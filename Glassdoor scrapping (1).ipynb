{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Inspired by code created by Diego De Lazzari\n",
    "Modified for Python 3 by Aungshuman Zaman\n",
    "\n",
    "\"\"\"\n",
    "from selenium import webdriver\n",
    "#from bs4 import BeautifulSoup # For HTML parsing\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "from selenium.webdriver.common import action_chains, keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import csv\n",
    "import os.path\n",
    "#from collections import OrderedDict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "def init_driver():\n",
    "    ''' Initialize chrome driver'''\n",
    "\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "    chrome_options.add_argument('--disable-extensions')\n",
    "    chrome_options.add_argument('--profile-directory=Default')\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "    chrome_options.add_argument(\"--disable-plugins-discovery\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    #browser = webdriver.Chrome(driver, chrome_options=chrome_options)\n",
    "    browser = webdriver.Chrome(chrome_options=chrome_options)\n",
    "    #browser = webdriver.Chrome()\n",
    "\n",
    "    return browser\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "###############################################################################\n",
    "\n",
    "def get_pause():\n",
    "    return np.random.choice(range(4,6))\n",
    "\n",
    "###############################################################################\n",
    "#utility function to get csv file from pickle.\n",
    "def get_csv(pickle_obj):  ####&&&& Don' use it for new files\n",
    "    my_dict = load_obj(pickle_obj)\n",
    "    csv_filename = 'mycsvfile.csv'\n",
    "    if os.path.isfile(csv_filename):\n",
    "        print('File already exists! Please rename it.')\n",
    "        return\n",
    "    with open(csv_filename, 'w') as f:  # Just use 'w' mode in 3.x\n",
    "        writer = csv.writer(f)\n",
    "\n",
    "        for k,v in my_dict.items():\n",
    "            if len(v) == 6:\n",
    "                new_dict = {}\n",
    "                new_dict['job_id'] = k\n",
    "                new_dict['rating'] = v[0]\n",
    "                new_dict['position'] = v[1]\n",
    "                new_dict['company_name'] = v[2]\n",
    "                new_dict['salary'] = v[3]\n",
    "                new_dict['link'] = v[4]\n",
    "                new_dict['description'] = v[5]  #[x.decode('ascii') for x in v[5]]\n",
    "                writer.writerow(new_dict.values())\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "def get_csv2(pickle_obj): ####&&&& Don' use it for new files\n",
    "    my_dict = load_obj(pickle_obj)\n",
    "    csv_filename = 'mycsvfile2.csv'\n",
    "    if os.path.isfile(csv_filename):\n",
    "        print('File already exists! Please rename it.')\n",
    "        return\n",
    "    with open(csv_filename, 'w') as f:  # Just use 'w' mode in 3.x\n",
    "        writer = csv.writer(f)\n",
    "        fieldnames = ['job_id','rating','position','company_name','salary','link',\\\n",
    "        'description','hq_city','hq_state_code','size','industry']\n",
    "\n",
    "        for k,v in my_dict.items():\n",
    "            if len(v) == 10:\n",
    "                new_dict = {}\n",
    "                new_dict['job_id'] = k\n",
    "                new_dict['rating'] = v[0]\n",
    "                new_dict['position'] = v[1]\n",
    "                new_dict['company_name'] = v[2]\n",
    "                new_dict['salary'] = v[3]\n",
    "                new_dict['link'] = v[4]\n",
    "                new_dict['description'] = v[5]  #[x.decode('ascii') for x in v[5]]\n",
    "                new_dict['hq_city'] = v[6]\n",
    "                new_dict['hq_state_code'] = v[7]\n",
    "                new_dict['size'] = v[8]\n",
    "                new_dict['industry'] = v[9]\n",
    "\n",
    "                #writer.writerow(new_dict.values())\n",
    "                writer.writerow([new_dict[i] for i in fieldnames]) #order preserved\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def get_csv3(pickle_obj): ####&&&&\n",
    "    my_dict = load_obj(pickle_obj)\n",
    "    csv_filename = 'mycsvfile3.csv'\n",
    "    if os.path.isfile(csv_filename):\n",
    "        print('File already exists! Please rename it.')\n",
    "        return\n",
    "\n",
    "    with open(csv_filename, 'w') as f:  # Just use 'w' mode in 3.x\n",
    "        writer = csv.writer(f)\n",
    "        fieldnames = ['job_id','rating', 'position', 'company', 'job_city', 'job_state_code',\\\n",
    "         'sal_low', 'sal_high', 'link','description','hq_city','hq_state_code','size','industry']\n",
    "\n",
    "        for k,v in my_dict.items():\n",
    "            if len(v) == 13:\n",
    "                new_dict = {}\n",
    "                new_dict['job_id'] = k\n",
    "                new_dict['rating'] = v[0]\n",
    "                new_dict['position'] = v[1]\n",
    "                new_dict['company'] = v[2]\n",
    "                new_dict['job_city'] = v[3]\n",
    "                new_dict['job_state_code'] = v[4]\n",
    "                new_dict['sal_low'] = v[5]\n",
    "                new_dict['sal_high'] = v[6]\n",
    "                new_dict['link'] = v[7]\n",
    "                new_dict['description'] = [x.decode('ascii') for x in v[8]]\n",
    "                #print(type(v[8]))\n",
    "                new_dict['hq_city'] = v[9]\n",
    "                new_dict['hq_state_code'] = v[10]\n",
    "                new_dict['size'] = v[11]\n",
    "                new_dict['industry'] = v[12]\n",
    "\n",
    "                #writer.writerow(new_dict.values())\n",
    "                writer.writerow([new_dict[i] for i in fieldnames]) #order preserved\n",
    "\n",
    "#######\n",
    "\n",
    "\n",
    "\n",
    "def searchJobs(browser, jobName, city=None, jobDict = None, link=None):\n",
    "    '''Scrape for job listing'''\n",
    "\n",
    "    ####&&&&\n",
    "    #q = input('Shall we scrape? (y/n)\\n') #q = raw_input('Shall we scrape? (y/n)')\n",
    "\n",
    "    #if q=='y': ####&&&&\n",
    "    if True:\n",
    "\n",
    "        print('hello')\n",
    "        job = browser.find_element_by_id(\"sc.keyword\") #job title, keywords, or company\n",
    "        \n",
    "        location = browser.find_element_by_id(\"sc.location\") #location search\n",
    "        sleep(3)\n",
    "        job.send_keys(jobName)  #type in job name in search\n",
    "        sleep(2)\n",
    "        #location form is already populated.\n",
    "        location.clear()\n",
    "        # can also execute JavaScript to clear it\n",
    "        #browser.execute_script(\"arguments[0].value = ''\", location)\n",
    "        location.send_keys(city) #type in location name in search\n",
    "\n",
    "        sleep(2)\n",
    "        browser.find_element_by_class_name('gd-btn-mkt').click()\n",
    "\n",
    "        sleep(5)\n",
    "        \n",
    "\n",
    "        # Find brief description\n",
    "\n",
    "\n",
    "        for i in range(20): #20  ####&&&&\n",
    "            try:\n",
    "                # Extract useful classes\n",
    "                jobPosting =browser.find_elements_by_class_name('jl')\n",
    "                sleep(get_pause())\n",
    "\n",
    "                # Create a job Dictionary. Every job in glassDoor has a unique data-id.\n",
    "                # data-id should be used as key for the dictionary\n",
    "                #create a map of 2-tuple. 2-tuple => data-id and selenium webElement.\n",
    "                jobTuple = map(lambda a: (a.get_attribute('data-id'), a), jobPosting)\n",
    "\n",
    "                # Filter picks out only those data-ids that are not in jobDict.keys()\n",
    "                newPost = list(filter(lambda b: b[0] not in jobDict.keys(),jobTuple) ) #list of 2-tuple\n",
    "\n",
    "                #If there are new posts, update job dict and link list\n",
    "                if newPost != []:\n",
    "\n",
    "                    # process the tuple\n",
    "                    #example of a[1].text ->\n",
    "                    #\"3.7\\nData Scientist, Analytics\\nEtsy – Brooklyn, NY\\n$114k-$167k  (Glassdoor Est.)\\nWe're Hiring\"\n",
    "                    #tuple structure ('job_id',['rating','position','company','salary'])\n",
    "                    #jobData = list(map(lambda a: (a[0],a[1].text.encode(\"utf8\")./\n",
    "                        #split('\\n')[0:4]),newPost))\n",
    "                    #jobData = list(map(lambda a: (a[0],a[1].text.split('\\n')[0:4]),newPost))\n",
    "                    #jobData = list(map(do_stuff, newPost)) ####&&&&\n",
    "                    # do_stuff returns many misplaced entries.\n",
    "                    #do_new_stuff uses regex to minimize bad data, it also splits up entries into more columns\n",
    "                    # new tuple structure ('job_id',[rating, position, company, job_city, job_state_code, sal_low, sal_high])\n",
    "                    print('starting do_new_stuff')\n",
    "                    jobData = list(map(do_new_stuff, newPost))\n",
    "                    print(\"I'm out of do_new_stuff.\")\n",
    "\n",
    "                    # Update job dictionary;\n",
    "                    # Convert tuple to dictionary. structure ('job_id',['rating',...]) -> {'job_id':['rating',...]}\n",
    "                    print('updating jobDict')\n",
    "                    tmp = dict((a[0],a[1]) for a in jobData)\n",
    "                    print('tmp created')\n",
    "                    jobDict.update(tmp) #add a new entry with unique key job_id\n",
    "                    # finally find the links:\n",
    "                    link_lst = list(map(lambda c: (c[0],c[1].find_element_by_tag_name('a').\\\n",
    "                        get_attribute('href')), newPost))\n",
    "                    #add the link to job dict\n",
    "                    print('Adding to link')\n",
    "                    tmp = [jobDict[c[0]].append(c[1]) for c in link_lst]\n",
    "                    # update link list. This will be used in get_data part.\n",
    "                    link += link_lst\n",
    "\n",
    "\n",
    "                browser.find_element_by_class_name('next').click() #next page\n",
    "                try:\n",
    "                    #browser.find_element_by_class_name('xBtn').click() #pop-up\n",
    "                    browser.find_element_by_xpath('//*[@id=\"JAModal\"]/div/div[2]/div').click() #pop up\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            except Exception as e:\n",
    "                #pass\n",
    "                print(type(e),e)\n",
    "\n",
    "    return jobDict, link\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def text_cleaner(text):\n",
    "    '''\n",
    "    This function just cleans up the raw html so that I can look at it.\n",
    "    Inputs: a URL to investigate\n",
    "    Outputs: Cleaned text only\n",
    "    '''\n",
    "    print('starting text_cleaner')\n",
    "    stopws = set(stopwords.words(\"english\"))\n",
    "    #print('initialized stopws')\n",
    "\n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    #lines = [line.strip() for line in text.splitlines()]\n",
    "\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) # break multi-headlines into a line each\n",
    "    #chunks = [phrase.strip() for line in lines for phrase in line.split(\"  \")]\n",
    "\n",
    "    def chunk_space(chunk):\n",
    "        chunk_out = chunk + ' ' # Need to fix spacing issue\n",
    "        return chunk_out\n",
    "\n",
    "    #print('Going for text!')\n",
    "    text = ''.join(chunk_space(chunk) for chunk in chunks if chunk).encode('utf-8') # Get rid of all blank lines and ends of line\n",
    "\n",
    "\n",
    "    # Now clean out all of the unicode junk (this line works great!!!)\n",
    "    #print('cleaning out unicode junc from text!')\n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "\n",
    "    #print('getting rid of non-words from text!')\n",
    "    text = re.sub(b\"[^a-zA-Z.+3]\",b\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                                # Also include + for C++\n",
    "\n",
    "    #print('make text lower case!')\n",
    "    text = text.lower()  # Go to lower case\n",
    "\n",
    "    #print('split text!')\n",
    "    text = text.split()  #  and split them apart\n",
    "\n",
    "    #print('removing stop words!')\n",
    "    text = [w for w in text if not w in stopws]\n",
    "\n",
    "\n",
    "    #print('set of text')\n",
    "    text = list(set(text)) # Last, just get the set of these. Ignore counts\n",
    "                           # we are just looking at whether a term existed or not on the website\n",
    "\n",
    "    #print(\"We are done! Let's return it!\")\n",
    "    return text\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "def string_from_text(pattern, tmp_txt):\n",
    "    lst  = tmp_txt.split('\\n')\n",
    "    for x in lst:\n",
    "        if x.find(pattern) != -1:\n",
    "            return x.replace(pattern, ' ')\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def do_stuff(a):\n",
    "    return (a[0],a[1].text.split('\\n')[0:4])\n",
    "\n",
    "##############################################################################\n",
    "\n",
    "def do_new_stuff(a):\n",
    "    print(\"I'm in do_new_stuff\")\n",
    "    if len(a) ==0:\n",
    "        print('object is empty')\n",
    "\n",
    "    tmp = a[1].text\n",
    "    print(temp)\n",
    "    raw_rating = re.findall('\\d\\.\\d',tmp )\n",
    "    print('raw_rating = ',raw_rating)\n",
    "    if len(raw_rating)==1:\n",
    "        rating =raw_rating[0]\n",
    "    else:\n",
    "        rating = ''\n",
    "    raw_sal_range = re.findall('\\d+k',tmp )\n",
    "    print('raw_sal_range = ',raw_sal_range)\n",
    "    if len(raw_sal_range)==2:\n",
    "        sal_low = int(raw_sal_range[0].replace('k',''))\n",
    "        sal_high = int(raw_sal_range[1].replace('k',''))\n",
    "    else:\n",
    "        sal_low = np.nan\n",
    "        sal_high = np.nan\n",
    "    raw_company = re.findall('.+–.+,.+',tmp)\n",
    "    print('raw_company = ',raw_company)\n",
    "    if len(raw_company)==1:\n",
    "        tt = raw_company[0].split('–')\n",
    "        company = tt[0].strip()\n",
    "        job_city = tt[1].split(',')[0].strip()\n",
    "        job_state_code = tt[1].split(',')[1].strip()\n",
    "    else:\n",
    "        company = ''\n",
    "        job_city = ''\n",
    "        job_state_code = ''\n",
    "    raw_position = re.findall('(.+sci.+|.+ana.+|.+eng.+)',tmp.lower())\n",
    "    print('raw_position = ',raw_position)\n",
    "    if len(raw_position)==1:\n",
    "        position = raw_position[0]\n",
    "    else:\n",
    "        position = tmp.split('\\n')[1].lower()\n",
    "    #return (a[0],tmp[0:4])\n",
    "    print('Will go out of do_new_stuff.')\n",
    "    return (a[0],[rating, position, company, job_city, job_state_code, sal_low, sal_high])\n",
    "\n",
    "##############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "#from bs4 import BeautifulSoup # For HTML parsing\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd # For converting results to a dataframe and bar chart plots\n",
    "from selenium.webdriver.common import action_chains, keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(jobDict) = 1897, len(link) = 1396\n"
     ]
    }
   ],
   "source": [
    "# 1- Load existing dictionary. Check for initial dictionary.\n",
    "# If empty initialize\n",
    "\n",
    "try:\n",
    "\tjobDict = load_obj('glassDoorDict')\n",
    "\tlink =    load_obj('glassDoorlink')\n",
    "except:\n",
    "\tsave_obj([], 'glassDoorlink')\n",
    "\tsave_obj({}, 'glassDoorDict')\n",
    "\n",
    "\tjobDict = load_obj('glassDoorDict')\n",
    "\tlink =    load_obj('glassDoorlink')\n",
    "\n",
    "print('len(jobDict) = '+str(len(jobDict))+ ', len(link) = '+str(len(link)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2- Choose what you want to do:\n",
    "#    get_link => Scraping for links and brief data,\n",
    "#    get_data => Scraping for detailed data.\n",
    "\n",
    "\n",
    "get_link = True ####&&&&\n",
    "#get_link = False\n",
    "\n",
    "\n",
    "website = \"https://www.glassdoor.com/index.htm\"\n",
    "browser = webdriver.Firefox(executable_path=r'C:\\Software\\geckodriver-v0.25.0-win64\\geckodriver.exe')\n",
    "browser.get(website)\n",
    "browser.find_element_by_css_selector('.gd-btn-locked-transparent').click()\n",
    "username = browser.find_element_by_css_selector(\".minh-modal > div:nth-child(1) > div:nth-child(5) > form:nth-child(1) > div:nth-child(3) > div:nth-child(1) > div:nth-child(2) > input:nth-child(1)\")\n",
    "password = browser.find_element_by_css_selector(\"div.mt-xsm:nth-child(4) > div:nth-child(1) > div:nth-child(2) > input:nth-child(1)\")\n",
    "username.send_keys(\"tayyibah.rasheed@okstate.edu\")\n",
    "password.send_keys(\"GRazta123$\")\n",
    "browser.find_element_by_css_selector(\"div.mt-std:nth-child(5) > div:nth-child(1) > button:nth-child(1)\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration number 0\n",
      "jobName = Data Scientist, city =  \n",
      "hello\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist', 'cape canaveral, fl']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['jr. data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['statistician - data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science', 'data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['jr. data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer i']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science internship']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['machine learning data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['informatics scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist – data connector']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['research scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['machine learning data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data & analytics consultant (bos)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "<class 'selenium.common.exceptions.ElementClickInterceptedException'> Message: Element <li class=\"next\"> is not clickable at point (463,680) because another element <div class=\"ModalStyle__modalContentLayout___YE_tc\"> obscures it\n",
      "\n",
      "len(update_jobDict) = 971, len(update_link) = 981\n",
      "Starting iteration number 1\n",
      "jobName = Data Engineer, city = Chicago\n",
      "hello\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['nxt – data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6', '4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer i']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science', 'big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['junior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['cloud data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data intelligence engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer (operations)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior technical support data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['1.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['python data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst - it']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sql database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science', 'senior big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior engineer - access management']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - data - bay area']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analytics engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior oracle database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst/engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer (chicago or remote)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['cloud data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science', 'big data engineer-data warehouse']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analytics engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['backend engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analytics engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - data']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - data pipeline platform']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data warehouse engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['python data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['automation anywhere engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer, test']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['insurance data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist - corn modeling']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['embedded systems engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior manager - big data architecture and engineering']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business intelligence engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['it systems engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['corporate intern: data analytics software engineer - summer 2020 (java/python)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer (mid - sr.)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['aws data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science', 'manager, data engineering']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst ii- chicago, detroit, des moines, kansas city, st. louis, indianapolis, columbus, cincinnati, louisville']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer - data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability - software engineer ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['full stack engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - delivery']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer - senior consultant']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['customer data scientist/sales engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['application engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['java engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['linux engineer (data capture)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ab initio developer/data engineer --- full time position']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['integral ad science']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sql server data engineer fte / contract']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data engineer (contract)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data platform engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['systems engineer - wheeling, il']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['principal software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior manager - data engineering portfolio delivery']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['apm senior engineer information technology systems']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist - credit risk']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer or data architect with performance tunin']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer (data)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['entry level software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['corporate intern: data analytics software engineer - summer 2020 (java/python/aws)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['machine learning engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['arity-senior data analytics engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['manager, data engineering']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead java engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['network engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['staff software engineer - data']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['principal data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['trading application support engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior network engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior big data engineer - procurement team']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior database reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior backend engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer ii - open location platform']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - events']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - data']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['advanced software engineer - data grid']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior network engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior gaming software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior site reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior android engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['java software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - fullstack']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software development engineer in test (sdet)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior network engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sterling engineering, inc', 'junior embedded software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer – intern (us)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business intelligence engineer - data visualization service administration']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['network engineer (data capture)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['graduate systems engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer midwest']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - tableau']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer - java, spark, kafka, aws']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['bi data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['research data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['junior data engineer (python/etl/sql)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['intern conversion - cio data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer, corp it (contractor)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['systems engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ab initio developer/data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer, zoro']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer/data scientist - validate health']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analytical engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer dba (data base admin)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer in test iii - new integration project']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data engineer, senior']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer -multiple levels available! - r']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer- pharma']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['intern: software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['desktop engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['2020 internship - enterprise transformation services: digital - advanced analytics & data engineering']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['azure data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['java data engineer -multiple levels available! - g']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist - procurement team']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr data engineer, zoro']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ai data scientist / data engineer - experienced associate']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and analytics quality engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist, business transformation']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer/architect (etl/sql)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data platform engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['spark with big data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / $60/hr']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['imanage.com', 'software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer - associate']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer, python sql server and etl - r']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['1.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / etl / pyspark']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['mid level data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['technical support engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior regulatory data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data/business intelligence engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / $150k']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior manager, data engineering']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['database engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer/ data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / c#']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / streaming']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineer / certifications']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['urgent requirement:: data engineer, chicago, il, 6+ months contract']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['1.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['python software and data engineer / iot sensor data / etl']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['reliability test development engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['contract data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer - $200']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['front - end software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['technical data engineer lead']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['2020 internship - enterprise transformation services: digital - advanced analytics & data engineering in chicago, illinois | car']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - data pipeline platform']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer with snowflake']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['urgent position :: sr. data engineer ( should have snowflake)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['enterprise data warehouse engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior market data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer (data integration/cloud/hadoop)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior principal software engineer – full stack']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr network engineer/data center']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability engineer, data']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer with snowflake']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['solutions architect (data engineer 5)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['embedded software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['network data l3 core engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior java engineer (data feeds/trading)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data warehouse engineer with snowflake']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer @ the mom project']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sql server senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['osp engineer - job located in wisconsin']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['front end software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['mid / senior data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer intern - summer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer in chicago, il at key bank']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['contract lead data engineer (python)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineering & analytics consultant']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data engineer / nlp']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['c# / .net software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['junior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['tax services senior – national tax – tax technology and transformation (ttt) – data scientist – advanced technologies']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead software engineer - java/aws/big data']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['principal software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['cloud application engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['.net software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['application engineer - sr specialist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['associate experience engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior java engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer (node stack)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead .net software engineer/technical specialist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr software engineer- olp core services.']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior full-stack engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior ui software engineer, applications']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['full - stack senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer – full stack']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - data grid']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr software engineer - olp core services']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['mobile/desktop support engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['presales software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer - reliability awareness']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software support engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software development engineer in test']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['aws engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - email solutions']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - crm solutions']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer (typescript), relocation to seattle, wa']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineering architect']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['advanced performance engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data-lead data analyst - 19']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['site reliability engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - corptax']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['1.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['microsoft data engineer / architect']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['1.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['front-end software engineer - email solutions']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['mzn management group, llc', 'software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer - python']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sterling engineering', 'jr. software engineer - mandarin speaking']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['microsoft data engineer/architect']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['linux engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  []\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['180 engineering', 'software engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - nav']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['devops engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['cloud data engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['software development engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data scientist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "len(update_jobDict) = 1454, len(update_link) = 1464\n",
      "Starting iteration number 2\n",
      "jobName = Data Analyst, city =  \n",
      "hello\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - ft day']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['big data analyst / engineer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data science & analytics analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['commercial & salesforce data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst, customer visibility']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['master data management business analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['applied data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst/project manager']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and reporting analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['vendor data quality analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['healthcare data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['machine learning/data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data intelligence analyst - hopkinton, ma']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data quality analyst', 'indianapolis, in']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['healthcare data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business reporting & data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['intern - data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sql data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['marketing data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['junior data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['web data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['programmatic data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['junior data analyst (ms excel, relational databases, data entry, data quality)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst, business development']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business/data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['procurement data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ua data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst-implementations']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - atl']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data services analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['product data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data visualization & reporting analyst large group, sparks - md']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data engineering analyst', 'indianapolis, in']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['bayview asset management', 'data and collateral analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data scientist (business intelligence analyst)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - customer analytics (ltv, usage, cohorts)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - lubrication industrial sales nam - saint louis, mo']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data intelligence analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['apex analytix', 'data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data solutions reporting analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst innovation - direct hire']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['technical data analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst, data products']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['compliance data analyst -']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst/trade remedies consultant']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - pharmacy']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['tgs management company', 'data quality analyst - 1 year contract - 3 openings']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - ts or ts/sci required']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['analyst - operations data analytics']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['product data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst/business analyst(entry level)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior bi data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and policy analyst - statistical programmer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['voloridge investment management', 'data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst (excel, relational databases) **tuscon']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business intelligence data analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data modeler & analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['xml data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['midland credit management', 'data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior operations data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst', 'indianapolis, in']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst/programmer']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['financial treasury data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior technical data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['applications & data services analyst i']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['procurement & business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['accounting data & reporting analyst intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data management analyst i']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data warehouse analyst-systems and data integration-analysis']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior analyst, data governance']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['southern management corporation', 'data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst ii (sql, tableau)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. data analyst - fraud investigation']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['power bi data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ri eohhs data quality analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['education assessment: data reporting analyst - online reporting system']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data business analyst (p&c insurance)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['operations data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - loan boarding']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data governance analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data governance analyst - junior']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['consumer lending data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data operations analyst (temporary)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['lead quantitative analyst, data science']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['xscion solutions', 'it data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['digital data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst- web analytics']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['marketing data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['marketing data & reporting analyst intern']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['office of the cao | reference data, operations analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['provider credentialing & data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior analyst, data strategy']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['product marketing analyst, data and analytics']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business / data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['research data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['survey data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst / reporting specialist']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['crystal report writer (data analyst) - starlims']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data governance analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data & analytics (python) analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sas data analyst with security clearance']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['internal fraud data analyst - sas/sql']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst (insider risk)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst, talent analytics']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['ngs data analyst, regeneron genetics center']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst 3']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sales operations data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['pharma r&d data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business analyst, data operations']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst (open to varying levels of exp.)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr risk control & data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst (sas, statistics, sql, healthcare)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['fund administration data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['strategic data analytics analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['mortgage data & collateral analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['associate technical data solutions, programmer/analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and reports analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['it technical business analyst - data integrations']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior health data analyst, star ratings']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['2020 data systems analyst inernship']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data / business analyst (sql/tableau)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior audit & data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst-statistical-sr- clinical programs']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['securities data management analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst in new york, ny at pyramid consulting, inc']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sql data analyst data cleaning, learning tableau, & earning bonuses']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data input analyst assistant']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst ii- chicago, detroit, des moines, kansas city, st. louis, indianapolis, columbus, cincinnati, louisville']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['audit senior data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sap materials master data analyst - temp']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. siu data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and policy analyst iii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['p&il - data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['sr. analyst / manager, data & process analysis (credit card)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['5.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['fusion life sciences', 'data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.4']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['advanced data warehouse/business intelligence analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['aia data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst- marketing']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.1']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['media data business analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.7']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "starting do_new_stuff\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.8']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['business data analyst ii']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.3']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.6']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.5']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.5', '1.0']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst - or main - 1.0 fte days']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['4.2']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['2.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data and attribution analyst - media']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  []\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst (contractor)']\n",
      "Will go out of do_new_stuff.\n",
      "I'm in do_new_stuff\n",
      "raw_rating =  ['3.9']\n",
      "raw_sal_range =  []\n",
      "raw_company =  []\n",
      "raw_position =  ['data analyst']\n",
      "Will go out of do_new_stuff.\n",
      "I'm out of do_new_stuff.\n",
      "updating jobDict\n",
      "tmp created\n",
      "Adding to link\n",
      "len(update_jobDict) = 1897, len(update_link) = 1907\n"
     ]
    }
   ],
   "source": [
    "# 4- Scrape for links and brief data\n",
    "\n",
    "if get_link :\n",
    "\titer_num = 0\n",
    "\twhile iter_num <3: # default 1 ####&&&&\n",
    "\t\tprint('Starting iteration number {}'.format(iter_num))\n",
    "\t\tsleep(get_pause())\n",
    "\t\tbrowser.get(website)\n",
    "\n",
    "\t\t# Initialize cities and jobs\n",
    "\n",
    "\t\tjobName_lst = ['Data Scientist', 'Data Analyst','Data Engineer']\n",
    "\t\tjobName = np.random.choice(jobName_lst)\n",
    "\t\t#jobName = 'Data Scientist' ####&&&&\n",
    "\n",
    "\t\tcity_lst = ['San Jose','New York','San Francisco','Detroit','Washington','Austin','Boston','Seattle','Chicago','Los Angeles',' ']\n",
    "\t\tcity = np.random.choice(city_lst)\n",
    "\t\t#city = ' '  ####&&&&\n",
    "\n",
    "\t\tprint('jobName = '+jobName+ ', city = '+city)\n",
    "\n",
    "\t\t# search for jobs (short description)\n",
    "\t\ttry:\n",
    "\t\t\t# jobDict structure {'job_id':['rating','position','company','salary']}\n",
    "\t\t\tupdate_jobDict, update_link = searchJobs(browser, jobName, city, jobDict, link)\n",
    "\t\t\tsleep(get_pause())\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(type(e),e)\n",
    "\t\t\tsys.exit(\"Error message\")\n",
    "\t\t\t#pass\n",
    "\n",
    "\n",
    "\t\tprint('len(update_jobDict) = '+str(len(update_jobDict))+ ', len(update_link) = '+str(len(update_link)))\n",
    "\n",
    "\t\t# save dictionary and link\n",
    "\n",
    "\t\tsave_obj(update_jobDict, 'glassDoorDict')\n",
    "\t\tsave_obj(update_link, 'glassDoorlink')\n",
    "\n",
    "\t\titer_num += 1\n",
    "\n",
    "\tbrowser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "website = \"https://www.glassdoor.com/index.htm\"\n",
    "browser = webdriver.Firefox(executable_path=r'C:\\Software\\geckodriver-v0.25.0-win64\\geckodriver.exe')\n",
    "browser.get(website)\n",
    "browser.find_element_by_css_selector('.gd-btn-locked-transparent').click()\n",
    "username = browser.find_element_by_css_selector(\".minh-modal > div:nth-child(1) > div:nth-child(5) > form:nth-child(1) > div:nth-child(3) > div:nth-child(1) > div:nth-child(2) > input:nth-child(1)\")\n",
    "password = browser.find_element_by_css_selector(\"div.mt-xsm:nth-child(4) > div:nth-child(1) > div:nth-child(2) > input:nth-child(1)\")\n",
    "username.send_keys(\"tayyibah.rasheed@okstate.edu\")\n",
    "password.send_keys(\"GRazta123$\")\n",
    "browser.find_element_by_css_selector(\"div.mt-std:nth-child(5) > div:nth-child(1) > button:nth-child(1)\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(link) = 1894\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372277973\n",
      "starting text_cleaner\n",
      "Scraped successfully 3277580140\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363055482\n",
      "starting text_cleaner\n",
      "Scraped successfully 3320766370\n",
      "starting text_cleaner\n",
      "Scraped successfully 3290650232\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374342143\n",
      "starting text_cleaner\n",
      "Scraped successfully 3343805917\n",
      "starting text_cleaner\n",
      "Scraped successfully 3359021883\n",
      "starting text_cleaner\n",
      "Scraped successfully 3334521649\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361681197\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378599427\n",
      "starting text_cleaner\n",
      "Scraped successfully 3203658495\n",
      "starting text_cleaner\n",
      "Scraped successfully 3349334324\n",
      "starting text_cleaner\n",
      "Scraped successfully 3328663903\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3286024103\n",
      "starting text_cleaner\n",
      "Scraped successfully 3244086072\n",
      "starting text_cleaner\n",
      "Scraped successfully 3234940925\n",
      "starting text_cleaner\n",
      "Scraped successfully 3083906595\n",
      "starting text_cleaner\n",
      "Scraped successfully 3349940293\n",
      "starting text_cleaner\n",
      "Scraped successfully 3257932432\n",
      "starting text_cleaner\n",
      "Scraped successfully 3341424800\n",
      "starting text_cleaner\n",
      "Scraped successfully 3129188835\n",
      "starting text_cleaner\n",
      "Scraped successfully 3354274605\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3368289967\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355327054\n",
      "starting text_cleaner\n",
      "Scraped successfully 3017282753\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371671338\n",
      "starting text_cleaner\n",
      "Scraped successfully 3278624040\n",
      "starting text_cleaner\n",
      "Scraped successfully 3300853973\n",
      "starting text_cleaner\n",
      "Scraped successfully 3157559938\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361833727\n",
      "starting text_cleaner\n",
      "Scraped successfully 3274853586\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363392581\n",
      "starting text_cleaner\n",
      "Scraped successfully 3234940630\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352062610\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350586957\n",
      "starting text_cleaner\n",
      "Scraped successfully 3354949821\n",
      "starting text_cleaner\n",
      "Scraped successfully 3332219523\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368522591\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366158130\n",
      "starting text_cleaner\n",
      "Scraped successfully 3365940563\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364766629\n",
      "starting text_cleaner\n",
      "Scraped successfully 3330108647\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370731048\n",
      "starting text_cleaner\n",
      "Scraped successfully 2845265634\n",
      "starting text_cleaner\n",
      "Scraped successfully 3343958694\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381476334\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374542505\n",
      "starting text_cleaner\n",
      "Scraped successfully 2494394491\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3247116677\n",
      "starting text_cleaner\n",
      "Scraped successfully 3359286905\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380176718\n",
      "starting text_cleaner\n",
      "Scraped successfully 2905794448\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374595377\n",
      "starting text_cleaner\n",
      "Scraped successfully 3346779973\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382286045\n",
      "starting text_cleaner\n",
      "Scraped successfully 3279844156\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348248144\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368349377\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381699711\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348226262\n",
      "starting text_cleaner\n",
      "Scraped successfully 3239089899\n",
      "starting text_cleaner\n",
      "Scraped successfully 3219783590\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371714538\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379945003\n",
      "starting text_cleaner\n",
      "Scraped successfully 3383402688\n",
      "starting text_cleaner\n",
      "Scraped successfully 3341272110\n",
      "starting text_cleaner\n",
      "Scraped successfully 3328148165\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378072172\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372448287\n",
      "starting text_cleaner\n",
      "Scraped successfully 3383437949\n",
      "starting text_cleaner\n",
      "Scraped successfully 2885641146\n",
      "starting text_cleaner\n",
      "Scraped successfully 3232526667\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352296279\n",
      "starting text_cleaner\n",
      "Scraped successfully 3383124143\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368843016\n",
      "starting text_cleaner\n",
      "Scraped successfully 3317243167\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1817 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3325373740\n",
      "starting text_cleaner\n",
      "Scraped successfully 3329596217\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368843004\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355772718\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382783203\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380691120\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360393035\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370106416\n",
      "starting text_cleaner\n",
      "Scraped successfully 3330317944\n",
      "starting text_cleaner\n",
      "Scraped successfully 3260984271\n",
      "starting text_cleaner\n",
      "Scraped successfully 3297991075\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368338939\n",
      "starting text_cleaner\n",
      "Scraped successfully 3328196981\n",
      "starting text_cleaner\n",
      "Scraped successfully 3311885574\n",
      "starting text_cleaner\n",
      "Scraped successfully 3232205492\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373909315\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370844052\n",
      "starting text_cleaner\n",
      "Scraped successfully 3369847571\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360835614\n",
      "starting text_cleaner\n",
      "Scraped successfully 3342753027\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381803597\n",
      "starting text_cleaner\n",
      "Scraped successfully 3285722142\n",
      "starting text_cleaner\n",
      "Scraped successfully 3314130131\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378151229\n",
      "3334289157 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1793 links\n",
      "3344619032 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1793 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3316719201\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352653054\n",
      "starting text_cleaner\n",
      "Scraped successfully 3367311072\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368573873\n",
      "starting text_cleaner\n",
      "Scraped successfully 3112996435\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360991161\n",
      "starting text_cleaner\n",
      "Scraped successfully 3383441687\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360736101\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378117146\n",
      "starting text_cleaner\n",
      "Scraped successfully 3342753057\n",
      "starting text_cleaner\n",
      "Scraped successfully 3111696201\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378458028\n",
      "starting text_cleaner\n",
      "Scraped successfully 3289820443\n",
      "starting text_cleaner\n",
      "Scraped successfully 3357284834\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371453452\n",
      "starting text_cleaner\n",
      "Scraped successfully 3130236684\n",
      "starting text_cleaner\n",
      "Scraped successfully 2696700525\n",
      "starting text_cleaner\n",
      "Scraped successfully 3326536561\n",
      "starting text_cleaner\n",
      "Scraped successfully 3351955208\n",
      "starting text_cleaner\n",
      "Scraped successfully 3285897329\n",
      "starting text_cleaner\n",
      "Scraped successfully 3214288107\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374375244\n",
      "starting text_cleaner\n",
      "Scraped successfully 3294822436\n",
      "starting text_cleaner\n",
      "Scraped successfully 3369830678\n",
      "starting text_cleaner\n",
      "Scraped successfully 3287011049\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348763517\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366254366\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371053961\n",
      "starting text_cleaner\n",
      "Scraped successfully 3313526137\n",
      "starting text_cleaner\n",
      "Scraped successfully 3346825539\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373039660\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379365938\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372657600\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364731535\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379366240\n",
      "starting text_cleaner\n",
      "Scraped successfully 3376966035\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382336114\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379485101\n",
      "starting text_cleaner\n",
      "Scraped successfully 3221972195\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379914357\n",
      "starting text_cleaner\n",
      "Scraped successfully 3329075752\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3379783992\n",
      "starting text_cleaner\n",
      "Scraped successfully 3310573898\n",
      "starting text_cleaner\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1749 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363774180\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355320064\n",
      "starting text_cleaner\n",
      "Scraped successfully 3328904357\n",
      "starting text_cleaner\n",
      "Scraped successfully 3162177887\n",
      "starting text_cleaner\n",
      "Scraped successfully 3306002330\n",
      "starting text_cleaner\n",
      "Scraped successfully 3309767524\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372535230\n",
      "starting text_cleaner\n",
      "Scraped successfully 3335619199\n",
      "starting text_cleaner\n",
      "Scraped successfully 3308916854\n",
      "starting text_cleaner\n",
      "Scraped successfully 3357878653\n",
      "starting text_cleaner\n",
      "Scraped successfully 3338501863\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3383173311\n",
      "starting text_cleaner\n",
      "Scraped successfully 3384086462\n",
      "starting text_cleaner\n",
      "Scraped successfully 3248518423\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371714160\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360214504\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348946671\n",
      "starting text_cleaner\n",
      "Scraped successfully 3375375566\n",
      "3355325640 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1731 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3297234118\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380143304\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381467080\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378618339\n",
      "starting text_cleaner\n",
      "Scraped successfully 3124905271\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381180354\n",
      "starting text_cleaner\n",
      "Scraped successfully 3262069247\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380930642\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377128597\n",
      "starting text_cleaner\n",
      "Scraped successfully 3272420202\n",
      "starting text_cleaner\n",
      "Scraped successfully 3330217908\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382824436\n",
      "starting text_cleaner\n",
      "Scraped successfully 3362291140\n",
      "starting text_cleaner\n",
      "Scraped successfully 3327032355\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377916128\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364692270\n",
      "starting text_cleaner\n",
      "Scraped successfully 3357208529\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379495034\n",
      "starting text_cleaner\n",
      "Scraped successfully 3354949822\n",
      "starting text_cleaner\n",
      "Scraped successfully 3293406680\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379102382\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3370865070\n",
      "starting text_cleaner\n",
      "Scraped successfully 3114489806\n",
      "starting text_cleaner\n",
      "Scraped successfully 3376943644\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377919508\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361755459\n",
      "starting text_cleaner\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1704 links\n",
      "starting text_cleaner\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1703 links\n",
      "starting text_cleaner\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tayyibah Kauser\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:273: DeprecationWarning: invalid escape sequence '\\ '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped successfully 3314068841\n",
      "starting text_cleaner\n",
      "Scraped successfully 3384313298\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368507422\n",
      "starting text_cleaner\n",
      "Scraped successfully 3272415482\n",
      "starting text_cleaner\n",
      "Scraped successfully 3289655136\n",
      "starting text_cleaner\n",
      "Scraped successfully 3291343494\n",
      "starting text_cleaner\n",
      "Scraped successfully 3003860377\n",
      "starting text_cleaner\n",
      "Scraped successfully 3375268777\n",
      "starting text_cleaner\n",
      "Scraped successfully 3306817399\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378032137\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364691231\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378032136\n",
      "starting text_cleaner\n",
      "Scraped successfully 3357212147\n",
      "starting text_cleaner\n",
      "Scraped successfully 3207258830\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379061371\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380373458\n",
      "starting text_cleaner\n",
      "Scraped successfully 3257853325\n",
      "starting text_cleaner\n",
      "Scraped successfully 3296757431\n",
      "starting text_cleaner\n",
      "Scraped successfully 3157221292\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379052005\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3329371658\n",
      "starting text_cleaner\n",
      "Scraped successfully 3205189506\n",
      "starting text_cleaner\n",
      "Scraped successfully 3238641055\n",
      "starting text_cleaner\n",
      "Scraped successfully 3030300317\n",
      "starting text_cleaner\n",
      "Scraped successfully 3288161955\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355325492\n",
      "starting text_cleaner\n",
      "Scraped successfully 3325392715\n",
      "starting text_cleaner\n",
      "Scraped successfully 3329133887\n",
      "starting text_cleaner\n",
      "Scraped successfully 3287011053\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3366022161\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363591604\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370983870\n",
      "starting text_cleaner\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1670 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3319185071\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366545905\n",
      "starting text_cleaner\n",
      "<class 'AttributeError'> 'NoneType' object has no attribute 'split'\n",
      "Scraped successfully 3335807126\n",
      "starting text_cleaner\n",
      "Scraped successfully 2884959733\n",
      "starting text_cleaner\n",
      "Scraped successfully 3141498457\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363413341\n",
      "starting text_cleaner\n",
      "Scraped successfully 3321198042\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366658901\n",
      "starting text_cleaner\n",
      "Scraped successfully 3262440992\n",
      "starting text_cleaner\n",
      "Scraped successfully 3291657622\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370857589\n",
      "starting text_cleaner\n",
      "Scraped successfully 3336894698\n",
      "starting text_cleaner\n",
      "Scraped successfully 3325934688\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378741424\n",
      "starting text_cleaner\n",
      "Scraped successfully 3222053252\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372756244\n",
      "starting text_cleaner\n",
      "Scraped successfully 3008611171\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372331235\n",
      "starting text_cleaner\n",
      "Scraped successfully 3337586013\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382783163\n",
      "starting text_cleaner\n",
      "Scraped successfully 3322609794\n",
      "3339084874 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1649 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3349347341\n",
      "starting text_cleaner\n",
      "Scraped successfully 3238854271\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366293333\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382298890\n",
      "starting text_cleaner\n",
      "Scraped successfully 3330159731\n",
      "starting text_cleaner\n",
      "Scraped successfully 3344814170\n",
      "starting text_cleaner\n",
      "Scraped successfully 3359852606\n",
      "3360978157 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1642 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3293818987\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348688378\n",
      "starting text_cleaner\n",
      "Scraped successfully 3358736001\n",
      "starting text_cleaner\n",
      "Scraped successfully 3317123480\n",
      "starting text_cleaner\n",
      "Scraped successfully 3318703997\n",
      "starting text_cleaner\n",
      "Scraped successfully 3308698561\n",
      "starting text_cleaner\n",
      "Scraped successfully 3347731742\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374114806\n",
      "starting text_cleaner\n",
      "Scraped successfully 3328314379\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379345051\n",
      "starting text_cleaner\n",
      "Scraped successfully 2355565782\n",
      "starting text_cleaner\n",
      "Scraped successfully 3286002947\n",
      "starting text_cleaner\n",
      "Scraped successfully 3351980228\n",
      "starting text_cleaner\n",
      "Scraped successfully 2776977673\n",
      "starting text_cleaner\n",
      "Scraped successfully 3103552735\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374972178\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382783211\n",
      "starting text_cleaner\n",
      "Scraped successfully 3351931875\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374526985\n",
      "starting text_cleaner\n",
      "Scraped successfully 3351029795\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368462259\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380905329\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350095969\n",
      "starting text_cleaner\n",
      "Scraped successfully 3356125528\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382549465\n",
      "starting text_cleaner\n",
      "Scraped successfully 3319144567\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355955878\n",
      "starting text_cleaner\n",
      "Scraped successfully 3353134607\n",
      "starting text_cleaner\n",
      "Scraped successfully 3104203573\n",
      "starting text_cleaner\n",
      "Scraped successfully 3383119458\n",
      "starting text_cleaner\n",
      "Scraped successfully 3181273272\n",
      "starting text_cleaner\n",
      "Scraped successfully 3376961345\n",
      "starting text_cleaner\n",
      "Scraped successfully 3227566503\n",
      "starting text_cleaner\n",
      "Scraped successfully 3369146451\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360775526\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371436377\n",
      "starting text_cleaner\n",
      "Scraped successfully 3341523795\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373178475\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379470704\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352851889\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379709394\n",
      "starting text_cleaner\n",
      "Scraped successfully 3189769788\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350256041\n",
      "starting text_cleaner\n",
      "Scraped successfully 3289279435\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368744398\n",
      "starting text_cleaner\n",
      "Scraped successfully 3315588472\n",
      "starting text_cleaner\n",
      "Scraped successfully 3346850609\n",
      "starting text_cleaner\n",
      "Scraped successfully 3353449453\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382925892\n",
      "starting text_cleaner\n",
      "Scraped successfully 3247149179\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378581714\n",
      "starting text_cleaner\n",
      "Scraped successfully 3116482311\n",
      "starting text_cleaner\n",
      "Scraped successfully 3167066599\n",
      "starting text_cleaner\n",
      "Scraped successfully 3325221821\n",
      "starting text_cleaner\n",
      "Scraped successfully 3285983029\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374063417\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3042963919\n",
      "starting text_cleaner\n",
      "Scraped successfully 3268496370\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381505069\n",
      "starting text_cleaner\n",
      "Scraped successfully 3338341009\n",
      "starting text_cleaner\n",
      "Scraped successfully 3362200736\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3355630811\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3351983853\n",
      "starting text_cleaner\n",
      "Scraped successfully 3304820296\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364095792\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380088412\n",
      "starting text_cleaner\n",
      "Scraped successfully 3316702634\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382808246\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373621453\n",
      "starting text_cleaner\n",
      "Scraped successfully 3358855560\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370928402\n",
      "starting text_cleaner\n",
      "Scraped successfully 3358478810\n",
      "starting text_cleaner\n",
      "Scraped successfully 3322534347\n",
      "starting text_cleaner\n",
      "Scraped successfully 3253711102\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361432710\n",
      "starting text_cleaner\n",
      "Scraped successfully 3289785693\n",
      "starting text_cleaner\n",
      "Scraped successfully 3375722934\n",
      "starting text_cleaner\n",
      "Scraped successfully 3356540539\n",
      "starting text_cleaner\n",
      "Scraped successfully 2450278877\n",
      "starting text_cleaner\n",
      "Scraped successfully 3374514829\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382073732\n",
      "starting text_cleaner\n",
      "Scraped successfully 3300702424\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368842093\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379268277\n",
      "starting text_cleaner\n",
      "Scraped successfully 3324404616\n",
      "starting text_cleaner\n",
      "Scraped successfully 3312287740\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3374326556\n",
      "starting text_cleaner\n",
      "Scraped successfully 3287011051\n",
      "starting text_cleaner\n",
      "Scraped successfully 3365461879\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3381877310\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364680972\n",
      "starting text_cleaner\n",
      "Scraped successfully 3322139025\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368362275\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350371131\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368589709\n",
      "starting text_cleaner\n",
      "Scraped successfully 3336201243\n",
      "starting text_cleaner\n",
      "Scraped successfully 3358798084\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377860239\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372225552\n",
      "starting text_cleaner\n",
      "Scraped successfully 3354845242\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380424641\n",
      "starting text_cleaner\n",
      "Scraped successfully 3369140162\n",
      "starting text_cleaner\n",
      "Scraped successfully 3288124726\n",
      "starting text_cleaner\n",
      "Scraped successfully 3362104432\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3379394303\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: [id=\"EmpBasicInfo\"]\n",
      "\n",
      "Scraped successfully 3376212929\n",
      "starting text_cleaner\n",
      "Scraped successfully 3306049584\n",
      "starting text_cleaner\n",
      "Scraped successfully 3343738094\n",
      "starting text_cleaner\n",
      "Scraped successfully 3348253402\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381577769\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3042963918\n",
      "starting text_cleaner\n",
      "Scraped successfully 3346593376\n",
      "starting text_cleaner\n",
      "Scraped successfully 3272420437\n",
      "3378391054 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1529 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378166709\n",
      "starting text_cleaner\n",
      "Scraped successfully 3310614402\n",
      "starting text_cleaner\n",
      "Scraped successfully 3292068782\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3042963884\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355299122\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380295617\n",
      "starting text_cleaner\n",
      "Scraped successfully 3375814371\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373100415\n",
      "3328724246 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1521 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370041879\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380205829\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3374278995\n",
      "starting text_cleaner\n",
      "Scraped successfully 3329772532\n",
      "3359006921 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1517 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3271950986\n",
      "starting text_cleaner\n",
      "Scraped successfully 3316151749\n",
      "starting text_cleaner\n",
      "Scraped successfully 3227567092\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370865343\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361666806\n",
      "starting text_cleaner\n",
      "Scraped successfully 3249159071\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377964270\n",
      "starting text_cleaner\n",
      "Scraped successfully 3362251926\n",
      "starting text_cleaner\n",
      "Scraped successfully 3304873744\n",
      "starting text_cleaner\n",
      "Scraped successfully 3295785820\n",
      "starting text_cleaner\n",
      "Scraped successfully 3269982506\n",
      "starting text_cleaner\n",
      "Scraped successfully 3073218737\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380989494\n",
      "starting text_cleaner\n",
      "Scraped successfully 3344718933\n",
      "starting text_cleaner\n",
      "Scraped successfully 3337654349\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382919758\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352098262\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370467703\n",
      "starting text_cleaner\n",
      "Scraped successfully 3312390904\n",
      "starting text_cleaner\n",
      "Scraped successfully 3371837410\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360752631\n",
      "starting text_cleaner\n",
      "Scraped successfully 3051765067\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366352131\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379785419\n",
      "starting text_cleaner\n",
      "Scraped successfully 3223711307\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350016466\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381568657\n",
      "starting text_cleaner\n",
      "Scraped successfully 3341515223\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361091681\n",
      "starting text_cleaner\n",
      "Scraped successfully 3341789396\n",
      "starting text_cleaner\n",
      "Scraped successfully 3138997534\n",
      "starting text_cleaner\n",
      "Scraped successfully 3114460128\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378431267\n",
      "starting text_cleaner\n",
      "Scraped successfully 2905794459\n",
      "starting text_cleaner\n",
      "Scraped successfully 3327849756\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3356057403\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381713489\n",
      "starting text_cleaner\n",
      "Scraped successfully 3367363035\n",
      "starting text_cleaner\n",
      "Scraped successfully 3335457872\n",
      "starting text_cleaner\n",
      "Scraped successfully 3349451599\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3372447859\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379223172\n",
      "starting text_cleaner\n",
      "Scraped successfully 3228781841\n",
      "starting text_cleaner\n",
      "Scraped successfully 3356188203\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382783185\n",
      "starting text_cleaner\n",
      "Scraped successfully 3370724753\n",
      "starting text_cleaner\n",
      "Scraped successfully 3306045920\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378463438\n",
      "starting text_cleaner\n",
      "Scraped successfully 3365496008\n",
      "starting text_cleaner\n",
      "Scraped successfully 3323168444\n",
      "starting text_cleaner\n",
      "Scraped successfully 3346825673\n",
      "starting text_cleaner\n",
      "Scraped successfully 3320727492\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373055955\n",
      "None is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1464 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3353327293\n",
      "starting text_cleaner\n",
      "Scraped successfully 3362126342\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378431266\n",
      "starting text_cleaner\n",
      "Scraped successfully 3322176515\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361091851\n",
      "starting text_cleaner\n",
      "Scraped successfully 3163161005\n",
      "starting text_cleaner\n",
      "Scraped successfully 3310366464\n",
      "starting text_cleaner\n",
      "Scraped successfully 3365147974\n",
      "starting text_cleaner\n",
      "Scraped successfully 3310725701\n",
      "starting text_cleaner\n",
      "Scraped successfully 3331695744\n",
      "starting text_cleaner\n",
      "Scraped successfully 3350327951\n",
      "starting text_cleaner\n",
      "Scraped successfully 3141189673\n",
      "starting text_cleaner\n",
      "Scraped successfully 3347992768\n",
      "starting text_cleaner\n",
      "Scraped successfully 3364962031\n",
      "starting text_cleaner\n",
      "Scraped successfully 3095721777\n",
      "starting text_cleaner\n",
      "Scraped successfully 3381341444\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360835620\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379918575\n",
      "starting text_cleaner\n",
      "Scraped successfully 3331834535\n",
      "starting text_cleaner\n",
      "Scraped successfully 3269670681\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382549415\n",
      "starting text_cleaner\n",
      "Scraped successfully 3295474012\n",
      "starting text_cleaner\n",
      "Scraped successfully 3174094527\n",
      "starting text_cleaner\n",
      "<class 'AttributeError'> 'NoneType' object has no attribute 'split'\n",
      "Scraped successfully 3319584379\n",
      "starting text_cleaner\n",
      "Scraped successfully 3344695146\n",
      "starting text_cleaner\n",
      "Scraped successfully 3363746127\n",
      "starting text_cleaner\n",
      "Scraped successfully 3342821448\n",
      "starting text_cleaner\n",
      "Scraped successfully 3378187219\n",
      "3383560689 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1436 links\n",
      "starting text_cleaner\n",
      "Scraped successfully 3310432329\n",
      "starting text_cleaner\n",
      "Scraped successfully 3372273896\n",
      "starting text_cleaner\n",
      "Scraped successfully 3235486587\n",
      "starting text_cleaner\n",
      "Scraped successfully 3325221767\n",
      "starting text_cleaner\n",
      "Scraped successfully 3286436384\n",
      "starting text_cleaner\n",
      "Scraped successfully 3192678635\n",
      "starting text_cleaner\n",
      "Scraped successfully 3342753276\n",
      "starting text_cleaner\n",
      "Scraped successfully 3365434463\n",
      "starting text_cleaner\n",
      "Scraped successfully 3356735145\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3068503436\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3381285424\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366361834\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.NoSuchElementException'> Message: Unable to locate element: //*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span\n",
      "\n",
      "Scraped successfully 3377090100\n",
      "starting text_cleaner\n",
      "Scraped successfully 3373032010\n",
      "starting text_cleaner\n",
      "Scraped successfully 3178644071\n",
      "starting text_cleaner\n",
      "Scraped successfully 3343700266\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382810447\n",
      "starting text_cleaner\n",
      "Scraped successfully 3361090471\n",
      "starting text_cleaner\n",
      "Scraped successfully 3355327748\n",
      "starting text_cleaner\n",
      "Scraped successfully 3100027717\n",
      "starting text_cleaner\n",
      "Scraped successfully 3360835798\n",
      "starting text_cleaner\n",
      "Scraped successfully 3226309275\n",
      "starting text_cleaner\n",
      "Scraped successfully 3379784036\n",
      "starting text_cleaner\n",
      "Scraped successfully 3275924868\n",
      "starting text_cleaner\n",
      "Scraped successfully 3382549470\n",
      "starting text_cleaner\n",
      "Scraped successfully 3368526261\n",
      "starting text_cleaner\n",
      "Scraped successfully 3352141878\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366052865\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380929833\n",
      "starting text_cleaner\n",
      "Scraped successfully 3351804999\n",
      "starting text_cleaner\n",
      "Scraped successfully 3280458632\n",
      "starting text_cleaner\n",
      "Scraped successfully 3366410030\n",
      "starting text_cleaner\n",
      "Scraped successfully 3174071440\n",
      "starting text_cleaner\n",
      "Scraped successfully 3375910249\n",
      "starting text_cleaner\n",
      "Scraped successfully 3314841644\n",
      "starting text_cleaner\n",
      "Scraped successfully 3377930442\n",
      "starting text_cleaner\n",
      "Scraped successfully 3380940200\n",
      "starting text_cleaner\n",
      "Scraped successfully 3294374243\n",
      "starting text_cleaner\n",
      "Scraped successfully 3356617744\n",
      "starting text_cleaner\n",
      "<class 'selenium.common.exceptions.TimeoutException'> Message: Timeout loading page after 300000ms\n",
      "\n",
      "Scraped successfully 3305673994\n",
      "3355000296 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3355387328 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3371043687 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3270736115 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3352849416 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3364657717 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3322458538 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3379919059 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n",
      "3317069219 is not working! Sleep for 6 seconds and retry\n",
      "Still missing 1396 links\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-198-d6008d5f4a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                         \u001b[1;31m#print(browser)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \"\"\"\n\u001b[1;32m--> 333\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGET\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: Tried to run command without establishing a connection\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-198-d6008d5f4a0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     78\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is not working! Sleep for 6 seconds and retry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'Still missing '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' links'\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                         \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Done for now!! len(link) = '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_data = True\n",
    "if get_data:\n",
    "\n",
    "\tprint('len(link) = '+str(len(link)))\n",
    "\twhile len(link) > 200: # originally 0, a hard coded solution for when only bad links are left.\n",
    "\t#for i in range(250): # debugging\t####&&&&\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\trnd_job = np.random.choice(range(len(link)))\n",
    "\t\t\t#print(rnd_job)\n",
    "\t\t\tids = link[rnd_job][0]\n",
    "\t\t\tpage = link[rnd_job][1]\n",
    "\t\t\t#print(ids)\n",
    "\t\t\t#print(page)\n",
    "\n",
    "\t\t\tbrowser.get(page)\n",
    "\t\t\t#print(browser)\n",
    "\t\t\tsleep(3)\n",
    "\n",
    "\t\t\t# Extract text\n",
    "\t\t\tdesc_list = browser.find_element_by_xpath('//*[@id=\"JobDescriptionContainer\"]/div[1]').text\n",
    "\t\t\t#print('desc_list '+ str(type(desc_list)))\n",
    "\t\t\tdescription = text_cleaner(desc_list)\n",
    "\t\t\t#description = desc_list\n",
    "\t\t\t#print('description '+ str(type(description)))\n",
    "\n",
    "\t\t\t# jobDict structure {'job_id':['rating','position','company','salary','descr']}\n",
    "\t\t\tjobDict[ids].append(description)\n",
    "\n",
    "\n",
    "\t\t\t#Additional information about company (size, revenue, industry)\n",
    "\t\t\tsleep(2)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tbrowser.find_element_by_xpath('//*[@class=\"jobDetailsInfoWrap\"]/header/div/div/div[2]/span').click()\n",
    "\t\t\t\ttmp_txt = browser.find_element_by_id('EmpBasicInfo').text\n",
    "\n",
    "\t\t\t\thq_city = string_from_text('Headquarters', tmp_txt).split(',')[0].strip()\n",
    "\t\t\t\t#print('hq_city = ',hq_city)\n",
    "\t\t\t\tjobDict[ids].append(hq_city)\n",
    "\t\t\t\t#print(' 1 = ',)\n",
    "\t\t\t\thq_state_code = string_from_text('Headquarters', tmp_txt).split(',')[0].strip()\n",
    "\t\t\t\t#print('hq_state_code = ',hq_state_code)\n",
    "\t\t\t\tjobDict[ids].append(hq_state_code)\n",
    "\t\t\t\t#print(' 2 = ',)\n",
    "\t\t\t\t#size_low = int(re.findall('\\d+',string_from_text('Size',tmp_txt))[0])\n",
    "\t\t\t\tsize = re.findall('\\d+',string_from_text('Size',tmp_txt))\n",
    "\t\t\t\t#print('size = ', size)\n",
    "\t\t\t\t#size_high = int(re.findall('\\d+',string_from_text('Size',tmp_txt))[1])\n",
    "\t\t\t\t#print(' = ',)\n",
    "\t\t\t\tjobDict[ids].append(size)\n",
    "\t\t\t\t#print(' 3 = ',)\n",
    "\t\t\t\t#jobDict[ids].append(size_low)\n",
    "\t\t\t\t#jobDict[ids].append(size_high)\n",
    "\t\t\t\tindustry = string_from_text('Industry',tmp_txt)\n",
    "\t\t\t\t#print('industry = ',industry)\n",
    "\n",
    "\t\t\t\tjobDict[ids].append(industry)\n",
    "\t\t\t\t#print(' 4 = ',)\n",
    "\t\t\t\t#jobDict[ids].append(revenue)\n",
    "\n",
    "\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(type(e),e)\n",
    "\n",
    "\t\t\t#remove links already used\n",
    "\n",
    "\t\t\tdummy=link.pop(rnd_job)\n",
    "\n",
    "\t\t\t# if everything is fine, save\n",
    "\t\t\t#print(\"Going to save data!!\")\n",
    "\t\t\tsave_obj(jobDict, 'glassDoorDict')\n",
    "\t\t\tsave_obj(link, 'glassDoorlink')\n",
    "\n",
    "\t\t\tprint('Scraped successfully ' + ids)\n",
    "\n",
    "\t\t\tsleep(get_pause())\n",
    "\t\texcept:\n",
    "\t\t\tprint( str(ids) + ' is not working! Sleep for 6 seconds and retry')\n",
    "\t\t\tprint( 'Still missing ' + str(len(link)) + ' links' )\n",
    "\t\t\tsleep(6)\n",
    "\tprint('Done for now!! len(link) = '+str(len(link)))\n",
    "\tbrowser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_dict = load_obj(pickle_obj)\n",
    "csv_filename = 'mycsvfile2.csv'\n",
    "if os.path.isfile(csv_filename):\n",
    "    print('File already exists! Please rename it.')\n",
    "    return\n",
    "\n",
    "with open(csv_filename, 'w') as f: \n",
    "   # Just use 'w' mode in 3.x\n",
    "    writer = csv.writer(f)\n",
    "    print('y')\n",
    "    fieldnames = ['job_id','rating', 'position', 'company', 'job_city', 'job_state_code',\\\n",
    "     'sal_low', 'sal_high', 'link','description','hq_city','hq_state_code','size','industry']\n",
    "\n",
    "    for k,v in my_dict.items():\n",
    "        if len(v) == 13:\n",
    "            new_dict = {}\n",
    "            new_dict['job_id'] = k\n",
    "            new_dict['rating'] = v[0]\n",
    "            new_dict['position'] = v[1]\n",
    "            new_dict['company'] = v[2]\n",
    "            new_dict['job_city'] = v[3]\n",
    "            new_dict['job_state_code'] = v[4]\n",
    "            new_dict['sal_low'] = v[5]\n",
    "            new_dict['sal_high'] = v[6]\n",
    "            new_dict['link'] = v[7]\n",
    "            new_dict['description'] = v[8]\n",
    "            #print(type(v[8]))\n",
    "            new_dict['hq_city'] = v[9]\n",
    "            new_dict['hq_state_code'] = v[10]\n",
    "            new_dict['size'] = v[11]\n",
    "            new_dict['industry'] = v[12]\n",
    "\n",
    "            #writer.writerow(new_dict.values())\n",
    "            writer.writerow([new_dict[i] for i in fieldnames]) #order preserved\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = load_obj('glassDoorDict')\n",
    "csv_filename = 'mycsvfile2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "8\n",
      "8\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "12\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "13\n",
      "9\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "12\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "11\n",
      "10\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "8\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for k in my_dict.keys():\n",
    "    print(len(my_dict[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3.9',\n",
       " 'data engineer',\n",
       " 'Seniorlink',\n",
       " 'Boston',\n",
       " 'MA',\n",
       " 64,\n",
       " 89,\n",
       " 'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=680969&s=149&guid=0000016de58a38219b7e47f6471ee345&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_833d2534&cb=1571514104766&jobListingId=3381578119',\n",
       " 'Position Summary:\\n\\n\\nThe Data Engineer will be a key player in Seniorlink’s Product & Technologies Organization (P&T). You will be the primary person building out the data integration framework capability within the ‘DataHub’ architecture. The DataHub uses a modern cloud based distributed computing architecture that utilizes modern technologies such as AWS S3, AWS EMR, Apache Spark, Redshift etc. You will work on the foundational infrastructure components within this architecture to expand the capability and make the system more robust. You will collaborate with senior engineers to design and engineer a robust, scalable integration framework that can transform the variety of incoming data formats from our customers. You will provide post-production monitoring and support for the components you build to ensure the system performs to specification. The ideal candidate will have a passion for the data domain, a strong background in data engineering, having designed and implemented data pipelines that can handle complex large scale data transformations.\\n\\nWhat You Will Do:\\nDevelop a sound understanding of the architecture already in place\\nDesign robust, scalable and secure data processing systems within the evolving data infrastructure of Seniorlink’s products and solutions\\nBuild frameworks, tooling, pipelines that address the partner data integration requirements\\nUse agile/scrum methodology to ensure sprint commitments are met regularly\\nPerform pull reports (PR) for other engineers\\nTroubleshoot processes in production to fix functional bugs or performance issues\\nPerform other duties as assigned\\nWhat You Will Bring:\\n\\n\\nBachelor’s Degree in Computer Science or related field\\n2+ years’ of experience doing data engineering work\\n1+ years’ experience in newer data lake type architectures\\nSolid understanding of algorithms, data structures, and data modeling (SQL a plus)\\nProficient with at least one of following programming languages - Python, Java, Scala\\nDemonstrated experience using ETL tools for a data pipeline\\nExperience working with cloud computing services\\nExperience working with Spark for large-scale data processing is a major plus\\nAbout Us:\\n\\n\\nHEALTH CARE SOLUTIONS THAT BLEND HUMAN TOUCH WITH TECHNOLOGY …\\n\\nSeniorlink is a tech-enabled health services company that builds care solutions to support family caregivers caring for loved ones at home. With nearly 20 years of care management expertise, Seniorlink blends experienced care coaches, proven protocols and an innovative app, to provide family health care solutions. The company’s commitment to and engagement of family caregivers paired with its reputation for delivering quality care, provides families with a high-touch, low cost alternative to facility-based care.',\n",
       " 'Boston',\n",
       " 'MA',\n",
       " '500',\n",
       " 'Health Care Services & Hospitals',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dict['3381578119']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3381578119\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'job_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-b48fa52b5ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m \u001b[1;36m13\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mnew_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'job_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mnew_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnew_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'position'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'job_id'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': '3361635360',\n",
       " 'rating': '4.2',\n",
       " 'position': 'data engineer',\n",
       " 'company': 'Keller Williams',\n",
       " 'job_city': 'Austin',\n",
       " 'job_state_code': 'TX',\n",
       " 'sal_low': 57,\n",
       " 'sal_high': 79,\n",
       " 'link': 'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=6101&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_24f9fe92&cb=1571515030030&jobListingId=3361635360',\n",
       " 'description': 'What youll be called: Data Engineer\\n\\nWhere youll work: KWRI HeadquartersAustin, TX\\n\\nNamed a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.\\n\\nKW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.\\n\\nWhat youll do:\\n\\nDesign, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency.\\n\\nEssential Duties and Responsibilities:\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources\\nParticipate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.\\nPerform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.\\nWork in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.\\nInvestigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.\\nPerform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.\\nMinimum Qualifications:\\nBachelors degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.\\n3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.\\n2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.\\nExperience with Spark, Presto, Hive and/or other map/reduce \"big data\" systems and services.\\nExperience in SQL and Python for scripting automation.\\nPreferred Qualifications:\\nMasters degree in Information Management, Data Science, Analytics or related field.\\nExperience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.\\nFamiliar working in a Cloud environment (AWS or GCP) with a subset of the following tools or their equivalent - Redshift, RDS, S3, EC2, Lambda, Kinesis, Elasticsearch, EMR, BigQuery, GCS.\\nWho are we?\\n\\nKeller Williams Realty Inc. is the largest real estate company by agent count across the globe and is number one in units and volume in the United States. Founded in 1983, we pride ourselves on an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders. Keller Williams Realty International (KWRI), is the companys corporate headquarters located in Austin, TX. Here, through a focus on cutting edge technology, education, and products and services, we support our agents and associates to create careers worth having, businesses worth owning, lives worth living, experiences worth giving and legacies worth leaving.',\n",
       " 'hq_city': 'Austin',\n",
       " 'hq_state_code': 'TX',\n",
       " 'size': '500',\n",
       " 'industry': 'Real Estate'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict = {'job_id' : [], \n",
    "            'rating' : [], \n",
    "            'position' : [], \n",
    "            'company' : [],\n",
    "            'job_city' : [],\n",
    "            'job_state_code' : [],\n",
    "            'sal_low' : [],\n",
    "            'sal_high' : [],\n",
    "            'link' : [],\n",
    "            'description' : [],\n",
    "            'hq_city' : [],\n",
    "            'hq_state_code' : [],\n",
    "            'size' : [],\n",
    "            'industry' : []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3381578119\n",
      "3320321158\n",
      "3284284037\n",
      "3298422474\n",
      "3352031827\n",
      "3196505395\n",
      "3388905001\n",
      "3374099933\n",
      "3379908362\n",
      "3356833111\n",
      "3350001434\n",
      "3386703038\n",
      "3378009255\n",
      "3354376776\n",
      "3180307018\n",
      "3376562653\n",
      "3254097283\n",
      "3359023976\n",
      "3238036556\n",
      "3296285380\n",
      "3350511635\n",
      "3356138485\n",
      "3356602294\n",
      "3328266779\n",
      "3322135751\n",
      "3391450036\n",
      "3365068364\n",
      "3205983875\n",
      "3353556402\n",
      "3318225561\n",
      "3042963959\n",
      "3328903440\n",
      "3253403496\n",
      "3308918595\n",
      "3379951822\n",
      "3358722522\n",
      "3373840950\n",
      "3328124307\n",
      "3335457872\n",
      "3351864423\n",
      "3365674970\n",
      "3219890595\n",
      "3279849002\n",
      "3246517577\n",
      "3385750069\n",
      "3374690234\n",
      "3178235988\n",
      "2994117434\n",
      "3346481137\n",
      "3353579290\n",
      "3374589739\n",
      "3329734179\n",
      "3326030111\n",
      "3325134847\n",
      "3322135794\n",
      "3383653153\n",
      "3300388779\n",
      "3374670113\n",
      "2725035780\n",
      "3227655543\n",
      "3325280156\n",
      "3272706711\n",
      "3323871719\n",
      "3265401585\n",
      "3364762246\n",
      "3361416053\n",
      "3384363812\n",
      "3302841314\n",
      "3363451785\n",
      "3358528772\n",
      "3365248211\n",
      "3344983747\n",
      "3332214881\n",
      "3280026843\n",
      "3311524017\n",
      "3391745761\n",
      "3388114590\n",
      "3317344619\n",
      "3181397648\n",
      "3093497333\n",
      "3272792918\n",
      "3349197179\n",
      "3348319164\n",
      "3377840501\n",
      "3379783823\n",
      "3388498166\n",
      "3280026844\n",
      "3386482594\n",
      "3364762219\n",
      "3391910866\n",
      "3389794716\n",
      "3375430663\n",
      "3379906488\n",
      "3374280684\n",
      "3389110658\n",
      "3346961508\n",
      "3389105500\n",
      "3328884452\n",
      "3382073732\n",
      "3364217382\n",
      "3310636246\n",
      "3391459118\n",
      "3363673653\n",
      "3250138590\n",
      "3365593070\n",
      "3391911999\n",
      "3294100326\n",
      "3357475761\n",
      "3303387708\n",
      "3361404447\n",
      "3390823608\n",
      "2843045883\n",
      "3390032181\n",
      "3364941187\n",
      "3374698577\n",
      "3385199416\n",
      "3370110149\n",
      "3371543219\n",
      "3388591670\n",
      "3365130651\n",
      "3362765832\n",
      "3283260659\n",
      "3226657886\n",
      "3361533371\n",
      "3026310092\n",
      "3365599355\n",
      "3376299212\n",
      "3380222486\n",
      "3026416102\n",
      "3368910375\n",
      "3390649636\n",
      "3365592745\n",
      "3270284414\n",
      "3024305913\n",
      "3379059074\n",
      "3391479780\n",
      "3348468246\n",
      "3381574280\n",
      "3391739330\n",
      "3272706709\n",
      "3359356708\n",
      "3365594257\n",
      "3383496027\n",
      "3337181340\n",
      "3364217639\n",
      "3391889987\n",
      "3364216369\n",
      "3368113759\n",
      "3367142807\n",
      "3382817541\n",
      "3097417370\n",
      "3310800877\n",
      "3345597620\n",
      "3387954063\n",
      "3353403038\n",
      "3256468388\n",
      "3375704792\n",
      "2805741309\n",
      "3297626736\n",
      "3342286414\n",
      "3089502227\n",
      "3347133133\n",
      "3112627652\n",
      "3357774174\n",
      "3385520868\n",
      "3377253166\n",
      "3294156050\n",
      "3373908447\n",
      "3380694219\n",
      "3328495749\n",
      "3028278579\n",
      "3304548552\n",
      "3356445098\n",
      "3221152643\n",
      "3379206501\n",
      "3276632357\n",
      "3283690987\n",
      "3357098747\n",
      "3389437981\n",
      "3390798686\n",
      "3347903964\n",
      "3204283190\n",
      "3365966803\n",
      "3389475634\n",
      "3279799442\n",
      "3145606361\n",
      "3378190508\n",
      "3319618262\n",
      "3360313684\n",
      "2728572590\n",
      "3366352741\n",
      "3353408638\n",
      "3250657259\n",
      "3386682367\n",
      "3221154178\n",
      "3385548830\n",
      "3222939647\n",
      "3356811831\n",
      "3385548831\n",
      "3361319289\n",
      "3390272571\n",
      "3196505400\n",
      "2894566897\n",
      "3391515796\n",
      "3374657738\n",
      "3009226251\n",
      "3336642327\n",
      "3388042390\n",
      "3299469030\n",
      "3358453194\n",
      "3305782144\n",
      "3042963953\n",
      "3391227608\n",
      "2830956947\n",
      "3379136111\n",
      "3354637243\n",
      "3182148363\n",
      "3344373156\n",
      "3024298943\n",
      "3381256451\n",
      "3299353201\n",
      "3391326038\n",
      "3388725698\n",
      "3366545905\n",
      "3336432464\n",
      "3382796198\n",
      "2805730584\n",
      "3326693262\n",
      "3378034906\n",
      "3344983178\n",
      "3389457036\n",
      "3216637790\n",
      "2682416895\n",
      "3389473881\n",
      "3391452699\n",
      "3350256041\n",
      "3325142817\n",
      "3167065760\n",
      "3131748220\n",
      "3362847573\n",
      "3369310948\n",
      "2905794448\n",
      "3389696568\n",
      "3360722678\n",
      "3383049801\n",
      "3375674746\n",
      "3375666714\n",
      "3386062239\n",
      "3380009451\n",
      "3368375402\n",
      "3344304777\n",
      "3302233792\n",
      "3381560563\n",
      "3374290487\n",
      "3379502612\n",
      "3360688963\n",
      "3367357617\n",
      "3381713488\n",
      "3390913389\n",
      "3387954064\n",
      "3386001293\n",
      "3236553500\n",
      "3354688738\n",
      "3250145116\n",
      "3305866748\n",
      "3390475593\n",
      "3366618209\n",
      "3349520525\n",
      "3383049979\n",
      "3310757778\n",
      "3291790167\n",
      "3370838217\n",
      "3390798705\n",
      "3147380766\n",
      "3343069387\n",
      "3241479300\n",
      "3390478898\n",
      "3381764730\n",
      "3386458364\n",
      "3390451028\n",
      "3018999533\n",
      "3391976062\n",
      "3290500590\n",
      "3353486745\n",
      "3365344025\n",
      "3141808620\n",
      "3354949828\n",
      "3389003813\n",
      "3344261552\n",
      "3368842694\n",
      "3358249232\n",
      "3320727492\n",
      "3284544349\n",
      "3385376472\n",
      "3372705977\n",
      "3372440269\n",
      "3342443987\n",
      "2815319335\n",
      "3264554349\n",
      "3391451709\n",
      "3358277816\n",
      "3365098427\n",
      "3390398637\n",
      "3334367086\n",
      "3318120120\n",
      "3314488612\n",
      "3374487547\n",
      "3357175641\n",
      "3227249258\n",
      "3248107443\n",
      "3355235575\n",
      "3097422861\n",
      "3358746665\n",
      "3357234804\n",
      "3356866240\n",
      "3275047315\n",
      "3335591784\n",
      "3387972104\n",
      "3350592560\n",
      "3388333820\n",
      "3143164701\n",
      "3329051603\n",
      "3321095757\n",
      "3358283996\n",
      "3342792120\n",
      "3370602991\n",
      "3261322462\n",
      "3353571054\n",
      "3386785122\n",
      "3347671898\n",
      "3361827429\n",
      "3349506039\n",
      "3386446902\n",
      "3312869918\n",
      "3205874097\n",
      "3284141945\n",
      "3375215414\n",
      "3389457397\n",
      "3386177145\n",
      "3374134883\n",
      "3389458872\n",
      "3226564805\n",
      "3384602825\n",
      "3359158121\n",
      "3265282666\n",
      "3389834808\n",
      "2399151229\n",
      "2643418786\n",
      "2846625727\n",
      "3352466057\n",
      "3346473096\n",
      "3162838785\n",
      "3329890320\n",
      "3300990307\n",
      "3353028670\n",
      "3024441198\n",
      "3370359504\n",
      "3030726619\n",
      "3218390878\n",
      "3390478442\n",
      "3241386207\n",
      "3277900840\n",
      "3390808457\n",
      "3358310830\n",
      "3223913105\n",
      "3304783500\n",
      "3365114392\n",
      "3389480540\n",
      "3361007067\n",
      "3298969858\n",
      "3349945478\n",
      "3204840537\n",
      "3227144875\n",
      "3042963544\n",
      "3297551623\n",
      "3391620329\n",
      "3363083070\n",
      "3360750231\n",
      "2756125191\n",
      "3356395660\n",
      "2613571011\n",
      "3347706928\n",
      "3351721043\n",
      "3330721722\n",
      "3279967546\n",
      "2699373757\n",
      "3104321391\n",
      "2802182714\n",
      "3391244549\n",
      "3368026226\n",
      "3152252849\n",
      "3360904461\n",
      "3304513767\n",
      "3390485177\n",
      "3390471460\n",
      "3183917356\n",
      "2701221094\n",
      "3249568868\n",
      "3373572181\n",
      "3336452888\n",
      "3310773089\n",
      "3353364228\n",
      "3235023173\n",
      "2625550835\n",
      "3261340591\n",
      "2205131791\n",
      "3390517528\n",
      "3255044377\n",
      "3268496370\n",
      "3261275142\n",
      "3241479496\n",
      "3347385784\n",
      "3316337052\n",
      "3157475402\n",
      "3362001647\n",
      "3016824335\n",
      "3391454096\n",
      "3305951022\n",
      "2586395394\n",
      "3278772035\n",
      "3355505851\n",
      "3378453884\n",
      "3206886489\n",
      "3334554957\n",
      "3320887905\n",
      "3335357407\n",
      "3314068899\n",
      "3368408073\n",
      "3371427451\n",
      "3355995533\n",
      "3382695723\n",
      "3358693766\n",
      "3390997824\n",
      "3366389666\n",
      "3386800704\n",
      "3284792099\n",
      "3374524978\n",
      "3380526201\n",
      "3334528706\n",
      "3383167200\n",
      "3368760213\n",
      "3226860130\n",
      "3370464306\n",
      "3372802823\n",
      "3315110012\n",
      "3378114189\n",
      "3382216306\n",
      "3372442313\n",
      "3372802654\n",
      "2738642001\n",
      "3386746978\n",
      "3372599684\n",
      "3343302019\n",
      "2999936335\n",
      "3288096218\n",
      "3369136637\n",
      "3310582184\n",
      "3255182106\n",
      "3207066944\n",
      "3373845720\n",
      "3389137352\n",
      "3344992472\n",
      "3368587818\n",
      "3160451404\n",
      "3377490685\n",
      "3272373666\n",
      "3388049781\n",
      "3243857019\n",
      "3390822737\n",
      "3252914685\n",
      "3374627658\n",
      "3392046125\n",
      "3269684505\n",
      "3381561468\n",
      "3253731394\n",
      "3380609121\n",
      "3280519916\n",
      "3303356588\n",
      "3313766242\n",
      "1518282148\n",
      "3389453304\n",
      "3344980725\n",
      "3003355143\n",
      "3389861910\n",
      "3386085524\n",
      "3358869187\n",
      "3380915652\n",
      "3371180768\n",
      "3382362638\n",
      "3390808593\n",
      "3377834680\n",
      "3374510907\n",
      "3349506201\n",
      "3341982548\n",
      "3198877873\n",
      "3390164202\n",
      "3377323295\n",
      "3149414104\n",
      "3385091901\n",
      "3388925956\n",
      "3250260342\n",
      "3357580393\n",
      "3302841072\n",
      "3232196825\n",
      "3121977624\n",
      "3361435662\n",
      "3261215240\n",
      "3316421998\n",
      "3338590194\n",
      "3343098374\n",
      "3219451475\n",
      "3388704069\n",
      "3368462259\n",
      "3363278108\n",
      "3353360125\n",
      "3301576190\n",
      "3390192157\n",
      "3318447073\n",
      "3241881621\n",
      "3315767058\n",
      "3359440335\n",
      "3091311554\n",
      "3346194713\n",
      "3380429816\n",
      "2883363101\n",
      "3349719006\n",
      "3388728004\n",
      "3181607110\n",
      "3303491640\n",
      "3360978453\n",
      "3380089023\n",
      "3289048728\n",
      "3188899572\n",
      "3355355556\n",
      "3359281012\n",
      "3380194068\n",
      "3361307072\n",
      "3242917002\n",
      "3388683899\n",
      "3377562203\n",
      "3318316541\n",
      "3390403255\n",
      "3326922684\n",
      "3245584756\n",
      "3307084492\n",
      "3366624100\n",
      "3142278445\n",
      "3382779017\n",
      "3382698741\n",
      "3385088517\n",
      "3371456396\n",
      "3307218779\n",
      "3389454472\n",
      "3381124433\n",
      "3385574029\n",
      "3193822281\n",
      "3360837707\n",
      "3388784064\n",
      "3387937743\n",
      "3391743602\n",
      "3042963917\n",
      "3389072701\n",
      "3371964899\n",
      "3176152088\n",
      "3372618360\n",
      "3344316315\n",
      "3316841130\n",
      "3384962076\n",
      "3314346296\n",
      "3390174271\n",
      "3332979497\n",
      "3127512802\n",
      "3241479497\n",
      "3205805597\n",
      "3239678914\n",
      "3387972106\n",
      "3380924781\n",
      "3380038893\n",
      "3391225784\n",
      "3281446074\n",
      "3367190454\n",
      "3307218778\n",
      "3375453305\n",
      "3225543594\n",
      "3379500410\n",
      "3359153199\n",
      "3364781986\n",
      "3391743583\n",
      "3378458028\n",
      "3388809191\n",
      "3131307964\n",
      "3242917004\n",
      "3216482166\n",
      "3284788525\n",
      "3389465996\n",
      "3275441294\n",
      "3385899396\n",
      "3332629132\n",
      "3374588528\n",
      "3391569956\n",
      "3330025890\n",
      "3391734415\n",
      "3365552325\n",
      "3386308459\n",
      "3390932691\n",
      "3244063645\n",
      "3389647362\n",
      "3042964097\n",
      "3304584449\n",
      "3307136055\n",
      "3090632882\n",
      "3302351543\n",
      "3353054982\n",
      "3292996965\n",
      "3270433241\n",
      "3216423001\n",
      "3366656533\n",
      "3371400250\n",
      "3358012561\n",
      "3333213051\n",
      "3315619807\n",
      "3374321497\n",
      "3296286018\n",
      "3372649424\n",
      "3123618711\n",
      "3382490177\n",
      "3252920071\n",
      "3293271462\n",
      "2762128593\n",
      "3331640007\n",
      "3387962862\n",
      "3370847809\n",
      "3111176014\n",
      "3300510911\n",
      "3390893169\n",
      "3367704241\n",
      "3383682931\n",
      "3377837817\n",
      "3289138501\n",
      "3180674154\n",
      "3372864281\n",
      "3381492602\n",
      "3366284500\n",
      "3341698187\n",
      "3376780434\n",
      "3391741103\n",
      "3385087325\n",
      "3234644718\n",
      "3356086361\n",
      "3377694578\n",
      "3391263157\n",
      "3378034865\n",
      "3280822180\n",
      "3332473060\n",
      "3347992370\n",
      "3203961865\n",
      "3021813740\n",
      "3326081261\n",
      "3391476518\n",
      "3368518290\n",
      "3331260098\n",
      "3314488459\n",
      "3318844170\n",
      "3042963963\n",
      "3329734202\n",
      "3036714790\n",
      "3358158942\n",
      "3310769855\n",
      "3348275855\n",
      "3288809116\n",
      "3296732312\n",
      "3280822365\n",
      "3318422270\n",
      "3379784823\n",
      "3280040412\n",
      "3387930147\n",
      "3375232853\n",
      "3361810789\n",
      "3388349947\n",
      "3361169736\n",
      "3146950754\n",
      "3361635360\n"
     ]
    }
   ],
   "source": [
    "for k,v in my_dict.items():\n",
    "    print(k)\n",
    "    if len(v)>= 13:\n",
    "        new_dict['job_id'].append(k)\n",
    "        new_dict['rating'].append(v[0])\n",
    "        new_dict['position'].append(v[1])\n",
    "        new_dict['company'].append(v[2])\n",
    "        new_dict['job_city'].append(v[3])\n",
    "        new_dict['job_state_code'].append(v[4])\n",
    "        new_dict['sal_low'].append(v[5])\n",
    "        new_dict['sal_high'].append(v[6])\n",
    "        new_dict['link'].append(v[7])\n",
    "        new_dict['description'].append(v[8])\n",
    "        #print(type(v[8]))\n",
    "        new_dict['hq_city'].append(v[9])\n",
    "        new_dict['hq_state_code'].append(v[10])\n",
    "        new_dict['size'].append(v[11])\n",
    "        new_dict['industry'].append(v[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job_id': ['3381578119',\n",
       "  '3320321158',\n",
       "  '3284284037',\n",
       "  '3298422474',\n",
       "  '3352031827',\n",
       "  '3196505395',\n",
       "  '3388905001',\n",
       "  '3374099933',\n",
       "  '3356833111',\n",
       "  '3350001434',\n",
       "  '3386703038',\n",
       "  '3378009255',\n",
       "  '3354376776',\n",
       "  '3180307018',\n",
       "  '3376562653',\n",
       "  '3254097283',\n",
       "  '3359023976',\n",
       "  '3238036556',\n",
       "  '3296285380',\n",
       "  '3350511635',\n",
       "  '3356138485',\n",
       "  '3356602294',\n",
       "  '3328266779',\n",
       "  '3322135751',\n",
       "  '3391450036',\n",
       "  '3365068364',\n",
       "  '3205983875',\n",
       "  '3353556402',\n",
       "  '3318225561',\n",
       "  '3328903440',\n",
       "  '3253403496',\n",
       "  '3308918595',\n",
       "  '3379951822',\n",
       "  '3358722522',\n",
       "  '3373840950',\n",
       "  '3335457872',\n",
       "  '3351864423',\n",
       "  '3365674970',\n",
       "  '3219890595',\n",
       "  '3279849002',\n",
       "  '3246517577',\n",
       "  '3385750069',\n",
       "  '3374690234',\n",
       "  '3178235988',\n",
       "  '2994117434',\n",
       "  '3346481137',\n",
       "  '3353579290',\n",
       "  '3374589739',\n",
       "  '3329734179',\n",
       "  '3325134847',\n",
       "  '3322135794',\n",
       "  '3383653153',\n",
       "  '3300388779',\n",
       "  '3374670113',\n",
       "  '2725035780',\n",
       "  '3227655543',\n",
       "  '3325280156',\n",
       "  '3272706711',\n",
       "  '3323871719',\n",
       "  '3265401585',\n",
       "  '3364762246',\n",
       "  '3361416053',\n",
       "  '3384363812',\n",
       "  '3302841314',\n",
       "  '3363451785',\n",
       "  '3358528772',\n",
       "  '3365248211',\n",
       "  '3344983747',\n",
       "  '3332214881',\n",
       "  '3280026843',\n",
       "  '3311524017',\n",
       "  '3391745761',\n",
       "  '3388114590',\n",
       "  '3317344619',\n",
       "  '3181397648',\n",
       "  '3093497333',\n",
       "  '3272792918',\n",
       "  '3349197179',\n",
       "  '3348319164',\n",
       "  '3377840501',\n",
       "  '3379783823',\n",
       "  '3388498166',\n",
       "  '3280026844',\n",
       "  '3386482594',\n",
       "  '3364762219',\n",
       "  '3391910866',\n",
       "  '3389794716',\n",
       "  '3379906488',\n",
       "  '3374280684',\n",
       "  '3389110658',\n",
       "  '3346961508',\n",
       "  '3389105500',\n",
       "  '3328884452',\n",
       "  '3382073732',\n",
       "  '3364217382',\n",
       "  '3310636246',\n",
       "  '3363673653',\n",
       "  '3250138590',\n",
       "  '3365593070',\n",
       "  '3391911999',\n",
       "  '3294100326',\n",
       "  '3357475761',\n",
       "  '3303387708',\n",
       "  '3361404447',\n",
       "  '3390823608',\n",
       "  '2843045883',\n",
       "  '3364941187',\n",
       "  '3374698577',\n",
       "  '3385199416',\n",
       "  '3371543219',\n",
       "  '3388591670',\n",
       "  '3365130651',\n",
       "  '3362765832',\n",
       "  '3283260659',\n",
       "  '3226657886',\n",
       "  '3361533371',\n",
       "  '3026310092',\n",
       "  '3365599355',\n",
       "  '3376299212',\n",
       "  '3380222486',\n",
       "  '3026416102',\n",
       "  '3368910375',\n",
       "  '3390649636',\n",
       "  '3365592745',\n",
       "  '3270284414',\n",
       "  '3024305913',\n",
       "  '3379059074',\n",
       "  '3391479780',\n",
       "  '3348468246',\n",
       "  '3381574280',\n",
       "  '3391739330',\n",
       "  '3272706709',\n",
       "  '3359356708',\n",
       "  '3365594257',\n",
       "  '3383496027',\n",
       "  '3337181340',\n",
       "  '3364217639',\n",
       "  '3364216369',\n",
       "  '3368113759',\n",
       "  '3367142807',\n",
       "  '3382817541',\n",
       "  '3097417370',\n",
       "  '3310800877',\n",
       "  '3345597620',\n",
       "  '3387954063',\n",
       "  '3353403038',\n",
       "  '3256468388',\n",
       "  '3375704792',\n",
       "  '2805741309',\n",
       "  '3297626736',\n",
       "  '3342286414',\n",
       "  '3089502227',\n",
       "  '3347133133',\n",
       "  '3112627652',\n",
       "  '3357774174',\n",
       "  '3385520868',\n",
       "  '3377253166',\n",
       "  '3294156050',\n",
       "  '3373908447',\n",
       "  '3380694219',\n",
       "  '3328495749',\n",
       "  '3028278579',\n",
       "  '3304548552',\n",
       "  '3356445098',\n",
       "  '3221152643',\n",
       "  '3379206501',\n",
       "  '3276632357',\n",
       "  '3357098747',\n",
       "  '3389437981',\n",
       "  '3390798686',\n",
       "  '3347903964',\n",
       "  '3204283190',\n",
       "  '3365966803',\n",
       "  '3389475634',\n",
       "  '3279799442',\n",
       "  '3145606361',\n",
       "  '3378190508',\n",
       "  '3319618262',\n",
       "  '3360313684',\n",
       "  '2728572590',\n",
       "  '3366352741',\n",
       "  '3353408638',\n",
       "  '3250657259',\n",
       "  '3386682367',\n",
       "  '3221154178',\n",
       "  '3385548830',\n",
       "  '3222939647',\n",
       "  '3356811831',\n",
       "  '3385548831',\n",
       "  '3361319289',\n",
       "  '3390272571',\n",
       "  '3196505400',\n",
       "  '2894566897',\n",
       "  '3391515796',\n",
       "  '3009226251',\n",
       "  '3336642327',\n",
       "  '3388042390',\n",
       "  '3299469030',\n",
       "  '3358453194',\n",
       "  '3305782144',\n",
       "  '2830956947',\n",
       "  '3379136111',\n",
       "  '3354637243',\n",
       "  '3182148363',\n",
       "  '3344373156',\n",
       "  '3381256451',\n",
       "  '3299353201',\n",
       "  '3391326038',\n",
       "  '3388725698',\n",
       "  '3366545905',\n",
       "  '3336432464',\n",
       "  '3382796198',\n",
       "  '2805730584',\n",
       "  '3326693262',\n",
       "  '3378034906',\n",
       "  '3344983178',\n",
       "  '3216637790',\n",
       "  '2682416895',\n",
       "  '3389473881',\n",
       "  '3350256041',\n",
       "  '3325142817',\n",
       "  '3167065760',\n",
       "  '3131748220',\n",
       "  '3362847573',\n",
       "  '3369310948',\n",
       "  '2905794448',\n",
       "  '3389696568',\n",
       "  '3360722678',\n",
       "  '3383049801',\n",
       "  '3375674746',\n",
       "  '3375666714',\n",
       "  '3380009451',\n",
       "  '3368375402',\n",
       "  '3344304777',\n",
       "  '3302233792',\n",
       "  '3374290487',\n",
       "  '3379502612',\n",
       "  '3360688963',\n",
       "  '3367357617',\n",
       "  '3381713488',\n",
       "  '3390913389',\n",
       "  '3387954064',\n",
       "  '3386001293',\n",
       "  '3236553500',\n",
       "  '3250145116',\n",
       "  '3305866748',\n",
       "  '3390475593',\n",
       "  '3366618209',\n",
       "  '3349520525',\n",
       "  '3383049979',\n",
       "  '3310757778',\n",
       "  '3291790167',\n",
       "  '3390798705',\n",
       "  '3147380766',\n",
       "  '3343069387',\n",
       "  '3241479300',\n",
       "  '3386458364',\n",
       "  '3390451028',\n",
       "  '3018999533',\n",
       "  '3391976062',\n",
       "  '3290500590',\n",
       "  '3353486745',\n",
       "  '3141808620',\n",
       "  '3354949828',\n",
       "  '3368842694',\n",
       "  '3358249232',\n",
       "  '3320727492',\n",
       "  '3284544349',\n",
       "  '3385376472',\n",
       "  '3372705977',\n",
       "  '3372440269',\n",
       "  '3342443987',\n",
       "  '3264554349',\n",
       "  '3391451709',\n",
       "  '3358277816',\n",
       "  '3365098427',\n",
       "  '3390398637',\n",
       "  '3334367086',\n",
       "  '3318120120',\n",
       "  '3314488612',\n",
       "  '3374487547',\n",
       "  '3357175641',\n",
       "  '3227249258',\n",
       "  '3248107443',\n",
       "  '3355235575',\n",
       "  '3097422861',\n",
       "  '3358746665',\n",
       "  '3357234804',\n",
       "  '3356866240',\n",
       "  '3275047315',\n",
       "  '3335591784',\n",
       "  '3387972104',\n",
       "  '3350592560',\n",
       "  '3388333820',\n",
       "  '3143164701',\n",
       "  '3329051603',\n",
       "  '3321095757',\n",
       "  '3358283996',\n",
       "  '3342792120',\n",
       "  '3370602991',\n",
       "  '3261322462',\n",
       "  '3353571054',\n",
       "  '3386785122',\n",
       "  '3347671898',\n",
       "  '3361827429',\n",
       "  '3349506039',\n",
       "  '3386446902',\n",
       "  '3312869918',\n",
       "  '3205874097',\n",
       "  '3284141945',\n",
       "  '3375215414',\n",
       "  '3389457397',\n",
       "  '3386177145',\n",
       "  '3374134883',\n",
       "  '3389458872',\n",
       "  '3226564805',\n",
       "  '3384602825',\n",
       "  '3359158121',\n",
       "  '3265282666',\n",
       "  '3389834808',\n",
       "  '2399151229',\n",
       "  '2643418786',\n",
       "  '2846625727',\n",
       "  '3352466057',\n",
       "  '3346473096',\n",
       "  '3162838785',\n",
       "  '3329890320',\n",
       "  '3300990307',\n",
       "  '3353028670',\n",
       "  '3024441198',\n",
       "  '3030726619',\n",
       "  '3218390878',\n",
       "  '3390478442',\n",
       "  '3241386207',\n",
       "  '3277900840',\n",
       "  '3358310830',\n",
       "  '3223913105',\n",
       "  '3304783500',\n",
       "  '3389480540',\n",
       "  '3361007067',\n",
       "  '3298969858',\n",
       "  '3349945478',\n",
       "  '3204840537',\n",
       "  '3227144875',\n",
       "  '3297551623',\n",
       "  '3391620329',\n",
       "  '3363083070',\n",
       "  '3360750231',\n",
       "  '2756125191',\n",
       "  '3356395660',\n",
       "  '2613571011',\n",
       "  '3347706928',\n",
       "  '3351721043',\n",
       "  '3330721722',\n",
       "  '3279967546',\n",
       "  '2699373757',\n",
       "  '3104321391',\n",
       "  '2802182714',\n",
       "  '3391244549',\n",
       "  '3368026226',\n",
       "  '3152252849',\n",
       "  '3360904461',\n",
       "  '3304513767',\n",
       "  '3390485177',\n",
       "  '2701221094',\n",
       "  '3373572181',\n",
       "  '3336452888',\n",
       "  '3310773089',\n",
       "  '3353364228',\n",
       "  '3235023173',\n",
       "  '2625550835',\n",
       "  '3261340591',\n",
       "  '2205131791',\n",
       "  '3255044377',\n",
       "  '3268496370',\n",
       "  '3241479496',\n",
       "  '3347385784',\n",
       "  '3316337052',\n",
       "  '3157475402',\n",
       "  '3362001647',\n",
       "  '3016824335',\n",
       "  '3305951022',\n",
       "  '2586395394',\n",
       "  '3278772035',\n",
       "  '3355505851',\n",
       "  '3378453884',\n",
       "  '3206886489',\n",
       "  '3334554957',\n",
       "  '3320887905',\n",
       "  '3335357407',\n",
       "  '3314068899',\n",
       "  '3371427451',\n",
       "  '3382695723',\n",
       "  '3358693766',\n",
       "  '3390997824',\n",
       "  '3366389666',\n",
       "  '3284792099',\n",
       "  '3334528706',\n",
       "  '3383167200',\n",
       "  '3368760213',\n",
       "  '3226860130',\n",
       "  '3370464306',\n",
       "  '3315110012',\n",
       "  '3382216306',\n",
       "  '3372442313',\n",
       "  '3372802654',\n",
       "  '2738642001',\n",
       "  '3386746978',\n",
       "  '3372599684',\n",
       "  '2999936335',\n",
       "  '3310582184',\n",
       "  '3255182106',\n",
       "  '3207066944',\n",
       "  '3373845720',\n",
       "  '3344992472',\n",
       "  '3368587818',\n",
       "  '3160451404',\n",
       "  '3272373666',\n",
       "  '3390822737',\n",
       "  '3252914685',\n",
       "  '3374627658',\n",
       "  '3392046125',\n",
       "  '3269684505',\n",
       "  '3380609121',\n",
       "  '3280519916',\n",
       "  '3303356588',\n",
       "  '3313766242',\n",
       "  '3389453304',\n",
       "  '3344980725',\n",
       "  '3003355143',\n",
       "  '3389861910',\n",
       "  '3386085524',\n",
       "  '3358869187',\n",
       "  '3380915652',\n",
       "  '3371180768',\n",
       "  '3382362638',\n",
       "  '3390808593',\n",
       "  '3377834680',\n",
       "  '3374510907',\n",
       "  '3349506201',\n",
       "  '3341982548',\n",
       "  '3198877873',\n",
       "  '3390164202',\n",
       "  '3377323295',\n",
       "  '3149414104',\n",
       "  '3385091901',\n",
       "  '3388925956',\n",
       "  '3250260342',\n",
       "  '3302841072',\n",
       "  '3232196825',\n",
       "  '3121977624',\n",
       "  '3361435662',\n",
       "  '3261215240',\n",
       "  '3316421998',\n",
       "  '3338590194',\n",
       "  '3343098374',\n",
       "  '3219451475',\n",
       "  '3388704069',\n",
       "  '3368462259',\n",
       "  '3353360125',\n",
       "  '3301576190',\n",
       "  '3390192157',\n",
       "  '3318447073',\n",
       "  '3241881621',\n",
       "  '3315767058',\n",
       "  '3359440335',\n",
       "  '3091311554',\n",
       "  '3346194713',\n",
       "  '3380429816',\n",
       "  '2883363101',\n",
       "  '3349719006',\n",
       "  '3388728004',\n",
       "  '3303491640',\n",
       "  '3380089023',\n",
       "  '3289048728',\n",
       "  '3188899572',\n",
       "  '3355355556',\n",
       "  '3359281012',\n",
       "  '3361307072',\n",
       "  '3242917002',\n",
       "  '3388683899',\n",
       "  '3377562203',\n",
       "  '3318316541',\n",
       "  '3390403255',\n",
       "  '3326922684',\n",
       "  '3245584756',\n",
       "  '3307084492',\n",
       "  '3382779017',\n",
       "  '3382698741',\n",
       "  '3385088517',\n",
       "  '3371456396',\n",
       "  '3307218779',\n",
       "  '3389454472',\n",
       "  '3193822281',\n",
       "  '3360837707',\n",
       "  '3388784064',\n",
       "  '3387937743',\n",
       "  '3391743602',\n",
       "  '3389072701',\n",
       "  '3371964899',\n",
       "  '3176152088',\n",
       "  '3372618360',\n",
       "  '3344316315',\n",
       "  '3316841130',\n",
       "  '3384962076',\n",
       "  '3314346296',\n",
       "  '3390174271',\n",
       "  '3332979497',\n",
       "  '3127512802',\n",
       "  '3241479497',\n",
       "  '3205805597',\n",
       "  '3239678914',\n",
       "  '3387972106',\n",
       "  '3380924781',\n",
       "  '3380038893',\n",
       "  '3391225784',\n",
       "  '3281446074',\n",
       "  '3307218778',\n",
       "  '3375453305',\n",
       "  '3225543594',\n",
       "  '3379500410',\n",
       "  '3359153199',\n",
       "  '3364781986',\n",
       "  '3391743583',\n",
       "  '3378458028',\n",
       "  '3388809191',\n",
       "  '3131307964',\n",
       "  '3242917004',\n",
       "  '3216482166',\n",
       "  '3284788525',\n",
       "  '3389465996',\n",
       "  '3275441294',\n",
       "  '3385899396',\n",
       "  '3332629132',\n",
       "  '3374588528',\n",
       "  '3391569956',\n",
       "  '3330025890',\n",
       "  '3391734415',\n",
       "  '3365552325',\n",
       "  '3386308459',\n",
       "  '3390932691',\n",
       "  '3244063645',\n",
       "  '3389647362',\n",
       "  '3304584449',\n",
       "  '3307136055',\n",
       "  '3302351543',\n",
       "  '3353054982',\n",
       "  '3292996965',\n",
       "  '3270433241',\n",
       "  '3216423001',\n",
       "  '3366656533',\n",
       "  '3371400250',\n",
       "  '3358012561',\n",
       "  '3333213051',\n",
       "  '3315619807',\n",
       "  '3374321497',\n",
       "  '3296286018',\n",
       "  '3372649424',\n",
       "  '3123618711',\n",
       "  '3382490177',\n",
       "  '3252920071',\n",
       "  '3293271462',\n",
       "  '3331640007',\n",
       "  '3387962862',\n",
       "  '3370847809',\n",
       "  '3111176014',\n",
       "  '3300510911',\n",
       "  '3390893169',\n",
       "  '3367704241',\n",
       "  '3383682931',\n",
       "  '3377837817',\n",
       "  '3289138501',\n",
       "  '3372864281',\n",
       "  '3381492602',\n",
       "  '3366284500',\n",
       "  '3341698187',\n",
       "  '3391741103',\n",
       "  '3385087325',\n",
       "  '3234644718',\n",
       "  '3356086361',\n",
       "  '3391263157',\n",
       "  '3378034865',\n",
       "  '3280822180',\n",
       "  '3332473060',\n",
       "  '3347992370',\n",
       "  '3203961865',\n",
       "  '3021813740',\n",
       "  '3326081261',\n",
       "  '3391476518',\n",
       "  '3368518290',\n",
       "  '3331260098',\n",
       "  '3314488459',\n",
       "  '3318844170',\n",
       "  '3329734202',\n",
       "  '3036714790',\n",
       "  '3358158942',\n",
       "  '3310769855',\n",
       "  '3348275855',\n",
       "  '3288809116',\n",
       "  '3296732312',\n",
       "  '3280822365',\n",
       "  '3318422270',\n",
       "  '3379784823',\n",
       "  '3280040412',\n",
       "  '3387930147',\n",
       "  '3375232853',\n",
       "  '3361810789',\n",
       "  '3388349947',\n",
       "  '3361169736',\n",
       "  '3146950754',\n",
       "  '3361635360'],\n",
       " 'rating': ['3.9',\n",
       "  '3.6',\n",
       "  '2.7',\n",
       "  '4.0',\n",
       "  '4.3',\n",
       "  '3.4',\n",
       "  '3.5',\n",
       "  '2.8',\n",
       "  '4.3',\n",
       "  '3.6',\n",
       "  '3.9',\n",
       "  '4.5',\n",
       "  '4.0',\n",
       "  '',\n",
       "  '3.7',\n",
       "  '4.7',\n",
       "  '3.3',\n",
       "  '3.8',\n",
       "  '3.7',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '3.4',\n",
       "  '3.4',\n",
       "  '4.0',\n",
       "  '5.0',\n",
       "  '4.6',\n",
       "  '4.0',\n",
       "  '3.7',\n",
       "  '3.8',\n",
       "  '3.6',\n",
       "  '3.6',\n",
       "  '3.2',\n",
       "  '3.7',\n",
       "  '4.6',\n",
       "  '4.4',\n",
       "  '2.6',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '2.8',\n",
       "  '3.6',\n",
       "  '3.1',\n",
       "  '3.5',\n",
       "  '3.7',\n",
       "  '3.2',\n",
       "  '3.3',\n",
       "  '3.5',\n",
       "  '4.0',\n",
       "  '3.3',\n",
       "  '4.0',\n",
       "  '2.9',\n",
       "  '4.5',\n",
       "  '2.8',\n",
       "  '4.6',\n",
       "  '3.5',\n",
       "  '4.0',\n",
       "  '3.2',\n",
       "  '4.1',\n",
       "  '3.8',\n",
       "  '4.1',\n",
       "  '3.8',\n",
       "  '3.3',\n",
       "  '4.2',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '4.3',\n",
       "  '3.6',\n",
       "  '3.6',\n",
       "  '3.5',\n",
       "  '4.6',\n",
       "  '4.1',\n",
       "  '4.2',\n",
       "  '3.0',\n",
       "  '3.3',\n",
       "  '2.8',\n",
       "  '4.2',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '',\n",
       "  '3.8',\n",
       "  '3.4',\n",
       "  '4.0',\n",
       "  '3.0',\n",
       "  '4.1',\n",
       "  '4.0',\n",
       "  '3.3',\n",
       "  '4.4',\n",
       "  '3.8',\n",
       "  '3.6',\n",
       "  '4.9',\n",
       "  '3.9',\n",
       "  '4.6',\n",
       "  '3.5',\n",
       "  '3.4',\n",
       "  '2.3',\n",
       "  '4.4',\n",
       "  '3.1',\n",
       "  '5.0',\n",
       "  '2.7',\n",
       "  '3.9',\n",
       "  '4.9',\n",
       "  '4.1',\n",
       "  '4.1',\n",
       "  '4.6',\n",
       "  '4.3',\n",
       "  '3.8',\n",
       "  '3.3',\n",
       "  '3.8',\n",
       "  '3.9',\n",
       "  '4.6',\n",
       "  '3.9',\n",
       "  '3.9',\n",
       "  '3.1',\n",
       "  '3.4',\n",
       "  '3.4',\n",
       "  '3.4',\n",
       "  '4.0',\n",
       "  '3.9',\n",
       "  '3.9',\n",
       "  '3.2',\n",
       "  '3.9',\n",
       "  '4.7',\n",
       "  '4.6',\n",
       "  '3.5',\n",
       "  '3.9',\n",
       "  '3.8',\n",
       "  '2.8',\n",
       "  '4.0',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '2.5',\n",
       "  '3.3',\n",
       "  '3.8',\n",
       "  '3.7',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '3.8',\n",
       "  '4.4',\n",
       "  '4.4',\n",
       "  '2.9',\n",
       "  '3.5',\n",
       "  '5.0',\n",
       "  '4.9',\n",
       "  '3.7',\n",
       "  '3.5',\n",
       "  '4.9',\n",
       "  '3.5',\n",
       "  '4.6',\n",
       "  '3.9',\n",
       "  '3.4',\n",
       "  '3.7',\n",
       "  '4.7',\n",
       "  '4.6',\n",
       "  '4.3',\n",
       "  '3.4',\n",
       "  '3.6',\n",
       "  '2.7',\n",
       "  '3.8',\n",
       "  '4.5',\n",
       "  '3.3',\n",
       "  '4.6',\n",
       "  '4.2',\n",
       "  '3.5',\n",
       "  '3.3',\n",
       "  '4.0',\n",
       "  '3.9',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '4.2',\n",
       "  '3.3',\n",
       "  '3.4',\n",
       "  '3.7',\n",
       "  '4.3',\n",
       "  '2.8',\n",
       "  '4.3',\n",
       "  '4.6',\n",
       "  '5.0',\n",
       "  '4.4',\n",
       "  '3.2',\n",
       "  '4.1',\n",
       "  '4.9',\n",
       "  '4.4',\n",
       "  '',\n",
       "  '4.0',\n",
       "  '',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '4.3',\n",
       "  '4.3',\n",
       "  '3.6',\n",
       "  '3.4',\n",
       "  '3.8',\n",
       "  '3.4',\n",
       "  '3.0',\n",
       "  '3.5',\n",
       "  '3.7',\n",
       "  '3.8',\n",
       "  '3.2',\n",
       "  '4.3',\n",
       "  '4.5',\n",
       "  '',\n",
       "  '3.2',\n",
       "  '5.0',\n",
       "  '4.2',\n",
       "  '4.4',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '4.8',\n",
       "  '3.5',\n",
       "  '3.0',\n",
       "  '4.4',\n",
       "  '5.0',\n",
       "  '3.9',\n",
       "  '4.9',\n",
       "  '',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '4.2',\n",
       "  '4.4',\n",
       "  '3.0',\n",
       "  '3.0',\n",
       "  '4.1',\n",
       "  '2.9',\n",
       "  '4.4',\n",
       "  '3.5',\n",
       "  '4.4',\n",
       "  '4.9',\n",
       "  '3.4',\n",
       "  '2.8',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '4.4',\n",
       "  '5.0',\n",
       "  '3.5',\n",
       "  '4.1',\n",
       "  '3.8',\n",
       "  '3.1',\n",
       "  '4.2',\n",
       "  '3.6',\n",
       "  '3.4',\n",
       "  '4.8',\n",
       "  '4.4',\n",
       "  '4.9',\n",
       "  '3.1',\n",
       "  '4.9',\n",
       "  '3.4',\n",
       "  '3.2',\n",
       "  '4.7',\n",
       "  '1.0',\n",
       "  '4.0',\n",
       "  '4.4',\n",
       "  '2.8',\n",
       "  '4.6',\n",
       "  '3.4',\n",
       "  '4.9',\n",
       "  '3.4',\n",
       "  '4.6',\n",
       "  '4.9',\n",
       "  '4.2',\n",
       "  '2.1',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '4.3',\n",
       "  '4.2',\n",
       "  '3.4',\n",
       "  '4.1',\n",
       "  '4.5',\n",
       "  '3.6',\n",
       "  '3.6',\n",
       "  '3.8',\n",
       "  '2.9',\n",
       "  '3.9',\n",
       "  '4.5',\n",
       "  '3.1',\n",
       "  '4.2',\n",
       "  '4.5',\n",
       "  '4.3',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '4.4',\n",
       "  '3.7',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '4.5',\n",
       "  '4.4',\n",
       "  '4.5',\n",
       "  '3.4',\n",
       "  '5.0',\n",
       "  '3.8',\n",
       "  '3.1',\n",
       "  '4.6',\n",
       "  '4.4',\n",
       "  '3.8',\n",
       "  '3.1',\n",
       "  '3.4',\n",
       "  '4.4',\n",
       "  '3.4',\n",
       "  '4.5',\n",
       "  '4.9',\n",
       "  '4.0',\n",
       "  '4.2',\n",
       "  '4.4',\n",
       "  '3.5',\n",
       "  '3.6',\n",
       "  '3.7',\n",
       "  '4.9',\n",
       "  '4.4',\n",
       "  '3.7',\n",
       "  '5.0',\n",
       "  '3.7',\n",
       "  '4.7',\n",
       "  '3.7',\n",
       "  '3.7',\n",
       "  '5.0',\n",
       "  '3.7',\n",
       "  '3.2',\n",
       "  '4.7',\n",
       "  '3.6',\n",
       "  '2.9',\n",
       "  '3.5',\n",
       "  '4.3',\n",
       "  '',\n",
       "  '4.4',\n",
       "  '3.0',\n",
       "  '4.3',\n",
       "  '3.8',\n",
       "  '4.6',\n",
       "  '3.7',\n",
       "  '3.9',\n",
       "  '4.0',\n",
       "  '3.5',\n",
       "  '4.0',\n",
       "  '4.2',\n",
       "  '2.4',\n",
       "  '4.3',\n",
       "  '3.6',\n",
       "  '3.3',\n",
       "  '3.9',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '5.0',\n",
       "  '3.4',\n",
       "  '2.5',\n",
       "  '3.4',\n",
       "  '3.4',\n",
       "  '4.8',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '3.3',\n",
       "  '5.0',\n",
       "  '4.0',\n",
       "  '2.7',\n",
       "  '3.5',\n",
       "  '4.7',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '4.0',\n",
       "  '4.6',\n",
       "  '3.8',\n",
       "  '3.6',\n",
       "  '3.8',\n",
       "  '4.2',\n",
       "  '3.5',\n",
       "  '4.8',\n",
       "  '4.7',\n",
       "  '4.3',\n",
       "  '4.6',\n",
       "  '4.3',\n",
       "  '',\n",
       "  '3.3',\n",
       "  '2.7',\n",
       "  '3.1',\n",
       "  '4.1',\n",
       "  '3.4',\n",
       "  '4.4',\n",
       "  '4.6',\n",
       "  '3.5',\n",
       "  '3.0',\n",
       "  '3.8',\n",
       "  '5.0',\n",
       "  '4.9',\n",
       "  '3.8',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '3.8',\n",
       "  '3.4',\n",
       "  '3.6',\n",
       "  '2.9',\n",
       "  '3.9',\n",
       "  '4.2',\n",
       "  '3.9',\n",
       "  '3.1',\n",
       "  '3.7',\n",
       "  '4.5',\n",
       "  '4.1',\n",
       "  '3.9',\n",
       "  '4.2',\n",
       "  '3.5',\n",
       "  '4.0',\n",
       "  '3.9',\n",
       "  '2.8',\n",
       "  '3.3',\n",
       "  '3.4',\n",
       "  '4.1',\n",
       "  '4.8',\n",
       "  '3.6',\n",
       "  '4.0',\n",
       "  '3.3',\n",
       "  '3.6',\n",
       "  '4.1',\n",
       "  '3.8',\n",
       "  '3.4',\n",
       "  '4.0',\n",
       "  '3.7',\n",
       "  '3.9',\n",
       "  '3.5',\n",
       "  '4.9',\n",
       "  '4.3',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '3.9',\n",
       "  '4.6',\n",
       "  '',\n",
       "  '4.0',\n",
       "  '3.9',\n",
       "  '5.0',\n",
       "  '3.4',\n",
       "  '4.3',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.4',\n",
       "  '5.0',\n",
       "  '2.8',\n",
       "  '3.6',\n",
       "  '4.2',\n",
       "  '2.5',\n",
       "  '3.7',\n",
       "  '3.7',\n",
       "  '4.2',\n",
       "  '3.9',\n",
       "  '3.4',\n",
       "  '3.1',\n",
       "  '3.5',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '3.1',\n",
       "  '4.0',\n",
       "  '3.6',\n",
       "  '4.7',\n",
       "  '4.6',\n",
       "  '3.1',\n",
       "  '3.8',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '3.2',\n",
       "  '3.5',\n",
       "  '3.1',\n",
       "  '4.0',\n",
       "  '4.0',\n",
       "  '3.4',\n",
       "  '3.4',\n",
       "  '4.4',\n",
       "  '2.4',\n",
       "  '4.2',\n",
       "  '2.6',\n",
       "  '2.9',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '5.0',\n",
       "  '3.6',\n",
       "  '3.8',\n",
       "  '4.3',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '3.5',\n",
       "  '3.2',\n",
       "  '3.5',\n",
       "  '3.4',\n",
       "  '4.4',\n",
       "  '3.8',\n",
       "  '4.3',\n",
       "  '3.6',\n",
       "  '5.0',\n",
       "  '4.1',\n",
       "  '3.6',\n",
       "  '3.1',\n",
       "  '3.5',\n",
       "  '3.5',\n",
       "  '2.1',\n",
       "  '3.3',\n",
       "  '3.4',\n",
       "  '3.5',\n",
       "  '3.4',\n",
       "  '4.3',\n",
       "  '3.4',\n",
       "  '4.1',\n",
       "  '4.3',\n",
       "  '3.9',\n",
       "  '4.4',\n",
       "  '4.8',\n",
       "  '4.0',\n",
       "  '4.1',\n",
       "  '4.6',\n",
       "  '3.0',\n",
       "  '2.5',\n",
       "  '4.4',\n",
       "  '3.4',\n",
       "  '4.2',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.6',\n",
       "  '4.7',\n",
       "  '3.6',\n",
       "  '2.7',\n",
       "  '3.2',\n",
       "  '3.1',\n",
       "  '3.6',\n",
       "  '2.4',\n",
       "  '3.7',\n",
       "  '3.5',\n",
       "  '4.0',\n",
       "  '3.6',\n",
       "  '4.7',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '3.5',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.7',\n",
       "  '3.3',\n",
       "  '3.8',\n",
       "  '3.7',\n",
       "  '3.3',\n",
       "  '3.9',\n",
       "  '4.6',\n",
       "  '3.5',\n",
       "  '3.5',\n",
       "  '4.1',\n",
       "  '3.5',\n",
       "  '2.9',\n",
       "  '2.5',\n",
       "  '4.3',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '4.1',\n",
       "  '4.1',\n",
       "  '3.8',\n",
       "  '3.1',\n",
       "  '4.4',\n",
       "  '3.6',\n",
       "  '3.6',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '3.6',\n",
       "  '4.9',\n",
       "  '4.3',\n",
       "  '3.8',\n",
       "  '4.2',\n",
       "  '4.3',\n",
       "  '4.0',\n",
       "  '3.6',\n",
       "  '4.2',\n",
       "  '3.7',\n",
       "  '4.9',\n",
       "  '3.7',\n",
       "  '4.1',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '3.9',\n",
       "  '4.1',\n",
       "  '5.0',\n",
       "  '3.5',\n",
       "  '3.9',\n",
       "  '3.6',\n",
       "  '3.5',\n",
       "  '3.8',\n",
       "  '4.7',\n",
       "  '3.8',\n",
       "  '3.8',\n",
       "  '4.4',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '4.6',\n",
       "  '5.0',\n",
       "  '3.7',\n",
       "  '4.1',\n",
       "  '4.1',\n",
       "  '4.3',\n",
       "  '4.0',\n",
       "  '4.3',\n",
       "  '4.3',\n",
       "  '4.2',\n",
       "  '4.0',\n",
       "  '3.8',\n",
       "  '4.0',\n",
       "  '3.7',\n",
       "  '3.0',\n",
       "  '3.8',\n",
       "  '5.0',\n",
       "  '3.3',\n",
       "  '3.7',\n",
       "  '4.9',\n",
       "  '4.0',\n",
       "  '4.2'],\n",
       " 'position': ['data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer, data intelligence',\n",
       "  'data application engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'opengov',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer mastery',\n",
       "  'data engineer',\n",
       "  'data engineer / senior data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data analyst - on ramp - recent graduate program',\n",
       "  'data analyst',\n",
       "  'senior data engineer',\n",
       "  'software engineer, data',\n",
       "  'data engineer',\n",
       "  'data analyst',\n",
       "  'cengage',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data warehouse engineer',\n",
       "  'senior data engineer',\n",
       "  'data engineer',\n",
       "  'sr. data engineer',\n",
       "  'back end software engineer',\n",
       "  'data engineer',\n",
       "  'business system data analyst',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'sr. data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data software engineer',\n",
       "  'associate software engineer',\n",
       "  'data engineer',\n",
       "  'data analyst',\n",
       "  'cengage',\n",
       "  'software engineer',\n",
       "  'machine learning engineer',\n",
       "  'cloud data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'senior data engineer',\n",
       "  'senior data science engineer, marketplace',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'big data engineer',\n",
       "  'senior data engineer',\n",
       "  'data warehouse engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data analyst - marketing',\n",
       "  'data engineer',\n",
       "  'interconnect engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'big data engineer',\n",
       "  'software engineer',\n",
       "  'data engineer',\n",
       "  'data analyst',\n",
       "  'data analytics engineer',\n",
       "  'software engineer',\n",
       "  'data engineer',\n",
       "  'data engineer ii',\n",
       "  'data scientist - nationwide opportunities',\n",
       "  'data engineer',\n",
       "  'entry level software engineer (no coding experience required)',\n",
       "  'battery data analytics engineer',\n",
       "  'data analyst',\n",
       "  'software engineer - remote',\n",
       "  'senior software engineer',\n",
       "  'data analyst',\n",
       "  'software engineer',\n",
       "  'big data engineer',\n",
       "  'data scientist',\n",
       "  'data analyst',\n",
       "  'data analyst',\n",
       "  'data scientist',\n",
       "  'software engineer',\n",
       "  'data scientist',\n",
       "  'data engineer (onsite)',\n",
       "  'data analyst ii- chicago, detroit, des moines, kansas city, st. louis, indianapolis, columbus, cincinnati, louisville',\n",
       "  'mea test engineer - relocation to phoenix',\n",
       "  'data scientist',\n",
       "  'etl/azure data engineer',\n",
       "  'data scientist',\n",
       "  'software engineer',\n",
       "  'quality data analyst',\n",
       "  'salesforce engineer',\n",
       "  'software engineer (ios)',\n",
       "  'software engineer - detroit',\n",
       "  'big data engineer',\n",
       "  'data analyst',\n",
       "  'data scientist',\n",
       "  'entry level software engineer',\n",
       "  'data scientist - smart mobility analytics',\n",
       "  'senior big data engineer',\n",
       "  'data scientist - operations analytics',\n",
       "  'systems and simulation data scientist',\n",
       "  'data analyst',\n",
       "  'bigdata engineer(data engineer)',\n",
       "  'data analyst',\n",
       "  'water engineer - data analyst',\n",
       "  'cengage',\n",
       "  'software engineer - fordlabs',\n",
       "  'data scientist - connected vehicle analytics',\n",
       "  'software engineer',\n",
       "  'ici data access software engineer',\n",
       "  'software engineer',\n",
       "  'business analyst data analyst',\n",
       "  'health care data analyst',\n",
       "  'software engineer (application development)',\n",
       "  'quicken loans',\n",
       "  'big data engineer',\n",
       "  'dte energy',\n",
       "  '4.0',\n",
       "  'big data engineer',\n",
       "  'data scientist',\n",
       "  'software engineer',\n",
       "  'principal software engineer',\n",
       "  'software engineer',\n",
       "  'software engineer (middleware / os / bsp)',\n",
       "  'software engineer',\n",
       "  'azure data engineer',\n",
       "  'senior ios software engineer - relocation to phoenix',\n",
       "  'senior software engineer (front end) - relocation to phoenix',\n",
       "  'activations engineer ii - digital services',\n",
       "  'software engineer',\n",
       "  'reporting analyst/data analyst',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'entry level software development engineer 2019 graduates',\n",
       "  'data engineer - nyc',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'software engineer, data',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data platform engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer (python)',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer - product',\n",
       "  'senior data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data scientist, ad-tech',\n",
       "  'data scientist',\n",
       "  'big data analyst / engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'software engineer, data',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'lead data engineer',\n",
       "  'junior software engineer, data / data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'junior machine learning engineer / data scientist',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data scientist',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data analyst',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'sirius xm',\n",
       "  'data analyst',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'software engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data platform engineer',\n",
       "  'data engineer',\n",
       "  'data scientist',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data scientist',\n",
       "  'data engineer trainee',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'software engineer',\n",
       "  'software data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data scientist',\n",
       "  'lead data engineer',\n",
       "  'data engineer',\n",
       "  'senior data engineer',\n",
       "  'success academy charter schools',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data scientist',\n",
       "  'data engineer',\n",
       "  'software engineer, atlas data lake',\n",
       "  'senior data engineer',\n",
       "  'senior data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'software engineer - data',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'azure database/big data devops engineer',\n",
       "  'senior data engineer',\n",
       "  'ios engineer',\n",
       "  'data analyst',\n",
       "  'software engineer',\n",
       "  'software engineer (security team)',\n",
       "  'data analyst',\n",
       "  'data scientist / ml engineer',\n",
       "  'computer vision application engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'cloud data engineer',\n",
       "  'roivant sciences',\n",
       "  'aws data engineer',\n",
       "  'data science internship',\n",
       "  'data scientist – strategic data science',\n",
       "  'associate data scientist snkrs',\n",
       "  'data scientist, customer operations',\n",
       "  'data scientist, analytics - instagram ml',\n",
       "  'entry level data scientist',\n",
       "  'data scientist, basketball integrity',\n",
       "  'data scientist - experimentation',\n",
       "  'data scientist',\n",
       "  'data scientist - journey analytics',\n",
       "  'data scientist / data analytics manager',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'associate data scientist - university grad',\n",
       "  'data scientist / analyst',\n",
       "  'data scientist',\n",
       "  'associate data scientist',\n",
       "  'data scientist- 62819br',\n",
       "  'internship in quantitative research (data science, machine learning, mathematical modelling)',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'senior data scientist, product analytics',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'machine learning engineer',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'audience data analyst',\n",
       "  'data scientist - natural language processing',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'bioinformatics analyst/data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist internship',\n",
       "  'scientist data analytics',\n",
       "  'data scientist',\n",
       "  'cimd - marcus by goldman sachs - decision and data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'healthcare data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data science/advanced analytics consultant',\n",
       "  'data analyst',\n",
       "  'data scientist, analytics & inference',\n",
       "  'senior data scientist',\n",
       "  'business intelligence analyst',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist -quality operations-corporate',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  \"moody's analytics\",\n",
       "  'data scientist',\n",
       "  'data scientist, digital machine learning',\n",
       "  'data scientist',\n",
       "  'data scientist - bomoda',\n",
       "  'senior data scientist',\n",
       "  'pdt partners',\n",
       "  'data scientist, engineer',\n",
       "  'data scientist',\n",
       "  'customer data scientist/sales engineer',\n",
       "  'data scientist, risk',\n",
       "  'data scientist',\n",
       "  'point72 data scientist',\n",
       "  'data scientist, analytics',\n",
       "  'data scientist',\n",
       "  'analyst, data science',\n",
       "  'data scientist, recommendation and ranking (ny)',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data analyst/data scientist (2020 start) - analytics - multioffice',\n",
       "  'two sigma',\n",
       "  'data scientist',\n",
       "  'data scientist (1801)',\n",
       "  'data scientist',\n",
       "  'data scientist (nlp)',\n",
       "  'quantitative research analyst',\n",
       "  'data scientist',\n",
       "  'data analyst',\n",
       "  'data scientist',\n",
       "  'data scientist, rpsg',\n",
       "  'data scientist',\n",
       "  'data scientist, computer vision (ny)',\n",
       "  'data scientist',\n",
       "  'senior data scientist',\n",
       "  'data scientist',\n",
       "  'machine learning engineer',\n",
       "  'senior data scientist',\n",
       "  'internship data scientist, product analytics',\n",
       "  '2020 data science intern',\n",
       "  'associate data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist (eft analytics)',\n",
       "  'data scientist - it transformation',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data analyst',\n",
       "  'data scientist- machine learning engineer',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'senior data scientist',\n",
       "  'data scientist (nlp)',\n",
       "  'data scientist',\n",
       "  'data scientist ii',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'senior scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data analyst',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist / statistician',\n",
       "  'marketing data scientist',\n",
       "  'data scientist - data engineer',\n",
       "  'data science and analytics',\n",
       "  'senior data scientist',\n",
       "  'data scientist - actimize division',\n",
       "  'business intelligence analyst',\n",
       "  'nyc data science academy',\n",
       "  'sr. business analyst, data science',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'sagence',\n",
       "  'data scientist',\n",
       "  'data support analyst',\n",
       "  'senior data scientist',\n",
       "  'senior data scientist',\n",
       "  'data scientist',\n",
       "  'revenue operations business intelligence analyst',\n",
       "  'machine learning research scientist',\n",
       "  'senior data scientist',\n",
       "  'machine learning engineer',\n",
       "  'ultrasound research scientist',\n",
       "  'verisk analytics',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'senior data scientist - threat intelligence researcher',\n",
       "  'marketing analytics manager',\n",
       "  'medical data scientist - clinical safety data review - oncology',\n",
       "  'summer internship 2020 - data scientist',\n",
       "  'data scientist',\n",
       "  'analytics manager',\n",
       "  'data scientist',\n",
       "  'senior data scientist',\n",
       "  'machine learning engineer',\n",
       "  'data scientist',\n",
       "  'data scientist',\n",
       "  'senior data scientist, identity and cross-device',\n",
       "  'senior data scientist',\n",
       "  'senior data scientist, measurement',\n",
       "  'senior data scientist - algorithms',\n",
       "  'nlp data scientist',\n",
       "  'director, data science',\n",
       "  'business intelligence analyst',\n",
       "  'sr. enterprise account exec- data science / ml - nyc',\n",
       "  'business intelligence analyst',\n",
       "  'staff data scientist - machine learning',\n",
       "  'data scientist, computer vision',\n",
       "  'data analyst',\n",
       "  'data analyst',\n",
       "  'data scientist - ml/ai',\n",
       "  'mid level data scientist',\n",
       "  'senior data scientist (payer)',\n",
       "  'machine learning engineer',\n",
       "  'data analyst',\n",
       "  'data scientist - 64411br',\n",
       "  'senior data scientist',\n",
       "  'data analyst - news partnerships, media partnerships',\n",
       "  'data analyst',\n",
       "  'senior data scientist',\n",
       "  'data analyst',\n",
       "  'firstsense applied data scientist',\n",
       "  'sr data engineer - product',\n",
       "  'applied scientist',\n",
       "  'senior data scientist - nationwide opportunities',\n",
       "  'data analyst',\n",
       "  'data science practice lead',\n",
       "  'machine learning engineer',\n",
       "  'data analyst',\n",
       "  'research scientist - machine learning',\n",
       "  'life data engineer (2020 permanent',\n",
       "  'quantitative research analyst',\n",
       "  'people analytics data scientist',\n",
       "  'data analyst',\n",
       "  'data scientist - 63289br',\n",
       "  'lead data scientist',\n",
       "  'ai research scientist - nlp',\n",
       "  'data intelligence analyst',\n",
       "  'quantitative analyst',\n",
       "  'data scientist - python, r, sql',\n",
       "  'data scientist - nlp',\n",
       "  'distinguished data scientist',\n",
       "  'data scientist with sql, python, r, tableau.',\n",
       "  'edl big data engineer - corporate- information technology (no agencies)',\n",
       "  'nlp data scientist',\n",
       "  'content, data scientist',\n",
       "  'senior data scientist',\n",
       "  'product developer and data scientist',\n",
       "  'data analyst',\n",
       "  'senior data scientist',\n",
       "  'data engineer',\n",
       "  'data analyst',\n",
       "  'data analyst',\n",
       "  'senior data scientist',\n",
       "  'ccb - quantitative modeler / data scientist - associate',\n",
       "  'people research scientist, people analytics',\n",
       "  'data scientist with sql, python, r, tableau',\n",
       "  'senior data scientist',\n",
       "  'business intelligence analyst',\n",
       "  'data scientist ii',\n",
       "  'rmbs data scientist',\n",
       "  'senior data scientist - natural language processing (nlp)',\n",
       "  'senior data scientist',\n",
       "  'field data scientist',\n",
       "  'data analyst',\n",
       "  'senior data scientist',\n",
       "  'applied scientist - amazon ai',\n",
       "  'principal data scientist',\n",
       "  'data analyst',\n",
       "  'data scientist (c13/vp) new york, ny',\n",
       "  'data analyst ii (sql, tableau)',\n",
       "  'r&d filter design scientist',\n",
       "  'data scientist, principal',\n",
       "  'product developer and data scientist',\n",
       "  'data analyst',\n",
       "  'sr. associate, data scientist, natural language processing',\n",
       "  'data scientist principal',\n",
       "  'research scientist - robotics',\n",
       "  'ccb - quantitative modeler / data scientist - associate',\n",
       "  'senior data scientist',\n",
       "  'machine learning engineer',\n",
       "  'data scientist - data engineer',\n",
       "  'machine learning engineer',\n",
       "  'crm analytics manager',\n",
       "  'ai data scientist / data engineer - experienced associate',\n",
       "  'data analyst / developer',\n",
       "  'senior data scientist',\n",
       "  'lead decision scientist',\n",
       "  'data analyst',\n",
       "  'senior data scientist',\n",
       "  'senior data scientist - risk mitigation',\n",
       "  'medical data scientist',\n",
       "  'data scientist (advanced analytics)',\n",
       "  'manager, data sciences',\n",
       "  'senior data analyst',\n",
       "  'senior data scientist',\n",
       "  'business intelligence analyst',\n",
       "  'sr. scientist i',\n",
       "  'staff machine learning engineer',\n",
       "  'data engineer',\n",
       "  'marketing data analyst',\n",
       "  'marketing data analyst- contract role',\n",
       "  '2020 phd university graduate - data scientist - uber eats (new york)',\n",
       "  'data platform engineer',\n",
       "  'senior data scientist',\n",
       "  'data science manager - instagram shopping',\n",
       "  'business data analyst',\n",
       "  'data scientist (nlp), decision analytics',\n",
       "  'senior data scientist',\n",
       "  'principal associate, data scientist',\n",
       "  'manager, data science',\n",
       "  'nyc data science academy',\n",
       "  'lead data scientist, two sigma private investments',\n",
       "  'data science architect - data manufacturing',\n",
       "  'data modeler/analyst',\n",
       "  'senior data scientist',\n",
       "  'sr. product manager, product data and analytics',\n",
       "  'data analyst 4',\n",
       "  'senior data scientist',\n",
       "  'microsoft advanced analytics consultant',\n",
       "  'sr. data scientist',\n",
       "  'senior product data analyst - adtech',\n",
       "  'senior data scientist',\n",
       "  'senior data scientist',\n",
       "  'data engineer',\n",
       "  'memorial sloan-kettering',\n",
       "  'lead data scientist',\n",
       "  'principal data scientist',\n",
       "  'field applications scientist',\n",
       "  'java/data engineer',\n",
       "  'sr data scientist (us citizen/ greencard)',\n",
       "  'research scientist (computer vision)',\n",
       "  'data engineer',\n",
       "  'staff data engineer',\n",
       "  'data science engineer',\n",
       "  'data support engineer',\n",
       "  'big data engineer',\n",
       "  'data platform engineer',\n",
       "  'data analyst',\n",
       "  'data analyst',\n",
       "  'data engineer',\n",
       "  'entry level data scientist',\n",
       "  'data engineer',\n",
       "  'senior data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data science engineer',\n",
       "  'senior data engineer',\n",
       "  'data intelligence engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data engineer',\n",
       "  'data support engineer',\n",
       "  'data engineer',\n",
       "  'data engineer'],\n",
       " 'company': ['Seniorlink',\n",
       "  'Blue Cross and Blue Shield of Massachusetts',\n",
       "  'COTA',\n",
       "  'Indigo',\n",
       "  'CarGurus',\n",
       "  'Digilant',\n",
       "  '3Pillar Global',\n",
       "  'Berg Health',\n",
       "  'Mondo',\n",
       "  'EF Educational Tours',\n",
       "  'Cynet Systems',\n",
       "  '4.5',\n",
       "  'DraftKings',\n",
       "  'Quantexa',\n",
       "  'MassMutual',\n",
       "  'Hometap',\n",
       "  'Liberty Mutual Insurance',\n",
       "  'Zylotech',\n",
       "  'Cervello Inc',\n",
       "  'Jebbit, Inc.',\n",
       "  'CarGurus',\n",
       "  'The Phia Group',\n",
       "  'L.E.K. Consulting',\n",
       "  'Kensho',\n",
       "  'Buoy Health',\n",
       "  'Robin Powered',\n",
       "  '4.0',\n",
       "  'Recorded Future',\n",
       "  'Capital One',\n",
       "  'EF Educational Tours',\n",
       "  'Bose',\n",
       "  'PatientsLikeMe',\n",
       "  'MassMutual',\n",
       "  'Yesware, Inc.',\n",
       "  'MIT',\n",
       "  'Harbor Health Services',\n",
       "  'Vertex Pharmaceuticals',\n",
       "  'Amazon',\n",
       "  'Vectra',\n",
       "  'Blue Cross and Blue Shield of Massachusetts',\n",
       "  'Insulet Corporation',\n",
       "  'Smart Source Technologies',\n",
       "  'MassMutual',\n",
       "  'Cognex',\n",
       "  'Plymouth Rock Assurance',\n",
       "  'Equian LLC',\n",
       "  '4.0',\n",
       "  'Liberty Mutual Insurance',\n",
       "  'Kensho',\n",
       "  'Homesite Group',\n",
       "  'Klaviyo',\n",
       "  'Mindteck',\n",
       "  'Systems & Technology Research',\n",
       "  'Humana',\n",
       "  'Indigo',\n",
       "  'Virgin Pulse',\n",
       "  'StockX',\n",
       "  'Quicken Loans',\n",
       "  'StockX',\n",
       "  'Quicken Loans',\n",
       "  'Stefanini',\n",
       "  'Relapath',\n",
       "  'SunSoft Technologies Inc.',\n",
       "  'Quicken Loans',\n",
       "  'OtterBase',\n",
       "  'Verizon',\n",
       "  'Comau LLC',\n",
       "  'Altimetrik Corp',\n",
       "  'Rocket Homes',\n",
       "  'StockX',\n",
       "  'OneMagnify',\n",
       "  'Quantum Technologies Inc.',\n",
       "  'Ciber Global LLC',\n",
       "  'Mitsubishi Electric',\n",
       "  'Plante Moran',\n",
       "  'Amazon',\n",
       "  'Amazon',\n",
       "  'Global Information Technology',\n",
       "  'Revature',\n",
       "  'General Motors',\n",
       "  'WalkerHealthcareIT',\n",
       "  'DIVERSANT, LLC',\n",
       "  'StockX',\n",
       "  'Wayne State',\n",
       "  'Stefanini',\n",
       "  'iTech US, Inc.',\n",
       "  'RGBSI',\n",
       "  'Collabera',\n",
       "  'GoAhead Solutions',\n",
       "  'Ford Motor Company',\n",
       "  'Rocket Homes',\n",
       "  'Meridian',\n",
       "  'Detroit Labs',\n",
       "  'CoventBridge Group',\n",
       "  'Nikola',\n",
       "  'GM Financial',\n",
       "  'Sara Software Systems',\n",
       "  'Meridian',\n",
       "  'Ford Motor Company',\n",
       "  'GoAhead Solutions',\n",
       "  'StockX',\n",
       "  'StockX',\n",
       "  'Connected',\n",
       "  'HCL Global Systems',\n",
       "  'Quicken Loans',\n",
       "  'Trinity Health',\n",
       "  'Revature',\n",
       "  'Ford Motor Company',\n",
       "  'Rocket Homes',\n",
       "  'Ford Motor Company',\n",
       "  'Ford Motor Company US',\n",
       "  'ePromptus Inc.',\n",
       "  'Urpan Technologies, Inc.',\n",
       "  'Collaborate Solutions, Inc.',\n",
       "  'DLZ Corporation',\n",
       "  '4.0',\n",
       "  'Ford Motor Company',\n",
       "  'Ford Motor Company',\n",
       "  'TPS, Incorporated',\n",
       "  'Ford Motor Company',\n",
       "  'Winkler Web Designs',\n",
       "  'ektello',\n",
       "  'BSC Solutions',\n",
       "  'Ford Motor Company',\n",
       "  '3.8',\n",
       "  'Unified Tech Group Inc',\n",
       "  '4.0',\n",
       "  'Lantana Consulting Group Logo',\n",
       "  'Diverse Lynx',\n",
       "  'Synergy Solutions',\n",
       "  'Amtec Inc.',\n",
       "  'Quicken Loans',\n",
       "  'Randstad US',\n",
       "  'Ford Motor Company',\n",
       "  'Collabera',\n",
       "  'Accenture',\n",
       "  'Nikola',\n",
       "  'Nikola',\n",
       "  'BullsEye Telecom',\n",
       "  'EPITEC',\n",
       "  'blueStone Staffing Solutions',\n",
       "  'GameChanger',\n",
       "  'Better.com',\n",
       "  'Audible',\n",
       "  'Looker',\n",
       "  'Rocketrip',\n",
       "  'MongoDB',\n",
       "  'FanDuel',\n",
       "  'Lancer Insurance',\n",
       "  'Paperless Post',\n",
       "  'Domio',\n",
       "  'Sagence',\n",
       "  'Medidata Solutions',\n",
       "  'Foursquare',\n",
       "  'Crux Informatics',\n",
       "  'Rent the Runway',\n",
       "  'TWDC',\n",
       "  'thoughtbot',\n",
       "  'IntraEdge',\n",
       "  'Conductor',\n",
       "  'Sony Music Entertainment',\n",
       "  'EPIX',\n",
       "  'Remedy BPCI Partners, LLC.',\n",
       "  'Showtime Networks',\n",
       "  'ThoughtWorks',\n",
       "  'Microsoft',\n",
       "  'Slalom LLC.',\n",
       "  'Slice (NY)',\n",
       "  'Entera',\n",
       "  'Warby Parker',\n",
       "  'Dailymotion',\n",
       "  'Hinge',\n",
       "  'Affinity Solutions',\n",
       "  'Newsela',\n",
       "  'Dashlane',\n",
       "  'Cherre',\n",
       "  'Caserta',\n",
       "  'Sapphire Digital',\n",
       "  'NJF Global Holdings',\n",
       "  'Beeswax',\n",
       "  'CB Insights',\n",
       "  'Quantexa',\n",
       "  'Knotel',\n",
       "  'Haas and Riley',\n",
       "  'ThoughtWorks',\n",
       "  'Fareportal Inc.',\n",
       "  'Noom US',\n",
       "  'Mondo',\n",
       "  'Fareportal Inc.',\n",
       "  'Guardian Life',\n",
       "  'InfoObjects Inc',\n",
       "  'Digilant',\n",
       "  'Birchbox',\n",
       "  'D.A.S.',\n",
       "  'Better.com',\n",
       "  'Group One Trading',\n",
       "  'Caresoft',\n",
       "  'SeatGeek',\n",
       "  'pymetrics',\n",
       "  'Course5 Intelligence',\n",
       "  'Getty Images',\n",
       "  'Button',\n",
       "  'Kinetix Trading Solutions Inc',\n",
       "  'Reonomy',\n",
       "  'Amazon',\n",
       "  'Capital One',\n",
       "  'Parsley Health',\n",
       "  'Capgemini',\n",
       "  '3.0',\n",
       "  'Reonomy',\n",
       "  'YieldStreet',\n",
       "  'Memorial Sloan-Kettering',\n",
       "  'Reaktor',\n",
       "  'Teampay',\n",
       "  'UncommonGoods',\n",
       "  'HelloFresh',\n",
       "  'Hospital for Special Surgery',\n",
       "  'White Ops',\n",
       "  'ShopKeep',\n",
       "  'Major League Baseball',\n",
       "  'Oloop Technology Solutions',\n",
       "  'Vroom',\n",
       "  'Ippon Technologies',\n",
       "  'Willis Towers Watson',\n",
       "  'Harrington Starr',\n",
       "  'C3.ai',\n",
       "  'GNY Insurance Companies',\n",
       "  'Healthfirst',\n",
       "  'Vettery',\n",
       "  'Accenture',\n",
       "  'Scalable Systems',\n",
       "  'Techlink, Inc.',\n",
       "  'LTI',\n",
       "  'ApTask',\n",
       "  'Feedzai',\n",
       "  '1010data',\n",
       "  'X4 Tech',\n",
       "  'CipherHealth',\n",
       "  'Fahrenheit IT',\n",
       "  'Butterfly Network',\n",
       "  'Tradeweb Markets LLC',\n",
       "  'Looker',\n",
       "  'Infinity Consulting Solutions',\n",
       "  'Urbint',\n",
       "  '3.4',\n",
       "  'Blueocean Market Intelligence',\n",
       "  'Qstride',\n",
       "  'Building Service 32BJ Benefit Funds',\n",
       "  'DemystData',\n",
       "  'Smartling',\n",
       "  'Group Nine Media',\n",
       "  'MongoDB',\n",
       "  'Warby Parker',\n",
       "  'Octane Lending',\n",
       "  'eclaro',\n",
       "  'TripleLift',\n",
       "  'IANULY Talent Accelerators',\n",
       "  'Ordergroove',\n",
       "  'Village Care of New York',\n",
       "  'CyberCoders',\n",
       "  'TD Bank',\n",
       "  'Andiamo Partners',\n",
       "  'JOOR',\n",
       "  'Zeta Global',\n",
       "  'Yext',\n",
       "  'G-Research',\n",
       "  'Crux Informatics',\n",
       "  'PA Consulting',\n",
       "  'AnyVision',\n",
       "  'ERGO Interactive',\n",
       "  'Software Guidance & Assistance',\n",
       "  'USEReady',\n",
       "  '3.1',\n",
       "  'Reliable Software Resources',\n",
       "  'G-Research',\n",
       "  'Two Sigma',\n",
       "  'Nike',\n",
       "  'Squarespace',\n",
       "  'Facebook',\n",
       "  'IBM',\n",
       "  'National Basketball Association',\n",
       "  'Walt Disney Company',\n",
       "  'Quartet Health',\n",
       "  'McKinsey & Company',\n",
       "  'Zola',\n",
       "  'Telaria',\n",
       "  'SmartThings',\n",
       "  'BlockFi',\n",
       "  'Amazon',\n",
       "  'Prognos',\n",
       "  'Cadre',\n",
       "  'Gallup',\n",
       "  'Squarespace',\n",
       "  'Workbridge Associates',\n",
       "  'viagogo',\n",
       "  'Simon Data',\n",
       "  'Aetna',\n",
       "  'G-Research',\n",
       "  'HOMER',\n",
       "  'Qualia Investments',\n",
       "  'Slice (NY)',\n",
       "  'Skillshare',\n",
       "  'Themesoft Inc',\n",
       "  'Crossix Solutions',\n",
       "  'Samsung NEXT',\n",
       "  'NS1',\n",
       "  'Pivotal Software',\n",
       "  'TED Conferences LLC',\n",
       "  \"The Farmer's Dog\",\n",
       "  'Rockstar Games',\n",
       "  'Bowery Farming',\n",
       "  'Systech Corp, Inc.',\n",
       "  'IBM',\n",
       "  'Attune',\n",
       "  'Booz Allen Hamilton Inc.',\n",
       "  'Shareablee',\n",
       "  'Gro Intelligence',\n",
       "  'Weill Cornell Medical College',\n",
       "  'Vroom',\n",
       "  'S&P Global',\n",
       "  'Two Sigma',\n",
       "  'CompIQ',\n",
       "  'Simon Data',\n",
       "  'Blue Apron',\n",
       "  'Kinship',\n",
       "  'FactSet',\n",
       "  'TEEMA',\n",
       "  'Bank of America',\n",
       "  'Goldman Sachs',\n",
       "  'Point72',\n",
       "  'Via',\n",
       "  'Milliman',\n",
       "  'APN Consulting Inc',\n",
       "  'Revlon',\n",
       "  'Slalom LLC.',\n",
       "  'Blink Health',\n",
       "  'Codecademy',\n",
       "  'Hopper',\n",
       "  'Industrious',\n",
       "  'SoftVision',\n",
       "  'EDO, Inc.',\n",
       "  'Mount Sinai Health System',\n",
       "  'Starz Entertainment',\n",
       "  'United Technologies',\n",
       "  '3.4',\n",
       "  'Butterfly Network',\n",
       "  'Capital One',\n",
       "  'Oracle',\n",
       "  'Weber Shandwick',\n",
       "  'Feather',\n",
       "  '4.0',\n",
       "  'Transit Wireless (USD)',\n",
       "  'Prudential',\n",
       "  'h2o.ai',\n",
       "  'Square',\n",
       "  'Vettery',\n",
       "  'Point72',\n",
       "  'TripleLift',\n",
       "  'Deloitte',\n",
       "  'J. Crew Group, Inc.',\n",
       "  'Etsy',\n",
       "  'NYU',\n",
       "  'Audible',\n",
       "  'TechProjects',\n",
       "  'SparkBeyond',\n",
       "  'Burtch Works',\n",
       "  'Charles River Associates',\n",
       "  '4.3',\n",
       "  'InvenTech Info',\n",
       "  'IBKR',\n",
       "  'Rent the Runway',\n",
       "  'Prognos',\n",
       "  'Tower Research Capital',\n",
       "  'Aetna',\n",
       "  'Doximity',\n",
       "  'TripleLift',\n",
       "  'New York City Department of Education .',\n",
       "  'Aramark',\n",
       "  'Etsy',\n",
       "  'Averity',\n",
       "  'Urbint',\n",
       "  'Albert Einstein College of Medicine',\n",
       "  'PulsePoint',\n",
       "  'SeatGeek',\n",
       "  'Criteo',\n",
       "  'Guardian Life',\n",
       "  'Macmillan Publishers',\n",
       "  'Aegis Media',\n",
       "  'Koch Industries',\n",
       "  'New York-Presbyterian Hospital',\n",
       "  'Eyeview',\n",
       "  'BNY Mellon',\n",
       "  'Bazaarvoice',\n",
       "  'pymetrics',\n",
       "  'Harnham',\n",
       "  'Unilever',\n",
       "  'Tephra Inc.',\n",
       "  'Priceline.com',\n",
       "  'Apidel Technologies',\n",
       "  'Bayer',\n",
       "  'OpenX',\n",
       "  'Dow Jones',\n",
       "  'Axelon, Inc.',\n",
       "  'Schrödinger',\n",
       "  'Princeton IT Services',\n",
       "  \"Moody's\",\n",
       "  'Point72',\n",
       "  'Remedy BPCI Partners, LLC.',\n",
       "  'Momentum Resource Solutions',\n",
       "  'NJF Global Holdings',\n",
       "  'SmartAsset',\n",
       "  'Simon & Schuster',\n",
       "  'Two95 International Inc.',\n",
       "  'Tapad',\n",
       "  'NICE Actimize',\n",
       "  'CBS Interactive',\n",
       "  '4.9',\n",
       "  'Medidata Solutions',\n",
       "  'S&P GLOBAL MARKET INTELLIGENCE',\n",
       "  '7Park Data',\n",
       "  'INgrooves Music Group',\n",
       "  'National Debt Relief',\n",
       "  '4.6',\n",
       "  'RiVi Consulting Group',\n",
       "  'Point72',\n",
       "  'Strategic Financial Solutions',\n",
       "  '605',\n",
       "  'Ztek Consulting',\n",
       "  'OnDeck',\n",
       "  'Riverside Research Institute',\n",
       "  'Synechron',\n",
       "  'S&P GLOBAL MARKET INTELLIGENCE',\n",
       "  'Riverside Research Institute',\n",
       "  '3.4',\n",
       "  'HR Acuity',\n",
       "  'DISH',\n",
       "  'Verizon',\n",
       "  'Chobani',\n",
       "  'Eisai Inc.',\n",
       "  'Tapad',\n",
       "  'ADP',\n",
       "  'Qapital',\n",
       "  'CyberCoders',\n",
       "  'Warby Parker',\n",
       "  'Information Builders',\n",
       "  'People Make Us',\n",
       "  'Momentum Solar',\n",
       "  'The Trade Desk',\n",
       "  'S&P GLOBAL MARKET INTELLIGENCE',\n",
       "  'The Trade Desk',\n",
       "  'Quartet Health',\n",
       "  'S&P Global Ratings',\n",
       "  'Transfix',\n",
       "  'Fareportal Inc.',\n",
       "  'h2o.ai',\n",
       "  'SecurityScorecard',\n",
       "  'The Walmart eCommerce Family of Brands',\n",
       "  'Etsy',\n",
       "  'Bounce Exchange',\n",
       "  'Tachyon Technologies',\n",
       "  'Getty Images',\n",
       "  'Jobspring Partners',\n",
       "  'Prognos',\n",
       "  'American Express',\n",
       "  'NYU Langone Health',\n",
       "  'Aetna',\n",
       "  'sparks & honey',\n",
       "  'Facebook',\n",
       "  'Forrest Solutions',\n",
       "  'Arena',\n",
       "  'Outcome Health',\n",
       "  'First Data Corporation',\n",
       "  'Showtime Networks',\n",
       "  'Amazon',\n",
       "  'Amazon',\n",
       "  'EDO, Inc.',\n",
       "  'Spotify',\n",
       "  'Amazon',\n",
       "  'Nomad Health',\n",
       "  'Samsung Research America',\n",
       "  'Munich Re',\n",
       "  'New York Life',\n",
       "  'Q Systems',\n",
       "  'Global Employment Solutions',\n",
       "  'Aetna',\n",
       "  'Enigma',\n",
       "  'Bloomberg',\n",
       "  'Medidata Solutions',\n",
       "  \"Moody's\",\n",
       "  'Jobot',\n",
       "  'Harnham',\n",
       "  'Verizon',\n",
       "  'TECH Projects',\n",
       "  'Jefferies',\n",
       "  'S&P Global',\n",
       "  'ION Media Networks',\n",
       "  'Entera',\n",
       "  'Citibank',\n",
       "  'Accountemps',\n",
       "  'Aetna',\n",
       "  'Slalom LLC.',\n",
       "  'Fora Financial',\n",
       "  'ApTask',\n",
       "  'neo4j',\n",
       "  'J.P.Morgan',\n",
       "  'Facebook',\n",
       "  'TechProjects',\n",
       "  'Sartorius',\n",
       "  'Braze',\n",
       "  'TripleLift',\n",
       "  'London Stock Exchange Group',\n",
       "  'Lynk',\n",
       "  'Gallup',\n",
       "  'ADARA',\n",
       "  'APN Consulting',\n",
       "  'D.A.S.',\n",
       "  'Amazon',\n",
       "  'Verizon',\n",
       "  'RiceFW Technologies Inc',\n",
       "  'Citi',\n",
       "  'Conduent',\n",
       "  'Pall',\n",
       "  'BNY Mellon',\n",
       "  'Citi',\n",
       "  'Advantage Technical Resources',\n",
       "  'KPMG',\n",
       "  'Via',\n",
       "  'Samsung Research America',\n",
       "  'Chase',\n",
       "  'Socure',\n",
       "  'Turner Broadcasting',\n",
       "  'CBS Corporation',\n",
       "  'S&P Global',\n",
       "  'HelloFresh',\n",
       "  'PwC',\n",
       "  'Bank of America',\n",
       "  'Equinox Fitness',\n",
       "  'PayPal',\n",
       "  'MassMutual',\n",
       "  'Digitas',\n",
       "  'Fidelity Investments',\n",
       "  'PSG Global Solutions',\n",
       "  'Genpact',\n",
       "  'Wavemaker',\n",
       "  'Yext',\n",
       "  'S&P Global',\n",
       "  'Shutterstock',\n",
       "  'Takasago International Corp',\n",
       "  'Compass',\n",
       "  'Hospital for Special Surgery',\n",
       "  'Netamorphosis',\n",
       "  'Harnham',\n",
       "  'Uber',\n",
       "  'Quartet Health',\n",
       "  'BNY Mellon',\n",
       "  'Facebook',\n",
       "  'Fareportal Inc.',\n",
       "  'Exl Service',\n",
       "  'Vettery',\n",
       "  'Capital One',\n",
       "  'Blink Health',\n",
       "  '4.9',\n",
       "  'Two Sigma',\n",
       "  'Bloomberg',\n",
       "  'Reliable Software Resources',\n",
       "  'Komodo Health',\n",
       "  'Showtime Networks',\n",
       "  'Softpath System LLC',\n",
       "  'X4 Tech',\n",
       "  'IBM',\n",
       "  'Octane Lending',\n",
       "  'Dailymotion',\n",
       "  'Harnham',\n",
       "  'Prudential',\n",
       "  'Group One Trading',\n",
       "  '3.9',\n",
       "  'Harnham',\n",
       "  'Jobot',\n",
       "  'GenapSys, Inc.',\n",
       "  'Memorial Sloan-Kettering',\n",
       "  'Delphi-US',\n",
       "  'Dataminr',\n",
       "  'PIMCO',\n",
       "  'SailPoint Technologies',\n",
       "  'Opcity',\n",
       "  'Opcity',\n",
       "  'SpringML',\n",
       "  'Sysco Labs',\n",
       "  'PIMCO',\n",
       "  'Raybeam',\n",
       "  'Cerity',\n",
       "  'IBM',\n",
       "  'Pluralsight',\n",
       "  'RetailMeNot',\n",
       "  'Jask',\n",
       "  'Saatva',\n",
       "  'Atlassian',\n",
       "  'Square Root',\n",
       "  'H E B',\n",
       "  'Apple',\n",
       "  'PIMCO',\n",
       "  'Dell',\n",
       "  'Persistent Systems Inc',\n",
       "  'Bright Health',\n",
       "  'Dosh',\n",
       "  'Crowdskout',\n",
       "  'Rev.com',\n",
       "  'Compunnel Inc',\n",
       "  'Treehouse Technology Group, LLC',\n",
       "  'Apple',\n",
       "  'Keller Williams'],\n",
       " 'job_city': ['Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  '',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  '',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  '',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  '',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin'],\n",
       " 'job_state_code': ['MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  '',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'Des Moines',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  '',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  '',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  '',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX'],\n",
       " 'sal_low': [64,\n",
       "  83,\n",
       "  86,\n",
       "  100,\n",
       "  61,\n",
       "  90,\n",
       "  79,\n",
       "  82,\n",
       "  nan,\n",
       "  62,\n",
       "  93,\n",
       "  89,\n",
       "  74,\n",
       "  nan,\n",
       "  82,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  85,\n",
       "  nan,\n",
       "  53,\n",
       "  45,\n",
       "  126,\n",
       "  103,\n",
       "  nan,\n",
       "  nan,\n",
       "  80,\n",
       "  nan,\n",
       "  90,\n",
       "  62,\n",
       "  112,\n",
       "  83,\n",
       "  116,\n",
       "  80,\n",
       "  84,\n",
       "  44,\n",
       "  91,\n",
       "  90,\n",
       "  87,\n",
       "  108,\n",
       "  69,\n",
       "  61,\n",
       "  103,\n",
       "  64,\n",
       "  82,\n",
       "  61,\n",
       "  69,\n",
       "  70,\n",
       "  nan,\n",
       "  nan,\n",
       "  79,\n",
       "  67,\n",
       "  88,\n",
       "  81,\n",
       "  108,\n",
       "  87,\n",
       "  nan,\n",
       "  88,\n",
       "  nan,\n",
       "  72,\n",
       "  72,\n",
       "  nan,\n",
       "  81,\n",
       "  41,\n",
       "  74,\n",
       "  nan,\n",
       "  65,\n",
       "  80,\n",
       "  74,\n",
       "  70,\n",
       "  nan,\n",
       "  nan,\n",
       "  68,\n",
       "  59,\n",
       "  83,\n",
       "  87,\n",
       "  106,\n",
       "  60,\n",
       "  35,\n",
       "  60,\n",
       "  nan,\n",
       "  91,\n",
       "  90,\n",
       "  35,\n",
       "  49,\n",
       "  68,\n",
       "  86,\n",
       "  nan,\n",
       "  nan,\n",
       "  84,\n",
       "  71,\n",
       "  59,\n",
       "  70,\n",
       "  38,\n",
       "  nan,\n",
       "  75,\n",
       "  nan,\n",
       "  80,\n",
       "  62,\n",
       "  nan,\n",
       "  51,\n",
       "  70,\n",
       "  nan,\n",
       "  77,\n",
       "  48,\n",
       "  91,\n",
       "  35,\n",
       "  76,\n",
       "  76,\n",
       "  84,\n",
       "  84,\n",
       "  42,\n",
       "  nan,\n",
       "  48,\n",
       "  40,\n",
       "  74,\n",
       "  68,\n",
       "  84,\n",
       "  53,\n",
       "  77,\n",
       "  nan,\n",
       "  nan,\n",
       "  65,\n",
       "  71,\n",
       "  103,\n",
       "  nan,\n",
       "  75,\n",
       "  nan,\n",
       "  82,\n",
       "  66,\n",
       "  50,\n",
       "  93,\n",
       "  nan,\n",
       "  69,\n",
       "  nan,\n",
       "  67,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  67,\n",
       "  nan,\n",
       "  117,\n",
       "  127,\n",
       "  100,\n",
       "  115,\n",
       "  nan,\n",
       "  123,\n",
       "  105,\n",
       "  75,\n",
       "  99,\n",
       "  nan,\n",
       "  80,\n",
       "  112,\n",
       "  108,\n",
       "  nan,\n",
       "  65,\n",
       "  96,\n",
       "  148,\n",
       "  100,\n",
       "  90,\n",
       "  nan,\n",
       "  114,\n",
       "  82,\n",
       "  94,\n",
       "  125,\n",
       "  109,\n",
       "  85,\n",
       "  nan,\n",
       "  87,\n",
       "  77,\n",
       "  97,\n",
       "  88,\n",
       "  43,\n",
       "  134,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  75,\n",
       "  nan,\n",
       "  nan,\n",
       "  98,\n",
       "  nan,\n",
       "  88,\n",
       "  nan,\n",
       "  131,\n",
       "  71,\n",
       "  100,\n",
       "  nan,\n",
       "  67,\n",
       "  91,\n",
       "  59,\n",
       "  102,\n",
       "  81,\n",
       "  nan,\n",
       "  116,\n",
       "  83,\n",
       "  80,\n",
       "  60,\n",
       "  108,\n",
       "  nan,\n",
       "  98,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  92,\n",
       "  102,\n",
       "  nan,\n",
       "  86,\n",
       "  76,\n",
       "  nan,\n",
       "  nan,\n",
       "  85,\n",
       "  104,\n",
       "  nan,\n",
       "  95,\n",
       "  69,\n",
       "  82,\n",
       "  nan,\n",
       "  77,\n",
       "  95,\n",
       "  nan,\n",
       "  108,\n",
       "  72,\n",
       "  87,\n",
       "  nan,\n",
       "  103,\n",
       "  80,\n",
       "  73,\n",
       "  80,\n",
       "  68,\n",
       "  127,\n",
       "  108,\n",
       "  82,\n",
       "  108,\n",
       "  99,\n",
       "  102,\n",
       "  nan,\n",
       "  90,\n",
       "  nan,\n",
       "  nan,\n",
       "  93,\n",
       "  143,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  92,\n",
       "  nan,\n",
       "  nan,\n",
       "  113,\n",
       "  78,\n",
       "  91,\n",
       "  97,\n",
       "  99,\n",
       "  nan,\n",
       "  nan,\n",
       "  85,\n",
       "  nan,\n",
       "  60,\n",
       "  74,\n",
       "  nan,\n",
       "  87,\n",
       "  nan,\n",
       "  66,\n",
       "  44,\n",
       "  101,\n",
       "  nan,\n",
       "  nan,\n",
       "  113,\n",
       "  nan,\n",
       "  106,\n",
       "  nan,\n",
       "  88,\n",
       "  nan,\n",
       "  81,\n",
       "  nan,\n",
       "  126,\n",
       "  90,\n",
       "  92,\n",
       "  111,\n",
       "  100,\n",
       "  84,\n",
       "  95,\n",
       "  120,\n",
       "  97,\n",
       "  nan,\n",
       "  88,\n",
       "  104,\n",
       "  nan,\n",
       "  112,\n",
       "  97,\n",
       "  nan,\n",
       "  106,\n",
       "  86,\n",
       "  nan,\n",
       "  84,\n",
       "  72,\n",
       "  83,\n",
       "  nan,\n",
       "  nan,\n",
       "  100,\n",
       "  nan,\n",
       "  129,\n",
       "  101,\n",
       "  103,\n",
       "  nan,\n",
       "  nan,\n",
       "  101,\n",
       "  59,\n",
       "  nan,\n",
       "  92,\n",
       "  nan,\n",
       "  103,\n",
       "  92,\n",
       "  nan,\n",
       "  82,\n",
       "  97,\n",
       "  nan,\n",
       "  73,\n",
       "  113,\n",
       "  86,\n",
       "  114,\n",
       "  nan,\n",
       "  102,\n",
       "  79,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  105,\n",
       "  113,\n",
       "  99,\n",
       "  71,\n",
       "  97,\n",
       "  89,\n",
       "  81,\n",
       "  98,\n",
       "  53,\n",
       "  94,\n",
       "  115,\n",
       "  84,\n",
       "  96,\n",
       "  nan,\n",
       "  84,\n",
       "  118,\n",
       "  92,\n",
       "  102,\n",
       "  nan,\n",
       "  91,\n",
       "  87,\n",
       "  nan,\n",
       "  nan,\n",
       "  151,\n",
       "  80,\n",
       "  79,\n",
       "  110,\n",
       "  89,\n",
       "  72,\n",
       "  103,\n",
       "  88,\n",
       "  106,\n",
       "  nan,\n",
       "  100,\n",
       "  81,\n",
       "  103,\n",
       "  106,\n",
       "  nan,\n",
       "  nan,\n",
       "  53,\n",
       "  nan,\n",
       "  nan,\n",
       "  86,\n",
       "  102,\n",
       "  101,\n",
       "  nan,\n",
       "  83,\n",
       "  nan,\n",
       "  78,\n",
       "  76,\n",
       "  73,\n",
       "  100,\n",
       "  nan,\n",
       "  nan,\n",
       "  70,\n",
       "  nan,\n",
       "  143,\n",
       "  nan,\n",
       "  nan,\n",
       "  65,\n",
       "  67,\n",
       "  99,\n",
       "  89,\n",
       "  139,\n",
       "  74,\n",
       "  64,\n",
       "  98,\n",
       "  nan,\n",
       "  84,\n",
       "  nan,\n",
       "  125,\n",
       "  101,\n",
       "  104,\n",
       "  82,\n",
       "  88,\n",
       "  nan,\n",
       "  93,\n",
       "  90,\n",
       "  77,\n",
       "  73,\n",
       "  85,\n",
       "  95,\n",
       "  nan,\n",
       "  95,\n",
       "  72,\n",
       "  nan,\n",
       "  149,\n",
       "  94,\n",
       "  51,\n",
       "  76,\n",
       "  97,\n",
       "  82,\n",
       "  nan,\n",
       "  74,\n",
       "  49,\n",
       "  90,\n",
       "  97,\n",
       "  67,\n",
       "  104,\n",
       "  nan,\n",
       "  nan,\n",
       "  70,\n",
       "  77,\n",
       "  90,\n",
       "  nan,\n",
       "  73,\n",
       "  124,\n",
       "  nan,\n",
       "  68,\n",
       "  144,\n",
       "  87,\n",
       "  96,\n",
       "  nan,\n",
       "  106,\n",
       "  nan,\n",
       "  nan,\n",
       "  101,\n",
       "  nan,\n",
       "  nan,\n",
       "  60,\n",
       "  114,\n",
       "  106,\n",
       "  114,\n",
       "  150,\n",
       "  99,\n",
       "  86,\n",
       "  55,\n",
       "  nan,\n",
       "  80,\n",
       "  131,\n",
       "  100,\n",
       "  52,\n",
       "  49,\n",
       "  93,\n",
       "  nan,\n",
       "  124,\n",
       "  nan,\n",
       "  51,\n",
       "  83,\n",
       "  nan,\n",
       "  73,\n",
       "  nan,\n",
       "  nan,\n",
       "  57,\n",
       "  73,\n",
       "  113,\n",
       "  nan,\n",
       "  125,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  131,\n",
       "  104,\n",
       "  87,\n",
       "  nan,\n",
       "  nan,\n",
       "  83,\n",
       "  127,\n",
       "  90,\n",
       "  nan,\n",
       "  92,\n",
       "  nan,\n",
       "  nan,\n",
       "  132,\n",
       "  nan,\n",
       "  112,\n",
       "  98,\n",
       "  99,\n",
       "  135,\n",
       "  76,\n",
       "  nan,\n",
       "  103,\n",
       "  85,\n",
       "  41,\n",
       "  62,\n",
       "  120,\n",
       "  86,\n",
       "  107,\n",
       "  106,\n",
       "  128,\n",
       "  nan,\n",
       "  75,\n",
       "  85,\n",
       "  nan,\n",
       "  130,\n",
       "  95,\n",
       "  59,\n",
       "  nan,\n",
       "  nan,\n",
       "  141,\n",
       "  59,\n",
       "  nan,\n",
       "  52,\n",
       "  nan,\n",
       "  79,\n",
       "  65,\n",
       "  70,\n",
       "  100,\n",
       "  81,\n",
       "  131,\n",
       "  82,\n",
       "  nan,\n",
       "  nan,\n",
       "  80,\n",
       "  nan,\n",
       "  69,\n",
       "  102,\n",
       "  77,\n",
       "  74,\n",
       "  76,\n",
       "  68,\n",
       "  86,\n",
       "  77,\n",
       "  nan,\n",
       "  96,\n",
       "  64,\n",
       "  87,\n",
       "  111,\n",
       "  67,\n",
       "  85,\n",
       "  nan,\n",
       "  82,\n",
       "  nan,\n",
       "  nan,\n",
       "  95,\n",
       "  91,\n",
       "  99,\n",
       "  71,\n",
       "  48,\n",
       "  83,\n",
       "  93,\n",
       "  97,\n",
       "  59,\n",
       "  76,\n",
       "  154,\n",
       "  92,\n",
       "  71,\n",
       "  nan,\n",
       "  88,\n",
       "  63,\n",
       "  nan,\n",
       "  79,\n",
       "  nan,\n",
       "  79,\n",
       "  nan,\n",
       "  99,\n",
       "  83,\n",
       "  72,\n",
       "  nan,\n",
       "  nan,\n",
       "  81,\n",
       "  85,\n",
       "  nan,\n",
       "  97,\n",
       "  108,\n",
       "  138,\n",
       "  70,\n",
       "  37,\n",
       "  nan,\n",
       "  nan,\n",
       "  57,\n",
       "  nan,\n",
       "  nan,\n",
       "  92,\n",
       "  111,\n",
       "  128,\n",
       "  nan,\n",
       "  77,\n",
       "  94,\n",
       "  73,\n",
       "  87,\n",
       "  116,\n",
       "  142,\n",
       "  nan,\n",
       "  84,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  66,\n",
       "  76,\n",
       "  nan,\n",
       "  110,\n",
       "  57],\n",
       " 'sal_high': [89,\n",
       "  113,\n",
       "  117,\n",
       "  136,\n",
       "  93,\n",
       "  123,\n",
       "  106,\n",
       "  113,\n",
       "  nan,\n",
       "  86,\n",
       "  126,\n",
       "  120,\n",
       "  101,\n",
       "  nan,\n",
       "  111,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  116,\n",
       "  nan,\n",
       "  82,\n",
       "  75,\n",
       "  161,\n",
       "  169,\n",
       "  nan,\n",
       "  nan,\n",
       "  134,\n",
       "  nan,\n",
       "  123,\n",
       "  77,\n",
       "  146,\n",
       "  116,\n",
       "  150,\n",
       "  135,\n",
       "  115,\n",
       "  67,\n",
       "  124,\n",
       "  118,\n",
       "  118,\n",
       "  138,\n",
       "  97,\n",
       "  85,\n",
       "  171,\n",
       "  110,\n",
       "  112,\n",
       "  96,\n",
       "  116,\n",
       "  121,\n",
       "  nan,\n",
       "  nan,\n",
       "  107,\n",
       "  92,\n",
       "  122,\n",
       "  110,\n",
       "  147,\n",
       "  121,\n",
       "  nan,\n",
       "  120,\n",
       "  nan,\n",
       "  94,\n",
       "  99,\n",
       "  nan,\n",
       "  110,\n",
       "  63,\n",
       "  103,\n",
       "  nan,\n",
       "  90,\n",
       "  109,\n",
       "  102,\n",
       "  110,\n",
       "  nan,\n",
       "  nan,\n",
       "  110,\n",
       "  103,\n",
       "  113,\n",
       "  113,\n",
       "  145,\n",
       "  84,\n",
       "  61,\n",
       "  98,\n",
       "  nan,\n",
       "  161,\n",
       "  135,\n",
       "  58,\n",
       "  87,\n",
       "  93,\n",
       "  121,\n",
       "  nan,\n",
       "  nan,\n",
       "  116,\n",
       "  123,\n",
       "  84,\n",
       "  99,\n",
       "  62,\n",
       "  nan,\n",
       "  107,\n",
       "  nan,\n",
       "  110,\n",
       "  106,\n",
       "  nan,\n",
       "  84,\n",
       "  110,\n",
       "  nan,\n",
       "  104,\n",
       "  73,\n",
       "  124,\n",
       "  61,\n",
       "  108,\n",
       "  104,\n",
       "  116,\n",
       "  116,\n",
       "  67,\n",
       "  nan,\n",
       "  72,\n",
       "  65,\n",
       "  126,\n",
       "  117,\n",
       "  116,\n",
       "  92,\n",
       "  131,\n",
       "  nan,\n",
       "  nan,\n",
       "  100,\n",
       "  97,\n",
       "  146,\n",
       "  nan,\n",
       "  108,\n",
       "  nan,\n",
       "  113,\n",
       "  92,\n",
       "  87,\n",
       "  155,\n",
       "  nan,\n",
       "  118,\n",
       "  nan,\n",
       "  94,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  117,\n",
       "  nan,\n",
       "  156,\n",
       "  172,\n",
       "  131,\n",
       "  158,\n",
       "  nan,\n",
       "  167,\n",
       "  142,\n",
       "  103,\n",
       "  164,\n",
       "  nan,\n",
       "  150,\n",
       "  150,\n",
       "  146,\n",
       "  nan,\n",
       "  108,\n",
       "  132,\n",
       "  197,\n",
       "  138,\n",
       "  123,\n",
       "  nan,\n",
       "  156,\n",
       "  112,\n",
       "  130,\n",
       "  169,\n",
       "  147,\n",
       "  116,\n",
       "  nan,\n",
       "  121,\n",
       "  106,\n",
       "  137,\n",
       "  125,\n",
       "  70,\n",
       "  178,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  104,\n",
       "  nan,\n",
       "  nan,\n",
       "  162,\n",
       "  nan,\n",
       "  122,\n",
       "  nan,\n",
       "  176,\n",
       "  97,\n",
       "  134,\n",
       "  nan,\n",
       "  94,\n",
       "  124,\n",
       "  83,\n",
       "  139,\n",
       "  112,\n",
       "  nan,\n",
       "  169,\n",
       "  116,\n",
       "  110,\n",
       "  95,\n",
       "  145,\n",
       "  nan,\n",
       "  132,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  119,\n",
       "  138,\n",
       "  nan,\n",
       "  118,\n",
       "  106,\n",
       "  nan,\n",
       "  nan,\n",
       "  118,\n",
       "  176,\n",
       "  nan,\n",
       "  131,\n",
       "  94,\n",
       "  114,\n",
       "  nan,\n",
       "  106,\n",
       "  139,\n",
       "  nan,\n",
       "  146,\n",
       "  101,\n",
       "  120,\n",
       "  nan,\n",
       "  140,\n",
       "  108,\n",
       "  101,\n",
       "  109,\n",
       "  97,\n",
       "  173,\n",
       "  146,\n",
       "  113,\n",
       "  147,\n",
       "  175,\n",
       "  138,\n",
       "  nan,\n",
       "  125,\n",
       "  nan,\n",
       "  nan,\n",
       "  136,\n",
       "  193,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  126,\n",
       "  nan,\n",
       "  nan,\n",
       "  151,\n",
       "  110,\n",
       "  126,\n",
       "  165,\n",
       "  132,\n",
       "  nan,\n",
       "  nan,\n",
       "  118,\n",
       "  nan,\n",
       "  103,\n",
       "  101,\n",
       "  nan,\n",
       "  117,\n",
       "  nan,\n",
       "  98,\n",
       "  72,\n",
       "  167,\n",
       "  nan,\n",
       "  nan,\n",
       "  163,\n",
       "  nan,\n",
       "  142,\n",
       "  nan,\n",
       "  120,\n",
       "  nan,\n",
       "  112,\n",
       "  nan,\n",
       "  182,\n",
       "  131,\n",
       "  130,\n",
       "  157,\n",
       "  139,\n",
       "  123,\n",
       "  142,\n",
       "  140,\n",
       "  143,\n",
       "  nan,\n",
       "  129,\n",
       "  143,\n",
       "  nan,\n",
       "  155,\n",
       "  137,\n",
       "  nan,\n",
       "  153,\n",
       "  119,\n",
       "  nan,\n",
       "  118,\n",
       "  102,\n",
       "  122,\n",
       "  nan,\n",
       "  nan,\n",
       "  135,\n",
       "  nan,\n",
       "  179,\n",
       "  151,\n",
       "  151,\n",
       "  nan,\n",
       "  nan,\n",
       "  138,\n",
       "  96,\n",
       "  nan,\n",
       "  129,\n",
       "  nan,\n",
       "  140,\n",
       "  128,\n",
       "  nan,\n",
       "  125,\n",
       "  145,\n",
       "  nan,\n",
       "  109,\n",
       "  155,\n",
       "  127,\n",
       "  166,\n",
       "  nan,\n",
       "  143,\n",
       "  112,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  156,\n",
       "  165,\n",
       "  145,\n",
       "  109,\n",
       "  141,\n",
       "  133,\n",
       "  122,\n",
       "  135,\n",
       "  86,\n",
       "  134,\n",
       "  161,\n",
       "  115,\n",
       "  139,\n",
       "  nan,\n",
       "  124,\n",
       "  168,\n",
       "  133,\n",
       "  149,\n",
       "  nan,\n",
       "  134,\n",
       "  120,\n",
       "  nan,\n",
       "  nan,\n",
       "  243,\n",
       "  118,\n",
       "  117,\n",
       "  154,\n",
       "  125,\n",
       "  103,\n",
       "  150,\n",
       "  130,\n",
       "  159,\n",
       "  nan,\n",
       "  150,\n",
       "  119,\n",
       "  143,\n",
       "  144,\n",
       "  nan,\n",
       "  nan,\n",
       "  85,\n",
       "  nan,\n",
       "  nan,\n",
       "  127,\n",
       "  147,\n",
       "  142,\n",
       "  nan,\n",
       "  122,\n",
       "  nan,\n",
       "  117,\n",
       "  117,\n",
       "  108,\n",
       "  150,\n",
       "  nan,\n",
       "  nan,\n",
       "  103,\n",
       "  nan,\n",
       "  197,\n",
       "  nan,\n",
       "  nan,\n",
       "  96,\n",
       "  100,\n",
       "  143,\n",
       "  128,\n",
       "  193,\n",
       "  110,\n",
       "  101,\n",
       "  139,\n",
       "  nan,\n",
       "  125,\n",
       "  nan,\n",
       "  172,\n",
       "  148,\n",
       "  154,\n",
       "  117,\n",
       "  130,\n",
       "  nan,\n",
       "  131,\n",
       "  134,\n",
       "  114,\n",
       "  115,\n",
       "  127,\n",
       "  141,\n",
       "  nan,\n",
       "  134,\n",
       "  101,\n",
       "  nan,\n",
       "  205,\n",
       "  128,\n",
       "  72,\n",
       "  118,\n",
       "  151,\n",
       "  121,\n",
       "  nan,\n",
       "  106,\n",
       "  73,\n",
       "  160,\n",
       "  143,\n",
       "  103,\n",
       "  150,\n",
       "  nan,\n",
       "  nan,\n",
       "  97,\n",
       "  110,\n",
       "  128,\n",
       "  nan,\n",
       "  105,\n",
       "  172,\n",
       "  nan,\n",
       "  100,\n",
       "  193,\n",
       "  120,\n",
       "  139,\n",
       "  nan,\n",
       "  143,\n",
       "  nan,\n",
       "  nan,\n",
       "  145,\n",
       "  nan,\n",
       "  nan,\n",
       "  91,\n",
       "  158,\n",
       "  153,\n",
       "  158,\n",
       "  180,\n",
       "  142,\n",
       "  130,\n",
       "  77,\n",
       "  nan,\n",
       "  110,\n",
       "  181,\n",
       "  150,\n",
       "  84,\n",
       "  82,\n",
       "  128,\n",
       "  nan,\n",
       "  172,\n",
       "  nan,\n",
       "  75,\n",
       "  122,\n",
       "  nan,\n",
       "  119,\n",
       "  nan,\n",
       "  nan,\n",
       "  91,\n",
       "  102,\n",
       "  153,\n",
       "  nan,\n",
       "  171,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  183,\n",
       "  141,\n",
       "  150,\n",
       "  nan,\n",
       "  nan,\n",
       "  122,\n",
       "  177,\n",
       "  130,\n",
       "  nan,\n",
       "  149,\n",
       "  nan,\n",
       "  nan,\n",
       "  176,\n",
       "  nan,\n",
       "  152,\n",
       "  141,\n",
       "  142,\n",
       "  192,\n",
       "  102,\n",
       "  nan,\n",
       "  148,\n",
       "  116,\n",
       "  69,\n",
       "  105,\n",
       "  162,\n",
       "  127,\n",
       "  153,\n",
       "  144,\n",
       "  182,\n",
       "  nan,\n",
       "  112,\n",
       "  124,\n",
       "  nan,\n",
       "  181,\n",
       "  133,\n",
       "  95,\n",
       "  nan,\n",
       "  nan,\n",
       "  189,\n",
       "  101,\n",
       "  nan,\n",
       "  86,\n",
       "  nan,\n",
       "  117,\n",
       "  89,\n",
       "  117,\n",
       "  145,\n",
       "  122,\n",
       "  183,\n",
       "  120,\n",
       "  nan,\n",
       "  nan,\n",
       "  111,\n",
       "  nan,\n",
       "  96,\n",
       "  137,\n",
       "  112,\n",
       "  109,\n",
       "  116,\n",
       "  107,\n",
       "  126,\n",
       "  114,\n",
       "  nan,\n",
       "  140,\n",
       "  108,\n",
       "  133,\n",
       "  160,\n",
       "  95,\n",
       "  121,\n",
       "  nan,\n",
       "  114,\n",
       "  nan,\n",
       "  nan,\n",
       "  130,\n",
       "  144,\n",
       "  143,\n",
       "  117,\n",
       "  74,\n",
       "  125,\n",
       "  131,\n",
       "  142,\n",
       "  100,\n",
       "  118,\n",
       "  219,\n",
       "  130,\n",
       "  100,\n",
       "  nan,\n",
       "  123,\n",
       "  104,\n",
       "  nan,\n",
       "  111,\n",
       "  nan,\n",
       "  121,\n",
       "  nan,\n",
       "  141,\n",
       "  116,\n",
       "  109,\n",
       "  nan,\n",
       "  nan,\n",
       "  117,\n",
       "  118,\n",
       "  nan,\n",
       "  136,\n",
       "  145,\n",
       "  180,\n",
       "  96,\n",
       "  59,\n",
       "  nan,\n",
       "  nan,\n",
       "  89,\n",
       "  nan,\n",
       "  nan,\n",
       "  129,\n",
       "  148,\n",
       "  168,\n",
       "  nan,\n",
       "  105,\n",
       "  125,\n",
       "  98,\n",
       "  117,\n",
       "  153,\n",
       "  183,\n",
       "  nan,\n",
       "  113,\n",
       "  nan,\n",
       "  nan,\n",
       "  nan,\n",
       "  91,\n",
       "  103,\n",
       "  nan,\n",
       "  147,\n",
       "  79],\n",
       " 'link': ['https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=680969&s=149&guid=0000016de58a38219b7e47f6471ee345&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_833d2534&cb=1571514104766&jobListingId=3381578119',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=655639&s=149&guid=0000016de58a38219b7e47f6471ee345&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_939d7df3&cb=1571514104770&jobListingId=3320321158',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=680891&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3c6553e2&cb=1571514104773&jobListingId=3284284037',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=755298&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5734911d&cb=1571514104777&jobListingId=3298422474',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=103&ao=654214&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_c2deb6b3&cb=1571514104781&jobListingId=3352031827',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=104&ao=4120&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_0b063ea0&cb=1571514104784&jobListingId=3196505395',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=105&ao=4120&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_201c4bf9&cb=1571514104787&jobListingId=3388905001',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=106&ao=669065&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_04a181e9&cb=1571514104791&jobListingId=3374099933',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=108&ao=352789&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_365d834a&cb=1571514104796&jobListingId=3356833111',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=109&ao=272736&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_caed0379&cb=1571514104799&jobListingId=3350001434',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=110&ao=735718&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_87f49db8&cb=1571514104802&jobListingId=3386703038',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=111&ao=233406&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_08273c12&cb=1571514104805&jobListingId=3378009255',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=112&ao=348370&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c21247b9&cb=1571514104809&jobListingId=3354376776',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=113&ao=14295&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_0cbffddd&cb=1571514104812&jobListingId=3180307018',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=114&ao=755474&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_64bac242&cb=1571514104815&jobListingId=3376562653',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=115&ao=215203&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5c1a37b5&cb=1571514104818&jobListingId=3254097283',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=116&ao=795245&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b84fa0ac&cb=1571514104820&jobListingId=3359023976',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=117&ao=681763&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_42aaf1dc&cb=1571514104823&jobListingId=3238036556',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=118&ao=799030&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b8f05888&cb=1571514104826&jobListingId=3296285380',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=119&ao=215203&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a2c7f8b4&cb=1571514104829&jobListingId=3350511635',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=120&ao=654207&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f81ad33f&cb=1571514104832&jobListingId=3356138485',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=121&ao=665607&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_c9a80859&cb=1571514104836&jobListingId=3356602294',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=122&ao=764538&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_200d305b&cb=1571514104839&jobListingId=3328266779',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=123&ao=751416&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_55e5965f&cb=1571514104843&jobListingId=3322135751',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=124&ao=148364&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cdb3fa9f&cb=1571514104846&jobListingId=3391450036',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=125&ao=708544&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c81184d6&cb=1571514104849&jobListingId=3365068364',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=126&ao=128225&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_21b58ec2&cb=1571514104852&jobListingId=3205983875',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=127&ao=8095&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f3a478f9&cb=1571514104855&jobListingId=3353556402',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=133055&s=58&guid=0000016de58a3821ad8e9c62f1fcc0b6&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_11a0c12f&cb=1571514104858&jobListingId=3318225561',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=202&ao=272736&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0416ba98&cb=1571514127598&jobListingId=3328903440',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=203&ao=782434&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_62b0b986&cb=1571514127602&jobListingId=3253403496',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=204&ao=8095&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_34dc5a94&cb=1571514127605&jobListingId=3308918595',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=205&ao=571138&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dfe4985a&cb=1571514127609&jobListingId=3379951822',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=206&ao=783913&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8205846d&cb=1571514127613&jobListingId=3358722522',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=207&ao=4130&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d4b821a7&cb=1571514127617&jobListingId=3373840950',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=209&ao=662817&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0742b02e&cb=1571514127627&jobListingId=3335457872',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=210&ao=773111&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_86c8e757&cb=1571514127637&jobListingId=3351864423',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=211&ao=133043&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_92926a32&cb=1571514127643&jobListingId=3365674970',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=212&ao=8095&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9a8fe5f7&cb=1571514127647&jobListingId=3219890595',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=213&ao=655639&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_76026359&cb=1571514127651&jobListingId=3279849002',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=214&ao=483589&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3f01c3e4&cb=1571514127654&jobListingId=3246517577',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=215&ao=735718&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e9164346&cb=1571514127658&jobListingId=3385750069',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=216&ao=755474&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4d96ff3a&cb=1571514127662&jobListingId=3374690234',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=217&ao=273207&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0b84aca5&cb=1571514127666&jobListingId=3178235988',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=218&ao=133665&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4c60b877&cb=1571514127670&jobListingId=2994117434',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=219&ao=664679&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f44b0250&cb=1571514127674&jobListingId=3346481137',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=220&ao=128225&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_131a37ac&cb=1571514127678&jobListingId=3353579290',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=221&ao=795245&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_04577a99&cb=1571514127681&jobListingId=3374589739',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=222&ao=672651&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ee81b4c2&cb=1571514127685&jobListingId=3329734179',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=224&ao=658663&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a6b55b0a&cb=1571514127691&jobListingId=3325134847',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=225&ao=190222&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ff4b2264&cb=1571514127694&jobListingId=3322135794',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=226&ao=352789&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c148a396&cb=1571514127698&jobListingId=3383653153',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=227&ao=795780&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_137218da&cb=1571514127701&jobListingId=3300388779',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=228&ao=120264&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0788edae&cb=1571514127704&jobListingId=3374670113',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=229&ao=755298&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a3f2a3a8&cb=1571514127708&jobListingId=2725035780',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=230&ao=286132&s=58&guid=0000016de58a918d958e88de2d444918&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_502ec3f3&cb=1571514127711&jobListingId=3227655543',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=747662&s=149&guid=0000016de58cb6278c2401d289649780&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d81ed68c&cb=1571514267923&jobListingId=3325280156',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=226254&s=149&guid=0000016de58cb6278c2401d289649780&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_45d38db9&cb=1571514267926&jobListingId=3272706711',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=745969&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_4745302b&cb=1571514267930&jobListingId=3323871719',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=226254&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1c82760f&cb=1571514267933&jobListingId=3265401585',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=103&ao=758431&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_7b1a74f0&cb=1571514267936&jobListingId=3364762246',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=104&ao=85058&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_72791d30&cb=1571514267940&jobListingId=3361416053',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=105&ao=85058&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0006d003&cb=1571514267944&jobListingId=3384363812',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=106&ao=747535&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_186c6726&cb=1571514267947&jobListingId=3302841314',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=107&ao=389273&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d1fc89f9&cb=1571514267954&jobListingId=3363451785',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=108&ao=784225&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0e13da3e&cb=1571514267957&jobListingId=3358528772',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=109&ao=782058&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_269476c3&cb=1571514267960&jobListingId=3365248211',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=110&ao=4348&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_cc00770a&cb=1571514267963&jobListingId=3344983747',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=111&ao=759479&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2f2434b2&cb=1571514267966&jobListingId=3332214881',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=112&ao=747662&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_de27e797&cb=1571514267969&jobListingId=3280026843',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=113&ao=4484&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5a7659ee&cb=1571514267972&jobListingId=3311524017',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=114&ao=389273&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5be4f38f&cb=1571514267975&jobListingId=3391745761',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=115&ao=735729&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a1168c14&cb=1571514267978&jobListingId=3388114590',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=116&ao=687186&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6f9102cb&cb=1571514267981&jobListingId=3317344619',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=117&ao=133143&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fc5293e1&cb=1571514267984&jobListingId=3181397648',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=118&ao=133043&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c5a330e6&cb=1571514267987&jobListingId=3093497333',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=119&ao=133043&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f4f0b5a6&cb=1571514267990&jobListingId=3272792918',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=120&ao=352789&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_abe7dbcf&cb=1571514267994&jobListingId=3349197179',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=121&ao=567544&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cce5a2a5&cb=1571514267998&jobListingId=3348319164',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=122&ao=133779&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1bbb7af8&cb=1571514268001&jobListingId=3377840501',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=123&ao=735718&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_648a2617&cb=1571514268004&jobListingId=3379783823',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=124&ao=242478&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_700b9431&cb=1571514268011&jobListingId=3388498166',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=125&ao=745969&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_c38273ee&cb=1571514268014&jobListingId=3280026844',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=126&ao=4120&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f80038bb&cb=1571514268017&jobListingId=3386482594',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=127&ao=758431&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_dc60f9df&cb=1571514268020&jobListingId=3364762219',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=372858&s=58&guid=0000016de58cb627a06e845f00e30c9c&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_130a4aab&cb=1571514268024&jobListingId=3391910866',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=201&ao=85058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_05a74bad&cb=1571514293142&jobListingId=3389794716',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=203&ao=278067&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b3fd873d&cb=1571514293148&jobListingId=3379906488',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=204&ao=454826&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_22343c1d&cb=1571514293151&jobListingId=3374280684',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=205&ao=782058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4a1c5ef6&cb=1571514293154&jobListingId=3389110658',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=206&ao=680245&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_da1f9cf2&cb=1571514293158&jobListingId=3346961508',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=207&ao=782058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6507e4f3&cb=1571514293162&jobListingId=3389105500',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=208&ao=14295&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_2913d583&cb=1571514293165&jobListingId=3328884452',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=209&ao=782856&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0edee113&cb=1571514293169&jobListingId=3382073732',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=210&ao=769231&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_51fe7af3&cb=1571514293172&jobListingId=3364217382',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=211&ao=588939&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_585e4bfc&cb=1571514293175&jobListingId=3310636246',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=213&ao=389273&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a4711ca2&cb=1571514293180&jobListingId=3363673653',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=214&ao=588939&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ef2266e8&cb=1571514293183&jobListingId=3250138590',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=215&ao=782058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d1c96ff6&cb=1571514293186&jobListingId=3365593070',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=216&ao=372858&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7527d829&cb=1571514293189&jobListingId=3391911999',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=217&ao=747662&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ef963506&cb=1571514293192&jobListingId=3294100326',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=218&ao=747662&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_9277e1b6&cb=1571514293195&jobListingId=3357475761',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=219&ao=721547&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_14392cd5&cb=1571514293198&jobListingId=3303387708',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=220&ao=4120&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_6891b11d&cb=1571514293201&jobListingId=3361404447',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=221&ao=132962&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6173a321&cb=1571514293204&jobListingId=3390823608',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=222&ao=359914&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cfeda2da&cb=1571514293207&jobListingId=2843045883',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=224&ao=567544&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_987f7876&cb=1571514293212&jobListingId=3364941187',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=225&ao=782058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_42da14ec&cb=1571514293215&jobListingId=3374698577',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=226&ao=37049&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b6ed3a20&cb=1571514293218&jobListingId=3385199416',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=228&ao=782058&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f251678f&cb=1571514293224&jobListingId=3371543219',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=229&ao=182894&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_64b1cb17&cb=1571514293227&jobListingId=3388591670',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=230&ao=454826&s=58&guid=0000016de58d18af888fc28ef57c3ca9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f21c3bd0&cb=1571514293230&jobListingId=3365130651',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=301&ao=454823&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f0abe632&cb=1571514311090&jobListingId=3362765832',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=302&ao=454823&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fe309478&cb=1571514311093&jobListingId=3283260659',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=303&ao=588939&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d4779011&cb=1571514311096&jobListingId=3226657886',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=304&ao=128226&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9f0e0b94&cb=1571514311099&jobListingId=3361533371',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=305&ao=37049&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9b00b46c&cb=1571514311102&jobListingId=3026310092',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=306&ao=782058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0dbbb666&cb=1571514311105&jobListingId=3365599355',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=307&ao=763249&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_96a8146c&cb=1571514311108&jobListingId=3376299212',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=308&ao=782058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_10933671&cb=1571514311111&jobListingId=3380222486',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=309&ao=796429&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_e8c14192&cb=1571514311118&jobListingId=3026416102',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=310&ao=85058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5b73add7&cb=1571514311121&jobListingId=3368910375',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=311&ao=85058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a8a4e981&cb=1571514311124&jobListingId=3390649636',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=312&ao=782058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4b0cf1d3&cb=1571514311127&jobListingId=3365592745',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=313&ao=226254&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5455063a&cb=1571514311130&jobListingId=3270284414',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=314&ao=66506&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_610c6545&cb=1571514311133&jobListingId=3024305913',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=315&ao=297281&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cc487b37&cb=1571514311136&jobListingId=3379059074',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=316&ao=389273&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_05c23b4a&cb=1571514311139&jobListingId=3391479780',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=317&ao=352789&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5e3d0699&cb=1571514311141&jobListingId=3348468246',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=318&ao=389273&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f0b7b78e&cb=1571514311145&jobListingId=3381574280',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=319&ao=85058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_374dead7&cb=1571514311148&jobListingId=3391739330',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=320&ao=226254&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_91f3a0c2&cb=1571514311151&jobListingId=3272706709',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=321&ao=45367&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f4c25610&cb=1571514311154&jobListingId=3359356708',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=322&ao=782058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3ddaa498&cb=1571514311157&jobListingId=3365594257',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=323&ao=278067&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bb27466a&cb=1571514311160&jobListingId=3383496027',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=324&ao=400784&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_60785cc6&cb=1571514311163&jobListingId=3337181340',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=325&ao=777964&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0361463b&cb=1571514311166&jobListingId=3364217639',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=327&ao=777964&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6b242075&cb=1571514311173&jobListingId=3364216369',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=328&ao=766126&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_691ba271&cb=1571514311176&jobListingId=3368113759',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=329&ao=242359&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ed18d0a4&cb=1571514311180&jobListingId=3367142807',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=330&ao=85058&s=58&guid=0000016de58d5e41acf6ebd3d5230a06&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b7561024&cb=1571514311183&jobListingId=3382817541',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=735890&s=149&guid=0000016de590290ca9c5bd6f21889f68&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_88d49e67&cb=1571514494146&jobListingId=3097417370',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=787414&s=149&guid=0000016de590290ca9c5bd6f21889f68&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_4c709875&cb=1571514494150&jobListingId=3310800877',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=485502&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_81989ffc&cb=1571514494154&jobListingId=3345597620',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=783510&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_46dc73d1&cb=1571514494158&jobListingId=3387954063',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=103&ao=701633&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7a0746c4&cb=1571514494161&jobListingId=3353403038',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=104&ao=153358&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_63e55d95&cb=1571514494165&jobListingId=3256468388',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=105&ao=655756&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_16e06a0b&cb=1571514494169&jobListingId=3375704792',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=106&ao=669910&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_6a5a489f&cb=1571514494172&jobListingId=2805741309',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=107&ao=738506&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0fdbff6e&cb=1571514494176&jobListingId=3297626736',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=108&ao=766674&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e19cc6a2&cb=1571514494180&jobListingId=3342286414',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=109&ao=654006&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fd5403e7&cb=1571514494183&jobListingId=3089502227',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=110&ao=120626&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f853ea77&cb=1571514494187&jobListingId=3347133133',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=111&ao=8095&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c16c37f0&cb=1571514494191&jobListingId=3112627652',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=112&ao=684389&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f66125a0&cb=1571514494194&jobListingId=3357774174',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=113&ao=404790&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f31ab0eb&cb=1571514494198&jobListingId=3385520868',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=114&ao=372858&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_613585d3&cb=1571514494201&jobListingId=3377253166',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=115&ao=14295&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9a4081fb&cb=1571514494205&jobListingId=3294156050',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=116&ao=352788&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9ab48476&cb=1571514494209&jobListingId=3373908447',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=117&ao=656630&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5f8c36bd&cb=1571514494212&jobListingId=3380694219',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=118&ao=588939&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d27ceb31&cb=1571514494216&jobListingId=3328495749',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=119&ao=4372&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fdb0c20d&cb=1571514494219&jobListingId=3028278579',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=120&ao=659809&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_24b6a126&cb=1571514494223&jobListingId=3304548552',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=121&ao=664018&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d9aef41d&cb=1571514494226&jobListingId=3356445098',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=122&ao=98222&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_62de8ac0&cb=1571514494230&jobListingId=3221152643',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=123&ao=37049&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f4b15f26&cb=1571514494234&jobListingId=3379206501',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=124&ao=4470&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9bc49b52&cb=1571514494237&jobListingId=3276632357',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=126&ao=4120&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dcf263b3&cb=1571514494244&jobListingId=3357098747',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=127&ao=8095&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_43966a96&cb=1571514494247&jobListingId=3389437981',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=85058&s=58&guid=0000016de590290ca542c595e8b7108b&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ee537822&cb=1571514494251&jobListingId=3390798686',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=201&ao=725519&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7d763429&cb=1571514517888&jobListingId=3347903964',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=202&ao=683108&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3d86439f&cb=1571514517891&jobListingId=3204283190',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=203&ao=782620&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_61d6b51a&cb=1571514517894&jobListingId=3365966803',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=204&ao=135984&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_6f21e927&cb=1571514517901&jobListingId=3389475634',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=205&ao=8095&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_12083817&cb=1571514517905&jobListingId=3279799442',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=206&ao=148364&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5debe25c&cb=1571514517908&jobListingId=3145606361',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=207&ao=85058&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0743c7a0&cb=1571514517911&jobListingId=3378190508',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=208&ao=749489&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c3fe6d10&cb=1571514517914&jobListingId=3319618262',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=209&ao=782186&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_eddeb073&cb=1571514517917&jobListingId=3360313684',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=210&ao=8095&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b8b63630&cb=1571514517920&jobListingId=2728572590',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=211&ao=704084&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_533c5bf8&cb=1571514517923&jobListingId=3366352741',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=212&ao=14295&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_34b12017&cb=1571514517926&jobListingId=3353408638',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=213&ao=4120&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fb1b39e3&cb=1571514517929&jobListingId=3250657259',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=214&ao=63368&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e66a6a16&cb=1571514517932&jobListingId=3386682367',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=215&ao=98222&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7e1ae0ac&cb=1571514517935&jobListingId=3221154178',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=216&ao=654227&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_2b7ec9b4&cb=1571514517939&jobListingId=3385548830',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=217&ao=798911&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bd5de7f6&cb=1571514517942&jobListingId=3222939647',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=218&ao=352789&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1cd41a04&cb=1571514517945&jobListingId=3356811831',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=219&ao=654219&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_79e1629f&cb=1571514517948&jobListingId=3385548831',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=220&ao=432451&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e4bfab9b&cb=1571514517951&jobListingId=3361319289',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=221&ao=389273&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_80a395fb&cb=1571514517958&jobListingId=3390272571',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=222&ao=4120&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_22cd7d13&cb=1571514517961&jobListingId=3196505400',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=223&ao=8095&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7d277b72&cb=1571514517964&jobListingId=2894566897',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=224&ao=4120&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7ad931cc&cb=1571514517967&jobListingId=3391515796',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=226&ao=787414&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a52fe632&cb=1571514517973&jobListingId=3009226251',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=227&ao=4120&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7472c23f&cb=1571514517976&jobListingId=3336642327',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=228&ao=372858&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_96e12150&cb=1571514517979&jobListingId=3388042390',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=229&ao=708251&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fbf8727e&cb=1571514517982&jobListingId=3299469030',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=230&ao=14295&s=58&guid=0000016de59085e3b669148337522311&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_20dde841&cb=1571514517985&jobListingId=3358453194',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=301&ao=4341&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_aea77682&cb=1571514536294&jobListingId=3305782144',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=304&ao=799375&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_432f6827&cb=1571514536304&jobListingId=2830956947',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=305&ao=148364&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cf1095ad&cb=1571514536308&jobListingId=3379136111',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=306&ao=66506&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ff7037cd&cb=1571514536311&jobListingId=3354637243',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=307&ao=409327&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d74cfa2c&cb=1571514536315&jobListingId=3182148363',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=308&ao=133043&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fe1a2815&cb=1571514536318&jobListingId=3344373156',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=310&ao=133055&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5333167c&cb=1571514536326&jobListingId=3381256451',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=311&ao=215203&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_29853917&cb=1571514536329&jobListingId=3299353201',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=312&ao=37049&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_13612ff1&cb=1571514536332&jobListingId=3391326038',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=313&ao=182894&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1affc821&cb=1571514536336&jobListingId=3388725698',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=314&ao=777342&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5bf70b77&cb=1571514536340&jobListingId=3366545905',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=315&ao=215203&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fa2b6cf0&cb=1571514536343&jobListingId=3336432464',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=316&ao=133018&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_21f8563f&cb=1571514536345&jobListingId=3382796198',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=317&ao=664801&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_755e5b36&cb=1571514536349&jobListingId=2805730584',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=318&ao=8095&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_c813c77a&cb=1571514536352&jobListingId=3326693262',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=319&ao=8095&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_9fa3c480&cb=1571514536354&jobListingId=3378034906',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=320&ao=8095&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dedb86cd&cb=1571514536357&jobListingId=3344983178',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=322&ao=588939&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_be74a4da&cb=1571514536363&jobListingId=3216637790',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=323&ao=681698&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4d25a15f&cb=1571514536366&jobListingId=2682416895',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=324&ao=472290&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3db52b1e&cb=1571514536369&jobListingId=3389473881',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=326&ao=305335&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5ed08ad6&cb=1571514536375&jobListingId=3350256041',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=327&ao=120264&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_771b3226&cb=1571514536378&jobListingId=3325142817',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=328&ao=148364&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f2328798&cb=1571514536381&jobListingId=3167065760',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=329&ao=215203&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b5f9bf3d&cb=1571514536384&jobListingId=3131748220',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=330&ao=133269&s=58&guid=0000016de590ccd0bdc9f414b02f5d46&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6e14c0db&cb=1571514536387&jobListingId=3362847573',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=401&ao=85058&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8c708756&cb=1571514555903&jobListingId=3369310948',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=402&ao=733661&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0bda29c5&cb=1571514555906&jobListingId=2905794448',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=403&ao=717762&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ed95d10a&cb=1571514555910&jobListingId=3389696568',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=404&ao=133671&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f05855ac&cb=1571514555913&jobListingId=3360722678',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=405&ao=8095&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6de84b55&cb=1571514555916&jobListingId=3383049801',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=406&ao=37049&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f66b4409&cb=1571514555922&jobListingId=3375674746',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=407&ao=735718&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bcb2b89f&cb=1571514555925&jobListingId=3375666714',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=409&ao=372858&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d78499f4&cb=1571514555931&jobListingId=3380009451',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=410&ao=4120&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1c3fff05&cb=1571514555934&jobListingId=3368375402',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=411&ao=352789&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_75f4b5fd&cb=1571514555937&jobListingId=3344304777',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=412&ao=732468&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_dd661fb6&cb=1571514555940&jobListingId=3302233792',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=414&ao=724806&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_27ac96d1&cb=1571514555946&jobListingId=3374290487',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=415&ao=85058&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c3ec7296&cb=1571514555949&jobListingId=3379502612',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=416&ao=8095&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_efc91cea&cb=1571514555952&jobListingId=3360688963',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=417&ao=454823&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c1b7561c&cb=1571514556060&jobListingId=3367357617',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=418&ao=352789&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a212dc96&cb=1571514556063&jobListingId=3381713488',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=419&ao=713616&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_53937e3c&cb=1571514556067&jobListingId=3390913389',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=420&ao=783510&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6a8d44af&cb=1571514556070&jobListingId=3387954064',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=421&ao=454823&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2110b4cc&cb=1571514556074&jobListingId=3386001293',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=422&ao=709129&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_922b198c&cb=1571514556077&jobListingId=3236553500',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=424&ao=731205&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_efe2a865&cb=1571514556083&jobListingId=3250145116',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=425&ao=4120&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a1b15d35&cb=1571514556086&jobListingId=3305866748',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=426&ao=735718&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6f0fa625&cb=1571514556089&jobListingId=3390475593',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=427&ao=735722&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_46286e97&cb=1571514556092&jobListingId=3366618209',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=428&ao=14295&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_eb530fb2&cb=1571514556095&jobListingId=3349520525',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=429&ao=654269&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_612ac0c7&cb=1571514556098&jobListingId=3383049979',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=430&ao=4120&s=58&guid=0000016de59119d28ceb668d6ec54b9f&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a3d39b1e&cb=1571514556101&jobListingId=3310757778',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=501&ao=153358&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_25afb749&cb=1571514574995&jobListingId=3291790167',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=503&ao=85058&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_01056f2b&cb=1571514575002&jobListingId=3390798705',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=504&ao=716977&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_50b80396&cb=1571514575007&jobListingId=3147380766',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=505&ao=735722&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_55f69fae&cb=1571514575010&jobListingId=3343069387',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=506&ao=8095&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_81483b5c&cb=1571514575013&jobListingId=3241479300',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=509&ao=14295&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_1d67d95a&cb=1571514575023&jobListingId=3386458364',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=510&ao=8095&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_52c9031d&cb=1571514575027&jobListingId=3390451028',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=511&ao=3949&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_78ba6227&cb=1571514575030&jobListingId=3018999533',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=512&ao=782827&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2395f253&cb=1571514575037&jobListingId=3391976062',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=513&ao=313674&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3ebe9db9&cb=1571514575039&jobListingId=3290500590',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=514&ao=737412&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7d5839ac&cb=1571514575043&jobListingId=3353486745',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=516&ao=654822&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b130a838&cb=1571514575049&jobListingId=3141808620',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=517&ao=595971&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_48fe8cca&cb=1571514575052&jobListingId=3354949828',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=520&ao=657873&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_cd4013d6&cb=1571514575062&jobListingId=3368842694',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=521&ao=768248&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1e4820cd&cb=1571514575066&jobListingId=3358249232',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=522&ao=723634&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9417013a&cb=1571514575069&jobListingId=3320727492',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=523&ao=649735&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_87b159c2&cb=1571514575072&jobListingId=3284544349',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=524&ao=795829&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dd32ead7&cb=1571514575078&jobListingId=3385376472',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=525&ao=4120&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_216ad06a&cb=1571514575081&jobListingId=3372705977',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=526&ao=454823&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cb9f22a1&cb=1571514575085&jobListingId=3372440269',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=527&ao=685372&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_811ace74&cb=1571514575091&jobListingId=3342443987',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=529&ao=8095&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b221cb62&cb=1571514575098&jobListingId=3264554349',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=530&ao=735722&s=58&guid=0000016de591652f885c7a2958991a68&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_99beb6fc&cb=1571514575101&jobListingId=3391451709',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=777574&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c89be549&cb=1571514712024&jobListingId=3358277816',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=783177&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f192abde&cb=1571514712027&jobListingId=3365098427',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=105&ao=191997&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b1101017&cb=1571514712037&jobListingId=3390398637',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=106&ao=659951&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ff0b51e9&cb=1571514712040&jobListingId=3334367086',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=107&ao=456220&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bdcdf7a2&cb=1571514712043&jobListingId=3318120120',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=108&ao=140152&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6f2f612e&cb=1571514712046&jobListingId=3314488612',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=110&ao=372858&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c358a8a8&cb=1571514712052&jobListingId=3374487547',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=112&ao=37049&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e666e819&cb=1571514712059&jobListingId=3357175641',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=113&ao=695667&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_e99d7dfc&cb=1571514712062&jobListingId=3227249258',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=114&ao=4120&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dc93b4bb&cb=1571514712065&jobListingId=3248107443',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=115&ao=8095&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_02ed5c9b&cb=1571514712068&jobListingId=3355235575',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=116&ao=669271&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ed6ca40e&cb=1571514712071&jobListingId=3097422861',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=117&ao=148364&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4ca1d2bc&cb=1571514712074&jobListingId=3358746665',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=119&ao=8095&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_9f91b069&cb=1571514712080&jobListingId=3357234804',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=121&ao=133043&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c790b3cf&cb=1571514712087&jobListingId=3356866240',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=122&ao=4341&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_86a846bc&cb=1571514712090&jobListingId=3275047315',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=123&ao=8095&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_73808551&cb=1571514712093&jobListingId=3335591784',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=124&ao=133142&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ddad6e1d&cb=1571514712096&jobListingId=3387972104',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=125&ao=177247&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_93585407&cb=1571514712099&jobListingId=3350592560',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=126&ao=321255&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ce4f20e1&cb=1571514712102&jobListingId=3388333820',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=127&ao=8095&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_8bbfc878&cb=1571514712104&jobListingId=3143164701',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=8095&s=58&guid=0000016de5937d12b8ddd5b5eb074c47&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ea90fa0c&cb=1571514712107&jobListingId=3329051603',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=201&ao=187657&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_74b146a0&cb=1571514732935&jobListingId=3321095757',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=202&ao=777586&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3503b8fc&cb=1571514732941&jobListingId=3358283996',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=203&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_808c273a&cb=1571514732945&jobListingId=3342792120',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=204&ao=786530&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b1525274&cb=1571514732948&jobListingId=3370602991',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=205&ao=4120&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f13af61b&cb=1571514732960&jobListingId=3261322462',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=206&ao=683308&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c3e35e78&cb=1571514732966&jobListingId=3353571054',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=207&ao=316669&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0014625a&cb=1571514732976&jobListingId=3386785122',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=208&ao=692386&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d2ef5903&cb=1571514732988&jobListingId=3347671898',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=209&ao=780414&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_66b223a6&cb=1571514732991&jobListingId=3361827429',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=210&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_36df173b&cb=1571514732994&jobListingId=3349506039',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=211&ao=135842&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8a5064bd&cb=1571514732997&jobListingId=3386446902',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=212&ao=4341&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_dfa1eeb2&cb=1571514733000&jobListingId=3312869918',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=213&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_0386d03a&cb=1571514733004&jobListingId=3205874097',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=214&ao=4120&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_152d44ba&cb=1571514733011&jobListingId=3284141945',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=215&ao=215203&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8e042f9f&cb=1571514733014&jobListingId=3375215414',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=216&ao=735722&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f7f9396c&cb=1571514733017&jobListingId=3389457397',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=217&ao=140152&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_63ed517e&cb=1571514733021&jobListingId=3386177145',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=218&ao=796190&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d7a375d6&cb=1571514733024&jobListingId=3374134883',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=219&ao=37049&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4581e5fe&cb=1571514733027&jobListingId=3389458872',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=220&ao=14295&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_6aa368f6&cb=1571514733030&jobListingId=3226564805',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=221&ao=37049&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a8f701c6&cb=1571514733033&jobListingId=3384602825',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=222&ao=389273&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ed19a2d6&cb=1571514733036&jobListingId=3359158121',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=223&ao=148364&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_996ffec3&cb=1571514733040&jobListingId=3265282666',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=224&ao=4120&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b32690e8&cb=1571514733043&jobListingId=3389834808',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=225&ao=134123&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_11ad9b30&cb=1571514733046&jobListingId=2399151229',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=226&ao=215203&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4c39a3f2&cb=1571514733049&jobListingId=2643418786',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=227&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_3366912b&cb=1571514733052&jobListingId=2846625727',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=228&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d1640d54&cb=1571514733055&jobListingId=3352466057',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=229&ao=8095&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_c3a73367&cb=1571514733058&jobListingId=3346473096',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=230&ao=717982&s=58&guid=0000016de593cf14b960a5f7cd4527ac&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_65dca466&cb=1571514733066&jobListingId=3162838785',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=301&ao=9619&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d06f682e&cb=1571514750222&jobListingId=3329890320',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=302&ao=133647&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dc6c4465&cb=1571514750225&jobListingId=3300990307',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=303&ao=4008&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_34094602&cb=1571514750228&jobListingId=3353028670',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=304&ao=135777&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_853de956&cb=1571514750231&jobListingId=3024441198',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=307&ao=8095&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_03bb83fe&cb=1571514750241&jobListingId=3030726619',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=308&ao=4121&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_23c02a31&cb=1571514750245&jobListingId=3218390878',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=309&ao=735722&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8ceedb1d&cb=1571514750249&jobListingId=3390478442',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=310&ao=4121&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d957e08b&cb=1571514750253&jobListingId=3241386207',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=311&ao=4470&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ea62888b&cb=1571514750256&jobListingId=3277900840',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=313&ao=702951&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a7cdb0d3&cb=1571514750263&jobListingId=3358310830',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=314&ao=478897&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_acf72f77&cb=1571514750267&jobListingId=3223913105',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=316&ao=673044&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8f308747&cb=1571514750274&jobListingId=3304783500',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=318&ao=669935&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d247ad98&cb=1571514750281&jobListingId=3389480540',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=319&ao=37049&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_edd18094&cb=1571514750285&jobListingId=3361007067',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=320&ao=215203&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f4145e67&cb=1571514750288&jobListingId=3298969858',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=321&ao=37049&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c687bcb1&cb=1571514750291&jobListingId=3349945478',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=322&ao=4375&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cafec82f&cb=1571514750294&jobListingId=3204840537',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=323&ao=37049&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5d1bd81d&cb=1571514750297&jobListingId=3227144875',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=325&ao=433326&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ecae7e5f&cb=1571514750303&jobListingId=3297551623',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=326&ao=4134&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d9980f77&cb=1571514750306&jobListingId=3391620329',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=327&ao=133055&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9fad9348&cb=1571514750309&jobListingId=3363083070',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=328&ao=133057&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c8c37310&cb=1571514750312&jobListingId=3360750231',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=329&ao=3839&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ab07e4e6&cb=1571514750314&jobListingId=2756125191',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=330&ao=8095&s=58&guid=0000016de59411a18a1d822d71882f5d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_7f01b4c6&cb=1571514750317&jobListingId=3356395660',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=401&ao=664952&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d26cb1f1&cb=1571514772892&jobListingId=2613571011',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=402&ao=3839&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_95b83753&cb=1571514772895&jobListingId=3347706928',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=403&ao=133022&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_1e21c76b&cb=1571514772898&jobListingId=3351721043',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=404&ao=154438&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_22734b6c&cb=1571514772901&jobListingId=3330721722',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=405&ao=4134&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_04a7c694&cb=1571514772904&jobListingId=3279967546',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=406&ao=4120&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_544972c9&cb=1571514772907&jobListingId=2699373757',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=407&ao=135777&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c971cd57&cb=1571514772910&jobListingId=3104321391',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=408&ao=8095&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b4c7f05c&cb=1571514772913&jobListingId=2802182714',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=409&ao=191997&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b36757be&cb=1571514772916&jobListingId=3391244549',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=410&ao=212128&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_814f135d&cb=1571514772919&jobListingId=3368026226',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=411&ao=148364&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6cfd2d29&cb=1571514772922&jobListingId=3152252849',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=412&ao=588939&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6f1a4d35&cb=1571514772925&jobListingId=3360904461',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=413&ao=133255&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_daf175e7&cb=1571514772928&jobListingId=3304513767',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=414&ao=37049&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a3b8bcc4&cb=1571514772931&jobListingId=3390485177',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=417&ao=78823&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c54b7ece&cb=1571514772939&jobListingId=2701221094',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=419&ao=242478&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_038d511e&cb=1571514772945&jobListingId=3373572181',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=420&ao=774115&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_faddeb5f&cb=1571514772948&jobListingId=3336452888',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=421&ao=783084&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_56d4c5c4&cb=1571514772951&jobListingId=3310773089',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=422&ao=37049&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6da07bd5&cb=1571514772953&jobListingId=3353364228',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=423&ao=8095&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_150cd1fc&cb=1571514772956&jobListingId=3235023173',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=424&ao=401459&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_7ea38669&cb=1571514772959&jobListingId=2625550835',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=425&ao=4341&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d51c946c&cb=1571514772962&jobListingId=3261340591',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=426&ao=718387&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9c4bbb0e&cb=1571514772965&jobListingId=2205131791',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=429&ao=187657&s=58&guid=0000016de5946b24a2ce07cb420bb1e1&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6bcf7ea5&cb=1571514772974&jobListingId=3255044377',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=501&ao=753112&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b6f6fb07&cb=1571514789587&jobListingId=3268496370',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=503&ao=8095&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c81af3a6&cb=1571514789593&jobListingId=3241479496',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=504&ao=4011&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_50e974eb&cb=1571514789596&jobListingId=3347385784',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=505&ao=409319&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_29d6d439&cb=1571514789599&jobListingId=3316337052',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=506&ao=148364&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_089034fe&cb=1571514789602&jobListingId=3157475402',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=507&ao=36811&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_dbd3a0eb&cb=1571514789605&jobListingId=3362001647',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=508&ao=709127&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d327782b&cb=1571514789608&jobListingId=3016824335',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=510&ao=4008&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_13a4a66d&cb=1571514789614&jobListingId=3305951022',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=511&ao=679693&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_50764c3a&cb=1571514789617&jobListingId=2586395394',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=512&ao=708251&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_709fb0a0&cb=1571514789620&jobListingId=3278772035',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=513&ao=777271&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_667740b3&cb=1571514789623&jobListingId=3355505851',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=514&ao=353605&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_43f4381f&cb=1571514789625&jobListingId=3378453884',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=515&ao=4121&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_538f6310&cb=1571514789628&jobListingId=3206886489',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=516&ao=242900&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_18918e88&cb=1571514789631&jobListingId=3334554957',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=517&ao=135311&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_71221a8e&cb=1571514789634&jobListingId=3320887905',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=518&ao=37049&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_212faf4a&cb=1571514789637&jobListingId=3335357407',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=519&ao=8095&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c43563c3&cb=1571514789640&jobListingId=3314068899',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=521&ao=3839&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6d98dba6&cb=1571514789646&jobListingId=3371427451',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=523&ao=85944&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_959052e6&cb=1571514789652&jobListingId=3382695723',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=524&ao=14295&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a52bec1d&cb=1571514789655&jobListingId=3358693766',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=525&ao=85058&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2532ce97&cb=1571514789658&jobListingId=3390997824',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=526&ao=4120&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bff83265&cb=1571514789660&jobListingId=3366389666',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=528&ao=735722&s=58&guid=0000016de594ab12936c330529ebe8d2&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cf17a0bf&cb=1571514789666&jobListingId=3284792099',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=601&ao=743194&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3d8965b3&cb=1571514806679&jobListingId=3334528706',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=602&ao=372858&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_78c98e85&cb=1571514806683&jobListingId=3383167200',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=604&ao=388174&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_29816888&cb=1571514806689&jobListingId=3368760213',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=605&ao=8095&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_500fbfc0&cb=1571514806692&jobListingId=3226860130',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=606&ao=135398&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_df89a17e&cb=1571514806695&jobListingId=3370464306',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=608&ao=136681&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ef6741b3&cb=1571514806701&jobListingId=3315110012',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=610&ao=674588&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f417bcdf&cb=1571514806706&jobListingId=3382216306',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=611&ao=735718&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f9226b59&cb=1571514806709&jobListingId=3372442313',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=612&ao=454826&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_060201fb&cb=1571514806712&jobListingId=3372802654',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=613&ao=135777&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2a8d9642&cb=1571514806715&jobListingId=2738642001',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=614&ao=136869&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f95654e0&cb=1571514806718&jobListingId=3386746978',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=616&ao=85058&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ddba54ca&cb=1571514806724&jobListingId=3372599684',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=619&ao=413413&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_cb36e371&cb=1571514806732&jobListingId=2999936335',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=623&ao=8095&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_15541fb6&cb=1571514806744&jobListingId=3310582184',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=624&ao=37049&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0e778070&cb=1571514806747&jobListingId=3255182106',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=626&ao=14295&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f58ea262&cb=1571514806753&jobListingId=3207066944',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=627&ao=663856&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b51997d9&cb=1571514806756&jobListingId=3373845720',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=629&ao=136126&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e339880a&cb=1571514806761&jobListingId=3344992472',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=630&ao=797733&s=58&guid=0000016de594eebebfcf8d4fa1cb4fcb&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6210280f&cb=1571514806764&jobListingId=3368587818',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=702&ao=7438&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_377b2190&cb=1571514831689&jobListingId=3160451404',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=705&ao=120626&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c7011e9a&cb=1571514831698&jobListingId=3272373666',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=711&ao=133023&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_774cc03f&cb=1571514831716&jobListingId=3390822737',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=712&ao=136348&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3defad95&cb=1571514831719&jobListingId=3252914685',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=713&ao=52448&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f4e0e264&cb=1571514831722&jobListingId=3374627658',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=714&ao=135416&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0d28b924&cb=1571514831725&jobListingId=3392046125',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=715&ao=654006&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2d5b5a31&cb=1571514831728&jobListingId=3269684505',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=719&ao=215203&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8b5c9b83&cb=1571514831739&jobListingId=3380609121',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=720&ao=656978&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_244e1b21&cb=1571514831742&jobListingId=3280519916',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=721&ao=762104&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_4086aa95&cb=1571514831745&jobListingId=3303356588',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=723&ao=14295&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_244972e2&cb=1571514831752&jobListingId=3313766242',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=725&ao=735724&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d593c208&cb=1571514831757&jobListingId=3389453304',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=726&ao=718045&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_42023501&cb=1571514831760&jobListingId=3344980725',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=727&ao=658826&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e5f880fe&cb=1571514831763&jobListingId=3003355143',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=728&ao=653445&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_300341c9&cb=1571514831766&jobListingId=3389861910',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=729&ao=650587&s=58&guid=0000016de5954fa795d6179c9627893d&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1bc724a8&cb=1571514831769&jobListingId=3386085524',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=801&ao=658828&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_af5cc840&cb=1571514847359&jobListingId=3358869187',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=802&ao=792898&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c2e41ed9&cb=1571514847363&jobListingId=3380915652',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=803&ao=85058&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1d96be5d&cb=1571514847366&jobListingId=3371180768',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=804&ao=132977&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9fd3b5b1&cb=1571514847369&jobListingId=3382362638',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=805&ao=786227&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7aaba9d7&cb=1571514847372&jobListingId=3390808593',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=806&ao=705947&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fa0b15f1&cb=1571514847375&jobListingId=3377834680',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=807&ao=74280&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f8faf185&cb=1571514847378&jobListingId=3374510907',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=809&ao=663856&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_caddaa91&cb=1571514847384&jobListingId=3349506201',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=810&ao=132959&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6a9e8067&cb=1571514847387&jobListingId=3341982548',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=811&ao=687173&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1b029799&cb=1571514847390&jobListingId=3198877873',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=812&ao=782827&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_44576973&cb=1571514847393&jobListingId=3390164202',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=814&ao=85058&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9ce57e9f&cb=1571514847398&jobListingId=3377323295',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=815&ao=721727&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_07bb6408&cb=1571514847401&jobListingId=3149414104',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=817&ao=85058&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4e2a7bae&cb=1571514847407&jobListingId=3385091901',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=818&ao=4341&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_b22ae844&cb=1571514847410&jobListingId=3388925956',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=819&ao=652636&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_8914d4ce&cb=1571514847413&jobListingId=3250260342',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=821&ao=740682&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_397606d2&cb=1571514847419&jobListingId=3302841072',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=823&ao=652636&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_70287518&cb=1571514847425&jobListingId=3232196825',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=824&ao=695667&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d249dfb7&cb=1571514847428&jobListingId=3121977624',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=825&ao=792108&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f56038ba&cb=1571514847430&jobListingId=3361435662',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=826&ao=674844&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fafdc3a1&cb=1571514847433&jobListingId=3261215240',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=827&ao=654219&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_d702f93a&cb=1571514847436&jobListingId=3316421998',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=828&ao=796639&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_287fc04e&cb=1571514847439&jobListingId=3338590194',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=829&ao=778338&s=58&guid=0000016de5958e5290b32e7d71787000&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_0e21fab3&cb=1571514847442&jobListingId=3343098374',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=902&ao=8095&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_cf511585&cb=1571514867581&jobListingId=3219451475',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=912&ao=182894&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4f98ceb6&cb=1571514867610&jobListingId=3388704069',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=913&ao=8095&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_25488df3&cb=1571514867613&jobListingId=3368462259',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=918&ao=37049&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6db3405c&cb=1571514867628&jobListingId=3353360125',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=919&ao=799375&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_26fd9aa6&cb=1571514867631&jobListingId=3301576190',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=921&ao=321255&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e852a8be&cb=1571514867637&jobListingId=3390192157',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=924&ao=4341&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_dd9a59f7&cb=1571514867646&jobListingId=3318447073',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=927&ao=132945&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5f08c6eb&cb=1571514867655&jobListingId=3241881621',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=929&ao=4375&s=58&guid=0000016de595dc9f853475de0f4729ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_71cb85bb&cb=1571514867661&jobListingId=3315767058',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1002&ao=187657&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2a9781df&cb=1571514882831&jobListingId=3359440335',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1003&ao=8095&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5f0fb657&cb=1571514882834&jobListingId=3091311554',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1005&ao=132920&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_23e98299&cb=1571514882840&jobListingId=3346194713',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1007&ao=85058&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2fa37cbb&cb=1571514882846&jobListingId=3380429816',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1008&ao=4341&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_0f48c7e3&cb=1571514882849&jobListingId=2883363101',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1010&ao=215203&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7f10ddd7&cb=1571514882855&jobListingId=3349719006',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1011&ao=182894&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_663c5076&cb=1571514882859&jobListingId=3388728004',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1019&ao=664018&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6338eb48&cb=1571514882881&jobListingId=3303491640',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1021&ao=133043&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_18d8c547&cb=1571514882887&jobListingId=3380089023',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1025&ao=133043&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0949c3d6&cb=1571514882899&jobListingId=3289048728',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1030&ao=215203&s=58&guid=0000016de596176486c5681b33637594&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cc561eb0&cb=1571514882913&jobListingId=3188899572',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1102&ao=4128&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_29bd87d5&cb=1571514896052&jobListingId=3355355556',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1104&ao=133043&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8dc3e106&cb=1571514896058&jobListingId=3359281012',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1107&ao=148364&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6c2c5040&cb=1571514896067&jobListingId=3361307072',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1109&ao=653260&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_83395636&cb=1571514896073&jobListingId=3242917002',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1111&ao=182894&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_720ef1d1&cb=1571514896079&jobListingId=3388683899',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1112&ao=135718&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7b1f067c&cb=1571514896082&jobListingId=3377562203',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1119&ao=4321&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_e780c373&cb=1571514896103&jobListingId=3318316541',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1120&ao=85058&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bd0c4bc0&cb=1571514896106&jobListingId=3390403255',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1121&ao=187657&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3fbcb34b&cb=1571514896109&jobListingId=3326922684',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1122&ao=8095&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fd959fa2&cb=1571514896112&jobListingId=3245584756',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1123&ao=37049&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cf781b06&cb=1571514896115&jobListingId=3307084492',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1128&ao=120626&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fc83a2bc&cb=1571514896135&jobListingId=3382779017',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1130&ao=454826&s=58&guid=0000016de5964bbc92173c0095321332&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_59bf4610&cb=1571514896141&jobListingId=3382698741',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1201&ao=771118&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a84bcc9f&cb=1571514911566&jobListingId=3385088517',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1202&ao=85058&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f499796b&cb=1571514911570&jobListingId=3371456396',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1203&ao=133279&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_be6fcdd3&cb=1571514911573&jobListingId=3307218779',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1204&ao=735718&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4714e6b9&cb=1571514911579&jobListingId=3389454472',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1210&ao=3839&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ad69667f&cb=1571514911596&jobListingId=3193822281',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1213&ao=4120&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_61ae2585&cb=1571514911605&jobListingId=3360837707',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1216&ao=182894&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_93897e61&cb=1571514911614&jobListingId=3388784064',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1218&ao=8095&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_8e4a1d8b&cb=1571514911620&jobListingId=3387937743',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1219&ao=37049&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_03af1ea1&cb=1571514911623&jobListingId=3391743602',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1225&ao=763249&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_83eac96e&cb=1571514911640&jobListingId=3389072701',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1227&ao=187657&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0defa1d1&cb=1571514911646&jobListingId=3371964899',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1228&ao=4470&s=58&guid=0000016de596887ea75fd60954282b38&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cd0e9860&cb=1571514911649&jobListingId=3176152088',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1301&ao=764280&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b8eecd63&cb=1571514925879&jobListingId=3372618360',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1302&ao=352789&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3b795ec7&cb=1571514925883&jobListingId=3344316315',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1303&ao=215203&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2d7c9313&cb=1571514925887&jobListingId=3316841130',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1305&ao=454826&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4217f451&cb=1571514925893&jobListingId=3384962076',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1306&ao=457223&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_43b621e1&cb=1571514925896&jobListingId=3314346296',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1310&ao=37049&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b544ba31&cb=1571514925909&jobListingId=3390174271',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1311&ao=178335&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6a81c237&cb=1571514925912&jobListingId=3332979497',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1312&ao=670288&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_41971360&cb=1571514925915&jobListingId=3127512802',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1313&ao=8095&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_109f8ff5&cb=1571514925918&jobListingId=3241479497',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1314&ao=242900&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d40bb387&cb=1571514925921&jobListingId=3205805597',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1316&ao=14295&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9ad3e6cf&cb=1571514925927&jobListingId=3239678914',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1317&ao=133142&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8b9f7eaf&cb=1571514925929&jobListingId=3387972106',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1320&ao=148364&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_b181cbb8&cb=1571514925942&jobListingId=3380924781',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1321&ao=352789&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4a77c579&cb=1571514925945&jobListingId=3380038893',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1324&ao=4120&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_98d887c5&cb=1571514925954&jobListingId=3391225784',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1327&ao=133043&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6fa1ddc4&cb=1571514925963&jobListingId=3281446074',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1329&ao=133279&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_fbd0f014&cb=1571514925969&jobListingId=3307218778',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1330&ao=389273&s=58&guid=0000016de596bfcb9d86622395fbc287&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_60b49c81&cb=1571514925972&jobListingId=3375453305',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1401&ao=133266&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_a1eedc05&cb=1571514940766&jobListingId=3225543594',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1403&ao=786622&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_31a8c8fb&cb=1571514940772&jobListingId=3379500410',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1404&ao=720147&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8860fba9&cb=1571514940775&jobListingId=3359153199',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1405&ao=37049&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_94aa60cd&cb=1571514940778&jobListingId=3364781986',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1407&ao=133266&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8b85f0c5&cb=1571514940784&jobListingId=3391743583',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1408&ao=735722&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e59a6f38&cb=1571514940787&jobListingId=3378458028',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1409&ao=182894&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_eef74949&cb=1571514940791&jobListingId=3388809191',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1410&ao=8095&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_42b91378&cb=1571514940793&jobListingId=3131307964',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1411&ao=653260&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_aa354b93&cb=1571514940797&jobListingId=3242917004',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1412&ao=3839&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_4aa15c1e&cb=1571514940800&jobListingId=3216482166',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1413&ao=148364&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_beb632ff&cb=1571514940803&jobListingId=3284788525',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1414&ao=37049&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ce9dc240&cb=1571514940806&jobListingId=3389465996',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1415&ao=400784&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e77ce60b&cb=1571514940808&jobListingId=3275441294',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1416&ao=4120&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bdf487e7&cb=1571514940812&jobListingId=3385899396',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1418&ao=191997&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_38b4d17a&cb=1571514940817&jobListingId=3332629132',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1419&ao=133105&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_58551456&cb=1571514940820&jobListingId=3374588528',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1420&ao=133647&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_69686088&cb=1571514940823&jobListingId=3391569956',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1422&ao=4120&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d96cdb65&cb=1571514940830&jobListingId=3330025890',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1423&ao=133838&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d4e4cb06&cb=1571514940832&jobListingId=3391734415',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1424&ao=374003&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5258d154&cb=1571514940835&jobListingId=3365552325',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1426&ao=4134&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d95f1bf0&cb=1571514940842&jobListingId=3386308459',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1429&ao=133036&s=58&guid=0000016de596fad795195cbfbfd49ea9&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_75238b73&cb=1571514940851&jobListingId=3390932691',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1501&ao=798301&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5dbfb45a&cb=1571514954382&jobListingId=3244063645',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1502&ao=4120&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_26ca2e21&cb=1571514954385&jobListingId=3389647362',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1504&ao=4128&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0973364b&cb=1571514954391&jobListingId=3304584449',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1505&ao=136020&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_463a53e9&cb=1571514954396&jobListingId=3307136055',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1509&ao=4120&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_1c60efb4&cb=1571514954410&jobListingId=3302351543',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1510&ao=636124&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_108a16cc&cb=1571514954413&jobListingId=3353054982',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1511&ao=4348&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_5134ca06&cb=1571514954416&jobListingId=3292996965',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1512&ao=4120&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_91a13b85&cb=1571514954420&jobListingId=3270433241',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1513&ao=4008&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e9ac08b3&cb=1571514954423&jobListingId=3216423001',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1514&ao=74183&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_8d305c58&cb=1571514954426&jobListingId=3366656533',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1515&ao=85058&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_bf2a106f&cb=1571514954430&jobListingId=3371400250',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1517&ao=134571&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_98a87cdb&cb=1571514954436&jobListingId=3358012561',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1518&ao=356134&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_76c61053&cb=1571514954440&jobListingId=3333213051',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1521&ao=3839&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f1fb0ed4&cb=1571514954451&jobListingId=3315619807',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1523&ao=456220&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0b902fea&cb=1571514954458&jobListingId=3374321497',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1524&ao=654220&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_ea5bc903&cb=1571514954462&jobListingId=3296286018',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1526&ao=588939&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3143d422&cb=1571514954469&jobListingId=3372649424',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1527&ao=4120&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_93ec541c&cb=1571514954472&jobListingId=3123618711',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1529&ao=133055&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f8e815b8&cb=1571514954478&jobListingId=3382490177',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1530&ao=358687&s=58&guid=0000016de5972ee4aaf64babc11490ee&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_a5f96468&cb=1571514954482&jobListingId=3252920071',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1601&ao=7438&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_aa9bd1c8&cb=1571514969296&jobListingId=3293271462',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1604&ao=783177&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_70cb7019&cb=1571514969306&jobListingId=3331640007',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1605&ao=735722&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_47d1fcdc&cb=1571514969309&jobListingId=3387962862',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1606&ao=735718&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d2fa1ef4&cb=1571514969312&jobListingId=3370847809',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1607&ao=4120&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_94488383&cb=1571514969316&jobListingId=3111176014',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1608&ao=704440&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_54cc8cfa&cb=1571514969319&jobListingId=3300510911',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1609&ao=352789&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c06aac59&cb=1571514969322&jobListingId=3390893169',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1610&ao=85058&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_62d78482&cb=1571514969325&jobListingId=3367704241',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1611&ao=140152&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ec7f906a&cb=1571514969328&jobListingId=3383682931',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1613&ao=14295&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_da68cc30&cb=1571514969335&jobListingId=3377837817',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1614&ao=725519&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_19957ab8&cb=1571514969338&jobListingId=3289138501',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1616&ao=85058&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_761b4039&cb=1571514969344&jobListingId=3372864281',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1619&ao=133022&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_1f02ed4c&cb=1571514969358&jobListingId=3381492602',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1620&ao=152916&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_bd1738e2&cb=1571514969361&jobListingId=3366284500',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1621&ao=243879&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3c95bd80&cb=1571514969365&jobListingId=3341698187',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1624&ao=85058&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0257cb31&cb=1571514969374&jobListingId=3391741103',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1625&ao=771118&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_6d4d14bc&cb=1571514969378&jobListingId=3385087325',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1626&ao=322930&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_48f3e52e&cb=1571514969381&jobListingId=3234644718',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1627&ao=243878&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_407a9b20&cb=1571514969385&jobListingId=3356086361',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1629&ao=85058&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_7727d365&cb=1571514969392&jobListingId=3391263157',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=1630&ao=185086&s=58&guid=0000016de597679e8aefe8e7a29246df&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_9d4639e4&cb=1571514969395&jobListingId=3378034865',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=484493&s=149&guid=0000016de59855dab286e7a21db61d66&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9e642d23&cb=1571515029942&jobListingId=3280822180',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=396821&s=149&guid=0000016de59855dab286e7a21db61d66&src=GD_JOB_AD&t=SRFJ&extid=4&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_58f122b8&cb=1571515029946&jobListingId=3332473060',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=101&ao=687526&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3b27beb3&cb=1571515029949&jobListingId=3347992370',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=102&ao=687520&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_0c1e0b28&cb=1571515029952&jobListingId=3203961865',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=103&ao=674073&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_f15bc0ca&cb=1571515029955&jobListingId=3021813740',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=104&ao=657995&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_811ece4c&cb=1571515029958&jobListingId=3326081261',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=105&ao=484493&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d7fdde3e&cb=1571515029961&jobListingId=3391476518',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=106&ao=669768&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_7246f4e6&cb=1571515029964&jobListingId=3368518290',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=107&ao=14295&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_226a45c5&cb=1571515029967&jobListingId=3331260098',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=108&ao=140152&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_9378d003&cb=1571515029970&jobListingId=3314488459',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=109&ao=442378&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_cf782b9f&cb=1571515029974&jobListingId=3318844170',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=111&ao=372631&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_5083d50d&cb=1571515029979&jobListingId=3329734202',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=112&ao=8095&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_242d5dfd&cb=1571515029982&jobListingId=3036714790',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=113&ao=4341&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&ea=1&cs=1_8f1c9f48&cb=1571515029985&jobListingId=3358158942',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=114&ao=148364&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_c70be464&cb=1571515029988&jobListingId=3310769855',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=115&ao=352789&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2b9bd5b7&cb=1571515029991&jobListingId=3348275855',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=116&ao=256754&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2289fd29&cb=1571515029995&jobListingId=3288809116',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=117&ao=135079&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_054311cd&cb=1571515029998&jobListingId=3296732312',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=118&ao=484493&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_d3599508&cb=1571515030001&jobListingId=3280822365',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=119&ao=596254&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_84b17c94&cb=1571515030004&jobListingId=3318422270',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=120&ao=454826&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_2c4e634a&cb=1571515030006&jobListingId=3379784823',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=121&ao=148364&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_f7125964&cb=1571515030009&jobListingId=3280040412',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=122&ao=215203&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_ad64e3e6&cb=1571515030012&jobListingId=3387930147',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=123&ao=8095&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_58487125&cb=1571515030015&jobListingId=3375232853',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=124&ao=8095&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_21723923&cb=1571515030018&jobListingId=3361810789',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=125&ao=763249&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_e9e28ea0&cb=1571515030021&jobListingId=3388349947',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=126&ao=85058&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_775ecf46&cb=1571515030024&jobListingId=3361169736',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=127&ao=135079&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_3a259af4&cb=1571515030027&jobListingId=3146950754',\n",
       "  'https://www.glassdoor.com/partner/jobListing.htm?pos=128&ao=6101&s=58&guid=0000016de59855da9a2f9fc34270d041&src=GD_JOB_AD&t=SR&extid=1&exst=OL&ist=&ast=OL&vt=w&slr=true&cs=1_24f9fe92&cb=1571515030030&jobListingId=3361635360'],\n",
       " 'description': ['Position Summary:\\n\\n\\nThe Data Engineer will be a key player in Seniorlink’s Product & Technologies Organization (P&T). You will be the primary person building out the data integration framework capability within the ‘DataHub’ architecture. The DataHub uses a modern cloud based distributed computing architecture that utilizes modern technologies such as AWS S3, AWS EMR, Apache Spark, Redshift etc. You will work on the foundational infrastructure components within this architecture to expand the capability and make the system more robust. You will collaborate with senior engineers to design and engineer a robust, scalable integration framework that can transform the variety of incoming data formats from our customers. You will provide post-production monitoring and support for the components you build to ensure the system performs to specification. The ideal candidate will have a passion for the data domain, a strong background in data engineering, having designed and implemented data pipelines that can handle complex large scale data transformations.\\n\\nWhat You Will Do:\\nDevelop a sound understanding of the architecture already in place\\nDesign robust, scalable and secure data processing systems within the evolving data infrastructure of Seniorlink’s products and solutions\\nBuild frameworks, tooling, pipelines that address the partner data integration requirements\\nUse agile/scrum methodology to ensure sprint commitments are met regularly\\nPerform pull reports (PR) for other engineers\\nTroubleshoot processes in production to fix functional bugs or performance issues\\nPerform other duties as assigned\\nWhat You Will Bring:\\n\\n\\nBachelor’s Degree in Computer Science or related field\\n2+ years’ of experience doing data engineering work\\n1+ years’ experience in newer data lake type architectures\\nSolid understanding of algorithms, data structures, and data modeling (SQL a plus)\\nProficient with at least one of following programming languages - Python, Java, Scala\\nDemonstrated experience using ETL tools for a data pipeline\\nExperience working with cloud computing services\\nExperience working with Spark for large-scale data processing is a major plus\\nAbout Us:\\n\\n\\nHEALTH CARE SOLUTIONS THAT BLEND HUMAN TOUCH WITH TECHNOLOGY …\\n\\nSeniorlink is a tech-enabled health services company that builds care solutions to support family caregivers caring for loved ones at home. With nearly 20 years of care management expertise, Seniorlink blends experienced care coaches, proven protocols and an innovative app, to provide family health care solutions. The company’s commitment to and engagement of family caregivers paired with its reputation for delivering quality care, provides families with a high-touch, low cost alternative to facility-based care.',\n",
       "  'Ready to help us transform healthcare? Bring your true colors to blue.\\nWe need Data Engineer for our newly established Big Data engineering practice. While primary focus of this role is to work in multiple Pods alongside with other engineers to build & deliver data integration pipeline. Other element of the role is to work in conjunction with Senior Data Engineer to contribute towards the delivery of Data Platform. This is hands on position with 80% focus towards data engineering and 20% towards best practice, unit testing and reviewing code.\\n\\nTechnical Delivery Experience:\\n\\n· Self-learner and networking skills that are required to be successful in this role given the rapid evolution of the New Platforms and Tools.\\n\\n· Experience indeveloping new applications and features within a scrum team providing\\n\\ndata and data services to the enterprise, other engineering teams, data science, analysts, product, management/executives, and other business teams\\n\\n· Experience in building high performing and scalable data ingestion patterns to support multiple batch processing and real-time data streaming.\\n\\n· Work with various project teams to deliver on commitments within time and scope\\n\\n· Work with Platform Operations team to ensure solutions are releasable, maintainable, and scalable\\n\\n· Experience or a good understanding and willingness to working in DevSecOps and Agile Scrum based Software Development.\\n\\n· Experience in developing scalable data engineering platform using cloud infrastructure.\\n\\n· Experience with configuration management, continuous integration, continuous deployment and continues security.\\n\\n· Experience or a good understanding and willingness to include Security at design, development and testing\\n\\n· Familiarity with container tools and serverless technologies.\\n\\n· Understanding of servers, storage, and networking roles\\n\\n· Experience with Lean, Agile and iterative delivery frameworks, metrics and methodologies.\\n\\n· Experience in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\n\\n· Strong analytic skills related to working with unstructured datasets.\\n\\n· Experience in building processes for supporting data transformation, data structures, metadata, dependency and workload management.\\n\\nLearning Experience:\\n\\n· Experience or a good understanding and willingness to commit to and practice agility and speed to deliver using agile, sprint-based working style.\\n\\n· Experience in ensuring the right level of Communication to keep all stakeholders, team members and users in sync and on the same page\\n\\n· Ability to be open to learn, to give and take feedback, to put team above own interest, to excel in team success and to strive for excellence\\n\\nLeadership Experience:\\nGreat communication skills coupled with a strong desire for personal development and learning\\nStrong verbal and written communication skills\\nSkills to Establish and Maintain Delivery Based Business and Peer relationship\\nExperience in managing Service Providers and Holding them accountable\\nPartnership Experience:\\n\\n· Experience in establishing effective partnerships across teams, peers and subordinates in an inclusive style to leverage their abilities and knowledge.\\n\\n· Ability to Align with Leadership Guidance and Provide recommendations on engagement, measurements to maximize collaboration, eliminating handoffs and high paced agile delivery environment\\n\\nEducation:\\n\\nBachelor’s degree in a field linked to data engineering, business analytics, applied mathematics, computer science, IT, computer applications, engineering or related field is required.\\n\\nTechnical Skills Required:\\n3+ years of experience in a Data Engineer role, experience using the following software/tools:\\nExperience with big data tools: Hadoop, Spark, Kafka, etc.\\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\\nExperience with AWS cloud services\\nExperience with stream-processing systems: Storm, Spark-Streaming, etc.\\nExperience in object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\\nExperience with Testing Driven Development process (TDD)\\nExperience with conducting code reviews.\\nLocation\\nBoston\\nTime Type\\nFull time',\n",
       "  'POSITION: DATA ENGINEER\\n\\nABOUT COTA\\nCOTA was founded by doctors, engineers, and data scientists to create clarity from fragmented and often-inaccessible real-world data. By using our proprietary technology, advanced analytics and deep expertise to organize complex data, we provide a comprehensive picture of cancer that can be used to advance care and research. We believe that everyone touched by cancer deserves a clear path to care. Together, we can make that vision a reality. We’re searching for smart, motivated people who share our passion for bringing clarity to cancer. Connect with us, introduce yourself, and apply to one of our current openings.\\n\\nOVERVIEW\\nCota is looking for a Data Engineer to help us architect how we ingest, store, and query data across the Cota platform. You’ll be making pivotal decisions on how we manage our data at scale, and how we make it available to multiple engineers and analysts across the organization. Additionally, you will be an individual contributor following through on those design decisions.\\n\\nOur data is primarily medical data, so any experience with EMR systems and other HIPAA practices will be a strong plus. Over time, we expect you to become an expert on the data model and be able to translate requests from other departments into specific data queries/reports.\\n\\nOur ideal candidate will have experience managing multiple streams of data, and have strong experience and opinions on the ETL process we should follow, where to use relational databases, where to use services like BigQuery/Redshift, and ElasticSearch.\\n\\nABOUT ENGINEERING AT COTA\\nWe help others make and understand connections in a complex web of data. On one end of the spectrum, we ingest large amounts of heterogeneous data, ranging from multi-GB exports from large data providers to individual facts extracted by abstractors from scanned paper records. On the other end of the spectrum, we serve information derived from the process data to consumers like doctors and other healthcare decision makers who draw meaningful conclusions about the current state of their patients.\\n\\nAs an engineering team, we build robust scalable platforms and products that help further the business. Being data-driven, and entering the realm of Fast Data, we solve interesting technical challenges on a daily basis. This is an opportunity to join a company embracing a modern Technology culture, where you will be able to impact many lives and have a major impact on COTA’s offerings.\\n\\nRESPONSIBILITIES\\nDevelop and maintain various data ETL processes and the data warehouse\\nImplement quality monitoring to report on the accuracy and relevancy of processed data\\nSupport data analytics requests for bespoke reports and data exports\\nUnderstand the available architectures and technologies, assess the available options\\nCatalog our overall schema design and reference data\\nAs part of a team, own data-centric processes, develop alerts for errors and service issues, and respond to alerts\\nContribute, shape, improve the way we use and manage infrastructure\\nPerform specialized data investigations to support analytics and custom reporting scenarios\\nParticipate in code reviews with a goal of understanding the overall data pipeline and ensuring data quality\\n\\nREQUIREMENTS\\nHolds a Bachelor’s degree in Computer Science, Information Systems, or related major, or equivalent work experience\\nAble to write well-documented, reusable, and testable code\\nStrong working knowledge of Postgres or other relational databases\\nAbility to write complex SQL queries for ETL or reporting\\nExperience with R or Pandas a plus\\nProficiency in distributed version control systems such as git or mercurial (we use git).\\nProficiency working as part of an Agile development team\\nExperience with workflow/scheduling frameworks a plus.\\nDesign experience a plus\\nAbility to interact and communicate effectively with colleagues on requirements and set expectations accordingly\\nAbility to work independently as well as with a team\\nExcellent written and oral communications\\nEnergetic and self-starting\\n\\nAt Cota, we are passionate about creating an inclusive workplace that celebrates and values diversity with the belief that it drives our innovation. Our commitment to diversity and inclusion is a guiding principle on how we build teams and develop leaders. As part of our commitment to building a respectful culture that encourages, develops and celebrates different backgrounds, experiences, abilities and perspectives all qualified applicants will receive consideration for employment without regard to race, color, religion, culture, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, veteran status or other applicable legally protected characteristics. All employment decisions, including decisions to hire and promote, will be based on merit, competence, business need and performance.\\n\\nWe are a proud equal opportunity employer.',\n",
       "  'Indigo improves grower profitability, environmental sustainability, and consumer health through the use of natural microbiology and digital technologies. Utilizing beneficial plant microbes and agronomic insights, Indigo works with growers to sustainably produce high quality harvests. The company then connects growers and buyers directly to bring these harvests to market. Working across the supply chain, Indigo is forwarding its mission of harnessing nature to help farmers sustainably feed the planet. The company is headquartered in Boston, MA, with additional offices in Memphis, TN, Research Triangle Park, NC, Sydney, Australia, Buenos Aires, Argentina, and São Paulo, Brazil. http://www.indigoag.com/\\n\\nThe Data Intelligence team at Indigo captures, integrates, and visualizes complex scientific datasets to speed R+D and product development at Indigo. This requires data engineers to effectively enable support for R+D data processes. We are looking for someone with experience in handling, integrating and visualizing complex data sets. The candidate should also have experience in implementing quality control processes for experimental data including developing and training on tools for evaluating key performance indicators for data generating processes in the company. The ideal candidate will have experience with and strong interest in scientific data and empathy for customers working in R+D functions.\\n\\nResponsibilities:\\nReview and understand all existing R&D databases and data pipeline needs, and create prioritized map of needed pipelines to build and improve\\nCreate prioritized list of needed data dashboarding apps, in collaboration with head of data intelligence; plan implementation\\nHarden and update existing code into reusable pipelines and tools for data intelligence team\\nEnsure smooth operation of all R&D data pipelines\\nSupport prototyping of new visuals, data dashboarding and upload tools & apps, and data analytic pipelines\\nContribute to long-term Data Intelligence strategy development\\nCompetencies:\\nProficient in building and supporting ETL and data analytic pipelines\\nAble to build intuitive visualizations (preferably using dashboard/app development tools such as R Shiny, Dash, Flask)\\nSuperior communication skills; able to understand user needs and requirements\\nAble to prioritize conflicting demands and understand user needs\\nInteracts well with engineering and product management\\nWorks well in cross-functional environments with frequent context switching and can keep cross-functional projects moving\\nPassion for concise and thorough documentation\\nComfortable with ambiguity and challenging company goals\\nAble to work independently and drive to solutions with minimal direction\\nQualifications:\\n5+ years experience with data engineering, data management, data operations or analytics OR equivalent combination of relevant masters/PHD + industry experience\\nBroad science or industry experience preferred (data science, tech, life sciences, etc.); experience working with scientific research data or engineering process development data\\nFluency in Python, SQL; experience in R is a plus\\nProfessional experience with github and AWS\\nExperience working with agile software development teams\\nExperience working in fast-paced, quickly pivoting environment\\nIndigo is committed to living our values, specifically \"creating a work environment where everyone feels respected, connected, and has opportunities to learn and grow.\" As part of living our values, we strive to create a diverse and inclusive work environment where everyone feels they can be themselves and has an equal opportunity of succeeding.',\n",
       "  \"Meet CarGurus—the #1 visited online car shopping website in the US. At CarGurus, we're building the world's most trusted and transparent automotive marketplace where it's easy to find great deals from top-rated dealers.\\n\\nFounded in 2006 by Langley Steinert (co-founder of TripAdvisor), CarGurus is a technology company with a passion for data and its power to simplify every aspect of the car shopping experience. Using proprietary technology, search algorithms and innovative data analytics, we provide unbiased validation on pricing, dealer reputation and vehicle history.\\n\\nAre you someone who loves to solve problems by bringing data to the table? Are you passionate about writing SQL? Do you like taking creative approaches to problem solving in a collaborative environment? We need your help!\\n\\nCarGurus is looking to hire a highly motivated Data Applications Engineer to help shape our growing data infrastructure. You will work closely and build relationships with analysts and leadership in all areas of the organization. Become an expert in building data transformations and visualizations and turn data into insight. At CarGurus, we are data-driven. Help us realize this goal by democratizing our data.\\n\\nWhat You'll Do\\nSupport our efforts to develop and implement a single source of truth for our metrics.\\nWork with stakeholders across all levels of the Sales and Marketing organizations to translate business analysis requirements into logical data models and visualizations using our analytics platform.\\nProvide excellent customer service as an internal consultant.\\nAdminister and support analytics tools.\\nWho You Are:\\n2-5 years experience in data modeling, data engineering, business intelligence, or other quantitative field.\\nTeam player who thrives in a collaborative environment with strong interpersonal skills.\\nCreative thinker, with an interest in solving business problems with data.\\nKnack for telling a compelling story with data visualizations.\\nExperience in SQL with ability to design and validate complex queries.\\nFamiliar with Looker, Tableau, or other business intelligence and data visualization tools.\\nFamiliarity with Redshift, Snowflake, BigQuery, or other large-scale databases a plus.\\nSoftware engineering skills in Python a plus, but not required.\\nFamiliarity with Google Analytics a plus, but not required.\\nWe have a great respect for testing and learning and a healthy aversion to scheduling meetings to discuss meetings. Lunch is catered daily. Gym membership is free. Foosball and ping pong are played often. Now a publicly-traded company, we're as committed as ever to cultivating the culture that got us here.\\n\\nIn addition to the US, CarGurus operates sites in Canada, the UK and Germany with other markets on the horizon. Our offices are located in Cambridge, MA, Detroit, MI and Dublin, Ireland. If you'd like to learn more, please visit our careers page.\",\n",
       "  'Do you like working with big data? Are you passionate about using Artificial Intelligence, Machine Learning and Deep Learning to influence product & business decisions? Do you enjoy helping customers in building solutions leveraging the state-of-the-art AI/ML/DL tools? If Yes, we want to talk to you.\\n\\nDigilant is seeking a Data Engineer passionate about using Machine Learning & Data Science to help design and build analytics and optimization tools to support ongoing delivery of key insights to drive business growth and overall impact. The most exciting part about working at Digilant is the enormous potential for personal and professional growth. You will be part of a small but growing team and help shape the way we develop, deploy, and operate production quality analytics systems and processes and have an impact on how Digilant uses data in the years to come.\\n\\nThis is a full time position based in US and you will work from our offices in New York, Boston or work remotely.\\n\\nWhat you’ll be doing:\\n\\nHave a lot of fun:\\nCollecting, processing and cleansing data from a wide variety of sources.\\nExploring datasets in notebooks, revealing trends and patterns, communicating insights to business users.\\nDeveloping and prototyping predictive models.\\nBuilding, maintaining and owning scalable data pipelines to support our platform and business needs.\\nWorking with Analytics to understand and leverage our data assets to solve client problems and needs.\\nBeing a team player, and bring the team and company forward by solving team and company priorities.\\nTechnical Qualifications:\\n2+ years experience working with data\\nExperienced in writing readable, re-usable code SQL and Python\\nExperienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark\\nProficiency in linux\\nSelf-driven, with a hunger to learn and spread knowledge by teaching others\\nExcellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders\\nBonus points:\\nWorked as a data scientist.\\nProduction experience with machine learning and deep learning models.\\nSolid grounding in statistics, probability theory, data modeling, machine learning algorithms and software development techniques and languages used to implement analytics solutions.\\nExperience with data modeling and Big Data solution stacks.\\nExperience with one or more deep learning frameworks.\\nKnowledge of digital marketing & programmatic space.\\nTechnologies we use:\\nPython\\nDjango\\ndocker\\nAWS\\nPostgres & Redshift\\nApache Spark & Hadoop ecosystem\\nWhat you’ll get:\\nCompetitive salary commensurate with experience and bonus opportunities.\\nDigilant offers an excellent benefits package including 401K with a matching contribution, Medical/Dental/Vision effective Day 1, Group Life Insurance, AD&D, Long and Short-‐Term Disability, Flexible Spending Accounts, Pre-‐tax Commuter Benefit Programs and an Uncapped Paid Time Off Policy!\\nTo apply for this job email your details to eng-jobs@digilant.com',\n",
       "  'Data Engineer\\n\\nCluj NapocaFull-time\\n3Pillar Global is looking for a Data Engineer. The SQL Engineer will drive complex ETL development processes and be part of a high-performing team.\\n\\nWe are 3PILLAR GLOBAL\\n\\n3Pillar Global is a global product development services company. We drive innovation and disruption for our customers. We are a catalyst for the digital economy, building software products for sector-leading clients . We are in the business of producing measurable results for our clients by driving and sustaining product development partnerships that deliver revenue generating and industry changing products to market. We believe that to achieve long term client success, we must help our clients innovate to remain leaders in their markets.\\nREQUIREMENTS\\nData development experience\\nData warehousing experience\\nVery good knowledge of ETL processes and development\\nStrong SQL experience\\nBENEFITS\\nFull employee labor contract\\nOpportunity to work on challenging projects (education/finance/healthcare/media) using the latest technologies\\nYearly performance bonus\\nMonthly Spot bonus\\nReferral bonuses\\nCompany supports exams and certifications for professional growth\\nInternal Trainings\\nTechnical Events\\nMedical package\\nChair massage\\nTransportation allowance\\nGym membership allowance\\nSoccer and basketball subscription\\nPluralsight subscription\\nInternal library\\nBookster subscription\\nUnlimited work from home\\n\\nWe take data privacy very seriously and we want to make sure you understand and agree to the way we collect, store and use your personal data.\\nBy filling in and submitting your application you provide us with your personal data.\\nWe collect your data in order to process your job application.\\nWe typically process and store your data for the duration of 10 years or until you withdraw your consent.\\nLearn more about our Privacy Policy and your Data Rights Here',\n",
       "  'POSITION SUMMARY:\\n\\nThe Data Engineer is a highly motivated and experienced individual who enjoys working in a dynamic multi-disciplinary team. This individual is instrumental in the management and processing of molecular and clinical data. The Data Engineer has a solid ETL background and is strongly goal-oriented with a focus on real-world impact.\\n\\nESSENTIAL DUTIES AND RESPONSIBILITIES:\\nInteract with internal and external collaborators to understand and deploy solutions that ensure data safety, integrity and availability.\\nWork with the Analytics team in the design, development, and execution of intelligent ETL pipelines.\\nResponsible for the harmonization of internal and external datasets towards the requirements of BERG AI platform.\\nResponsible to generate reports that allow for the evaluation of data quality and reliability of processing algorithms.\\nWork with IT on defining data-based solutions for upcoming projects.\\nRecommend new policies for data collection and documentation.\\nOversee compliance with the data-related policies.\\nDevelop and write technical documentation for internal and external collaborators.\\nProvide guidance and training to Berg staff as necessary.\\nOther duties as assigned.\\nREQUIREMENTS:\\nRequires an MS in Information Technology, Computer Sciences, Bioinformatics or a related field with 2+ years’ experience in data management in healthcare-related fields or research setting.\\nProficiency in R is required, and in Python is a plus.\\nExperience with SQL is required. Experience with non-relational databases is a plus.\\nExperience in Linux and Big Data technologies is preferred.\\nProven ability to demonstrate attention to detail and record-keeping.\\nQuick learner, extremely flexible and adaptable to the needs of internal collaborators in a dynamic environment.\\nAbility to efficiently work on multiple projects.\\nAbility to work independently, with minor supervision.\\nExcellent communication and interpersonal skills.\\nMust be able to work in a team-oriented environment.',\n",
       "  \"My direct client, one of Boston's top gaming studios, is building out their Data Engineering team tasked with creating automations to solve their largest Big Data problems in relation to customer data and the recent California Consumer Privacy Act.\\n\\nWe are currently building out a team of mid-level and senior engineers, which will focus on hands-on engineering work in their enterprise AWS instance.\\n\\nTitle: Data Engineer\\n\\nDuration: 9 month contract with likely extension or conversion\\nCompensation: Up to $150/hr dependent upon experience; negotiable for senior candidates\\nLocation: REMOTE\\nStart Date: ASAP\\n\\nMy client in the gaming space is looking for a REMOTE Data Engineer to join their team.\\n\\nCandidates must have some understanding of 3 of the following 4 technologies:\\nRedshift\\nS3\\nAthena\\nParquet\\nExperience with the following would be a plus:\\nScala\\nPython\\nData architecture experience\\nPlease note that while we are able to work C2C, we cannot work with third party employers at this time.\",\n",
       "  'EF Tours helps students gain new perspectives and build skills for the future through experiential learning. As an accredited institution, we partner with educators across the world to create global education programs that blend classroom, digital, and experiential learning for students.\\n\\nThe Tours Engineering team @ EF Tours is the group of engineering and creative professionals empowering EF to send groups of students and educators on guided educational tours across the world. We power every step of our customer’s journey – from showcasing our tour offerings, to creating personalized itineraries, and collecting their feedback after the tour. Over the course of your career with Tours Engineering, you’ll have the opportunity to build beautiful front-end interactions for our customers, solve large scale data optimization problems, and get your hands on everything in between.\\n\\nWhat you’ll do:\\n\\nYou will be part of a cross-functional, autonomous data team, collaborating closely with analysts, data scientists, engineers and executive stakeholders. You will be deeply involved in building out a data lake that will serve as the new source of truth for our business. As part of that effort, you will leverage your mastery of Spark and ETL to transition work loads from our legacy relational data warehouse into modern, open-source data processing platforms.\\n\\nWho you are:\\n\\nTo succeed in this role, you’ll need excellent problem-solving skills, strong attention to detail, and the technical know-how to independently build solutions from start to finish. We are looking for a true data expert, someone who is comfortable working with and shaping datasets of varying latencies, size, and format.\\n\\nWe ask that you have:\\n5+ years of hands-on industry experience with Python and AWS\\n2+ years of proven ability developing ETL in Spark (PySpark preferred)\\nAdvanced SQL (ANSI SQL or Transact-SQL)\\nWorking knowledge of Data Lake patterns: partitioning, multi-step transformations, data cataloging\\nWorking knowledge of self-describing, compressed data file formats: Parquet, Avro\\nWorking knowledge of event streaming platforms: Kinesis, Kafka, Flink\\nWorking knowledge of Domain Driven Design (DDD) and event storming\\nExperience with AWS data processing services: EMR, Athena, Redshift\\nExperience with AWS serverless infrastructure: API Gateway, Lambda, DynamoDB, S3\\nExperience with NoSQL/non-relational databases, especially document stores\\nExperience building data models intended for data visualization solutions\\nDemonstrable experience implementing business logic into well structured data models that have been successfully applied to BI\\nExcellent verbal and written skills in order to effectively communicate business concepts to partner analysts and teams\\nA drive to learn the intricacies of the business, with a versatile and dynamic understanding of businesses you’ve worked with in the past\\nOther useful skills include:\\nExperience coding in Java or Scala\\nDocker or other containerization tooling\\nCI/CD exposure using git based deployment automation.\\nInfrastructure-as-code: Terraform, CloudFormation\\nExperience working on a SCRUM Team\\nExperience using global data catalogs for either end user reference or data automation\\nExperience with relational modeling, star schemas and Kimball Data Warehousing\\nLet’s talk about the Perks:\\n\\nWe believe that happy people do great work. What makes us happy? Things like:\\nThe opportunity to travel internationally\\nDiscounts on travel\\nThree weeks paid vacation for your first year and then four weeks after your first year\\nFree language classes, taught in-house\\nAccess to a brand-new fitness center and rock-climbing wall on the EF campus\\nA restaurant and bar located in 2 of our EF campus buildings\\nMarket-leading benefits package including top of the line health coverage, 401k with company match, tuition reimbursement, and more…\\nEF is the world’s largest international education company – providing life-changing educational experiences to create global citizens. We offer millions of people the opportunity to learn a language, travel abroad, experience another culture or earn an academic degree. Since 1965, EF’s mission has been opening the world through education. Our 52,000 employees have a worldwide presence in over 600 schools and offices within 50 countries.\\n\\nFor more information about our career opportunities, visit www.careers.ef.com',\n",
       "  'We are looking for Data Engineer for our client in Boston, MA\\nJob Title: Data Engineer\\nJob Location: Boston, MA\\nJob Type: Contract\\nW2 candidates are encouraged to apply. We are unable to sponsor H1b or work with third-party candidates at this time\\nJob Description:\\nThe client is looking for several accomplished data engineers to join their Advanced Data Analytics Team in their Asset Management organization.\\nThis role is ideal for someone with an enterprise development background, with a strong technology and coding skills, looking to operate in a less constrained environment, as part of an accelerated development team.\\n\\nThe Team:\\nAsset Management Technology (AMT) provides worldwide technology and support to all Investment Management, Research, Trading, and Investment Operations functions.\\nAMT is an integral partner for Asset Management to deliver creative, scalable, industry-leading investment tools that enable Asset Management to achieve competitive advantage globally.\\n\\nThe Expertise You Have:\\n3-10 years of professional development experience.\\nDatabase development background, SQL, PL/SQL.\\nStrong background with object-oriented programming, with projects completed leveraging many of the following technologies, including Python, Java, or .Net, .Net Core, C#.\\nExperience in Pattern Matching to extract data from Web Sites (Web Scraping), Text Documents, etc.\\nKnowledge of Cloud computing concepts (AWS) and working experience with deploying and managing applications in the Cloud.\\n\\nThe Skills You Bring:\\nYou have experience with extracting text through parsing HTML, XML, JSON, text, PDF, Word, and other types of documents.\\nYour demonstrated experience developing RESTful API based web services on Windows and Linux.\\nYou thrive in environments involving everything from Conceptual Design to Rapid Prototyping.\\nYou are familiar with extracting data from REST APIs and parallel processing large datasets.\\nYou have knowledge or interest in developing custom Data Pipelines to extract data, map data, transform data, and to load data in various data stores like Oracle, S3, and/or shared drives.\\nParticipating in problem-solving, troubleshooting, performance tuning, production support, and maintenance of existing APIs.\\n- provided by Dice',\n",
       "  'Apply Now!',\n",
       "  \"We're reimagining sports and technology.\\n\\nDraftKings is bringing sports fans closer to the games they love and becoming an essential part of their experience in the process. An industry pioneer since our founding in 2012, we believe we can continue to define what it means to be a technology company in sports entertainment. We love what we do and we think you will too.\\n\\nLove data? We do too.\\n\\nAs a Data Engineer, you'll be a creative contributor to our data analysis and scalability processes, and you will use your experience to provide key insights that help us make smarter decisions. Analytical thinking drives our business and when you join our team, you'll not only solve new problems every day, you'll see your data solutions immediately improve our users' experience.\\n\\nWhat You'll Do as a Data Engineer\\nYou'll build tools to provide actionable insights into key business metrics\\nCare about agility like you care about scalability. We roll out products very quickly and priority management is key\\nDesign processes that support data transformation and structures, metadata, dependency and workload management\\nYou will also focus on performance analysis, optimization, and tuning\\nHave the opportunity to see your personal work make an immediate impact on influential products\\nWhat Skills You Will Use\\n2+ years in aspects of business intelligence and data engineering, including data warehousing, delivery, and operations\\nThe ability to build and optimize data pipelines, data models and transformations.\\nA deep knowledge of a variety of data engines like SQL Server, MySQL, and Redshift\\n2+ years experience writing advanced SQL\\nYou'll also have a solid understanding of dimensional modeling\\nHands on experience with at least one programming language such as Python, C#, Java\\nFamiliarity with source control tools like Git, Bitbucket, SVN, etc...\\nExperience working in AWS, Terraform, is a plus\\nWho are we a good fit for?\\n\\nWe love working with talented people but more than that, we seek out compassionate co-workers with a collaborative spirit. Our work moves quickly and we're great at coming together to find creative solutions to some of tech's most interesting problems. If that sounds good to you, join us.\\n\\nApply now\\n\\nWe're proud to believe that your gender, race, nationality, religion, sexual orientation, status as a protected veteran, or status as an individual with a disability should have nothing to do with our hiring practices. We'll never discriminate against anyone's background or creed. If you're good at what you do, we want you to do it at DraftKings.\",\n",
       "  'Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.\\n\\nOur success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industry’s most challenging problems.\\n\\nWhat does a Data Engineer role at Quantexa look like?\\n\\nIn order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.\\n\\nBeing Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.\\n\\nWe want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.\\n\\nRequirements\\n\\nWhat do I need to have?\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\nExperience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.\\nPassion and drive to grow within one of the UK’s fastest growing Start-ups\\nBenefits\\n\\nWhy join Quantexa?\\n\\nWe know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying “thank you” for all your hard work!\\nCompetitive Salary\\nCompany Bonus\\nExcellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage\\n401k where we’ll match up to 5%\\nOnline training customized to your personal preferences\\nGenerous annual leave\\nAmazing working environment - Ranging from regular social events, free beverages and a very good location right by south station in Boston.',\n",
       "  'At MassMutual, were passionate about helping millions of people find financial freedom and this passion has driven our approach to developing meaningful experiences for our customers. The Data Engineering team , part of MassMutuals Enterprise Technology and Experience organization, is comprised of highly skilled, collaborative, problem solvers who are motivated to create innovative solutions that exceed the changing needs of our\\n\\ncustomers and move MassMutual and the industry forward.\\n\\nTo continue our cutting-edge work, we are hiring a Sr. Data Engineer to join our team.\\nIncumbents work with data scientists, engineers, product managers, and enterprise stakeholders to help deliver innovative, data-driven systems aimed at transforming the insurance industry. As part of a Data Engineering team incumbents may be responsible for any of the following.\\nDaily and monthly responsibilities\\nDesign and implement the enterprise data platform.\\nDevelop advanced distributed analytics workflows on distributed data processing and computing platforms.\\nSupport the integration, management and analysis of very large and diverse financial and life insurance datasets.\\nBuild pipelines to clean, prepare, and enrich data and ultimately integrate into a consolidation layer presented to customers and enterprise stakeholders.\\nWork closely with highly skilled data engineers to analyze and develop predictive data products that advance and transform MassMutuals business.\\nDesign, develop, test and maintain highly scalable database applications on SQL and NoSQL databases and large-scale data processing and warehousing systems.\\nSpecify, install and update all maintenance procedure such as backup and recovery, high availability and disaster recovery.\\nRecommend ways to improve data reliability, efficiency and quality.\\nCollaborate with data architects, modelers and IT team members on project goals. Domain expert and mentors others. Provides temp leadership when needed across domains.\\nManages projects that integrate across multiple areas.\\nDesign, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset.\\nExecutes and provides feedback for data modeling policies, procedure, processes, and standards.\\nAssists with capturing and documenting system flow and other pertinent technical information about data, database design, and systems.\\nDevelop data quality standards and tools for ensuring accuracy.\\nWork across departments to understand new data patterns\\nTranslate high-level business requirements into technical specs.\\n\\nWhat great looks like in this role\\n\\nOur ideal Sr. Data Engineer is a collaborative leader skilled in data analytics, data modeling, and database design. Youre also committed to data integrity, are highly analytical, and can work on multiple projects at once.\\n\\nYoull use your skills to develop, monitor, and manage data systems across our platform. Additionally, you will act as a mentor to junior team members and coach them on best practices and engineering standards.\\n\\nThe team culture of working collaboratively, cross-functionally, using new technologies combined with the work/life balance provided by MassMutual are core reasons people enjoy working on the Data Engineering team at MassMutual.\\n\\nObjectives of the role\\n\\nDesign, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset.\\nWorking on a range of projects including batch pipelines, data modeling, and data mart solutions youll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business need.\\n\\nBasic Qualifications\\n\\n• Bachelors degree in computer science or engineering.\\n• 5+ years of experience with data analytics, data modeling, and database design.\\n• 3+ years of coding and scripting (Python, Java, Scala) and design experience.\\n• 3+ years of experience with Spark framework.\\n• Experience with ELT methodologies and tools.\\n• Expertise in tuning and troubleshooting SQL.\\n• Strong data integrity, analytical and multitasking skills.\\n• Excellent communication, problem solving, organizational and analytical skills.\\n• Able to work independently.\\n• Authorized to work in the USA with or without sponsorship.\\n\\nPreferred Qualifications\\n\\n• Masters degree in computer science or engineering.\\n• Familiar with agile project delivery process.\\n• Knowledge of SQL and use in data access and analysis.\\n• Ability to manage diverse projects impacting multiple roles and processes.\\n• Able to troubleshoot problem areas and identify data gaps and issues.\\n• Ability to adapt to fast changing environment.\\n• Experience with Python.\\n• Basic knowledge of database technologies (Vertica, Redshift, etc.).\\n• Experience designing and implementing automated ETL processes.',\n",
       "  \"Who We Are\\nAt Hometap, we believe you can have a house and a life. Our vision is to empower current and prospective homeowners to access their home's equity, free of debt. By tapping into the built-in value of a home, we help homeowners do the big-ticket stuff that makes life better: start a business, buy a second home, get out from under crippling debt, or pay for college tuition.\\nHometap makes investments in residential homes, buying a small equity stake against the home’s future value. For homeowners, this is a simplified, sensible alternative to mortgage debt and monthly payments. For investors, this is a new asset class, allowing them to participate in the multi-trillion-dollar residential real estate market without the obligations and downsides of homeownership.\\nWho you are:\\nAs a Data Engineer at Hometap, you’ll be the crucial glue that makes a whole new category possible. We need people who are excited about building the architecture and infrastructure for processing the data that powers the business.\\nYou’ll be responsible for:\\n\\nOrganize, clean and analyze raw data sets to continually test hypotheses\\nPlan, organize and build infrastructure that supports the dynamic environment of a young company.\\nCreate and maintain ETLs to and from a variety of sources and destinations. Familiarity or interest in tools in the space of Spark, Dask, Hadoop, and/or Airflow, Kafka, Alooma, AWS Data Pipeline is desired.\\nFollow best-practice engineering standards, such as architectural design, unit testing and test driven development\\n\\nYou will have:\\n\\n2+ years of related experience in data engineering.\\nCloud computing infrastructure services.\\nManagement of relational databases and or no-SQL databases.\\nETL tools\\n(preferred) Python ORMs\\nA degree in computer science, mathematics, statistics, or a related field\\nA strong understanding of algorithms, statistics, and data interpretation\\nProficiency in Python\\nThe ability to understand business problems and translate them into data science and data engineering projects.\\nOur Team & Culture\\nAt Hometap, we pride ourselves on being Owners -- of our jobs, our output, our careers -- and Good Neighbors -- we believe helping each other means helping the company, homeowners, and investors succeed.\\nEqually important to your skill set is the way you fit within our team and business. We are looking for a creative thinker who looks for opportunities within challenges, rolls up their sleeves to get things done, values collaboration and communication, and embraces the Homeowner-first mentality. Critical to our mission is being thoughtful in everything we build. If you’re someone who embraces this builder mentality, you’ll be right at home here.\",\n",
       "  \"At Liberty Mutual Investments (LMI), we manage a high-quality investment portfolio utilizing a disciplined strategy. We have a large and varied customer base, supported by strategic business units that function as a complete investment firm within our Fortune 100 Company. Our Investment professionals are critical to our ability to keep our promises to policyholders, claimants and their families.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\n\\nLMI Technology is actively searching for a Data Engineer within Data Platform Investments Technology who will be responsible for participating on a team to develop and enhance applications from general specifications. Our team is responsible for driving the LMI Data Strategy through the collection, maintenance, improvement, and manipulation of data within the operational and analytics databases. The candidate should be a creative thinker with a strong passion for building data products using the latest technology focused on data analytics.\\n\\nPlease note this role is posted as a range 14-16 grade level depending on experience.\\n\\nJoin us!\\nParticipate in scrums as part of Agile development team\\nEstimate size and Implementation of backlog items, translate into engineering design and logical units of work (tasks)\\nEvaluation of technical feasibility\\nWrites and verifies code which adheres to the acceptance criteria\\nApplication of product development standard methodologies\\nDevelops domain expertise (Investment data and processes) and applies to development of solutions.\\nWork with investments and financial data in a variety of formats across structured and unstructured sources.\\nPerforms data research / profiling and applies results to solution design.\\nSupports the data environment by releasing new features, resolving issues for users, and working with other technology teams to define standards.\\nIdentifies and implements appropriate continuous improvement opportunities.\\nCollaborates with teams to integrate systems.\\nDevelops and updates technical documentation.\\nBuilds positive relationships with key investments business partners and users to understand, gather, and develop products to meet their needs.\\nPreferred Qualifications:\\nBachelor's degree in technical or business discipline or equivalent experience\\nGenerally 3-5 years of professional experience\\nPossess/develop in-depth understanding of data environment and leverages knowledge to build robust, scalable solutions.\\nPossess/develop domain expertise around Investments data and processes and applies to development of solutions.\\nExperienced with SQL and ideally the Microsoft SQL Server stack (SSIS, SSAS)\\nExperience with data management systems; both relational and NoSQL\\nExperience with business intelligence, analytics, and reporting tools (Power BI, Tableau, etc.)\\nExperience with Python or other programming languages\\nUnderstanding of data warehousing, modern Cloud / Hybrid data architecture concepts\\nFamiliarity with Agile development concepts - Test-Driven Design, MVP, iterative and incremental design and delivery\\nFamiliarity with ETL tools and concepts\\nFamiliarity with Data Modeling\\nFamiliarity with full stack application development\\nAbility to adhere to guidelines and standards while innovating and contributing to evolution of those standards.\\nAbility to balance support of legacy solutions with delivery of new business capabilities\\nBenefits:\\nWe value your hard work, integrity and commitment to positive change. In return for your service, it's our privilege to offer you benefits and rewards that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits\\nOverview:\\nAt Liberty Mutual, we give motivated, accomplished professionals the opportunity to help us redefine what insurance means; to work for a global leader with a deep sense of humanity and a focus on improving and protecting everyday lives. We create an inspired, collaborative environment, where people can take ownership of their work; push breakthrough ideas; and feel confident that their contributions will be valued and their growth championed.\\nWe're dedicated to doing the right thing for our employees, because we know that their fulfillment and success leads us to great places. Life. Happiness. Innovation. Impact. Advancement. Whatever their pursuit, talented people find their path at Liberty Mutual.\",\n",
       "  \"Spun out of MIT in 2014, Zylotech is a disruptive AI-powered customer data analytics company located in Central Square. We are ZyloChamps, big thinkers with a sense of fun who love working in an open environment full of start-up perks.\\n\\nWhy you'll love working here:\\n\\nZylotech is a fast-growing company that rewards autonomy and creativity. Our team works hard, is fast and collaborative, and we encourage personal growth. There are many opportunities to work with other departments where you can interact with some incredibly talented people.\\n\\nWhat youll do:\\nArchitect, implement and deploy new data models and data processes in production.\\nManage data warehouse plans for a product or a group of products.\\nExplore client data, analyze and Implement ETL process leveraging open source frameworks\\nWrite python/spark scripts for Model verification and/or query database.\\nOrchestrate APIs and automate data processes\\nMonitor model performance and advising any necessary DevOps changes.\\nBuild quick POCs / prototypes around data problems\\nWhat we seek:\\nA Hacker. We encourage hacking out creative ways to find and build simple and effective solutions to complex problems\\nAt least 2 years of Experience in Python and spark\\nExperience in Databases (NoSQL and SQL), preferably in PostgreSQL, MongoDB, ElasticSearch, Neo4j.\\nExperience in creating Data-warehouses and scheduling automated ETL jobs\\nExperience in Docker and Kubernetes\\nFlexible to learn, solve problems and continuously hack solutions\\nBackground in building, maintaining, and shipping sophisticated enterprise software systems through multiple release cycles, especially through upgrades and changes to APIs and data formats\\nExperience in an agile environment and a rapidly expanding product market\\nBachelors or Masters degree in Computer Science or a related field of study.\\nImportant Personal Attributes:\\nYou are curious about technology & science.\\nTeam matters.\\nChallenging yourself is an intrinsic part of who you are.\\nSharing and debating ideas to get to the right answer is important to you.\\nYou seek to act and dont wait to be told.\\nYou like to teach those around you.\\nYou are serious about your work but also believe that laughter and levity are important.\\nYou work hard and you also have passions outside of work that enrich your life.\\nWinning is important to you, but so is how you win.\\nYou take pride & ownership in building a Product and company who is disrupting an industry.\\nAbout Zylotech:\\n\\nZylotech is a self-learning customer intelligence platform that helps marketers create complete customer profiles for targeting revenue opportunities more effectively. Powered by AutoML, the platform continuously unifies and enriches internal and external data, and performs ongoing micro-segmentation, pattern discovery, and recommendations, all while integrating with a variety of marketing clouds for on-demand accessibility. Zylotechs cross-industry clients have reported up to a 6x increase in customer lift. For more information, visit Zylotech.com.\\n\\nZylotech is an equal employment opportunity employer and does not discriminate against any applicant because of race, creed, color, age, national origin, ancestry, religion, gender, sexual orientation, disability, genetic information, veteran status, military status, application for military service or any other class protected by state or federal law.\\n\\nTo all recruitment agencies: Zylotech does not accept agency resumes. Please do not forward resumes to our jobs alias, Zylotech employees or any other company location. Zylotech is not responsible for any fees related to unsolicited resumes. Unsolicited resumes received will be considered our property and will be processed accordingly.\\n\\nQualified Applicants must be legally authorized for employment in the United States. Qualified Applicants will not require employer-sponsored work authorization now or in the future for employment in the United States.\",\n",
       "  'Summary:\\n\\nYou have experience with client projects and in handling vast amounts of data - working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Analytics team. Play a key role on projects from a data engineering perspective, working with our Architects and clients to model the data landscape, obtain data extracts and define secure data exchange approaches.\\nPlan and execute secure, good practice data integration strategies and approaches\\nAcquire, ingest, and process data from multiple sources and systems into Big Data platforms\\nCreate and manage data environments in the Cloud\\nCollaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models\\nHave a strong understanding of Information Security principles to ensure compliant handling and management of client data\\nThis is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science\\nQualifications:\\nExperience on client-facing projects, including working in close-knit teams\\nExperience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)\\nExperience or familiarity with real-time ingestion and streaming frameworks is a plus\\nExperience and desire to work with open source and branded open source frameworks\\nExperience working on projects within the cloud ideally AWS or Azure\\nExperience with NLP, Machine Learning, etc. is a plus\\nExperience working on lively projects and a consulting setting, often working on different and multiple projects at the same time\\nStrong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R\\nData Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models\\nExcellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.\\nA deep personal motivation to always produce outstanding work for your clients and colleagues\\nExcel in team collaboration and working with others from diverse skill-sets and backgrounds\\nCervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches.',\n",
       "  \"Senior Software Engineer - Data | Location: Boston.\\nHello, We're Jebbit!\\n\\nThe world’s 1st declared data platform. Founded by two entrepreneurs sick of online advertising’s status quo, Jebbit envisions a future of total data transparency between brands and consumers. Our mission is to make every experience assumption free by asking people explicitly and directly about their interests, motivations, and preferences. Originally founded at Boston College, Jebbit was named one of the Top 25 Most Promising Companies in the World by CNBC and our co-founders are Forbes 30 Under 30 honorees. We’re a graduate of Techstars and located in Boston's Seaport near South Station.\\n\\nOur base technology stack consists of postgres (SQL), Ruby on Rails, EmberJS, and NodeJS. Most of our infrastructure resides on AWS and we also employ Kubernetes, ElasticSearch, and Redshift. Having a firm grasp of fundamentals and having experience scaling architecture is more important than being a master of any singular domain. We are looking for a mid- to senior-level data engineer who enjoys problem solving at a fast-pace in a flexible environment.\\nThe Role\\nGet to learn a lot of new skills and demonstrate your existing chops as you work with department and team leads to build cool features as we scale and optimize our platform. Your work will cover the entire range of features from database optimization, performance fine-tuning, building data-pipelines, data integrations, analytics, reporting, and working closely with our data scientists.\\nAt certain times you will be building new Capabilities such as proof of concepts using new frameworks, new database technologies, new machine learning deployment technologies and some stream data processing in the future.\\nAt other times you will be improving existing aspects of the platform doing performance fine-tuning, improving on some of our external data integrations and data management for BI and Analytics related tasks.\\nYou will interact with product management, and client facing teams to better understand the market and what role our technology plays in it.\\n\\nSkills:\\nSelf Learner with a tremendous curiosity to solve complicated technical puzzles/issues\\nEffective verbal and written technical communication\\nRequired skills: 3+ years as a software developer with strong background in developing and deploying python (must have code in production and not just course work) and database fundamentals (SQL).\\nNice to have skills: Prior knowledge of Ruby On Rails, Previous work with work with Redshift, Cassandra, ElasticSearch or other data frameworks such as Apache Kafka, Apache Storm, Apache Flink etc.\\nWhy Jebbit\\nJebbiters pride ourselves in taking a fresh perspective at everything, so while we’re fast-paced and getting things done, we also keep things balanced with flexible schedule and WFH policy\\nIn addition to offering top of standard benefits package (medical\",\n",
       "  \"Meet CarGurus—the #1 most visited online car shopping website in the US. At CarGurus, we're building the world's most trusted and transparent automotive marketplace where it's easy to find great deals from top-rated dealers.\\n\\nFounded in 2006 by Langley Steinert (co-founder of TripAdvisor), CarGurus is a technology company with a passion for data and its power to simplify every aspect of the car shopping experience. Using proprietary technology, search algorithms and innovative data analytics, we provide unbiased validation on pricing, dealer reputation and vehicle history.\\n\\nAbout the Role\\n\\n\\nAt CarGurus we use data, not hunches, to inform decisions. Analysts play a key role in crafting the data narrative for these decisions. We love to collaborate, we're excited by a challenge, and we believe that ownership enables us to grow.\\n\\nWe are looking for thoughtful, courageously curious candidates who can dive into complex data and draw insightful conclusions. The one-year rotational program is designed to give graduating college seniors an accelerated path towards analytics roles within CarGurus. This program provides a unique chance to create a strong analytical foundation while exploring several facets of a rapidly-growing tech company. You'll work cross-functionally with many departments and stakeholders, gain exposure to multiple dimensions of the business, and work on projects with outsized impact.\\n\\nCarGurus values diversity in experience and backgrounds—we do our best work when we create space for different voices and perspectives. Whatever unique experiences or skill sets you bring, we look forward to learning from each other!\\n\\nWhat You'll Do:\\nParticipate in bespoke four- to six-month rotations over the one-year program; rotations may be in product, financial planning/analysis, consumer marketing, and business development\\nDefine metrics to evaluate business performance, products and experiments; build reports and dashboards to monitor and understand them\\nCommunicate and present data analysis, visualizations, and insights to key stakeholders\\nBe a voice and advocate for using data to improve the quality of our decision-making\\nWho You Are:\\n\\n\\nThis position is for students who are receiving their bachelors degree and graduating in spring of 2020. While we expect you have a strong academic background, no automotive or technology experience is required. Coursework and/or work experience involving statistics, mathematics, or other quantitative fields are preferred; nevertheless, the position is not limited to any specific majors or minors. Regardless of your background, please consider applying if you are:\\nOpen to learn new reporting or Business Intelligence tools; previous exposure to programming languages a plus\\nNaturally inquisitive and excited by solving consumer challenges with technology and data\\nAble to work in independent and cross-functional settings within a fast paced environment\\nPossess excellent verbal and written communication skills\\nCarGurus Culture\\n\\n\\nAt the core of our company culture is a spirit of innovation, curiosity and collaboration. True to our start-up roots, we're nimble, flexible and data-driven. We have a respect for testing and learning, and a healthy aversion to meaningless meetings. We're committed to diversity and inclusion because we want all employees to have a sense of belonging. To power you through the day, our kitchens are stocked with snacks and drinks. Lunch is catered daily. All employees are shareholders. We believe giving back is important, so we have established an Open Source Fund and Charitable Giving program which make quarterly donations to employee recommended organizations. We recognize that everyone has a life outside of the office which is why we have flexible schedules and unlimited personal time.\\n\\nIn addition to the US, CarGurus operates sites in Canada, the UK, Germany, Italy, and Spain with other markets on the horizon. Our offices are located in Cambridge, MA, Detroit, MI and Dublin, Ireland. If you'd like to learn more, please visit our careers page.\\n\\nProgram will begin Spring/ Summer 2020.\",\n",
       "  'Summary:\\n\\nThe Phia Group is comprised of The Phia Group, LLC and Phia Group Consulting, LLC (“PGC”), and is a service-oriented organization assisting employee health plans nationwide. We provide our clients with innovative cost-cutting solutions and constantly expanding service offerings. We continue to enjoy growth thanks to our most valuable resource – our talented and committed team.\\n\\nThe Data Analyst will be an integral cog in the life cycle of the claims data process. The Data Analyst will develop and maintain ETL Maps using Altova Mapforce, assist in preserving the quality and integrity of the enterprise data, collaborate with internal business users to triage data anomalies and coordinate with external clients to resolve any data quandaries. The Data Analyst must analyze data and find data discrepancies using business rules.\\n\\nEssential Duties and Responsibilities include the following. Other duties may be assigned.\\n\\n· Develop ETL Maps in Altova for imports of client claim data\\n\\n· Database and ETL performance tuning\\n\\n· Design, implement and understand Dimensional Models and query tools\\n\\n· Design and development of relational database management objects (tables, schemas, indexes, materialized views, partitions, stored procedures, macros, etc.) , database design principles, data administration standards, and information security terminology\\n\\n· Ability to solve complex technical problems, assimilate information rapidly, and work under time constraints\\n\\n· Strong SQL capability for identifying and resolving data related issues\\n\\n· Responsible for gathering and assessing business information needs and preparing requirements, driving requirements through to implementation\\n\\n· Lead source system analysis and design, help to design and develop data loading processes\\n\\n· Knowledge of claims or healthcare related data structures and entities\\n\\n· Help implement best practices and robust ETL solutions, create various ETL jobs based on EDW environment Architecture and Data Models that got created\\n\\n· Ensure Data Quality and Data Integrity is maintained\\n\\n· Understand our data sources, current data environment, collaborate with BI team and business Subject Matter Experts (SMEs) to understand immediate and upcoming data needs\\n\\n· Serves as SME in data modeling concepts (star schema, snowflake schemas, normalized & de-normalized data models) to help build EDW, ODS and Data Marts\\n\\n· Develops processes for capturing and maintaining metadata\\n\\n· Experience with 837 and 835 EDI Health Care claims files\\n\\n· Performs other duties as assigned.\\n\\nExperience and Qualifications\\n\\n· Must have good understanding of Microsoft SQL Server 2012 and 2016\\n\\n· Proficient in writing SQL queries and understanding data modals\\n\\n· Strong analytical approach to problem solving.\\n\\n· Must have experience in of the ETL tools preferably Altova Mapforce, Talend or Informatica\\n\\n· Experience implementing API solutions in the healthcare industry\\n\\n· Knowledge of claims or healthcare related data structures and entities is desirable\\n\\n· Basic knowledge of software coding, concepts or HTML would be a plus.\\n\\n· Excellent communication skills and customer service skills\\n\\n· Good Team player with the ability to work well with others\\n\\n· Bachelor’s degree or Associates degree in computer science or related field preferred\\n\\n· Minimum 2-4 years experience.\\n\\n· Ability to read, write and speak the English Language\\n\\n· Must have ability to define problems, collect data, establish facts, and draw valid conclusions',\n",
       "  'Data & Analytics is a Core Services function that helps case teams succeed with their analytics work, contributes to training Associates and Consultants, and supports Managing Directors with IP, developing commercial proposals, and ensuring timely data capture at the launch of casework. Data & Analytics is an important strategic capability for LEK in our quest to support our clients with actionable findings from data. LEK works in verticals like Consumer/Retail, Industrial, and Life Science/Healthcare practices. LEK has been a highly successful international management consulting firm, with a history of rapid yet controlled revenue growth. To further fuel this expansion we are seeking to grow a thriving analytics practice.\\n\\nThe Sr. Data Engineer plays a pivotal role in determining viable paths to sound and actionable results from analysis. Across the value chain from inception of client proposals, to capturing and cleaning of data sources, assessing data quality and completeness, running analyses and presenting findings in a compelling and actionable format, you hold your own. Versatility and flexibility of methods and tools is important, because no two cases will be alike. Our clients’ data assets never cease to amaze us. We blend data sources across proprietary data, client data, subscription data, and public sources to enrich our analyses. You play a key role in building frameworks and reproducible methods to make data engineering predictable, reliable, and scalable.\\n\\nAs part of a dynamic and growing team, there is every opportunity to carve out value added work, and to grow your skills and experience. You receive state-of-art training in a variety of domains, and lead the organization to adopting new and innovative methods to solve our clients’ hardest questions. You aren’t just a technical expert: you intersect between data science, statistics and computer science, with business expertise in a variety of domains, financial modeling to determine suitability of recommendations, and managerial and influencing skills to “sell” your recommendations to our clients.\\n\\nResponsibilities:\\nIdentifying and flagging potential data sources in support of client needs\\nSourcing, restructuring and cleaning of data to make them amenable to analysis\\nSupporting case teams with their analytics work in either a “consultancy/expert” mode, or as part of active case work\\nGuide the firm along a multi-year roadmap that you design to profitable and sustainable data monetization\\nBuild out our proprietary data sources by matching and merging of procured, proprietary, as well as public data sources in support of business development, client proposals, and generating IP that helps further establish our thought leadership\\nDetermine data needs for case team projects. Specify what data our clients are expected to deliver, and securely manage file exchanges with our clients’ IT departments\\nTrain and coach LEK staff on the use of our software and proposed data management methods\\nContribute to commercial proposals, as required by Managing Directors\\nDevelop templates and training materials to build out LEK’s analytical capabilities\\nQualifications and Experience:\\nExpert level knowledge of the MS SQL Server stack, including PowerPivot and PowerBI\\nHands-on experience with all major cloud providers: Azure, AWS, GCP, Snowflake\\nFamiliar with selecting, recommending, and tuning optimal cloud solutions\\n5+ years’ experience loading and updating temporal data solutions\\nA minimum of 10 years of experience in data engineering\\nExperience with Alteryx and Tableau, as well as Python/Pandas is preferable, but not required\\nExperience doing hands-on, practical casework and data migrations. This might have been at a consultancy, or on the corporate side.\\nIdeally, you have gained experience managing small teams.\\nCandidates responding to this posting must currently possess eligibility to work in the United States\\n\\nL.E.K. Consulting offers a competitive compensation and benefits package\\n\\nL.E.K Consulting is an Equal Opportunity Employer',\n",
       "  'At Kensho, we hire talented people and give them the autonomy, support, and resources needed to build cutting edge technology and products for our parent company, S&P Global. As a result, we produce technology that is scalable, robust, and solves the challenges of one of the world’s largest, most successful financial institutions.\\n\\nAs a software engineer on the Data Team, you will be responsible for aggregating thousands of messy data points and making sure they are integrated into Kensho’s systems and easily accessible to our teams. Our Data Team consists of a diverse mix of engineers working in Python to interface with many types of backends.\\n\\nAre you looking to make impactful, scalable contributions that could transform the way people think about data? If so, we would love to help you excel here at Kensho. We take pride in our team-based, tightly-knit startup Kenshin community that provides our employees with a collaborative, communicative environment that brings transparency to the biggest challenges in data.\\nWhat You’ll Do\\nYou will write robust, scalable, and well tested code for integrating all kinds of data into Kensho’s systems\\nYou will work closely with our knowledge graph, ensuring it’s synchronised with dozens of internal and external data sources\\nYou will help make Kensho’s data easily accessible internally through easy to use, performant and well documented APIs\\nYou will work on rapidly prototyping new services to accommodate other projects and teams’ data needs\\nWhat We Look For\\nYou have a love for all things data, data structures, and algorithms\\nYou’re excellent at coding and documentation\\nYou have a deep understanding of regular expressions\\nYou have experience with either relational or non-relational databases\\nYou have experience aggregating information from publicly-available sources\\nYou have outstanding communication skills, including the ability to convey technical concepts to a variety of technical and non-technical audiences\\nHow You Can Really Get Our Attention\\nYou have 2-4 years of experience working with large, unstructured datasets, graph databases, and data acquisition\\nYou have made meaningful, impactful contributions in a startup or top company using big data\\nYour open source project contributions, research, publications, or patents demonstrate innovation and initiative\\nYou participate in and have received awards at Hackathons and/or other competitions\\nYou demonstrate a positive attitude, self-awareness, and a willingness to learn\\nTechnologies You Will Use\\nPython and specifically Pandas, Numpy, scikit-learn, and celery\\nInfrastructure such as kubernetes, nginx, redis, and RabbitMQ\\nData stores like PostgreSQL and other NoSQL alternativesUbuntu linux or similar\\nBenefits & Perks\\nAt Kensho, we pride ourselves on providing top-of-market benefits, including:\\nMedical, Dental, and Vision insurance - 100% company paid premiums\\nUnlimited Paid Time Off\\n18 weeks of 100% paid Parental Leave (paternity and maternity)\\n401(k) plan with 6% employer matching\\nGenerous company matching on donations to non-profit charities\\nUp to $20,000 tuition assistance\\nPlentiful snacks, drinks, and regularly catered lunches\\nDog-friendly office (CAM office)In-office gyms and showers (CAM, DC) or Equinox membership (LA, NYC)\\nStipend towards commuter or gym reimbursement\\nBike sharing program memberships\\nCompassion leave and elder care leave\\nMentoring and additional learning opportunities\\nOpportunity to expand professional network and participate in conferences and events\\nAbout Kensho\\n\\nKensho uses machine learning, artificial intelligence, natural language processing and data visualization techniques to solve some of the hardest analytical problems and create breakthrough financial intelligence solutions for our parent company, S&P Global. Kensho was founded in 2013 by Harvard & MIT alums and was acquired by S&P Global in 2018. Kensho continues to operate as a startup in order to maintain our distinct, independent brand and to promote our breakthrough, innovative culture.\\n\\nOur team of Kenshins enjoy a dynamic and collaborative work environment that runs autonomously from S&P, while leveraging the unparalleled breadth and depth of data and resources available as part of S&P Global. As Kenshins, we pride ourselves on maintaining an innovative culture that depends on diversity and inclusion. We are an equal opportunity employer that welcomes future Kenshins with all experiences and perspectives. Kensho is headquartered in Cambridge, MA, with offices in New York City, Washington D.C. and Los Angeles.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.',\n",
       "  'Mission\\nWe create technology with heart for the health of every person in the world.\\n\\nAbout Buoy Health\\nBuoy builds a digital health tool that helps people – from the moment they get sick – start their health care on the right foot. Started by a team of doctors and computer scientists working at the Harvard Innovation Laboratory in Boston MA, Buoy was developed in direct response to the downward spiral we’ve all faced when we attempt to self-diagnose our symptoms online. Buoy leverages artificial intelligence – powered by advanced machine learning and proprietary granular data - to resemble an exchange you would have with your favorite doctor – to provide consumers with a real-time, accurate analysis of their symptoms and help them easily and quickly embark on the right path to getting better. Buoy is based in Boston and was founded in 2014.\\n\\nAbout the role\\nThe Data Engineer is responsible for deploying and maintaining data warehousing environments, managing ETL/ELT pipelines and job orchestration frameworks, and ensuring data quality. The charter of the data engineering and analytics team is to promote a data driven culture throughout the company, and to make high quality data broadly accessible and easy to work with for analysis, data science, and machine learning. The data engineer can expect to work closely with business analysts, data scientists, machine learning engineers, and developers to build the first generation of our data warehouse, ETL pipelines, and data models. We are currently building our data analytics ecosystem from the ground up, and our company and datasets are growing rapidly - so the data engineer will also have the opportunity to inform the design, implementation, and best practices or this system.\\n\\nResponsibilities\\n-- Build cloud-based data warehousing environments, data processing pipelines, and data models that support a variety of business needs\\n-- Support a variety of data processing pipelines, integrate new data sources into our data warehouse, and create jobs to load, transform, and QA vital datasets\\n-- Work with analysts and developers in the product development process to ensure that newly designed data models meet analytics requirements and follow best practices\\n-- Share your expertise on scalable data processing with analysts and data scientists to further our goal of being a truly data driven organization\\n\\nAbout you\\n-- 3+ years of experience as a data engineer using data warehousing technologies like Snowflake, Amazon Redshift, S3, Athena, EMR, and Hadoop/Hive/Spark\\n-- Proficient in SQL including one or more relational databases like MySQL, MariaDB, Oracle, Postgres, or similar\\n-- 3+ years experience with ETL and job scheduling or orchestration using tools like Airflow, Luigi, Oozie, or similar\\n-- 3+ years experience programming in python Familiarity with AWS and git\\n-- Excellent communication and ability to work on a growing team\\n\\nBonus points if you have\\n-- Experience with web-scale data or working with healthcare data in a HIPAA-compliant environment\\n-- Experience with data modeling, data visualization, and/or BI tools like Looker, Metabase, or Tableau\\n-- Experience with AB Testing\\n\\nBenefits\\n-- Stock Options\\n-- Unlimited PTO\\n-- Medical, Dental, Vision\\n-- Simple IRA with matching\\n-- Dogs in the office!',\n",
       "  \"At some point you’ve probably attended or scheduled a meeting -- maybe it was in a room, on video or over the phone -- then your meeting room gets stolen, you don't have the right resources, or you can't find where you need to be. Finding space and time to communicate can be painful. The problem is easy to understand but difficult to solve. That's where Robin comes in. We build software that coordinates meeting spaces, people and things in your workplace so you can get back to doing your best work.\\n\\nBased in Boston, the DNA of our eclectic team shapes our culture in a way that means we can show up to work as our whole selves. Our values guide the way we treat our customers, our coworkers, and our candidates. We are intentional with our words and actions. We’re helpful. And at the end of the day, we're all united by the mission of modernizing the open office so that businesses (like HubSpot, Twitter, Bumble and Kayak) are more enjoyable places to walk into each day, including our own.\\n\\nWe are looking for a Data Analyst to partner with engineering and product to support the development of customer-facing workplace analytics. If you consider yourself smart, have a drive for results, proven track record and are not shy to challenge the status quo -- we want you!\\nWork you’ll be responsible for:\\nQuery databases (Postgres/MySQL) to gather large amounts of complex calendar data. Identify trends, patterns, gaps and insights using descriptive analytics and data visualizations to further business insights.\\nLeverage best in class analytical programming tools, such as Jupyter notebooks, and create well documented and repeatable analytical processes.\\nPerform complex ad-hoc analyses in Excel to solve important business problems and formulate robust, actionable recommendations for leadership that support the product team in creating a data-driven company.\\nContribute to the development of data strategy tools such as a data dictionary and assist with the creation of team onboarding materials.\\n\\nYou are:\\nAnalytical. You’re passionate about using and understanding data to problem-solve.\\nCollaborative. You enjoy partnering with various teams to drive analytics on business trends through multiple systems. You are able clearly distill the essence of your technical work to audiences of all levels and across multiple functional areas.\\nAdaptable. You are able to work on multiple projects, meet deadlines and love a fast-paced environment.\\nCuriosity: You’re driven to learn the latest and greatest of an industry. You enjoy problem-solving and know-how to ask strategic questions rather than just talking about implementation details.\\nArticulate. You are able to communicate and simplify complex data into sensible business solutions. Experience articulating product questions and using statistics to arrive at an answer.\\nExperience you already have:\\n2-4 years of experience on a data or analytics team\\nKnowledge and experience applying statistical techniques to real business data\\nA Bachelors degree in a quantitative field such as Statistics, Computer Science, Engineering, Mathematics, Data Sciences or equivalent experience\\n2 or more years of experience working with Microsoft Excel\\n2 or more years of experience working with Python (preferable) / R\\nCompetency with relational databases, writing SQL scripts and working with large sets of data\\nFamiliar with Git\\nPlus if you are interested in learning Go or Java\\nWe’re creating the smart office of the future. We’d love to have you be a part of it.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We are ADA compliant and handicap accessible.\",\n",
       "  \".\\n\\nDo you dare to reinvent the future of education?\\n\\n\\nAt Cengage, we are harnessing the power of tech to build a future where all learners have the tools and confidence to achieve their goals.\\n\\nAs a Cengage employee, you will blaze a new trail to transform the way people learn. Collaborating with the best of the best, you will feel challenged and inspired to do breakthrough work. With the support of our united team, there is no limit to what you can imagine, create and set in motion.\\n\\nAre we right for you?\\n\\n\\nWe set the bar higher by bringing our unique talents and point of view to the table every day. We are curious and comfortable with change and are willing to take risks to transform education. Most importantly, with everything we do, we put learning first.\\n\\nWhat You'll Do Here:\\nDesign, develop and support applications for Cengage Learning’s edTech products.\\nWork within and across Agile teams to design, develop, test, implement, and support technical solutions.\\nParticipate in peer code reviews, knowledge sharing, and assist other engineers in their work.\\nContinuously improve software engineering practices and standards.\\nWork along with senior engineers to code solutions to specification.\\nParticipate in on-call rotation to support your engineering efforts.\\nSkills You Will Need Here:\\n\\n\\nRequired:\\n\\nMaster’s degree (or foreign education equivalent) in Computer Science, Software Engineering, Information Systems, Information Technology or a related field, plus one (1) year of experience analyzing, designing and coding large scale Java and JavaScript applications. Or, alternatively, Bachelor’s degree (or foreign education equivalent) in Computer Science, Software Engineering, Information Systems, Information Technology or a related field, plus three (3) years of experience analyzing, designing and coding large scale Java and JavaScript applications.\\n\\nCandidate must also possess:\\nDemonstrated Expertise (“DE”) creating and supporting Java and Javascript/Node.js applications in a production environment.\\nDE building Java applications within the Spring framework, and performing JavaScript programming utilizing NPM, Grunt, and Bower.\\nDE integrating and supporting REST services using SQL and NoSQL within an AWS environment.\\nDE building user interfaces utilizing React.JS and Redux.JS, and monitoring critical event logs using Splunk.\",\n",
       "  'Want to be part of shaping the future? Our breakthrough ability to unlock insights from the web radically improves intelligence and cyber threat visibility. We\\'re a high-energy, fast-paced, and fast-growing company that partners closely with our customers. This role is integral in delivering best practice threat intelligence solutions to our customers and building lasting, productive relationships between users and the company. Our ideal candidate has outstanding communication skills, proven history in customer community building or engagement, and is excited about building collaboration around cyber threat intelligence problem sets.\\n\\nThis Role:\\n\\nRecorded Future is a guardian of the internet. We defend against cyber attacks by analyzing the open web, deep web, and dark web. Our harvesting pipeline reads over 700,000 web sources and structured data feeds, and our real-time multilingual natural language processing technology takes that content from raw text to alerts and visualizations in minutes.\\n\\nThe Data Science team takes ownership of this unique dataset. We implement crucial harvesting capabilities, manipulate our data structures to obtain solutions and insights, and explore new use cases and capabilities for the data. We\\'re looking for smart and ambitious people with a strong engineering foundation and a quantitative mindset to help us take this on. In this role, you will implement new techniques for extracting high fidelity information from our vast collection of data, and what you build will become part of our Threat Intelligence product to help our customers stay protected. You will join a team of motivated people working on a tough problem, and help us shine a light into the dark corners of the internet.\\n\\nQualifications:\\n\\nRequired experience and skills:\\nTechnical education: BA, BS, MS, or PhD in Computer Science or related discipline, with a strong academic record.\\nPython Programming: 1-3 years work experience or equivalent academic background in data processing using Python. You are comfortable automating data flows with resilient, production-grade code.\\nData skills: Comfort working with large, complex data structures. You love working with heterogeneous data to answer real human questions, and you know how to write the code that makes that happen.\\nExcellent communication: Your clarity of thought is always apparent in your crisp and articulate emails, Slack chats, phone calls, and in-person conversations.\\nPreferred experience and skills:\\nPreference for prior experience working with SQL and NoSQL databases.\\nPreference for prior experience with the Elastic Stack.\\nPreference for prior experience working with data in Information Security, Cybersecurity, or Threat Intelligence.\\nWe realize we can only succeed with a team of very smart and passionate people. If you\\'re looking to work in a unique environment with ambitious, dedicated colleagues, the chance to collaborate with fantastic users and customers, then we have a lot in common! You\\'ll also be equipped with top technology, enjoy trips, \"the best\" coffee, great food and fun. We offer competitive compensation and a full range of benefits, as well as a great culture, commitment to professional development and social responsibility.\\n\\nDon\\'t forget to check out our podcast! Join the Recorded Future team, special guests, and our partners from the CyberWire to learn everything you want to know (and maybe some things you\\'d rather not know) about the world of cyber threat intelligence. All episodes are free and available on iTunes, GooglePlay, and Stitcher.',\n",
       "  \"1 Broadway (21026), United States of America, Cambridge, Massachusetts\\n\\nAt Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer\\n\\nThe Finance Technology team at Capital One is searching for innovative and analytical Data Engineersto join our team in Boston MA. Our data engineers are multilinguists who can speak the languages of how we operate as a business, how that business impacts our financials, and the latest technologies that are reshaping our Finance Tech landscape.\\n\\nIn this role, you will be responsible for building generic data pipelines & frameworks using open source tools on public Cloud platforms, as well as using complex algorithms to develop systems and applications for our Finance Business Partners.The right candidate for this role is someone who is passionate about technology, interacts with product owners and technical stakeholders, thrives under pressure, and is hyper-focused on delivering exceptional results with good teamwork skills. Human centered design is the underlying framework for this candidate who will have the opportunity to influence and interact with fellow technologists beyond his team and influence technology partners across the enterprise.\\n\\nThe Job & Expectations:\\nPartner with product owners, peers, and end-users to understand business requirements\\nBe the intersection with lines of business, Finance and technology. The ability to understand various perspectives is critical when optimizing an engineering solution\\nProvide technical guidance concerning business implications of application development projects\\nLeverage ETL programming skills in open source languages including Kafka, Python, Scala and SQL on various frameworks especially Apache Spark\\nDeploy DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test-Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Nexus, Git and Docker\\nManage multiple responsibilities in an unstructured environment where you're empowered to make a difference. In that context, you will be expected to research and develop cutting edge technologies to accomplish your goals\\nSkills and Experience\\nExperience with Cloud computing, preferably AWS and its services including S3, EMR/EC2, and Lambda functions\\n2 years of experience using open-source languages such as Python and Scala on Apache Spark framework\\n2 or more years experience working on data projects\\n2 years of work experience in data management engineering\\n2 years of experience in scripting languages (e.g. UNIX shell scripting, perl, etc.)\\nBasic Qualifications:\\nBachelor's Degree\\nAt least 2 year of experience in system analysis\\nAt least 2 years of experience in scripting languages\\nAt least 2 years of experience leveraging SQL for database querying\\nPreferred Qualifications:\\n3+ years experience in at least one scripting language\\n3+ years experience developing full stack software solutions\\nAt least 1 year of experience developing, deploying, testing in AWS public cloud\\n3+ years of experience in AWS Cloud computing\\n1+ years of experience in an Agile delivery environment\\nAWS certification\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\",\n",
       "  'Educational Tours helps students gain new perspectives and build skills for the future through experiential learning. As an accredited institution, we partner with educators across the world to create global education programs that blend classroom, digital, and experiential learning for students.\\n\\nThe Tours Engineering team @ EF Tours is the group of engineering and creative professionals empowering EF to send groups of students and educators on guided educational tours across the world. We power every step of our customer’s journey – from showcasing our tour offerings, to creating personalized itineraries, and collecting their feedback after the tour. Over the course of your career with Tours Engineering, you’ll have the opportunity to build beautiful front-end interactions for our customers, solve large scale data optimization problems, and get your hands on everything in between.\\n\\nWhat you’ll do:\\n\\nYou will be part of a cross-functional, largely autonomous data team, collaborating closely with analysts, data scientists, engineers and executive stakeholders in building a single source of truth. You’ll maintain and evolve our mature BI platform on SQL Server, optimize the existing Kimball data marts as they grow, build new models or data marts and support the many data analyst teams in further building and accessissing processed data. You will additionally be involved in augmenting the mature platform as we transition it to a data lake leveraging all the latest tools available anchoring on AWS resources.\\n\\nWho you are:\\n\\nTo succeed in this role, you’ll need excellent problem-solving skills, strong attention to detail, and the technical know-how to independently build solutions from start to finish. We are looking for a true expert on data modeling, who is comfortable working with datasets of varying latencies and size and disparate platforms.\\n\\nWe ask that you have:\\n7+ years of hands-on industry experience with data modeling and working in Business Intelligence\\nExtensive expertise with Microsoft SQL Server (2016+), SQL Server Integration Services (SSIS), ETL, Replication and Transact-SQL\\nAdvanced SQL expertise with multi-stage transformations and usage of more sophisticated transformation tools and methods, such as windowing functions and temp tables in Transact-SQL\\nDemonstrable experience implementing business logic into models that have been successfully applied for BI\\nExcellent verbal and written skills in order to effectively communicate business concepts to partner analysts and teams\\nApplied experience with relational modeling, star schemas and Kimball Data Warehousing\\nA drive to learn the intricacies of the business, with a versatile and dynamic understanding of businesses you’ve worked with in the past\\n1+ years of experience working with Python\\nOther useful skills include:\\nExperience with cloud hosting (AWS, Azure, etc.)\\nExperience with NoSQL/Non-relational databases, especially document stores\\nExperience defining and implementing a global data catalog for end user reference\\nA working knowledge of modeling with Analysis Services, both on-prem and Azure\\nFamiliarity modeling for BI visualization tools such as Power BI\\nLet’s talk about the Perks:\\nWe believe that happy people do great work. What makes us happy? Things like:\\nThe opportunity to travel internationally\\nDiscounts on travel\\nThree weeks paid vacation for your first year and then four weeks after your first year\\nFree language classes, taught in-house\\nAccess to a brand-new fitness center and rock-climbing wall on the EF campus\\nA restaurant and bar located in 2 of our EF campus buildings\\nMarket-leading benefits package including top of the line health coverage, 401k with company match, tuition reimbursement, and more…\\nEF is the world’s largest international education company – providing life-changing educational experiences to create global citizens. We offer millions of people the opportunity to learn a language, travel abroad, experience another culture or earn an academic degree. Since 1965, EF’s mission has been opening the world through education. Our 52,000 employees have a worldwide presence in over 600 schools and offices within 50 countries.\\n\\nFor more information about Education Tours, visit https://www.eftours.com\\n\\nFor more information about our career opportunities, visit www.careers.ef.com',\n",
       "  \"Job Description\\n\\n\\nPrincipal Duties and Responsibilities\\n\\nAt Bose we strive to “Wow” the customer, and we are driven by curiosity and perseverance. We like to concentrate on the job at hand. We value passionate, down to earth, “can do” people who enjoy fine-tuning small details, without losing sight of the big picture. We are looking for the type of person who loves to challenge the status quo, who isn’t afraid to give honest feedback, and feels uncomfortable when a day goes by without achieving something impactful. We tend to get excited when a challenge demands a creative solution. Above all else, this role requires someone who takes great pride in their work and is inspired and motivated by their role in improving the way millions of people listen to music world wide.\\n\\nAs a software engineer focusing on Big Data you will work with our IT team to develop data platforms that turn big data into big insights with tremendous value. Enable capabilities and provide business partners with the tools to make their decision making process more efficient and with greater speed. As part of an agile delivery team, you will design, develop, deploy and support the data ingestion pipeline and the data access solutions for our Big Data Analytics platform. Create new data sources for our dynamic customer experience strategies collaborating with touchpoint teams. This role requires knowledge and hands-on experience with big data technologies used throughout entire application stack include Spark, Cloudera Data Hub, and Python/Scala/R languages.\\nDesign and develop ETL pipelines connected devices, web applications, and mobile applications that support the customer experiences\\nCollaborate with front-end and mobile app development teams on user-facing features and services\\nWork with platform architects on software and system optimizations, helping to identify and remove potential performance bottlenecks\\nFocus on innovating new and better ways to create solutions that add value and amaze the end user, with a penchant for simple elegant design in every aspect from data structures to code to UI and systems architecture\\nStay up to date on relevant technologies, plug into user groups, understand trends and opportunities that ensure we are using the best techniques and tools\\nWork with other software leads on developing continuous integration (CI) pipeline and unit test automation\\nDocument the work you do, especially APIs that you create\\nQualifications (demonstrated competence):\\nDelivered the full lifecycle of a solution using Hadoop, AWS S3, AWS EMR\\nDelivered at least one Big data solution using cloud services & open source\\nExpert knowledge of programming languages such as Python, Java, or Scala\\nIngested data using Big Data ETL tools (Apache Spark)\\nSupport Data Science and ML tools like AWS Sagemaker, Cloudera CDSW, Google AI Platform\\nImplemented data security and privacy in a cloud environment\\nDelivered solutions using Agile methodology\\nHighly desirable but not required skills include:\\nExperience with cloud computing (Amazon Web Services preferred)\\nExperience with cloud computing services (Amazon Web Services like EC2, Dynamo, S3, RDS preferred)\\nExperience\\n8 or more years working in software development\\n4+ years developing, deploying and maintaining high volume production big data solutions\\nBachelor's degree in Computer Science, or equivalent\\nBose is an equal opportunity employer that is committed to inclusion and diversity. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, genetic information, national origin, age, disability, veteran status, or any other legally protected characteristics. For additional information, please review: (1) the EEO is the Law Poster (http://www.dol.gov/ofccp/regs/compliance/posters/pdf/OFCCP_EEO_Supplement_Final_JRF_QA_508c.pdf); and (2) its Supplements (http://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm). Please note, the company's pay transparency is available at http://www.dol.gov/ofccp/pdf/.pdf. Bose is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the application or employment process, please send an e-mail to Wellbeing@bose.com and let us know the nature of your request and your contact information.\",\n",
       "  \"Why PatientsLikeMe?\\n\\nThe healthcare system isn't doing enough for patients, but you can help change that. PatientsLikeMe is leading the social medicine space by allowing people to share their health data, track their progress, help others, and change medicine for good. Each time someone shares their experience, they're helping the next person diagnosed learn what could really work for them, and helping researchers shorten the path to new treatments. At PatientsLikeMe we're passionate about improving the lives of patients, improving the healthcare system, working on hard technical problems and doing all of this with teamwork. If you want to work with great people in an R&D organization on an initiative that matters, we want to talk to you. Come make a difference in patients' lives with us!\\n\\nWho are we hiring?\\n\\nPatientsLikeMe is looking for a data engineer to join our Cambridge, MA team. Remote and part-time (3-4 days per week) are possibilities. Our Data Engineers work with a multi-disciplinary team of Data Scientists, Clinical Data Managers, Research Scientists, Engineers and many others at PLM. We think about the process of getting good data and all of the potential opportunities for use in advancing our research and patient impact. If you are a software engineer who likes to focus your talents on creative data capture and manipulation and would like to use your skills to advance analysis of clinical data, we want to meet you!\\n\\nYour Roles and Responsibilities:\\nDesign and build software tools for data processing, analysis, and reporting, for use by data managers, data science, and research to automate their workflows\\nCreate data flow models and business logic for large-scale digital health studies using software and third-party clinical data management systems, in collaboration with clinical data managers, researchers, and other stakeholders\\nImplement data cleaning software and processes to ensure data integrity and identification and remediation of bugs in logic and software\\nBuild digital health data reporting functionality (automated, self-service, and manual) to support research and clinical operations\\nExperience and skill-sets to succeed in this role:\\nBachelor's Degree in a Technical or Analytical field of study OR 3+ years of relevant tech experience (data science, data engineering, software engineering, etc.).\\nProficiency with relational databases (we use Postgres) and queries (a love of SQL really helps!)\\nAbility to read, break down and eventually create scripts that process data (Python, R, Bash, etc.)\\nExperience developing software tools for data monitoring, reporting, and analysis in a general-purpose programming language (Python, Ruby, Java, etc.)\\nPassion for data integrity; close attention to detail\\nAbility to monitor and evaluate the performance of complex processes; considering What happened? What should have happened?\\nAbility to communicate process performance to cross-functional stakeholders\\nCompetitive salary, generous vacation package, flexible schedule, health and dental, parental leave, free lunch 2x/week and always with a vegetarian/vegan option. Friday Journal Club\\\\ Company Presentations — often by industry leaders in health and technology. Your teammates are also singers, cyclists, woodworkers, bookworms, animal lovers, artists, gardeners, athletes, parents, patients, caregivers, and all-around great people.\",\n",
       "  'Since 1851, MassMutuals commitment has always been to help people protect their families, support their communities, and help one another. This is why we want to inspire people to Live Mutual. We are people helping people.\\n\\nA career with us means you will work alongside exceptional people and be empowered to reach your professional and personal goals. Our employees are the foundation of what makes MassMutual a strong, stable and\\n\\nethical business. We seek and value unique and varied perspectives and experiences because we believe we are stronger when all voices are heard. We invite you to bring your bright, innovative ideas to MassMutual as we continue to help millions of Americans rely on each other.\\n\\nTogether, we are stronger.\\n\\nDescription\\n\\n\\nAt MassMutual, we are passionate about helping millions of people find financial freedom and this passion has driven our approach to developing meaningful experiences for our customers. The Data Engineering team, part of the Enterprise Technology and Experience, is comprised of highly skilled, collaborative, problem solvers who are motivated to create innovative solutions that exceed the changing needs of our customers and move MassMutual and the industry forward.\\n\\nTo continue our cutting-edge work, we are hiring a Sr. Data Engineer to join our team.\\n\\nWhat great looks like in this role\\n\\n\\nOur ideal Sr. Data Engineer is a collaborative leader skilled in data analytics, data modeling, and database design. Youre also committed to data integrity, are highly analytical, and can work on multiple projects at once. Youll use your skills to develop, monitor, and manage data systems across our platform. Additionally, you will act as a mentor to junior team members and coach them on best practices and engineering standards. The team culture of working collaboratively, cross-functionally, using new technologies combined with the work/life balance provided by MassMutual are core reasons people enjoy working on the Data Engineering team at MassMutual.\\n\\nObjectives of the role\\nDesign, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset.\\nWorking on a range of projects including batch pipelines, data modeling, and data mart solutions youll be part of collaborative project teams working to implement robust data collection and processing pipelines to meet specific business need.\\nDaily and Monthly Responsibilities\\nDesign, build, and measure complex ELT jobs to process disparate data sources and form a high integrity, high quality, clean data asset.\\nExecutes and provides feedback for data modeling policies, procedure, processes, and standards.\\nAssists with capturing and documenting system flow and other pertinent technical information about data, database design, and systems.\\nDevelop data quality standards and tools for ensuring accuracy.\\nWork across departments to understand new data patterns.\\nTranslate high-level business requirements into technical specs.\\nBasic Qualifications\\nBachelors degree in computer science or engineering.\\n4+ years of experience with data analytics, data modeling, and database design.\\n3+ years of coding and scripting (Python, Java, Scala) and design experience.\\n3+ years of experience with Spark framework.\\nExperience with Python\\nExperience with ELT methodologies and tools.\\nExpertise in tuning and troubleshooting SQL.\\nStrong data integrity, analytical and multitasking skills.\\nExcellent communication, problem solving, organizational and analytical skills.\\nAble to work independently.\\nAuthorized to work in the USA with or without sponsorship ow or in the future.\\nPreferred Qualifications\\nMasters degree in computer science or engineering.\\nFamiliar with agile project delivery process.\\nKnowledge of SQL and use in data access and analysis.\\nAbility to manage diverse projects impacting multiple roles and processes.\\nAble to troubleshoot problem areas and identify data gaps and issues.\\nAbility to adapt to fast changing environment.\\nBasic knowledge of database technologies (Vertica, Redshift, etc.).\\nWorking at MassMutual\\n\\n\\nA career at MassMutual means you will work alongside exceptional people and be empowered to reach your professional and personal goals. Our employees are the foundation of what makes MassMutual a strong, stable and ethical business. We seek and value unique and varied perspectives and experiences because we believe we are stronger when all voices are heard. We invite you to bring your bright, innovative ideas to MassMutual as we continue to help millions of Americans rely on each other.',\n",
       "  \"Yesware is seeking a Back End Software Engineer to join our talented development team. We have created a feature-rich backend architecture and data platform along with apps for Gmail, Outlook, and the web. Together these provide the leading prescriptive sales platform, which serves over 100K sales professionals worldwide.\\nWe value the insights and creativity of our engineers. You'll work side-by-side with product owners to create high-quality products at a fast pace. Our culture fosters innovation and excellence in engineering practices. Developers are responsible for the quality of what they produce, as well as putting that code into production and keeping it healthy. We have an internal open-source model in which you'll learn and extend the rest of our technology stack to realize the features you're working on.\\nBecause you will be tasked with delivering maintainable, well-documented software, and because some of your teammates may be remote, we highly value written communication skills at Yesware. As such, we require all engineering applicants to please provide a cover letter in their application. We look forward to hearing from you!\\nRequirements:\\nIndustry experience in software development. (Minimum of 1 year, additional experience highly valued.)\\nEngineering experience in a Ruby/Rails environment or similar web stack.\\nEvidence of exposure to architectural patterns of a large, high-scale web application (e.g., well-designed APIs, high volume data pipelines, efficient algorithms).\\nExcellent software development habits: object-oriented design, unit testing, integration testing, data structures, etc.\\nExperience with at least one SQL or NoSQL database.\\nA hands-on attitude and a willingness to get things done.\\nExcellent oral and written communication skills.\\nEmotional maturity and adaptability.\\nNice to Haves:\\nExperience developing email tools.\\nExperience developing SaaS offerings.\\nA desire to learn the other tools in our stack.\\nExperience developing strategies to handle database growth over time with both SQL and NoSQL databases.\\n\\nAbout the Company:\\nYesware is a team of smart, ambitious, kind people working together to empower each other and our customers. We build, market, and sell a one-of-a-kind sales platform that helps salespeople sell smarter, right from their inbox.\\nYou can find us on the top floor of our building, conveniently located in downtown Boston. We offer competitive compensation comprised of salary, equity and bonuses. We also offer a growing list of benefits: health, dental, 401(k), life insurance, unlimited vacation days, flexible spending account, as well as gym and public transportation reimbursement. Additional perks include weekly yoga and meditation classes, a fully stocked kitchen, free daily catered lunch, and a ride home with Lyft when you need it.\\nWe are proud to be venture-backed by investors including the Foundry Group, Battery Ventures, and Google Ventures. Our customers range from young startups of all sizes, to some of the biggest names in business, like Yelp, Acquia, Groupon, Zendesk, and many more. We live out our core values everyday in a mission to serve our customers and to make Yesware the best place we’ve ever worked.\\nYesware is committed to developing a culture that embraces diversity and fosters inclusivity. We do this because it makes us stronger as individuals, as a business and it is simply the right thing to do. We know that diversity comes in many forms, so we welcome employees from all walks of life to bring their differing experiences and perspectives to Yesware. Together, we can build an environment where every member of our team can bring their authentic self to work, be heard, and work toward achieving their professional potential.\\nRead more about us at www.yesware.com/company\",\n",
       "  'Working at MIT offers opportunities, an environment, a culture – and benefits – that just aren’t found together anywhere else. If you’re curious, motivated, want to be part of a unique community, and help shape the future – then take a look at this opportunity.\\n\\nDATA ENGINEER, Office of the Vice President for Finance (VPF)-Controllership, to participate in the full life cycle of business intelligence platform development, including design, coding, testing, and production support. Will implement innovative data solutions and maintain MIT financial data used for reporting and analytics; support VPF staff in its strategic, financial, and operational analytical needs; and work as a team member/lead on projects spanning a broad range of data-focused applications.\\n\\nA full description is available at https://vpf.mit.edu/about-vpf/jobs-at-vpf.\\n\\n</p>\\n\\nJob Requirements\\n\\nREQUIRED: bachelor’s degree (master’s preferred); five years’ experience building databases, data marts, or data warehouses with structured querying language (SQL) across multiple platforms, preferably Microsoft, Oracle, PostgreSQL, and MySQL; three years’ experience in or supporting financial/accounting functions; experience with ETL software, preferably Talend, Pentaho, SQL Server Integration Services (SSIS), and Informatica; experience with online analytical processing (OLAP) and visualization and reporting software, preferably SQL SSAS and SSRS, Tableau, Cognos, and BusinessObjects; advanced-level experience with Excel including modeling and PowerPivot and Visual Basic for Applications; experience with SAP or equivalent enterprise resource planning system; experience with data profiling, data quality, and master data management; detail orientation; ability to work independently and as part of a team; excellent interpersonal, communication, and problem-identification and -solving skills; and ability to collaborate with a diverse group of colleagues from varying backgrounds and meet deadlines in a busy, changing environment. PREFERRED: Data Management Association and/or Data Warehousing Institute membership; SQL, Visual Basic, and Java programming experience; knowledge of statistical tools (e.g., R, Python, SAS, SPSS); and experience with Agile methodology data governance and data vault design, SAP or similar accounting and reporting systems, and higher education or nonprofit accounting. Job #18069-8\\n\\n10/3/19\\n\\n</p>\\n\\nMIT is an equal employment opportunity employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, sex, sexual orientation, gender identity, religion, disability, age, genetic information, veteran status, ancestry, or national or ethnic origin.',\n",
       "  \"The Business Systems Data Analyst will assist the Director of Financial Planning and Reporting in leading, designing and execution of data analytics that will serve management needs, both internal and external. This role will provide primary business systems reporting, analysis and support for revenue cycle management as well as overall business performance.\\n\\nEssential Functions:\\n\\n• Performs business reporting and analysis utilizing the company's data environment, including EHR and finance/accounting systems.\\n\\n• Interprets data, analyzes and visualizes results to provide insight into business performance.\\n\\n• Monitors performance indicators and success metrics for proposed and existing initiatives, and/or services; may collect and analyze external market data to provide benchmarks for comparison purposes.\\n\\n• Supports the Patient Account team with reporting, month end close and support for EPM system.\\n\\n• Performs other related duties and ad hoc analysis as assigned.\\n\\nRequired and/or Preferred Education/Experience/Skills:\\n\\n• Bachelor’s degree or equivalent related experience; Field of Study: Finance, Accounting, Business Administration, Computer Science, or related field.\\n\\n• 3 years of experience in Health Care data analysis, reporting, etc. working directly with electronic health record systems.\\n\\n• Experience with SQL, and other Business Intelligence tools required.\\n\\n• Strong analytical and organizational skills.\\n\\n• Ability to manage large data sets and demonstrated experience in data analytics.\\n\\n• Ability to professionally communicate effectively, orally and in writing.\\n\\n• Ability to work under time constraints and deadlines.\\n\\nHarbor Health is an Equal Opportunity Employer M/F/V/D.\",\n",
       "  'Job Description:\\n\\nAre you adept at transforming and organizing varied, complex data? Do you have experience in data engineering with unstructured data? You might be the person we are looking for.\\n\\nOur enterprise-wide data science team is seeking a top-notch data engineer with strong technical knowledge and a real passion for addressing business needs through data analysis. As an individual contributor on this team, you will create tools and data pipelines that leverage the latest advances in data engineering to address high-impact research and business questions across research and development, clinical, commercial, and general and administrative areas of our business. Youll work side-by-side with internal partners from across the organization to develop creative solutions for our highest priority business needs.\\n\\nThe ideal candidate will be a data engineer who is driven by building pipelines that feed data scientists with data and can work both independently as well as part of a highly collaborative team. We are seeking a candidate with the demonstrable ability to find solutions where others cant, who has the drive and determination to pull the team forward and persevere. Were looking for self-starters with a strong sense of urgency who thrive when operating in a fast-paced environment.\\n\\nPrimary Responsibilities:\\nWrite clean, maintainable data pipelines that feed data scientists\\nCorrect, transform and enrich multiple sources of data\\nQuickly and efficiently load bulk and streaming data\\nWork closely with the data science team and internal business partners to identify the path to a successful product\\nQualification:\\nBachelors degree or higher in Computer Science or related discipline\\n3+ years of experience using an ETL tool. Informatica or Talend is preferred\\n5+ years of experience with SQL database queries and programming\\nExperience programming in Java or Python\\nFamiliarity with data quality, cleaning and masking techniques\\nExperience handling unstructured data\\nExperience working across multiple compute environments to create workflows and pipelines (e.g. HPC, cloud, Linux systems)\\nAn ability to interact with a variety of large-scale data structures (e.g. HDFS, SQL, noSQL)Strong adherence to data privacy standards and ethics\\nDeep understanding of algorithms and performance optimization\\nStrong interpersonal and communication skills and a demonstrated ability to work and collaborate in a team environment\\nPreferred Qualifications:\\nPrevious experience in healthcare, life sciences, or pharmaceutical industry is a plus\\nExperience with AWS cloud technologies and stack\\nKnowledge of distributed data processing and management systems\\nExperience with big data analytics platforms and/or workflow tools\\nDemonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively\\n#LI-MH1',\n",
       "  \"Do you want to build the analytics to understand and accelerate large scale migrations to AWS? Migrating to AWS is one of the most impactful business decisions AWS customers make and we want your help to better understand our customers' migration journeys. For customers large and small, migrating to AWS can have an enormously positive impact on their costs, agility, and employee growth. Here in the Migration Services team, were working closely with customers to invent new approaches to migrations, build scalable systems, and use machine learning to solve problems that havent been solved yet.\\n\\nAs a Data Engineer in AWS Migration Services you will work on the data pipeline and analytics to provide business and engineering stakeholders key insights into our customers migration journeys. You will get the exciting opportunity to interact with very large data sets in one of the most complex data warehouse environments. Our data pipeline combines metrics from multiple data sources including Amazon Redshift, Salesforce, and Amazon S3. You will have the opportunity to help business and engineering stakeholders determine what migration related metrics they should be tracking and establish new and expand existing automated data collection to feed into the data pipeline. You will regularly apply your analytical and problem solving skills and perform analysis with tools like Jupyter, SageMaker, and Pandas so we better understand customer migrations and how we can accelerate their migrations.\\n\\nDay-to-day you will:\\n· Work closely with product management, sales, and business stakeholders to analyze data from a multitude of sources about customers migrations and how we can accelerate their migrations.\\n· Design, implement, and maintain a data pipeline and analytical environment using third-party and in-house reporting tools, modeling metadata, and building reports and dashboards.\\n· Use creative problem-solving to automate the collection and analysis from available data sources in order to deliver actionable output.\\n· Iteratively improve analysis and identify new metrics to improve analytics.\\n\\n\\n\\n\\n\\nBasic Qualifications\\n\\n· 2+ years of relevant work experience in analytics, data engineering, business intelligence or related field\\n· 2+ years of programming experience in languages like Python\\n· Demonstrable ability in data modeling, ETL development, and data warehousing, or similar skills\\n· Experience with reporting tools like Tableau, Excel or other BI packages\\n· Experience in working and delivering end-to-end projects independently.\\n· SQL proficiency\\n· B.S. degree in mathematics, statistics, computer science or a similar quantitative field\\n\\nPreferred Qualifications\\n\\n· Graduate degree in mathematics, statistics, computer science or a similar quantitative field\\n· 5+ years of hands-on experience in writing complex, optimized SQL queries across large datasets.\\n· 5+ years of hands-on experience with data analysis tools like Jupyter and Pandas.\\n· Experience working with a diverse set of business and engineering stakeholders at all levels\\n· Experience with AWS technologies including Redshift, SageMaker, EMR, RDS, S3, and Kinesis\\n· Demonstrated ability to coordinate projects across functional teams, including engineering, sales, product management, finance, and operations\\n· Proven track record of successful communication of analytical outcomes through written communication, including an ability to effectively communicate with both business and technical teams\\n\\n\\nAmazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.\",\n",
       "  \"Company Overview\\n\\n\\nVectra Networks delivers a new class of real-time threat detection and advanced analysis of active network intrusions. Vectra Networks picks up where perimeter security leaves off using AI to provide deep, continuous analysis of both internal and Internet-facing network traffic for all phases of the attack progression as attackers attempt to breach, spy, spread, and steal within networks.\\n\\nVectra directly analyzes network traffic in real time using combination of patent-pending data science, machine learning, and behavioral analysis to detect attacker behaviors and user anomalies in the network. All detections are algorithmically correlated and prioritized to show an attack in context, and Vectra Networks' machine learning adapts as attacks evolve.\\n\\nPosition Overview\\n\\n\\nDetecting attackers in real-time requires robust data pipelines that enable machine learning and statistical techniques. As part of the Data Science team, you will transform rich network traffic data into meaningful features and develop data systems for collecting algorithm telemetry. You will build pipelines and tools for both on-prem and cloud deployments while collaborating with Data Scientists and Software Engineers in the process.\\n\\nResponsibilities\\nDevelop data pipelines for both research and production purposes, using a variety of distributed systems and databases.\\nImplement monitoring tools to track detection algorithm behavior and health.\\nCollaborate with Data Scientists and Software Engineers within the team to bring new algorithms to production.\\nInterface with Software and DevOps Engineers on our Platform team.\\nQualifications\\nRequired\\nBS or MS in Computer Science or related field (or equivalent experience)\\nStrong experience with Python\\nExperience with Docker, AWS/Azure/On-Prem deployments, and networking\\nLinux proficiency and administration\\nExperience with a source control system, preferably Git\\nDesirable\\nFamiliarity with Hadoop, Map/Reduce, Spark, and distributed computing\\nUnderstanding of data pipeline architectures (e.g. Lambda, Kappa)\\nDatabase hands-on experience (MySQL, MongoDB, couchdb, ElasticSearch, etc.)\\nKnowledge of real-time data pipelines (e.g. Kafka and Spark Streaming)\\nExperience with continuous integration and deployment workflows\",\n",
       "  'Ready to help us transform healthcare? Bring your true colors to blue.\\nWe need Senior Data Engineer for our newly established Big Data Engineering practice. While primary focus of the role is to lead Data development effort of data platform, architecture, working closely with Digital & Analytics team for opportunities to deliver “Data as service”, another key element of the role is to contribute towards the best practices and framework to deliver a scalable platform using newer tools and technology. This role requires a combination of hands of engineering (80%) as well Architecture (20%).\\n\\nTechnical Delivery Experience:\\n\\n· Self-learner and networking skills that are required to be successful in this role given the rapid evolution of the New Platforms and Tools.\\n\\n· Experience indeveloping new applications and features within a scrum team providing\\n\\ndata and data services to the enterprise, other engineering teams, data science, analysts, product, management/executives, and other business teams\\n\\n· Experience in building high performing and scalable data ingestion patterns to support multiple batch processing and real-time data streaming.\\n\\n· Work with various project teams to deliver on commitments within time and scope\\n\\n· Work with Platform Operations team to ensure solutions are releasable, maintainable, and scalable\\n\\n· Experience or a good understanding and willingness to working in DevSecOps and Agile Scrum based Software Development.\\n\\n· Experience in developing scalable data engineering platform using cloud infrastructure.\\n\\n· Experience with configuration management, continuous integration, continuous deployment and continues security.\\n\\n· Experience or a good understanding and willingness to include Security at design, development and testing\\n\\n· Familiarity with container tools and serverless technologies.\\n\\n· Understanding of servers, storage, and networking roles\\n\\n· Experience with Lean, Agile and iterative delivery frameworks, metrics and methodologies.\\n\\n· Experience in performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\n\\n· Strong analytic skills related to working with unstructured datasets.\\n\\n· Experience in building processes for supporting data transformation, data structures, metadata, dependency and workload management.\\n\\nLearning Experience:\\n\\n· Experience or a good understanding and willingness to commit to and practice Continuous improvement in people, process and technology to promote collaboration, agility and speed to deliver using agile, sprint-based working style.\\n\\n· Experience in ensuring the right level of Communication to keep all stakeholders, team members and users in sync and on the same page\\n\\n· Ability to be open to learn, to give and take feedback, to put team above own interest, to excel in team success and to strive for excellence\\n\\nLeadership Experience:\\nExtensive experience to reassess and adjust tactical and/or operational direction\\nGreat communication skills coupled with a strong desire for personal development and learning\\nStrong verbal and written communication skills with demonstrated technical leadership\\nSkills to Establish and Maintain Delivery Based Business and Peer relationship\\nExperience in managing Service Providers and Holding them accountable\\nPartnership Experience:\\n\\n· Experience in establishing effective partnerships across teams, peers and subordinates in an inclusive style to leverage their abilities and knowledge.\\n\\n· Ability to Align with Leadership Guidance and Provide recommendation on team sizes, engagement, measurements to maximize collaboration, eliminating handoffs and high paced agile delivery environment\\n\\nEducation:\\n\\nBachelor’s degree in a field linked to data engineering, business analytics, applied mathematics, computer science, IT, computer applications, engineering or related field is required.\\n\\nTechnical Skills Required:\\n5+ years of experience in a Data Engineer role, experience using the following software/tools:\\nExperience with big data tools: Hadoop, Spark, Kafka, etc.\\nExperience with relational SQL and NoSQL databases, including Postgres and Cassandra.\\nExperience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\\nExperience with AWS cloud services\\nExperience with stream-processing systems: Storm, Spark-Streaming, etc.\\nExpert in object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\\nExpertise in using PowerPoint/Confluence and clearly articulating findings / presenting solutions\\nExperience with Testing Driven Development process (TDD)\\nExperience with conducting code reviews.\\nLocation\\nBoston\\nTime Type\\nFull time',\n",
       "  'Position Summary\\n\\n\\nInsulet Corporation, maker of the OmniPod, is the leader in tubeless insulin pump. The Data Engineer role is responsible for data lake infrastructure, development of automated data uploads and scripting for data cleansing and analytics. Reporting to the Senior Director, Global Technology and Cloud Ops, you will develop tools and processes to transform data for use with Insulet’s Analytics team and senior technical leaders. We are a fast growing company that provides an energetic work environment and tremendous career growth opportunities.\\n\\nResponsibilities\\nDesign, implementation and maintenance of Insulet’s data lake, warehouse and overall architecture\\nWork with IT, analytics and cross functional teams to identify data sources, determine data collection and design aggregation mechanisms\\nPerform data quality checks and data clean up\\nInterface with business stakeholders in cross-functional teams, including manufacturing, quality assurance, and post-market surveillance in order to understand various applications and their data sets\\nDevelop data preprocessing tools as needed\\nMaintenance and understanding of the various business intelligence tools used to visualize and report team analytics results to the company\\nEducation and Experience\\nBachelors degree in Mathematics, Computer Science, Electrical and Computer Engineering, or a closely related STEM field is required\\nMaster’s degree in Mathematics, Computer Science, Electrical and Computer Engineering, or a closely related STEM field; or a BS with 2-3 year’s experience working with data technologies, is preferred\\nExperience in data quality assurance, control and lineage for large datasets in relational/non-relational databases\\nExperience managing robust ETL/ELT pipelines for big real-world datasets that could include messy data, unpredictable schema changes and/or incorrect data types\\nExperience with both batch data processing and streaming data\\nExperience in implementing and maintaining Business Intelligence tools linked to an external data warehouse or relational/non-relational databases is required\\nExperience in medical device, healthcare, or manufacturing industries is desirable\\nHIPAA experience a plus\\n\\n\\nSkills/Competencies:\\nDemonstrated knowledge in SQL and relational databases is required\\nKnowledge in non-relational databases (MongoDB) is a plus\\nDemonstrated knowledge of managing large data sets in the cloud (Azure SQL, Google BigQuery, etc) is required\\nKnowledge of ETL and workflow tools (Azure Data Factory, AWS Glue, etc) is a plus\\nDemonstrated knowledge of building, maintaining and scaling cloud architectures (Azure, AWS, etc), specifically cloud data tools that leverage Spark, is required\\nDemonstrated coding abilities in Python, Java, C or scripting languages\\nDemonstrated familiarity with different data types as inputs (e.g. CSV, XML, JSON, etc)\\nDemonstrated knowledge of database and dataset validation best practices\\nDemonstrated knowledge of software engineering principles and practices\\nAbility to communicate effectively and document objectives and procedures',\n",
       "  'Data Engineer\\n\\nDuration: Long term contract (6 month increments)\\n\\nJOB EXPERIENCE REQUIREMENT:\\n6-10 years professional development experience\\nDatabase development background, SQL, PL/SQL\\nStrong background with object oriented programming, with technologies including Python, Java or .Net, .Net Core, C#\\nExperience in Pattern Matching to extract data from Web Sites (Web Scraping), Text Documents, etc.\\nKnowledge of Cloud computing concepts (AWS) and working experience with deploying and managing applications in the Cloud\\nPlease send us the word format resume along with the day-time contact number and your work authorization.\\n\\n- provided by Dice',\n",
       "  \"Our Enterprise Data Analytics Platform (EDAP) is designed to explore and extract insights that our customers depend upon. Being the primary source for analytics, our customers both internal and external depend upon us to provide accurate, real-time, and fault tolerant solutions to their ever-growing data needs. The Enterprise Data Analytics Platform builds highly performant scalable analytics solutions varying from data storage systems to computation and serving solutions. We utilize a plethora of open source and industry accepted technologies for our big data problems such as Apache Spark, Apache Storm, Amazon Web services, and Apache Kafka.\\n\\nWe are looking for a skilled Software Engineer with an eye for building and optimizing distributed systems to join our team. From data ingestion, processing and storage, to serving and scale, we work closely with other engineers and product management to build consistent and highly available systems that tackle real world data and scale problems.\\nThis role is scoped as an individual contributor.\\nResponsibilities:\\n• Focus on the development of cloud computing infrastructure, and help build, distribute, scale and optimize these technologies\\n• Designs and implements cloud scale distributed data-focused Data Analytics Platform, services and frameworks including solutions to address high-volume and complex data collection, processing, transformation and reporting for analytical purposes\\n• Writes code and unit tests, works on API specs, automation, and conducts code reviews and testing\\n• Develop scalable, robust, and highly available services related to our data platform\\n• Implement new features and optimize existing ones to drive maximum performance\\n• Owns technical aspects of software development and identifies opportunities to adopt innovative technologies.\\n• Identifies continuous improvements for service availability\\n• Evaluates and recommends tools, technologies and processes to ensure that the services that the team provides achieve the highest standards of quality and performance\\n• Debugs and troubleshoots problems in data flow, lineage, transformation and other stages of the ETL pipelines.\\n• Collaborates with other peer organizations (e.g., Business Analyst, Data Modeler, QA, SRE, technical support, etc.) to prevent and resolve technical issues and provide technical guidance.\\n• Work in Agile development environment. Attend daily stand-up meetings, collaborate with your peers, prioritize features, and work with a sense of urgency to deliver value to your customers\\n\\nBasic Qualifications:\\nBachelors degree\\n5 years of meaningful experience\\nA strong level of curiosity paired with the ability to get things done.\\nStrong algorithms, data structures, and coding background.\\nExperience with software engineering standard methodologies (e.g. unit testing, code reviews, design document)\\nJava or Scala or other programming experience\\nStrong in SQL\\nExperience building products using 1 of the following distributed technologies:\\n** Relational Stores (i.e Postgres or MySQL or Oracle)\\n** Columnar or NoSQL Stores (i.e Vertica or Redshift or Cassandra or DynamoDB)\\n** Distributed Processing Engines (i.e Apache Spark or Apache Storm or Celery)\\n** Distributed Queues (i.e Apache Kafka or Kinesis or RabbitMQ)\\nHas worked with systems processing large amounts of data\\nHas worked with partner data scientist, data analysts and other domain experts to understand their needs and be able to develop solutions\\nExperience working with AWS or similar cloud platform technologies\\nAuthorized to work in the United States with or without sponsorship.\\nPreferred Qualifications:\\nBachelor's degree in Computer Science preferred\\n8 years of meaningful development, experience is preferred\\nKnowledge of open source/industry standard data processing, storage, and serving technologies.\\nAbility to take proactive, problem-solving/troubleshooting approach to identifying the root cause of issues and solving problems\\nWorking with Data Warehousing is a plus\\nExperience with source control systems, Git preferred\\nMust be able to quickly grasp the platform/application overarching design and ensure development is executed in accordance with the present design\\nAbility to drive change through persuasion and consensus\\nExperience in Financial/Insurance industrial\",\n",
       "  \"Job Description\\nThe Company\\n\\nCognex is the global leader in the exciting and growing field of machine vision. With over $800 million of cash in the bank and no debt, we are a financially strong international company with a culture that maintains the fast paced, creative environment of a startup. Our employees, proudly called “Cognoids,” take their work seriously, but don't take themselves seriously. Our Work Hard, Play Hard, Move Fast culture recognizes our employees for their innovation, perseverance and hard work in a fun, rewarding, and quirky environment.\\n\\nThe Team:\\n\\nDelivering Cognex ID and Vision Automation Solutions into various Retail/Commercial/Freight Distribution Centers, Parcel & Postal, and Airport Baggage Handling Industries, the logistics business is one of the fastest growing segments within Cognex. The Logistics Business Unit (LBU) is chartered to lead the engineering, delivery, and acceptance of our ID and Vision Automation Solutions to some of the largest global customers in these industries.\\n\\nThis position is in the Logistics Software Engineering team, focused on providing software that furthers the goals of the Logistics Business Unit. The software written by this team will help to reduce variability in standard designs, improve efficiency during installation and commissioning, and solve demanding new applications.\\n\\nThe Role:\\n\\nThe Logistics Business Unit is looking for well-rounded, intelligent, creative and motivated candidates with a passion for results!\\n\\nAs a member of the LBU, you will have opportunities to build your skill set with software development, key technology, and customer application knowledge. You will get the opportunity to visit some of our most important customers, collaborate with our cross-functional, multi-national engineering teams, and get exposure to complex and challenging problems involving hardware, software, and systems integration.\\n\\nEssential Functions:\\nWork on a small software team to prototype and develop innovative features and solutions on top of Cognex products and technology\\nLearn and improve methods for configuration and setup of logistics solutions (communications and networking protocols, lighting and optics, etc.)\\nEnsure high-quality product by developing test automation and test plans\\nDebug and troubleshoot in a hardware and software environment\\nUnderstand our products and our customers’ applications to provide optimal solutions\\nSupport and communicate with Cognex customers and internal teams\\nOn-site customer visits to support installation and diagnose issues (Typical 25%)\\nKnowledge, Skills , and Abilities:\\nHigh energy and motivated learner\\nStrong analytical and problem-solving skills\\n1-3 years or less of Software development knowledge (e.g. C, C#, and JavaScript) required\\nExperience with software development processes, source code control, formal bug tracking desired\\nExperience with embedded systems, networking, vision/image-processing, industrial programming, and optics all desired.\\nTeam player with ability to work effectively on multi-site development projects\\nGood presentation, communication, organizational and interpersonal skills\\nMinimum education and work experience required:\\nBachelor’s or master’s degree in a related technical field (Computer Science, Computer Engineering, or other related engineering field)\\nWe are an Equal Opportunity Employer. Protected Veterans and Individuals with Disabilities are encouraged to apply.\\n\\nAdditional Job Description\\n\\n\\nEqual Employment Opportunity\\n\\nCognex is an equal opportunity employer. Cognex evaluates qualified applicants without regard to race, color, religion, gender, national origin, age, sexual orientation, gender identity or expression, protected veteran status, disability/handicap status or any other legally protected characteristic.\",\n",
       "  'The Plymouth Rock Company and its affiliated group of companies write and manage over $1.3 billion in personal and commercial auto and homeowner’s insurance throughout the Northeast, where we have built an unparalleled reputation for service. We continuously invest in technology, our employees thrive in our empowering environment, and our customers are among the most loyal in the industry.\\n\\nThe Plymouth Rock group of Companies employs 1,800 people and is headquartered in Boston, Massachusetts. Plymouth Rock Assurance Corporation holds an A.M. Best rating of \"A-/Excellent\".\\n\\nThe Data Engineer’s mission will be to provide the in-depth technical and analytical support required to propel our analytics team forward during an exciting time of expansion at the Plymouth Rock companies. The successful candidate will participate in a broad range of data management, business intelligence and other analytics-based activities designed to ensure Plymouth Rock maintains and extends its competitive edge with respect to loss management, claims customer service, expense control, and employee engagement.\\n\\nEssential Duties and Responsibilities:\\nAct as primary architect and owner of our data infrastructure\\nPrincipal owner and administrator of our team’s SQL Server\\nDevelop and maintain jobs, stored procedures, SSIS packages and other ETL processes\\nTake the lead in assembling and structuring data for analysis and introducing automation techniques where appropriate\\nIdentify, design and implement technical process improvements, automate manual processes, optimize data delivery with a constant eye towards scalability\\nWork closely with Claims Department management and other business functions by developing and producing both recurring (production) and ad hoc reports and metrics, as well as assisting teams with data-related technical issues and support their data infrastructure needs\\nCreate new tools to help manage our business and optimize results\\nProvide other Claims Department analytical support and collaborate with other departments as projects require\\nTechnical Proficiency:\\nAdvanced SQL knowledge and comfort working with relational databases and complex query authoring\\nExperience managing data environments independently\\nExperience working with common business intelligence tools, preferably Tableau\\nAdvanced knowledge of Excel\\nExperience with object-oriented programming languages (such as Python, C++, etc.)\\nExperience working with big data and/or unstructured data a plus\\nEducation and Experience:\\nBachelor’s Degree in a technical discipline desired (engineering, statistics, computer science, economics, or mathematics). Master’s Degree a plus.\\nPrior experience working with insurance data preferred\\nSkills and Abilities:\\nExceptional analytical and creative problem solving abilities\\nIntrinsic sense of urgency with ability to prioritize workload, meet project deadlines and drive results\\nIntellectually curious and interested in exploring new ways to analyze data\\nAbility to manage and analyze data, determine trends, identify anomalous results, draw conclusions and communicate findings\\nAlways willing to wrestle even minor unexplained variances found in data to the ground\\nEffective communication skills (both verbal and written)\\nAbility to strike the right balance between working independently and collaboratively depending upon the individual task or project assigned',\n",
       "  'Equian has grown to become one of the nations leading companies for healthcare reimbursement analysis and payment integrity. We have been voted one of 40 Healthcares Hottest by Modern Healthcare and have been recognized by Inc. 5000 as one of the fastest growing companies in the United States. We are an employee-centric company and offer the most competitive salaries in the industry, exceptional benefits, wellness programs, and many other perks you find at many other companies. Come join one of the finest and most recognized organizations in the industry!\\n\\nAt Equian, we are developing the next generation of integrated technology solutions that brings data mining, data analytics, workflow, document management and process management into a seamless platform to meet the challenges of a rapidly changing business environment.\\n\\nWe offer a competitive compensation package, which includes Medical, Dental, Vision, Life, Disability, a flexible Paid Time Off program, scheduled and floating holidays, educational assistance, wellness program and a 401(k) Plan with Company Match.\\n\\nWe are seeking to expand our team with motivated individuals with a proven ability to prioritize multiple tasks and meet deadlines in a dynamic environment.\\n\\nWe have a current opportunity for Data Analyst located in the Woburn, MA location. Below you will find a summary of the role and skills required:\\n\\nROLE SUMMARY\\n\\nIn the Data Analyst role, you will take responsibility for troubleshooting data issues, developing reports, and managing our master data set. In order to succeed in this role, you must be comfortable working with others in a fast-paced environment. In addition, you will need an eye for detail, experience as a data analyst, and a deep understanding of Microsoft SQL Server and Microsoft Excel. The successful candidate will have a strong work ethic and a data-driven approach.\\n\\nDUTIES AND RESPONSIBILITIES\\nWork closely with service operations team to evaluate and price overpayments for our clients.\\nGenerate reports to highlight current state of business operations.\\nTraining end users on new reports and dashboards.\\nSupporting initiatives for data integrity and normalization.\\nLoading, cleaning, and manipulating data from multiple sources including internal, external, and 3rd parties.\\nQUALIFICATIONS\\nBachelors degree in MIS, CS, Statistics, Mathematics or related field, new grads welcome to apply!\\nProficiency in basic SQL commands and queries.\\nStrong analytical and quantitative skills.\\nKeen understanding of relational database theory, preferred but not required.\\nMust be a self-starter and eager to learn.\\nExcellent communication skills, both written and verbal.\\nDemonstrated ability to build relationships and work as part of a team.\\nMust be authorized to work in USA. Equian will not provide sponsorship.',\n",
       "  \".\\n\\nDo you dare to reinvent the future of education?\\n\\n\\nAt Cengage, we are harnessing the power of tech to build a future where all learners have the tools and confidence to achieve their goals. As a Cengage employee, you will blaze a new trail to transform the way people learn. Collaborating with the best of the best, you will feel challenged and inspired to do breakthrough work. With the support of our united team, there is no limit to what you can imagine, create and set in motion.\\n\\nAre we right for you?\\n\\n\\nWe set the bar higher by bringing our unique talents and point of view to the table every day. We are curious and comfortable with change and are willing to take risks to transform education. Most importantly, with everything we do, we put learning first.\\n\\nWhat You'll Do Here:\\n\\n\\nThe Software Engineer will be responsible for application development and support of Cengage Learning’s high-quality, electronic products. The Developer will work in a highly collaborative, cross functional and Agile team.\\n\\nResponsibilities:\\nTest-drives awesome code, working closely with other Agile team members.\\nGets things done. Takes ownership of tasks when others do not. Helps drive the completion of all kinds of work tasks.\\nHelps break down, estimate, and provide just-in-time design for small increments of work.\\nBuilds a strong team using their collaboration skills.\\nPromotes improvements in version control, continuous integration, project build, and project automation.\\nPair programs with different people in many situations.\\nPerforms root cause analysis, technology evaluation, and design spikes.\\nResponsible for developing unit testing and overall integration tests for the project team.\\nProvides direction and shares knowledge with team members in areas of expertise\\nContinuously learns, and is unafraid to learn new practices, processes, technologies, and languages.\\nContinuously acts to enhance knowledge of new technologies, business processes, and project management skills, staying abreast of trends, latest industry developments and knowledge sharing among colleagues\\nProvides development and support for portions of one or more applications which may span multiple product or platform development teams\\nCommits to completing well-defined work, and delivers on those commitments\\nParticipates in weekly iteration demos for product managers\\nReports status of assigned software development and/or maintenance tasks\\nConsistently follows software development methodology\\nSkills You Will Need Here:\\n\\n\\nSoftware skills:\\nBachelor degree in Computer Science or related field, or equivalent combination of education and recent, relevant work experience\\nMinimum 2-3 years software development experience including C# and JavaScript programming experience. Including a JavaScript framework such as AngularJS, React or JQuery\\nThorough understanding of object oriented design and programming\\nExperience using source code control systems, such as Git or Subversion\\nAbility to troubleshoot error from application and build logs\\nAccessibility and WCAG 2.0+ compliance preferred\\nStrong verbal and written English communication skills required\\nCreative problem solving skills and ability to effectively communicate and translate feedback, needs and solutions\\nMust have strong teamwork orientation and the ability to foster collaboration within and across teams\\nNeeds good work ethic and strong sense of ownership of end result\\nExperience using Jenkins (preferably with a master and slave architecture)\\nExperience with cloud platform deployment (in anticipation to moving to the cloud)\\nSoft Skills:\\nAble to shift focus when priorities change.\\nCapable of tracking the status of multiple in process tasks that have dependencies\\nClear communication in requests to other teams\\nMust be sensitive to cultural differences in interactions and in the way work is done\",\n",
       "  \"We deliver our customers peace of mind every day by helping them protect what they value most. Our passion for placing the customer at the center of everything we do is driving a transformational shift at Liberty Mutual. Operating as a tech startup within a Fortune 100 company, we are leading a digital disruption that will redefine how people experience insurance.\\n\\nAbout the job:\\n\\nThe candidate will be part of a talented agile team working to build the next generation of products and solutions within the Digital & Direct business unit. We are seeking experienced, motivated Software Engineers to participate in the development of enterprise class APIs and microservices. You will be engaged in challenging, innovative projects that impact millions of customers.\\n\\nThe industry changes quickly, so we are looking for candidates who can respond to change, pick up new technologies quickly and adapt to changing requirements. We also want candidates who take pride in their work and have strong design and development expertise. We value quality code delivery and expect candidates to demonstrate competence at writing, testing and debugging complex Java code.\\n\\nYour role and responsibilities\\n\\nYou will collaborate closely with a team of technologists, analysts, developers and test engineers to deliver complex software solutions.\\nDrive full cycle end to end development from design through implementation.\\nThe role requires hands-on development, problem resolution and knowledge of various horizontal and vertical packages - spanning across all layers of our technical stack.\\nIn an Agile environment, work with business team members, product owners, and other software engineers to review and qualify business requirements, functional specifications, use-cases, and test plans.\\nDesign, prototype and author code for software components and applications based on functional specification and optimize them for system performance.\\nQualifications:\\n\\nBachelor's Degree in technical discipline preferably computer science or software development.\\nGenerally 3+ years of professional development experience.\\nModerate knowledge of IT concepts, strategies, methodologies, architectures and technical standards.\\nExcellent analytical, problem solving, and communication skills.\\nExperience with layered system architectures and layered solutions; understanding of shared software concepts.\\nJava development experience\\nExperience working with agile methodologies (Scrum, Kanban, XP) and cross-functional teams (Product Owners, Scrum Masters, Developers, Designers, Test Engineers)\\nIdeally familiar with Design Thinking, Behavior and Test-Driven Development\\nExperience with HTML, JavaScript, XML/XSD, Web Services.\\nExperience with Unit Testing frameworks (e.g., Junit, Mockito).\\nExperience with IBM Integration Designer, WebSphere Integration Developer (WID) is a plus.\\nExperience with Spring Boot and/or Docker is a plus.\\nExperience with cloud services such as Amazon Web Services is a strong plus.\\nExperience integrating with and managing microservices exposed via SOAP and/or REST APIs including development and support of Java services a strong plus.\\nBenefits:\\n\\nWe value your hard work, integrity and commitment to positive change. In return for your service, it's our privilege to offer you benefits and rewards that support your life and well-being. To learn more about our benefit offerings please visit: https://LMI.co/Benefits\\nOverview:\\n\\nAt Liberty Mutual, we give motivated, accomplished professionals the opportunity to help us redefine what insurance means; to work for a global leader with a deep sense of humanity and a focus on improving and protecting everyday lives. We create an inspired, collaborative environment, where people can take ownership of their work; push breakthrough ideas; and feel confident that their contributions will be valued and their growth championed.\\n\\nWe're dedicated to doing the right thing for our employees, because we know that their fulfillment and success leads us to great places. Life. Happiness. Innovation. Impact. Advancement. Whatever their pursuit, talented people find their path at Liberty Mutual.\",\n",
       "  'At Kensho, we hire talented people and give them the autonomy, support, and resources needed to build cutting edge technology and products for our parent company, S&P Global. As a result, we produce technology that is scalable, robust, and solves the challenges of one of the world’s largest, most successful financial institutions.\\n\\nAs a Machine Learning Engineer, you will be responsible for tackling a wide range of problems from natural language processing to timeseries prediction. You will move beyond the theoretical confines of academia and apply your tradecraft to build machine learning systems on real world data. In this role you will constantly collaborate and work with groups of engineers. Our machine learning team is responsible for delivering ML-based solutions from problem framing, to prototyping, to production application.\\n\\nAre you looking to make impactful, scalable contributions that could transform the way people think about data? If so, we would love to help you excel here at Kensho. We take pride in our team-based, tightly-knit startup Kenshin community that provides our employees with a collaborative, communicative environment that allows us to tackle the biggest challenges in data.\\nYou Will\\nYou will conduct original research on large proprietary and open source data setsIdentify, research, prototype, and build predictive products\\nYou will build cutting-edge models for understanding vast amounts of textual dataWrite production code\\nYou will write tests to ensure the robustness and reliability of your productionized models\\nYou will work closely with software engineers to build incredible systems\\nWhat We Look For\\nYou have at least one core programming expertise, such as python (NumPy, SciPy, Pandas), MATLAB, or R\\nYou have experience with advanced machine learning methods\\nYou possess strong statistical knowledge, intuition, and experience modeling real data\\nYou have a stellar ability to communicate even the most complicated methods and results to a broad, often non-technical audience\\nYou demonstrate effective coding, documentation, and communication habits\\nSeveral of the following terms should hold deep meaning for you: LSTM, lookahead bias, bagging, boosting, stacking, information retrieval, batch norm, entity recognition, bootstrapping, Glorot initialization, Kullback-Leibler divergence, GLOVE, SMAPE, HMM, MAP, exponential family, VC dimension, EM, L1, TD(Lambda)\\nHow You Can Really Get Our Attention\\nYou have 3+ years of experience being a major machine learning contributor at a top company, hedge fund, or university\\nYour github/kaggle profile shows a project or problems you’ve tackled\\nYou have the ability and credibility to lead a team\\nTechnologies You Will Use\\nPython and specifically Numpy, SciPy, Pandas, scikit-learn\\nNeural network packages like TensorFlow and Keras\\nML packages like LightGBM and XGBoost\\nOpenFST\\nElasticsearch\\nBenefits and Perks\\nAt Kensho, we pride ourselves on providing top-of-market benefits, including:\\nMedical, Dental, and Vision insurance - 100% company paid premiums\\nUnlimited Paid Time Off\\n18 weeks of 100% paid Parental Leave (paternity and maternity)\\n401(k) plan with 6% employer matching\\nGenerous company matching on donations to non-profit charities\\nUp to $20,000 tuition assistance\\nPlentiful snacks, drinks, and regularly catered lunches\\nDog-friendly office (CAM office)In-office gyms and showers (CAM, DC) or Equinox membership (LA, NYC)\\nStipend towards commuter or gym reimbursement\\nBike sharing program memberships\\nCompassion leave and elder care leave\\nMentoring and additional learning opportunities\\nOpportunity to expand professional network and participate in conferences and events\\nAbout Kensho\\n\\nKensho uses machine learning, artificial intelligence, natural language processing and data visualization techniques to solve some of the hardest analytical problems and create breakthrough financial intelligence solutions for our parent company, S&P Global.\\n\\nKensho was founded in 2013 by Harvard & MIT alums and was acquired by S&P Global in 2018 for $550M, the largest FinTech AI acquisition ever. Kensho continues to operate as a startup in order to maintain our distinct, independent brand and to promote our breakthrough, innovative culture. Our team of Kenshins enjoy a dynamic and collaborative work environment that runs autonomously from S&P, while leveraging the unparalleled breadth and depth of data and resources available as part of S&P Global.\\n\\nAs Kenshins, we pride ourselves on maintaining an innovative culture that depends on diversity and inclusion. We are an equal opportunity employer that welcomes future Kenshins with all experiences and perspectives.\\n\\nKensho is headquartered in Cambridge, MA, with offices in New York City, Washington D.C. and Los Angeles.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or national origin.',\n",
       "  \"Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\n\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\n\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\n\\n• Homesite Insurance was founded in 1997 and was one of the first companies to enable customers to purchase home insurance directly online, during a single visit. Since then, we've continued to innovate rapidly to meet the needs of our customers and their changing expectations.\\nOne thing that's stayed the same since our founding: our commitment to our customers, partners and employees.\\nJoin us on our journey as we continue to grow into a powerful contender in the field of insurance.\\nWe’re looking for a Data Engineer to help us transform our data systems and architecture to support greater variety, volume, and velocity of data and data sources. You might be a good fit if:\\nYou enjoy extracting data from a variety of sources and find ways to connect them and make them suitable for use in software systems and for the development of models and algorithms.\\nYou enjoy interacting with new database systems and learning new data technologies and are interesting in developing your knowledge of new tools and techniques.\\nYou are interested in automating data engineering efforts to minimize human interaction and optimizing data quality.\\nYou have an interest in developing your knowledge of practical data science techniques and technologies in addition to your data engineering knowledge and experience.\\n\\nThis role requires comprehensive data engineering skills and is not a SQL developer role though SQL is a required skill.\\n\\nResponsibilities:\\n\\nWe’re looking for an experienced data engineer to help us:\\nBuild and Maintain serverless data ingestion and refresh pipelines in terabyte scale using AWS cloud services – Amazon Glue, Amazon Redshift, Amazon S3, Amazon Athena, DynamoDB, and others\\nIncorporate new data sources from external vendors using flat files, APIs, web-scraping, and databases.\\nMaintain and provide support for the existing data pipelines using Python, Glue, Spark, and SQL\\nWork to develop and enhance the database architecture of the new analytic data environment that includes recommending optimal choices between relational, columnar, and document databases based on requirement\\nIdentify and deploy appropriate file formats for data ingestion into various storage and/or compute services via Glue for multiple use cases\\nDevelop real-time/near real-time data ingestion from web and web service logs from Splunk\\nMaintain existing processes and develop new methods to match external data sources to Homesite data using exact and fuzzy methods\\nImplement and use machine learning based data wrangling tools like Trifacta to cleanse and reshape 3rd party data to make suitable for use.\\nDevelop and implement tests to ensure data quality across all integrated data sources.\\nServe as internal subject matter expert and coach to train team members in the use of distributed computing frameworks for data analysis and modeling including AWS services and Apache projects\\nQualifications:\\nMaster’s degree in Computer Science, Engineering, or equivalent work experience\\nTwo to four years’ experience working with datasets with hundreds of millions of rows using a variety of technologies\\nIntermediate to expert level programming experience in Python and SQL in Windows and Mac/Linux environment\\nIntermediate level experience working with distributed computing frameworks, especially Spark\\nIntermediate level experience working with relational databases including PostgreSQL and Microsoft SQL Server\\nExperience working with contemporary data file formats like Apache Parquet, Avro, and columnar databases like RedShift\\nExperience working with distributed SQL query engines like Presto DB and Athena\\nExperience with Amazon Web Services including Redshift, S3, Kinesis, Glue, and DynamoDB\\nExperience analyzing data for data quality and supporting the use of data in an enterprise setting.\\nNice to have:\\nSome experience working with clustering and classification models\\nSome experience working with Trifacta\\nSome experience working with Google Analytics\\nSome familiarity working with RDFs and SparQL and some experience working with Graph Databases\\nExperience with enterprise search engine systems including ElasticSearch and Apache Solr\",\n",
       "  \"We are looking for a Data Engineer who wants to launch a career in business analytics. You'll work on the Business Intelligence team helping answer the hardest questions at Klaviyo. We're looking for someone that has a background in working with billions of records, designing datasets, writing ETL's, and working in a process driven engineering environment.\\nThe Business Intelligence team is still in its early days and you'll have a big impact on our direction and how we operate. You'll be central to managing the data environment that everyone uses to understand the health of the business. You'll also get to help analyze business situations important to Klaviyo.\\nWe love data and the excitement that comes with using that data to help our business grow. We are the kind of people that are not afraid of any question or project. We love to move fast, keep learning and get stuff done.\\nResponsibilities:\\nThis is a cross functional role that requires framing business problems, identifying and obtaining data needed for analysis, synthesizing results and making recommendations to the executive team and other leaders in various departments\\nAbout 75% of this role is engineering. 25% is business analysis\\nDesigning Datasets, writing ETL’s and maintaining the BI data environment\\nTypical business issues will involve framing and solving problems around revenue like acquisition channels, drivers of churn and customer retention, customer lifetime values, product engagement etc. As Klaviyo grows rapidly, projects looking at costs and investments will become a larger focus\\nSome financial and quantitative modeling will also be required / can be learnt\\nThere will be frequent interaction with customer success, support, product, engineering, sales, marketing functions\\nIdeal Candidate:\\nEngineering or quantitative background (experience with AWS/Kafka/SQL/Python/R/Redshift/Snowflake or similar)\\nAn individual motivated by continuous personal growth and learning, ideally with a desire to obtain an MBA, or MS in Business Analytics in the future\\nFamiliarity with reporting tools a plus\\nSomeone who is very curious about what makes a business / operation function successful\\nGood writing and communication skills. Should be able to write 1- or 3-page analysis reports on various aspects of Klaviyo’s business or operations\\nDoes not have to be from a traditional business intelligence background\",\n",
       "  'Mindteck is hiring a Data Engineer. This position is based in Boston, MA\\n\\nThe Team\\n\\nAsset Management Technology (AMT) provides worldwide technology and support to all the Investment Management, Research, Trading and Investment Operations functions. AMT is an integral partner for Asset Management to deliver creative, scalable, industry-leading investment tools that enable Asset Management to achieve competitive advantage globally.\\n\\nThe Expertise You Have\\n3-10 years professional development experience\\nDatabase development background, SQL, PL/SQL\\nStrong background with object oriented programming, with projects completed leveraging many of the following technologies including Python, Java or .Net, .Net Core, C#\\nExperience in Pattern Matching to extract data from Web Sites (Web Scraping), Text Documents, etc.\\nKnowledge of Cloud computing concepts (AWS) and working experience with deploying and managing applications in the Cloud\\nThe Skills You Bring\\nYou have experience with extracting of text through parsing html, xml, json, text, pdf, word, and other types of documents\\nYour demonstrated experience developing RESTful API based web services on Windows and Linux\\nYou thrive in environments involving everything from Conceptual Design to Rapid Prototyping\\nYou are familiar with extracting data from REST APIs and parallel processing large datasets\\nYou have knowledge or interest in developing custom Data Pipelines to extract data, map data, transform data, and to load data in various data stores like Oracle, S3, and / or shared drives.\\nParticipating in problem solving, troubleshooting, performance turning, production support, and maintenance of existing APIs\\nCompany Overview\\n\\nAt Fidelity, we are focused on making our financial expertise broadly accessible and effective in helping people live the lives they want. We are a privately held company that places a high degree of value in creating and nurturing a work environment that attracts the best talent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace where we respect and value our associate for their unique perspectives and experiences. For information about working at Fidelity, visit FidelityCareers.com.\\n\\nWhy Mindteck?\\n\\nDo you have what it takes to be a Mindtecker? We\\'re a growth-oriented company that values our Consultants\\' collective expertise, experience, and enthusiasm!\\n\\nWhile others \"talk the talk”, we \"walk the walk”. Quite simply, WE CARE. About you, your career, our clients, and the communities where we live and work.\\n\\nWe\\'re proud of our long track record of placing thousands of talented individuals in positions where they thrive. We understand the importance of a healthy work-life balance and believe it is critical to our consultant\\'s productivity and overall happiness -- at work and at home.\\n\\nJust like life, at Mindteck you\\'ll experience a diverse and interesting mix of challenges and opportunities. We\\'ll be there -- by your side, and on your side -- every step of the way. That\\'s the Mindteck difference.\\n\\nAs a Mindteck Consultant, you will be eligible to benefit from our competitive, often above-market compensation structure as well as one of the most attractive and comprehensive benefit packages in the IT Consulting Industry.\\n\\nPackage Overview:\\nCompetitive Salary\\n401k Retirement Plan\\nAffordable Medical, Dental, and Vision Plans\\nHealth Savings Account (HSA)\\nLife/AD&D, Short- and Long-term Disability Benefits\\nPaid Time Off\\nPaid Holidays\\nSound like you? Join us. Apply today!\\n\\nPresently, Mindteck employs over 1,000 individuals throughout offices in:\\nUS\\nUK\\nSingapore\\nMalaysia\\nBahrain\\nIndia\\nAnd three development centers (Singapore and India [Kolkata, Bangalore]).\\nMindteck is listed on the Bombay Stock Exchange (BSE 517344) and is among a select group of global companies appraised at Maturity Level 5, Version 1.3 of the CMMI Institute\\'s Capability Maturity Model Integration (CMMI) www.Mindteck.com.\\n\\nFollow Mindteck:\\nFacebook:\\nhttps://www.facebook.com/Mindteckglobal/\\nLinkedIn:\\nhttps://www.linkedin.com/company/mindteck\\nYouTube:\\nhttps://www.youtube.com/user/MindteckVideos\\nTwitter:\\nhttps://twitter.com/mindteck?lang=en\\nPinterest:\\nhttps://www.pinterest.com/mindteck/',\n",
       "  'STR is a government research contractor specializing in advanced research and development for defense, intelligence, and homeland security applications. We pride ourselves in developing cutting-edge technologies with significant and immediate impact on our national security. Our product is our staff; we prioritize cultivating a team of driven and talented scientists and engineers that together culminate into a premier company.\\n\\nEvery Analyst a Scientist - One of our primary goals is to empower intelligence analysts to be able to study their data like scientists. The tools we develop focus on streamlining intelligence analysis through integrated algorithms and software that provide insight into the geopolitical landscape for use in operational intelligence missions around the world.\\n\\nThe Role:\\n\\nAs a Data Engineer, you will be part of a team that transforms large and complex customer data into real-world, high-impact solutions. You will work with researchers and engineers to design and implement solutions to challenging national security problems. You will be responsible for building the software infrastructure to clean, ingest, and expose datasets and algorithms to both developers and end users. You will deploy algorithms, generate workflows, create engineer-facing tools, and design customer-facing prototype systems. You will occasionally travel to customer sites to engage with end users, demonstrate prototypes, and integrate analytics into customer systems. If you would like to help intelligence and defense analysis keep pace with modern machine learning and software techniques, then this role is for you!\\n\\nWho you are:\\nA degree in a scientific or engineering field, such as Computer Science, Mathematics, Physics, or Software Engineering\\nProficiency with a scientific programming language such as Python, Java, or C++\\nExperience with database management and common query syntax\\nMotivated collaborator and excellent communicator of ideas to both technical and non-technical audiences\\nKnowledge of AWS, Spark, Dask, and/or similar technologies for working with data at scale\\nUS citizen and willing to obtain a U.S. Security Clearance\\nEven better:\\nTrack record of architecting, developing, deploying, or maintaining enterprise software\\nExperience with software development best practices and tools\\nUnderstanding of web development and visualization technologies, such as d3, Leaflet, Bootstrap, or others\\nFamiliarity with machine learning or statistical modeling techniques\\nActive U.S. Security Clearance\\nCompensation:\\nCompetitive salary\\nComprehensive benefits (Medical, Dental, Vision, Disability, Life)\\n401k company match\\nCompetitive and flexible paid time off\\nContinued higher education reimbursement\\nProfit sharing (Additional match to 401k)\\nPhone reimbursement plan\\nAnd more!\\nSTR is dedicated to fostering a diverse and inclusive workforce where all employees, regardless of race, ethnicity, gender, neurodiversity, or other personal characteristics, feel valued, included, and empowered to achieve their best. We recognize that each employee’s backgrounds, experiences, and perspectives are essential for providing our customers with innovative solutions to challenging national security problems. STR’s commitment to attracting, retaining, and engaging talented and diverse professionals is demonstrated by our participation, sponsorship, and support in local and national minority organizations.\\n\\nApplicants must be US Citizens.',\n",
       "  'DescriptionDo you get excited working with the Cloud, Big Data, and Machine Learning? Do you believe technology has a critical role to play in the ongoing healthcare industry transformation? Do you want to be part of a dynamic team that excels in implementing the best Data products and make the end to end machine learning platform tick? Are you ready to create something new that will positively affect the healthcare and technology industry in a noble and purposeful way?\\n\\nWe are seeking a Senior Data Engineer to design and implement cutting edge data products and to take the machine learning solutions from concepts to production.\\nResponsibilities\\nDesign and build the Minimum Viable Cloud Data Platform on Azure.\\nPossesses in depth understanding of the Azure ML DevOps to deploy/automate/monitor the Machine Learning models into production.\\nPossesses in depth understanding of building streaming, ingestion, processing data pipelines.\\nPossesses working knowledge of container architectures to streamline and simplify model development, test, and deployment.\\nSelf-directed and comfortable supporting the devOps needs of multiple teams, systems and products.\\nDesign and develop data products and machine learning solutions for Humana enterprise Analytics.\\nPossess in depth understanding of computer science parallelism concepts and efficient data processing techniques suitable for machine learning applications\\nArchitect and implement ETL / Machine learning product solutions using best practices in Azure Cloud environment.\\nUnderstand available data, including both in structured and unstructured formats, and recommend effective ways for storage and analytical processing in the cloud.\\nComfortable working in a fast paced agile environment, closely interacting with business and some of industrys best software engineers, machine learning scientists, data engineers, and data scientists.\\n\\nRequired Qualifications\\nBS in Computer Science or related field\\nSolid knowledge of data structures and algorithms\\n3+ years of experience designing and implementing data processing and/or machine learning applications in Azure cloud environment\\n3+ years of experience with PySpark\\n3+ years programming experience using one or more of Java, Scala, and Python\\nFamiliarity with open source machine learning frameworks, such as tensorflow, pytorch,etc\\n2+ years of experience with containerization using Docker or similar technologies.\\nWorking knowledge of agile development techniques\\nWillingness to learn new technologies and comprehend unfamiliar data domains and new business processes quickly\\n2+ years of experience working on Azure Platform.\\n2+ years of experience working on Dev Ops.\\nFamiliar with version control and best practices around CI/CD\\nPrior experience of designing and building APIs and services for data\\nPrior experience in any nosql DBs and Graph DB\\nPrior experience with using typical set of tools for building a data pipelines: streaming, ingestion, and processing pipelines.\\n\\nPreferred Qualifications\\nMS/Ph.D. in Computer Science or related field\\nExperience implementing applications around Spark ML libraries.\\nOpen source contributions and robust portfolio of shipped code at GitHub.\\nSolid knowledge of authentication and authorization for data access\\nAny Relevant certifications\\nAzure Certifications.\\nAdditional InformationScheduled Weekly Hours40',\n",
       "  \"Indigo is a company dedicated to harnessing nature to help farmers sustainably feed the planet. With a vision of creating a world where farming is an economically desirable and accessible profession, Indigo works alongside its growers to apply natural approaches, conserve resources for future generations, and grow healthy food for all. Utilizing beneficial plant microbes to improve crop health and productivity, Indigo's portfolio is focused on cotton, wheat, barley, corn, soybeans, and rice. The company, founded by Flagship Pioneering, is headquartered in Boston, MA, with additional offices in Memphis, TN, Research Triangle Park, NC, Sydney, Australia, Buenos Aires, Argentina, and São Paulo, Brazil. www.indigoag.com\\n\\nThe Senior Data Science Engineer will engineer interfaces for Indigo's Marketplace offering with a focus on backend applications supporting automated trading, financial engineering and operations research at scale. This person is responsible for maximizing code reusability while also considering API design, model deployment considerations, and the testability of the APIs. This person will also be a critical member of a highly efficient and cohesive engineering team that is focused on the customer's needs and requirements.\\n\\nOutcomes\\nOnboarded with at least one complete feature done within 30 days\\nDesign APIs that software developers love to use\\nCollaborate with architects to identify and gain sign-off for API deployment\\nPartner w/ UI & API team to ensure seamless integration of DSE deliverables into SW platform.\\nImplement microservices architecture in AWS\\nEnsure that all APIs are fully tested in an automated framework, including functional and performance tests\\nPartner with a data scientist/operations research scientist to productionize a predictive model prototype within 120 days\\nCompetencies\\nEager/open to learning core finance/trading system design/implementation\\nStatistics/ML expertise a plus\\nAPI scaling/performance profiling experience\\nPassion for developing easy-to-use and customer-delightful APIs, working directly with and demonstrating a deep understanding of the end-user / customer\\nPassion for software development and modern software development practices, including lean and agile\\nDeep commitment to quality, reliability, scalability and maintainability\\nEgoless and works and interacts well with software engineering, product management, data scientists and non-technical users\\nPassion for Indigo mission and values\\nGreat listener and communicator, written and oral\\nTechnically knowledgeable and not afraid to technically challenge engineers\\nAbility and passion to quickly learn new technologies and industries, staying up-to-date with technology and best practice trends\\nResults oriented, demonstrating a passion to release software, while not compromising on quality and being sensitive to the specific needs for the customer and the application's usability\\nExperience implementing complex algorithms at scale\\nQualifications\\n3+ years of experience building internal and / or external APIs required\\nDeep understanding of API design, including versioning, isolation and micro-services (REST or GraphQL)\\nExperience designing and documenting internal and external (commercial) APIs leveraging an API documentation framework (e.g. Apiary, Swagger)\\nExperience with variety of datastores and tradeoffs (SQL/NoSQL)\\nExperience with deployments to cloud environments\\nExperience with containerization (Docker)\\nExperience with version control (git)\\nExperience with python including Numpy/Pandas\\nActively practicing lean / agile software methodologies or similar\\nExperience writing functional and unit testing\",\n",
       "  'Framingham, MA / Providence, RI / Minneapolis, MN\\n\\nWho is Virgin Pulse?\\n\\nVirgin Pulse, founded as part of Sir Richard Branson’s famed Virgin Group, helps organizations build employee health and wellbeing into the DNA of their corporate cultures. As the only company to deliver a powerful, mobile-first digital platform infused with live services, including coaching and biometric screenings, Virgin Pulse’s takes a high-tech-meets-high-touch-approach to engage employees in improving across all aspects of their health and wellbeing, every day – from prevention and building a healthy lifestyle to condition and disease management to condition reversal, all while engaging users daily in building and sustaining healthy habits and behaviors. A global leader in health and wellbeing, Virgin Pulse is committed to helping change lives and businesses around the world for good so that people and organizations can thrive, together. Today, more than 3100 organizations across the globe are using Virgin Pulse’s solutions to improve health, employee wellbeing and engagement, reduce costs and create strong workplace cultures.\\n\\nWho are our employees?\\n\\nAt Virgin Pulse we’re passionate about changing lives for good. We want to make a difference in the world by helping people be healthy so they can perform at their best, every day, at work and home. Our award-winning solutions support leading employers in improving and simplifying the employee health and wellbeing journey and engaging people in all aspects of their health. But our world-class products and programs are nothing without our people – the employees who design, build, promote, sell, test and perfect the latest innovations in workplace health and wellbeing. Our people are our top priority and we invest in their health and happiness. At Virgin Pulse, we have so much more than a strong, supportive company culture – have a shared vision for a healthier, happier world.\\n\\nWho you are.\\n\\nAs a Data Engineer with Virgin Pulse you are capable of working on a fast-paced reporting pipeline. As a KEY member of the Data Systems & Reporting team, you will work closely with our client services and ETL team to clean data, design Tableau workbooks, automate SQL scripts, and respond to daily reporting requests.\\n\\nYou are passionate about top quality data software and knowledgeable about business insights, capable of sharing this knowledge with internal clients and customers. You understand SQL deeply and have extensive experience with data analytics using industry tools such as Tableau or Power BI. You also take pride in having an agile mindset and a deep commitment to the principles of agile software development. You should understand modern non-relational databases, and how they are different from traditional databases and take pride in your excellent verbal and written communication skills and desire for a strong team ethic.\\n\\nIn the role of Data Engineer for Virgin Pulse you will wear many hats but attention will be crucial in the following:\\nDemonstrate experience in Tableau or Power BI\\nDemonstrate capability in SQL\\nUsing Python to create clean, extensible code\\nDesigning and implementing data warehouse schemas for big data\\nEat, breathe, and think agile for all\\nYou embrace the full lifecycle of product development from inception to continual support\\nYou are passionate about quality. Ensuring accuracy and elegance in your work is something you do every day. You think carefully about anything before writing code, you code with fastidious attention to detail, you test obsessively, and you take criticism enthusiastically and improve continually.\\n\\nWhat you bring to the team.\\n\\nIn order to represent the best of what we have to offer you come to us with a multitude of positive attributes including:\\nYou have a degree in Computer Science, MiS or Mathematics\\nYou have outstanding problem solving and analytical skills\\nYou are an amazing communicator (written and verbal) and comfortable discussing software and features\\nYour ability to contribute to a planning or scheduling discussion mirrors your ability to listen and learn from your peers\\nWhat makes you stand out.\\n3 years of a major BI tool such as Tableau or PowerBI\\n3 years of SQL experience\\n3 years of Python experience\\nExperience with one or more non-relational database (Vertica, MongoDB, Cassandra, etc)\\nExperience with Jira and Git\\nSecurity Competencies:\\n\\nWork to ensure system and data security is maintained at a high standard, ensuring the confidentiality, integrity and availability of the Virgin Pulse application is not compromised. Ensure industry best practice coding standards are adhered to in particular ensure all code developed at Virgin Pulse is free from bugs and security vulnerabilities, such as those defined and published by OWASP.\\n\\nWhy work here?\\n\\nWe believe a career should provide a collaborative and supportive work environment, strong employee culture and cutting-edge technology and services — so many reasons to love it here.\\n\\n*We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to any protected class status.',\n",
       "  \"We're looking for a Data Engineer to join our growing and successful Data Team!\\n\\nAs a Data Engineer, you'll own a problem from end to end and will be empowered to take the lead with technology and implementation while joining a rare hyper-growth company.\\n\\nAs a data engineer, you may have experience spanning traditional DW and ETL architectures. But for this role, it is important to have industry experience working with big data ecosystems and relational databases.\\n\\nResponsibilities:\\nSupport mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation\\nBuild and support reusable framework to ingest, integration and provision data\\nAutomation of end to end data pipeline with metadata, data quality checks and audit\\nBuild and support a big data platform on the cloud\\nImplement automation of jobs and testing\\nOptimize the data pipeline to support multiple stakeholders\\nSupport mission critical applications and near real time data needs from the data platform\\nCapture and publish metadata and new data to subscribed users\\nWork collaboratively with product managers, data scientists as well as business partners and actively participate in design thinking session\\nQualifications:\\n3+ years' experience in data engineering\\nExperience with AWS - EMR, RDS, Redshift, S3, Kinesis, Athena, Glue\\nFamiliarity with big data technologies - Spark/Hadoop, Kafka, SQL, Python\\nGood interpersonal, verbal and written communication skills\\nExpertise in SQL, SQL tuning, schema design, Python and ETL processes\\nExperience in data pipelines with such workflow tools as Airflow, Oozie or Luigi\\nBS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines\\nNice to have:\\nExperience with Tableau\\nExperience with Machine Learning\\nHave worked with Data Scientists\",\n",
       "  'The Big Data Engineer is responsible for engaging in the design, development and maintenance of the big data platform and solutions at Quicken Loans. This includes the platform host data sets that support various business operations and enable data-driven decisions as well as the analytical solutions that provide visibility and decision support using big data technologies. The Big Data Engineer is responsible for administering a Hadoop cluster, developing data integration solutions, resolving technical issues, and working with Data Scientists, Business Analysts, System Administrators and Data Architects to ensure the platform meets business demands. This team member also ensures that solutions are scalable, include necessary monitoring, and adhere to best practices and guidelines. The Big Data Engineer helps mentor new team members and continues to grow their knowledge of new technologies.\\n\\nResponsibilities\\nDevelop ELT processes from various data repositories and APIs across the enterprise, ensuring data quality and process efficiency\\nDevelop data processing scripts using Spark\\nDevelop relational and NoSQL data models to help conform data to meet users needs using Hive and HBase\\nIntegrate platform into the existing enterprise data warehouse and various operational systems\\nDevelop administration processes to monitor cluster performance, resource usage, backup and mirroring to ensure a highly available platform\\nAddress performance and scalability issues in a large-scale data lake environment\\nProvide big data platform support and issue resolutions to Data Scientists and fellow engineers\\nRequirements\\nMaster\\'s degree in computer science, software engineering or a closely related field\\n2 years of experience with Hadoop distribution and ecosystem tools such as Hive, Spark, NiFi and Oozie\\n2 years of experience developing batch and streaming ETL processes\\n2 years of experience with relational and NoSQL databases, including modeling and writing complex queries\\nProficiency in at least one programming language, such as Python or Java\\nExperience with Linux system administration, scripting and basic network skills\\nExcellent communication, analytical and problem-solving skills\\nWho We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past nine consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for five consecutive years, 2014 through 2018, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top-30 for the past 15 years.',\n",
       "  \"We're looking for a Senior Data Engineer to join our growing and successful Data Team!\\n\\nAs a Senior Data Engineer, you'll own a problem from end to end and will be empowered to take the lead with technology and implementation while joining a rare hyper-growth company.\\n\\nResponsibilities:\\nDesign and build mission critical data pipelines with a highly scalable distributed architecture - including data ingestion (streaming, events and batch), data integration, data curation\\nBuild and support reusable framework to ingest, integration and provision data\\nAutomation of end to end data pipeline with metadata, data quality checks and audit\\nBuild and support a big data platform on the cloud\\nDefine and implement automation of jobs and testing\\nOptimize the data pipeline to support ML workloads and use cases\\nSupport mission critical applications and near real time data needs from the data platform\\nCapture and publish metadata and new data to subscribed users\\nWork collaboratively with product managers, data scientists as well as business partners and actively participate in design thinking session\\nParticipate in design and code reviews\\nMotivate, coach, and serve as a role model and mentor for other development team associates/members that leverage the platform\\nQualifications::\\n7+ years' experience in data warehouse / data lake technical architecture\\nMinimum 3 years of Big Data and Big Data tools in one or more of the following: Kafka, MapReduce, Spark or Python, Hadoop\\nExperience with Database Architecture/Schema design\\nStrong familiarity with batch processing and workflow tools such as AirFlow, NiFi\\nAbility to work independently with business partners and management to understand their needs and exceed expectations in delivering tools/solutions\\nStrong interpersonal, verbal and written communication skills and ability to present complex technical/analytical concepts to executive audience\\nStrong business mindset with customer obsession; ability to collaborate with business partners to identify needs and opportunities for improved data management and delivery\\nBS/BA degree in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines\\nNice to have:\\nMasters in Computer Science, Physics, Mathematics, Statistics or other Engineering disciplines\\nExperience with data visualization tools such as Tableau, Looker, PowerBI\\nExperience with Hadoop implementation\\nExperience with AWS\",\n",
       "  'Who We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past nine consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for five consecutive years, 2014 through 2018, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top-30 for the past 15 years.\\n\\nJob Summary\\n\\nThe Data Warehouse Engineer works on the back end development of the data warehouse and the maintenance of front-end reporting tools. This team member is a self-driven individual whos ready to dig deep into business needs and work closely with the Business Intelligence team to find solutions. This team member is also expected to think outside of the box and clearly communicate better solutions to our needs.\\n\\nResponsibilities\\nDesign and support the database and table schemas for new and evolving sources of data\\nMonitor and troubleshoot data issues on data warehouse servers\\nHandle dimensional design as new areas of the business are loaded into the data warehouse\\nDesign and support the analysis services cubes to help with analytics\\nDiagnose and resolve data warehouse ETL processes, data flows and business logic failures\\nWork with database architects to define resource utilization needs\\nProvide technical expertise and advice to business areas to help them smoothly transition manual reports and applications to the Business Intelligence environment\\nGuide business areas in identifying new data needs and delivery mechanisms for reporting information\\nEstablish documentation of reports and data definitions\\nRequirements\\nDegree in business management, computer science or information technology\\n5 years of experience with T-SQL coding\\n2 years of experience in performance tuning (SQL, ETL)\\n3 years of experience with SQL Server Integration Services or similar ETL tools\\n2 years of experience with SQL Server Analysis Services\\n2 years of experience with dimensional modeling and other data-warehousing techniques\\nWhatll Make You Special\\n3 years of experience with SQL Server database administration\\n1 year of experience using .NET, C#, VB.NET, VBA, ASP.NET and/or Java\\nExperience with SQL Server Analysis Services and MDX query language\\nFamiliarity with master data management concepts\\nExperience working in the Apache Hadoop ecosystem\\nExperience using Tableau\\nAbility to work cross-functionally\\nPowerShell',\n",
       "  \"Stefanini is looking for a Data Engineer in MI\\n\\nThis position will be part of the Data Supply Chain (DSC) product group Data fulfillment team using tools like Attunity, Sqoop etc. The product team's objective is to replicate data from hundreds of database sources within the company to the DSC Hadoop environment and do transformations to make it usable for data scientists in GDIA. This position will require an individual who has a strong background with multiple database technologies, who is process oriented and has knowledge of Java and expertise in the Hadoop environment. This role is part of a highly dynamic team supporting GDIA activities.\\n\\nSkills Required:\\n2 + years. experience with either SQL, Oracle or DB2.\\nExperience with more than one is preferred.\\n1 + years. experience working with the Hadoop ecosystem using tools like Sqoop, HIVE, and HBASE etc.\\nExperience with data replication such as ETL or within Hadoop using Kafka, Scoop, etc.\\nExcellent communication skills Strong team player with experience working as part of an agile team Independent self-starter who is able to work in a constantly evolving environment\\nExperience Preferred:\\nJava development experience\\nKnowledge of scripting tools like Python\\nKnowledge of big data tools like Spark and Kafka\\nOperational support experience\\nEducation Required:\\nBS in Computer Science or related or equivalent experience\",\n",
       "  \"Job Description\\nRelapath LLC is searching for (2) Consultants to perform in a Data Engineer role to be responsible for the full life cycle of the back-end development of a data platform.\\n\\nJOB DESCRIPTION\\nCreate new data pipelines, database architectures and ETL processes, and they observe and suggest what the go-to methodology should be.\\nGather requirements, perform vendor and product evaluations, deliver solutions, conduct trainings and maintain documentation.\\nHandle the design and development, tuning, deployment and maintenance of information, advanced data analytics and physical data persistence technologies.\\nEstablish analytic environments required for structured, semi-structured and unstructured data.\\nImplement the business requirements and business processes, build ETL configurations, create pipelines for the datalake and data warehouse, research new technologies and build proofs of concept around them.\\nCarry out monitoring, tuning and database performance analysis and performs the design and extension of data marts, metadata and data models.\\nEnsure all data platform architecture code is maintained in a version control system.\\nShare knowledge with fellow team members, allowing the entire team to grow and become proficient to further build out and enhance the data platform.\\nDAILY DUTIES\\nFocus on scalability, performance, service robustness and cost trade-offs\\nDesign and implement high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark\\nCreate prototypes and proofs of concept for iterative development\\nLearn new technologies and apply the knowledge in production systems\\nDevelop ETL processes to populate a datalake with large data sets from a variety of sources\\nCreate MapReduce programs in Java and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large data sets\\nMonitor and troubleshoot performance issues on the enterprise data pipelines and the datalake\\nFollow the design principles and best practices defined by the team for data platform techniques and architecture\\nREQUIRED EXPERIENCE:\\n\\nRequired 3 years:\\nObject-oriented/object function scripting languages: Java (preferred), Python and/or Scala\\nRequired 2 years:\\nBig data tools: Hadoop, Spark, Kafka, NiFi, Hive and/or Sqoop\\nAWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue\\nStream-processing systems: Spark-Streaming, Kafka Streams and/or Flink\\nRelational SQL and NoSQL databases like MySQL, Postgres, Cassandra and Elasticsearch\\nWorking experience in a Linux environment\\nExpertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\\nDemonstrated ability to performance-tune MapReduce jobs\\nStrong analytical and research skills\\nDemonstrated ability to work independently as well as with a team\\nAbility to troubleshoot problems and quickly resolve issues\\nStrong communication skills\\nEDUCATION\\n\\nBachelor’s degree in computer science or combination of education and work experience\\n\\nCandidates for this position with Relapath LLC must be legally authorized to work in the United States on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position. Relapath LLC is an equal opportunity employer.\\nCompany Description\\nAs a North American provider of customized IT Solutions, Managed Services, and smart staffing solutions, Relapath’s client base spans Fortune 1000 organizations. We are committed to providing the technology solutions that our clients need to move their business forward more strategically.\\n\\nRelapath offers employees benefits and PTO.\\n\\nOur awards include:\\nMetro Detroit's 2018 Best and Brightest Companies®\\nMetro Detroit's 2017 Best and Brightest Companies®\\nCorp! Magazine 2017 Michigan Economic Bright Spot Award\\n2017 Best and Brightest Companies to Work For in the Nation\\n\\nLet's succeed together!\",\n",
       "  'Job Description\\nPosition with our automotive client:\\n\\nData Engineer\\n\\nJob Summary:\\n\\nThe Data Engineer will be a member of the DETROIT Powertrain Engineering’s Data Analytics team. The main functions of this role will be to provide strategy and guidance as well as architecting, building, and maintaining data pipelines for transactional data, coming from various databases across the organization, as well as time-series data coming from powertrain testing. Additionally, the data engineer will be essential in guiding a future migration to cloud based analytics. The ideal candidate will be comfortable defining and building a system from the ground up, in an effort to support the team’s overall vision and goals of revolutionizing the way Powertrain Engineering uses and consumes data.\\n\\nResponsibilities:\\n\\n· Develop, build, and maintain data pipeline architecture to support the efforts of the data analytics’ team needs\\n\\n· Create extract, transform, and load (ETL) processes needed to build out and fill the data mart layer with data coming from both transactional data sources as well as time-series test data sources\\n\\n· Ensure data integrity, flow, consistency, and lineage for data coming through the pipeline\\n\\n· Define processes and best practices to be used by the team\\n\\n· Work to define the details on how cloud migration could be used to compliment on premise business intelligence\\n\\n· Build out a cloud based infrastructure strategy such that a demonstration of effectiveness can be made\\n\\nSkills:\\n\\n· Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\n\\n· Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\n\\n· Strong analytic skills related to working with unstructured datasets.\\n\\n· Build processes supporting data transformation, data structures, metadata, dependency and workload management.\\n\\n· Experience supporting and working with cross-functional teams in a dynamic environment.\\n\\n· Experience with big data tools: Hadoop, Spark, Kafka, etc.\\n\\n· Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.\\n\\n· Experience with stream-processing systems: Storm, Spark-Streaming, etc.\\n\\n· Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\\n\\n· Experience with BI tools such as Tableau, Power BI, etc.\\n\\nContact: Rashmi Upadhyaya\\n\\nrashmiu@sunsoft.us',\n",
       "  'Who We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past nine consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for five consecutive years, 2014 through 2018, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top-30 for the past 15 years.\\n\\nJob Summary\\n\\nThe Data Analyst is a highly trained individual who specializes in organizing and analyzing data from various resources with the purpose of turning numbers into information to influence decision making. The Data Analyst uses a range of tools and methodologies, including statistics and technical expertise, to generate results that highlight useful insights and expose areas of opportunity. Data Analysts are naturally curious, have a strong sense of urgency and focus on how their analyses impact our business results. This team member is specifically dedicated to the Marketing business area.\\n\\nResponsibilities\\nDrive analytical decision making on a wide range of initiatives\\nField and prioritize strategic requests from senior leaders to help everyone involved better understand the impact of key business decisions, make projections and enhance our mortgage-related processes\\nAnalyze data with standard statistical methods, interpret results, and provide written and verbal summaries of data analyses\\nEffectively summarize and simplify analysis results that are to be shared with teammates and business partners through a wide variety of mediums\\nWork closely with our business intelligence peers, including Business Analysts, Engineers, Data Scientists and Modelers to ensure we\\'re creating a holistic analytical approach that solves for analytics needed, technical solutions and models to transform a business initiative\\nWork as an extension of business partners to consult with and communicate needs back to our Technology team\\nRequirements\\nBachelors degree or higher preferred in economics, mathematics, applied statistics, computer science or finance or equivalent experience\\n1 year of applied experience working with complex data sets\\nBasic understanding of SQL or other querying languages used to create data sets\\nAdvanced Excel skills\\nGreat accuracy and a talent for identifying and resolving data issues\\nDemonstrated experience visualizing and presenting data (e.g., Tableau, PowerBI, etc.)\\nComfort with a high learning curve regarding software platforms and new analytical concepts\\nWhatll Make You Special\\nExperience with web analytics tools such as Adobe Analytics Suite and Google Analytics\\nExperience with customer relationship management software as a service, such as Salesforce Marketing Cloud\\nBasic statistical software program knowledge, including R and Python\\nAbility to ramp up quickly to understand the business and industry\\nExperience creating strategic actionable takeaways, providing solutions and exposing areas of opportunity\\nCapacity for being responsible for data within the organization to reach conclusions and drive solutions and/or recommendations\\nComfort presenting in front of a large group when necessary\\nSelf-starter with a strong sense of career ownership who seeks out opportunity for further growth and development\\nStrong project management and prioritization skills\\nAbility to work in fast-paced environment where flexibility is key and directions can change frequently\\nExtremely high level of attention to detail and outstanding organizational skills',\n",
       "  'This team member establishes analytic environments required for structured, semi-structured and unstructured data. They implement the business requirements and business processes, build ETL configurations, create pipelines for the data lake and data warehouse, research new technologies and build proofs of concept around them. This person carries out monitoring, tuning and database performance analysis and performs the design and extension of data marts, meta data and data models. They also ensure all data platform architecture code is maintained in a version control system.\\n\\nResponsibilities\\nFocus on scalability, performance, service robustness and cost trade-offs\\nDesign and implement high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark\\nCreate prototypes and proofs of concept for iterative development\\nLearn new technologies and apply the knowledge in production systems\\nDevelop ETL processes to populate a data lake with large data sets from a variety of sources\\nCreate MapReduce programs in Java and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large data sets\\nMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lake\\nFollow the design principles and best practices defined by the team for data platform techniques and architecture\\nRequirements\\nBachelor’s degree in computer science or equivalent experience\\n2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Hive and/or Sqoop\\n2 years of experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue\\n2 years of experience with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink\\n3 years of experience with object-oriented/object function scripting languages: Java (preferred), Python and/or Scala\\n2 years of experience with relational SQL and NoSQL databases like MySQL, Postgres, Cassandra and Elasticsearch\\n2 years of experience working in a Linux environment\\nExpertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\\nDemonstrated ability to performance-tune MapReduce jobs\\nJob Type: Contract\\n\\nSalary: $50.00 to $70.00 /hour\\n\\nContract Length:\\n3 - 4 months\\nContract Renewal:\\nPossible',\n",
       "  \"What you’ll be doing...\\n\\nYou will be part of a team that works with the latest technology to provide our customers access to our many network components including 5G systems. As a member of the team you will be responsible for providing complete technical oversight and project management for multiple transport network expansion projects. These initiatives will require a skill set including, router knowledge, dark fiber, and microwave network design.\\nProviding technical support and project management for the design, acquisition, and implementation for all equipment supporting Network infrastructure projects including DWDM and SONET Optical transport systems deployed throughout the Verizon Wireless (VZW) Network or Microwave point-to point or point to multi-point installations.\\nWorking closely with OEM Vendors, 3rd party DF/Lit providers, and internal teams to identify the best Value (quality, time, and cost).\\nManaging the vendor selection, contract negotiation, RFP and ongoing relationships.\\nMaintaining an engineering database for documentation of all fiber assets, plant, FDP panels, and IRU contract management.\\nHandling microwave design/review, regulatory filings as well as implementation management.\\nOverseeing the financial aspects of multi-million dollar projects, including establishing business cases, and developing and managing budgets, procurement and bill payments.\\nInterfacing with higher levels of the management team across various organizations as part of the process of managing all facets of large, complex infrastructure deployments.\\nCollaborating with other groups’ forecasts to ensure equipment growth projects meet expectant demands in support of the company’s objectives.\\nWhat we’re looking for...\\n\\nYou'll need to have:\\nBachelor’s degree or four or more years of work experience.\\nThree or more years of relevant work experience.\\nCisco router experience.\\nKnowledge of Optical transport systems.\\nA valid Driver’s license.\\nEven better if you have:\\nA degree.\\nFive or more years of experience in the cellular/ telecommunications industry in a Transport engineering role.\\nCCNA certification.\\nKnowledge in CWDM/DWDM Optical Transport systems including: Ciena, Alcatel-Lucent, and Fujitsu.\\nKnowledge with Dark Fiber vendors, their organizational processes and standards, as well as their network portfolio maps.\\nExperience in Microsoft Office Excel, Word, PowerPoint, and Project.\\nExperience with PSTN traffic reporting tools, and least cost routing.\\nWorking knowledge and experience with SevOne traffic data collection system or another traffic statistics reporting package.\\nExperience with Atoll or Pathloss for microwave design.\\nKnowledge of engineering and engineering economic principles.\\nKnowledge and experience any of the following platforms and/or technologies:\\nDWDM Optical Transport systems including: Ciena 6500 and Fujitsu 9500\\nALU/Nokia 7705 & 7750 routers. Cisco 920, 903, 5501, 540 routers\\nMicro Semi Edge Grand Master experience.\\nPSTN traffic engineering including E911 and SS7.\\nDigital Communications including: Fiber and Ethernet.\\nWireless voice and data service including: EVDO, CDMA, and LTE.\\nMicrowave experience including: Aviat, NEC , ALU/Nokia MPR9500.\\nTest equipment experience including: JDSU & EXFO, spectrum analyzers, and path analyzers.\\nWhen you join Verizon...\\n\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\n\\nEqual Employment Opportunity\\n\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.\\n\\n]]>\",\n",
       "  'Comau LLC is now creating a new team focused on Digital Manufacturing. For this reason we are looking for a Data Engineer, passionate for Automation, IoT technologies, sensors, robotics willing to challenge him/herself with the next generation of intelligent machines that will drive the digital revolution.\\n\\nTake ownership. Are you ready for a challenge?\\n\\nIn this role the ideal candidate will act as a network and think innovatively while executing the following responsibilities:\\n\\nIntegrate hardware/sensors and develop software to manage IoT/Industry 4.0 applications. Creates interfaces and mechanisms for the flow and access of information.\\n\\nBe able to critical analyze the data collected, applying data mining techniques, doing statistical analysis, and building high quality prediction systems integrated with products\\n\\nCollaborate/coordinate the work with start-ups and other digital companies\\n\\nDirect research on emerging trends//solutions/standards in Automation and IoT/Industry 4.0\\n\\nWork in strict collaboration with universities and students.\\n\\nOther duties as required\\n\\nReports to:\\n\\nHead of Digital Product Management',\n",
       "  'What are we looking for?\\n\\nSoftware Engineers comfortable working in a high intensity collaborative environment, delivering value through working software. Self-starters who can work side-by-side with product managers, product designers, product owners, collaborating, envisioning product features, taking technical decisions and delivering high quality software.\\n\\nData Engineer will be part of the Data Supply Chain. Big Data Engineer will be responsible for designing & developing data ingestion and data transformation framework for modern data lake solutions. Will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.\\nExperience working with big data tools: Hadoop, Spark, Kafka, etc.\\nImplement and support big data tools and frameworks such as HDFS, Hive, and Hbase or Cassandra\\nExperience working with object-oriented/object function scripting languages: Java, Scala, Python etc.\\nEducation requirements: Bachelors or Masters in CSE, MCA, Electronics and Electrical, Electronics and Communication and Information Technology.\\n\\nAltimetrik is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability status or protected veteran status or any other status protected by the laws or regulations in the locations where we operate.',\n",
       "  'The Rock Family of Companies is made up of nearly 100 separate businesses spanning fintech, sports, entertainment, real estate, startups and more. We’re united by our culture – a drive to find a better way that fuels our commitment to our clients, our community and our team members. We believe in and build inclusive workplaces, where every voice is heard and diverse perspectives are welcomed. Working for a company in the Family is about more than just a job – it’s about having the opportunity to become the best version of yourself.\\n\\nThe Big Data Engineer is responsible for the full life cycle of the back-end development of a data platform. The Big Data Engineer creates new data pipelines, database architectures and ETL processes, and they observe and suggest what the go-to methodology should be. They gather requirements, perform vendor and product evaluations, deliver solutions, conduct trainings and maintain documentation. They also handle the design and development, tuning, deployment and maintenance of information, advanced data analytics and physical data persistence technologies.\\n\\nThis team member establishes analytic environments required for structured, semi-structured and unstructured data. They implement the business requirements and business processes, build ETL configurations, create pipelines for the data lake and data warehouse, research new technologies and build proofs of concept around them. This person carries out monitoring, tuning and database performance analysis and performs the design and extension of data marts, meta data and data models. They also ensure all data platform architecture code is maintained in a version control system.\\n\\nThe Big Data Engineer is responsible for sharing knowledge with fellow team members, allowing the entire team to grow and become proficient to further build out and enhance the data platform.\\n\\nResponsibilities\\nFocus on scalability, performance, service robustness and cost trade-offs\\nDesign and implement high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark\\nCreate prototypes and proofs of concept for iterative development\\nLearn new technologies and apply the knowledge in production systems\\nDevelop ETL processes to populate a data lake with large data sets from a variety of sources\\nCreate MapReduce programs in Java and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large data sets\\nMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lake\\nFollow the design principles and best practices defined by the team for data platform techniques and architecture\\nRequirements\\nBachelor’s degree in computer science or equivalent experience\\n2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Hive and/or Sqoop\\n2 years of experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue\\n2 years of experience with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink\\n3 years of experience with object-oriented/object function scripting languages: Java (preferred), Python and/or Scala\\n2 years of experience with relational SQL and NoSQL databases like MySQL, Postgres, Cassandra and Elasticsearch\\n2 years of experience working in a Linux environment\\nExpertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\\nDemonstrated ability to performance-tune MapReduce jobs\\nStrong analytical and research skills\\nDemonstrated ability to work independently as well as with a team\\nAbility to troubleshoot problems and quickly resolve issues\\nStrong communication skills\\nWhat’ll Make You Special\\nExperience with managing real estate data\\nExperience leading a team of engineers on a large enterprise data platform build\\nWho We Are\\n\\nRocket Homes Real Estate LLC is a Detroit-based, tech-driven company with a passion for simplifying real estate. Our mission is to create a seamless home buying and selling experience by combining the process of searching for homes, connecting with a trusted real estate agent and getting a mortgage. Since 2006, we’ve partnered with our sister company, Rocket Mortgage® by Quicken Loans, and our nationwide network of top-rated real estate agents to help over 500,000 clients with their real estate needs.\\n\\nThe Company is an Equal Employment Opportunity employer, and does not discriminate in any hiring or employment practices. The Company provides reasonable accommodations to qualified individuals with disabilities in accordance with state and federal law. Applicants requiring reasonable accommodation in completing the application and/or participating in the employment application process should notify a representative of the Human Resources Team, The Pulse, at 1-800-411-JOBS.',\n",
       "  \"We're looking for a Software Engineer to join the Services Team at StockX.\\n\\nThe Services Team is responsible for powering the StockX experience across different platforms by providing a scalable and reliable set of microservices that implement our core business functionality. We utilize cutting edge tools and platforms such as Node.js, Kafka, Docker, Kubernetes, and AWS to handle our massive growth.\\n\\nAs a Software Engineer, you will be empowered to take ownership of technology decisions and solutions while playing a pivotal role in establishing a successful engineering culture at a fast-growing company.\\n\\nThis is a great opportunity to leverage your existing skills, to build a world-class team and to have a huge impact on how marketplaces can be redefined. At StockX, we are just getting started.\\n\\nResponsibilities\\nDesign, build, and evolve microservices used by StockX web and mobile applications\\nCollaborate with front-end and back-end engineers to build scalable services\\nResearch and implement cutting edge technology that can be applied to handle massive scale\\nDebug and monitor production systems\\nHelp define the way we work in the future including coding and design standards\\nWork effectively in an agile development process\\nRequirements\\nStrong experience and understanding of JavaScript (Node.js)\\nStrong experience and understanding of data storage, relational (particularly Postgres) and non-relational (particularly Redis and DynamoDB)\\nExperience in service oriented and/or microservice architectures\\nExperience with message queues, pub-sub systems, and/or event streams\\nExperience working with AWS or other cloud providers\\nYou have built highly resilient, scalable REST-based services\\nYou are product focused and collaborate to find the best possible solutions\",\n",
       "  'Job Description:\\n\\nOverview:\\n\\nThe Data Engineer will be responsible for expanding and optimizing our data and data pipeline architecture. The ideal candidate is experienced at data wrangling, enjoys data systems, and enjoys building things from the ground up. The data engineer will support the analytics teams at our client’s site by creating processes and programs to ingest the many unstructured files we receive, transform the data into a standard form and load it into the ecosystem. The right candidate will be excited with the prospect of optimizing and redesigning the client’s data architecture for the next set of data initiatives.\\n\\nResponsibilities:\\nUse tools to build processes to extract, transform and load data from multiple sources\\nAssemble complex data sets that meet business requirements\\nIdentify, design and implement processes to automate manual processes\\nCreate structured datasets from unstructured data from multiple sources\\nExperience working with cross-functional teams in a dynamic environment\\nRequired Experience:\\n\\nQualifications:\\nThree to five years of experience with SQL query language plus experience creating complex queries\\nThree to five years of experience with Hadoop; specifically, Hive, HiveQL and HDFS\\nThree to five years of experience building processes supporting ETL work\\nOne to three years of experience building and optimizing big data pipelines and data sets\\nOne to three years of experience with programming languages: Python, Scala, PySpark\\nStrong Analytical skills\\nBachelor’s Degree in Computer Science, Statistics, Information Systems or some other related field\\nKeyword: Data Engineer\\nFrom: OneMagnify\\n</br>Apply now',\n",
       "  \"Quantum Technologies is seeking multiple Data Analyst. US Citizens, Green Card holders, OPT candidates are encouraged to apply!\\n\\nResponsibilities Conduct data analysis and quality reviews of large datasets. Provide recommendations and implement database solutions to combine multiple data sources and feed data visualization tools and reports.\\n\\nQualifications\\nHigh level of familiarity and experience working with Excel\\nComfortable with large, complex data sets\\nHigh attention to detail\\nProven ability to conduct comprehensive, thorough review of datasets to identify data quality issues\\nExcellent written and verbal communication skills\\nComfortable engaging with government officials to provide technical support and coaching on data improvement\\nExperience working independently and with ambiguity Minimum Degree Required: Bachelor’s degree Desired\\nHigh level of familiarity and experience working with Access, Tableau, PowerBI or other data analysis / visualization tools\\nExperience building data visualizations in Tableau or PowerBI\\nExperience building databases in SQL, Access and other methods/tools\\nExperience creating application programming interfaces (APIs) with multiple data sources\\nCompany Profile:\\n\\nQuantum Technologies Inc. is a leading IT service provider. We offer professional staffing services for business consulting, project management, software solutions, implementation, and development across all IT disciplines. We assist our clients to deliver ventures from inception to completion with a determined competitive advantage\\n\\n***Serious Candidates only! No C2C or Third Party companies, please.\\n***Open to relocation is encouraged!\\n\\nJob Types: Full-time, Contract\\n\\nJob Type: Full-time\\n\\nExperience:\\nrelevant: 1 year (Preferred)\\nEducation:\\nBachelor's (Preferred)\\nWork Location:\\nOne location\\nBenefits:\\nHealth insurance\\nDental insurance\\nVision insurance\\nRetirement plan\\nRelocation assistance\\nThis Company Describes Its Culture as:\\nDetail-oriented -- quality and precision-focused\\nInnovative -- innovative and risk-taking\\nAggressive -- competitive and growth-oriented\\nOutcome-oriented -- results-focused with strong performance culture\\nStable -- traditional, stable, strong processes\\nPeople-oriented -- supportive and fairness-focused\\nTeam-oriented -- cooperative and collaborative\",\n",
       "  'Make a difference\\n\\nCiber Global wants you. Come build new things with us and advance your career. At Ciber Global you ll collaborate with experts. You ll join successful teams contributing to our clients success. You ll work side by side with our clients and have long-term opportunities to advance their top priorities.\\n\\nPosition Description:\\nClient is seeking a Systems Engineer for its big data analytics.\\nThis position does not have any code development responsibilities.\\n\\nResponsibilities include:\\nEngineer and manage the deployment of new Informatica solutions and components.\\nSupport the operations team with escalated incidents.\\nBe available for occasional after-hours escalations.\\nImplement security controls and separation of duties processes according to company policies.\\nDocument day-to-day processes, installation and desk procedures.\\nProvide SME support to the operations team for escalated issue resolution and incident management.\\n\\nSkills Required:\\nAbility to self-start, multitask and communicate effectively.\\n5+ years of experience with any of these Informatica products: IICS, DIH, BDM, DI, EIC, IDL, MDM (ideally in a Linux environment).\\nHands-on experience integrating various products into the Hadoop ecosystem.\\nStrong foundation in Linux system administration including LDAP and Kerberos.\\nStrong scripting skills in at least one language Solid understanding of TCP/IP networking.\\nTroubleshooting and problem resolution skills.\\n\\nExperience Preferred:\\nExperience with MicroFocus Voltage SecureData.\\nExperience with Ambari, Yarn, Storm, and/or Ranger.\\nExposure to Kerberos and MySQL.\\nInformatica development fundamentals.\\n\\nEducation Required:\\nBachelor\"s degree or equivalent experience in a relevant field.\\n\\nAt Ciber Global our consultants have access to a comprehensive benefits package. Benefits can include Paid-Time-Off, Paid Holidays, 401K matching, Life and Accidental Death Insurance, Short & Long Term Disability Insurance, and a variety of other perks.\\n\\nCiber Global is an Equal Opportunity Employer Minorities/Females/Gender Identity/Sexual Orientation/Protected Veterans/Individuals with Disabilities.\\n\\nFind a purpose\\n\\nHelp clients embrace emerging technologies. Create inventive solutions and meet intriguing client challenges. Solve, fix, design and innovate. Be a part of something bigger by helping clients go digital, create engaging customer experiences and transform their business.\\n\\nMove ahead\\n\\nCiber is helping clients bring their innovation initiatives to life - come join us and make a difference! No matter where you are in your career, you can work to move to the next level. You ll work hand in hand with top-tier talent and be part of a team focused on the technologies that help clients compete and win.\\n\\n- provided by Dice',\n",
       "  'Mitsubishi Electric Automotive America creates high-quality in-car systems for major OEMs around the world. Known for our innovative components, we leverage our deep technology expertise to deliver autonomous-ready infotainment and ADAS solutions, premium audio systems, high-definition displays, and powertrain electronics for standard, EV, and hybrid vehicles. This professional will be assisting our sales, advanced engineering team, and HMI lead in leveraging software engineering functions. Be a part of a select team that is making a real impact on the automotive mobility of tomorrow!\\n\\nWork with the advanced engineering HMI lead engineer and team in performing the following functions:\\nSoftware Engineering\\nProvide S/W implementation and integration support to local engineers, customers, and offsite teams.\\nUtilize a comprehensive understanding of current and future automotive systems operation (e.g. eAVB, different flavors of CAN, Etc.).\\nUtilize a working understanding of software design, specifically HMI and application design.\\nAssist customers in modifying/implementing software design.\\nFeature Management\\nPlan and communicate feature development with advanced engineering team leads.\\nDocument and track issues, maintain the list through the life of the program.\\nCommunicate information to assist hardware team in Japan.\\nSoftware “Management”\\nPlan and deliver to a software release schedule.\\nMaintain files and distribution to customer and other stakeholders.\\nHMI Development\\nCoordinate with HMI team(s) to evaluate, implement and integrate advanced HMI concepts.\\nRequirements Management and Validation\\nMaintain and understand requirements documents.\\nThoroughly explain the requirements (“Fill in the holes”) to our design team.\\nCreatively suggest additional features and requirements that a customer may want.\\nSales Support\\nSupport sales team in determining future technology strategies.\\nWill be onsite several days a week working with our customers.\\nAnalyzes current processes, procedures and systems and makes recommendations for improvement.\\n\\nBSCS, BSEE or equivalent training in software development and experience.\\n5+ years’ experience in embedded or systems engineering\\nAutomotive industry experience.\\nFluency in Java, C, C++, JavaScript, etc. is desired.\\nWorking knowledge in open source operating systems (Linux, Android, etc.)\\nGood written, verbal, and cross-cultural communication skills.\\nExperience with Jira, Gerrit, and Agile Software Development.\\n\\n1st',\n",
       "  'Make your mark. The Plante Moran Technology Services team has been recognized as a Computerworld Top 100 Places to Work in IT for 2018 and 2019 and won two prestigious awards in 2017: InformationWeek IT Excellence and the CIO 100. If you are seeking professional growth, like being innovative and challenged, and desire to work on impactful business technology projects, we want to hear from you. Join our award-winning technology team, and grow your career!\\n\\nWe are excited to expand our Business Intelligence and Analytics team to enable us to do even more for our clients and our staff. We are seeking candidates who are passionate about data and solving business problems with state-of-the-art technology solutions.\\n\\nYour Role.\\n\\nYour work will include, but not be limited to:\\nAll aspects of Extract, Transform, Load (ETL) and Data Integration Pipelines including:\\nDesign and Development\\nData Mapping and Modeling\\nData Investigation, Profiling, Cleansing and Transformations\\nError Handling, Logging, Troubleshooting\\nSecurity\\nTest Plan Development and Verification\\nWork with Data Architects and Solution Architects to:\\nImplement, maintain and support conceptual, logical and physical data models\\nEstablish, implement and uphold data integration standards and methodologies\\nImplement, maintain and support data marts and the data warehouse system\\nImplement, maintain and support the data quality, data catalog and master data management initiatives\\nParticipate in strategic and tactical architecture direction\\nProactively support activities around data integration; such as on-going data validation and performance tuning\\nParticipate in code and design review to ensure alignment to standards and best practices\\nCollaborate with others to:\\nDesign, implement, maintain and support semantic layers, data marts and tools that enable creation of reports and dashboards for firm-wide and self-service use cases\\nMentor and train on the appropriate usage of data marts, enterprise data warehouse and other data sources used in reporting and analytics\\nMentor and train on appropriate uses for tools for ETL, reporting and visualization use cases\\nCoordinate promote to production activities to publish reports and dashboards from business units to production deployments when deemed appropriate\\nDevelop, implement, maintain and support reporting, dashboards and analytical solutions\\n\\n\\nThe qualifications.\\nBS/BA is required, Computer Science, Business, Math or related field is preferred\\nRequired Experience\\nAt least 5+ years of recent experience working with Microsoft SQL Server 2012 or higher\\nAt least 5+ years of recent experience with T-SQL\\nAt least 5+ years of recent experience developing and maintaining ETL solutions with SQL Server Integration Services (SSIS) 2012 or higher\\n\\n\\nPreferred Experience.\\nAzure Data Factory\\nAzure SQL Database\\nPython, R, .Net, etc.\\nData Modeling\\nRecent versions of Microsoft SQL Server Analysis Services (SSAS)\\nRecent versions of Microsoft SQL Server Reporting Services (SSRS)\\nMicrosoft Power BI\\nStrong oral and written communication skills and the ability to communicate to multiple audiences such as within Technology Services and directly with business stakeholders\\nDemonstrated experience with planning, designing and delivering end-to-end ETL solutions\\nFull life-cycle development and maintenance leadership for data warehouse, ETL, and business intelligence solutions\\nDemonstrated ability and skills to drive projects and initiatives to completion\\nOur difference.\\nWe are a nationally recognized public accounting, consulting, and wealth management firm, consistently ranked as one of FORTUNE magazine’s “100 Best Companies to Work For.” At Plante Moran, we live by the Golden Rule, fostering a relatively “jerk-free” culture with the lowest staff turnover rate in the industry. Our supportive network of well-rounded professionals is excited to catapult your growth and help pave your pathway to professional excellence.\\n\\nApply now. Experience our difference.\\n\\nThis is an exempt position, so you may have to work hours that exceed the standard 40-hour work week.\\n\\nPlante Moran is an Equal Opportunity Employer, committed to a diverse workplace.\\n\\nInterested applicants must submit their resume for consideration using our applicant tracking system. Due to the high volume of applications received, only candidates selected for interviews will be contacted. Candidates must be legally authorized to work in the United States without sponsorship, with the exception of candidates that are bilingual in Japanese and English. Unsolicited resumes from search firms or employment agencies, or similar, will not be paid a fee and become the property of Plante Moran.\\n\\nThe specific statements above are not intended to be all-inclusive.',\n",
       "  'All communication between Customers and Selling Partners is supported by Amazons anonymous messaging system Buyer-Seller Messaging, which protects Buyer and Seller privacy and applies business rules to regulate communications. BSM enables a) Buyers to get post-order customer service, b) Buyers to ask Sellers pre-order product questions, and c) Sellers to contact Buyers post-order. Our vision is to empower Selling Partners and Buyers with a trustworthy, useful, and efficient experience that enables customer service for Seller fulfilled orders at par with Amazon.\\nWe are looking for a talented data engineer to help build/enhance the data platform needed to power a global communications experience between millions of Customers and third-party Sellers. You will own a large and truly big data-sets and your day to day work will impact millions of Customers and Sellers who communicate with each other for mission-critical use cases every day. You will set the long term vision as well as execute on the data roadmap with two teams of engineers, UX designers, and product managers. You will use advanced technologies (big data, natural language processing, digital forensics) to innovate on behalf of our Customers and Sellers, and solve complex problems to lead a global messaging platform.\\n\\nTechnologies used: S3, Spark, AWS Kinesis, Redshift, SQL, Tableau, Hive, Quicksight, EMR, Glue, ElasticSearch, Sagemaker, CI/CD pipelines, EC2\\n\\n\\n\\nBasic Qualifications\\n\\n· At least 3 years of data engineering experience\\n· Solid experience with REST, and web services\\n· Agile development practices\\n· Strong verbal and written communication skills and demonstrated technical leadership\\n· Works well in a fast-moving team environment and is able to effectively drive cross-team solutions having complex dependencies and requirements\\n· B.S. in Computer Science/a related field or relevant experience\\n· Technologies used: S3, Spark, AWS Kinesis, Redshift, SQL, Tableau, Hive, Quicksight, EMR, Glue, ElasticSearch, Sagemaker, CI/CD pipelines, EC2\\n\\n\\n\\nPreferred Qualifications\\n\\n· Experience building reliable, scalable, secured and complex software systems that have been successfully delivered to customers\\n· Excellent written and verbal communication skills to work with stake holders and partner teams',\n",
       "  'Excited by using massive amounts of data to develop Machine Learning (ML) and Deep Learning (DL) models? Want to help the largest global enterprises derive business value through the adoption of Artificial Intelligence (AI)? Eager to learn from many different enterprises use cases of AWS ML and DL? Zealous to be key part of Amazon, who has been investing in Machine Learning for decades, pioneering and shaping the worlds AI technology?\\n\\nAt Amazon Web Services (AWS), we are helping large enterprises build ML and DL models on the AWS Cloud. We are applying predictive technology to large volumes of data and against a wide spectrum of problems. Our Professional Services organization works together with our AWS customers to address their business needs using AI.\\n\\nAWS Professional Services is a unique consulting team. We pride ourselves on being customer obsessed and highly focused on the AI enablement of our customers. If you have experience with AI, including building ML or DL models, wed like to have you on our team. You will get to work with an innovative company, with great teammates, and have a lot of fun helping our customers.\\n\\nThis role will focus specifically on AWS most complex and largest customers in the world to help solve a wide range of business problems. Consultants will provide deep and broad insight to customers and partners to help remove constraints that prevent them from leveraging AWS services to create strategic value.\\n\\nA successful candidate will be a person who enjoys diving deep into data, doing analysis, discovering root causes, and designing long-term solutions. It will be a person who likes to have fun, loves to learn, and wants to innovate in the world of AI.\\n\\nMajor responsibilities include:\\n· Understand the customers business need and guide them to a solution using our AWS AI Services, AWS AI Platforms, AWS AI Frameworks, and AWS AI EC2 Instances .\\n·\\n· Assist customers by being able to deliver a ML / DL project from beginning to end, including understanding the business need, aggregating data, exploring data, building & validating predictive models, and deploying completed models to deliver business impact to the organization.\\n·\\n· Use Deep Learning frameworks like MXNet, Caffe 2, Tensorflow, Theano, CNTK, and Keras to help our customers build DL models.\\n·\\n· Use SparkML and Amazon Machine Learning (AML) to help our customers build ML models.\\n·\\n· Work with our Professional Services Big Data consultants to analyze, extract, normalize, and label relevant data.\\n·\\n· Work with our Professional Services DevOps consultants to help our customers operationalize models after they are built.\\n·\\n· Assist customers with identifying model drift and retraining models.\\n·\\n· Research and implement novel ML and DL approaches, including using FPGA.\\n·\\nThis is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.\\n\\nNote: If you do not live in a market where we have an open Data Scientist position, please feel free to apply. Our Data Scientists can live in any location that has an AWS office.\\n\\nBasic Qualifications\\n\\n· A Bachelor or Masters Degree in a highly quantitative field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.) or equivalent experience\\n· 4+ years of industry experience in predictive modeling, data science and analysis\\n· Previous experience in a ML or data scientist role and a track record of building ML or DL models\\n· Experience using Python and/or R\\n· Experience using ML libraries, such as scikit-learn, caret, mlr, mllib\\n· Experience in writing and tuning SQL\\n· Experience with SparkML\\n· Experience working with GPUs to develop model\\n· Experience handling terabyte size dataset\\n· Experience using data visualization tools\\n\\nPreferred Qualifications\\n\\n\\n· PhD in a highly quantitative field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.)\\n· 4+ years of industry experience in predictive modeling and analysis\\n· Skills with programming languages, such as Java or C/C++\\n· Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations\\n· Consulting experience and track record of helping customers with their AI needs\\n· Publications or presentation in recognized Machine Learning, Deep Learning and Data Mining journals/conferences\\n· Experience with AWS technologies like Redshift, S3, EC2, Data Pipeline, & EMR\\n· Combination of deep technical skills and business savvy enough to interface with all levels and disciplines within our customers organization\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment\\n· Experience diving into data to discover hidden patterns\\n· Able to write production level code, which is well-written and explainable\\nAmazon is an Equal Opportunity Employer Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.',\n",
       "  \"Job Code ATL - 341\\nJob Title Data Engineer\\nLocation Dearborn, MI or Southfield, MI,Michigan\\njob description\\nJob Description:\\nLocation Dearborn, MI or Southfield, MI\\nType: Full Time (H1 transfer acceptedif the candidate can join on Receipt Notice)\\n\\nSalary range - $70K - $100K per year + benefits.\\n\\nLinkedin profile MANDATORY.\\nData Engineer-Engineer will be part of the Data Supply Chain. The product team's objective is to replicate data from hundreds of database sources within the company to the Hadoop environment and do transformations to make it usable for data scientists. This position will require an individual who has a strong background with multiple database technologies, who are process-oriented and has knowledge of Java and expertise in the Hadoop environment\\n3+ years of software development experience, preferably using JAVA\\nExperience with either SQL, Oracle or DB2.\\nExperience working with the Hadoop ecosystem using tools like Sqoop, HIVE, and HBASE, etc.\\nStrong team player with experience working as part of an agile team Independent self-starter who is able to work in a constantly evolving environment\\nSkills Needed sql,Hive,JAVA.,Oracle or DB2,Hadoop ecosystem using tools like Sqoop,and HBASE\\n\\nJob Posted by ApplicantPro\",\n",
       "  \"Revature is the fastest growing employer of emerging technology talent in the US and we are currently looking to hire over 100 new Software Engineers.\\n\\nOur Software Engineers design, analyze and build next-gen software systems, including business applications, games, computer applications, middleware, and network control systems across a variety of industries, including finance, insurance, retail, healthcare and government.\\n\\nRevature has been featured in the Wall Street Journal, Money, Time, on MSN, and was recently named as one of the 8 Cool Companies to Apply to With Awesome Benefits by Glassdoor.\\n\\nJoin us and be part of the next generation of Software Engineers. Interviews are starting now!\\n\\nWhat We Are Looking For:\\n0-3 years experience\\nSolid foundational knowledge of SQL\\nA natural problem solver\\nStrong communication and interpersonal skills\\nAbility to relocate\\nEligible to work in the US\\nRevature is not currently sponsoring work visas or transfers at this time.\\n\\nWhat We Offer:\\nCompetitive Salary\\nRelocation Assistance\\nCorporate Housing\\nHealth, Vision and Dental Insurance\\nPaid Time Off\\nEnterprise level development training\\nLife Insurance\\n401K\\nMentoring and on-going support throughout your entire Revature career\\nExperience with one of the world's largest and most reputable companies in the US\\nSuitable candidates are encouraged to apply immediately\\n\\nNot Mentioned\",\n",
       "  'You will work in a collaborative and inclusive work environment with users and analysts enabling data collection and developing solutions for data management, analysis, and visualization\\n\\nKey Job Responsibilities:\\nDevelop battery data analytics processes and visualizations for vehicle programs using relevant tools:\\nData Management and retrieval using Spark, Hive, Hue, and Jupyter Notebooks.\\nData analysis using Python, MATLAB, and R.\\nData visualization with Power BI, Tableau, Matplotlib, and ggplot2.\\nCollaborate with battery data analytics and engineering groups on algorithm design, results, modifications, production issues, data consumption methods, and lessons learned.\\nApply analytical methods to gather insight from electric vehicle usage data. Use the appropriate statistical methods to make actionable inferences.\\nFit complex models to multi-factor datasets using Python, MATLAB, R.\\nModel and project the long-term trends of key electric vehicle metrics. Validate results.\\nApply statistical and machine learning models to understand battery production quality metrics.\\nContribute to analytics forums and cross functional groups by sharing learning within and beyond Global Battery Engineering.\\n\\n\\nRequired Skills and Experience:\\nB.S.\\nDegree in Engineering, Computer Science, Applied Mathematics, Statistics,\\nor Data Analytics.\\n5\\nyears minimum demonstrated technical knowledge of data analysis methods and\\nsuccess solving complex problems\\nProficiency\\nusing relevant tools (Python, Spark, Hive, MATLAB, Power BI, R)\\nReal-world experience using Hive to query Hadoop data file\\nstructures.\\nPractical Knowledge of machine\\nlearning and predictive methods\\nAbility\\nto manage multiple projects and assignments with high level of autonomy\\nand accountability for results\\nWillingness\\nto learn and quickly adjust to new tools and systems\\nCapable\\nof converting ambiguous problem statements into concrete project\\nrequirements\\nDemonstrated\\nproficiency in statistical analysis techniques\\nPractical\\nKnowledge of machine learning and predictive methods\\nPassion\\nfor data driven results and customer initiatives\\nExcellent verbal and\\nwritten communication skills including the ability to communicate\\neffectively and professionally with colleagues and stakeholders. Ability\\nto understand and explain technical concepts to non-technical stakeholders.\\nConfident teaching and\\ncoaching abilities\\n\\n\\nPreferred Skills and Experience:\\nMasters or PhD in Computer Science, Data Analytics, or Statistics\\nKnowledge of high voltage batteries and electrification subsystems\\nElectrochemistry Background\\n\\n\\nKey Behaviors:\\nExceptional communication and presentation skills\\nDealing with ambiguity\\nConflict management\\nPrioritization and decision-making skills\\nAction-oriented\\nCustomer-focused\\nIntegrity and Trust\\nProblem Solving',\n",
       "  'DATA ANALYST, DETROIT, MICHIGAN\\n\\nWalkerHealthcareIT is seeking a Data Analyst for a healthcare client located in Detroit, Michigan. This is a contract position.\\n\\nSTART DATE: ASAP\\n\\nON-SITE / REMOTE: 100% On-site\\n\\nWAGE TYPE: 1099, W2\\n\\nWalkerHealthcareIT Standard Perks\\n\\nWeekly pay via Direct Deposit\\n\\nDATA ANALYST JOB DESCRIPTION\\n\\nAnalyst is responsible for identifying, collecting, analyzing, and maintaining data to evaluate issues that support prospective business decisions.\\n\\nDATA ANALYST JOB REQUIREMENTS\\nBachelor degree required in a related field such as Computer Science, Business Information Systems, Applied Mathematics.\\nSQL Queries (Joins, Nested Joins, Complex Joins) 3 yrs. experience\\nTableau Report Writing using Desktop Server instance 3 yrs. experience\\nMicrosoft Office Products (Excel with macros/pivots 3 yrs. Exp., Access 3 yrs. Exp.)\\nEstablished communication skills are required to understand, interpret, and communicate ideas.\\nEstablished ability to learn new technology, techniques, and processes.\\nEstablished knowledge of database design, modeling, and implementation on SQL/Server (2012 and newer) or equivalent database experience.\\nEstablished knowledge of various operating systems such as Windows, LINUX, etc.; Strong knowledge of files/databases on the operating systems.\\nEstablished knowledge of various platforms such as Cloud PaaS, SaaS would be beneficial.\\nEstablished analytical, organizational, planning, and problem solving skills.\\nAbility to read/create data models, project design documents, and testing documents.\\nAbility to effectively interface with employees at all levels.\\nEstablished understanding of and ability to apply statistical inference.\\nAbility to read and interpret documents such as safety rules, operating and maintenance instructions, and procedure manuals. Ability to write reports and correspondence. Ability to speak effectively before groups of customers or employees of organization.\\nAbility to add, subtract, multiply, and divide in all units of measure, using whole numbers, common fractions, and decimals. Ability to compute rate, ratio, and percent, and to draw and interpret bar graphs.\\nprovided by Dice',\n",
       "  'Title: Software Engineer\\n\\nLocation: Detroit, Michigan\\n\\nType: Direct Hire *(will not consider contract)\\n\\nCompensation: up to 150k, stock options\\n\\nDescription:\\n\\nYoull work on full-stack components including UI/UX, API integration, SDK, and extensions for our next generation synchronization and migration framework. Using an Agile approach, engineers are involved in the full lifecycle of code design, delivery, automation and extension.\\n\\nEngineers help refine tasks from the JIRA backlog, adding acceptance criteria and then working with architects to build something that matches design guidelines. We use a TDD approach and our process adds an emphasis on code reviews and collaboration. You will deliver a completed User Story after code review approval to QA staff, but stay involved to ensure that testing and documentation is done correctly.\\n\\nBecause of the complexity of the product, engineers regularly work with client services, sales engineers, and customers to puzzle out specific challenges and business use cases.\\n\\nSoftware Engineers are required to be self-starters capable of working independently with minimal management oversight. Because we work remotely, good written and oral communication skills are a must.\\n\\nAs we extend our product, we are looking at integrating with AI/Machine Learning tools, so if you have interest or experience in that domain then we might be a good fit for you.\\n\\nRequired skills:\\nExtensive experience with software patterns and standards (MVVM, OAuth, other frameworks using OO principles)\\nFamiliarity with asynchronous programming\\nStrong skill in automated testing including mocking\\nExperience programming against Web services (primarily REST)\\nProficiency in Microsoft .Net, and C#\\nExperience with SQL databases (MS SQL, PostgreSQL)\\nAbility to work with React/Redux, Typescript and JavaScript ES6\\nFamiliarity with NodeJS and NPM\\nDesired skills\\nExperience with HTML5, React.js\\nExperience with Git, TeamCity\\nAbility to refine basic stories and find hidden requirements\\nMicrosoft SharePoint development, and/or any ECM platform development experience will be a big benefit.\\nEEOC: We are committed to equal employment opportunity without consideration of race, color, religion, ethnicity, citizenship, political activity or affiliation, marital status, age, national origin, ancestry, disability, veteran status, sexual orientation, gender identity, gender expression, sex or gender, or any other basis protected by law.',\n",
       "  \"We're looking for a Senior Software Engineer to join the Services Team at StockX.\\n\\nThe Services Team is responsible for powering the StockX experience across different platforms by providing a scalable and reliable set of microservices that implement our core business functionality. We utilize cutting edge tools and platforms such as Node.js, Kafka, Docker, Kubernetes, and AWS to handle our massive growth.\\n\\nAs a Senior Software Engineer, you will be empowered to take ownership of technology decisions and solutions while playing a pivotal role in establishing a successful engineering culture at a fast-growing company.\\n\\nThis is a great opportunity to leverage your existing skills, to build a world-class team and to have a huge impact on how marketplaces can be redefined. At StockX, we are just getting started.\\n\\nResponsibilities\\nDesign, build, and evolve microservices used by StockX web and mobile applications\\nCollaborate with front-end and back-end engineers to build scalable services\\nResearch and implement cutting edge technology that can be applied to handle massive scale\\nDebug and monitor production systems\\nHelp define the way we work in the future including coding and design standards\\nWork effectively in an agile development process\\nRequirements\\nStrong experience and understanding of JavaScript (Node.js)\\nStrong experience and understanding of data storage, relational (particularly Postgres) and non-relational (particularly Redis and DynamoDB)\\nExperience in service oriented and/or microservice architectures\\nExperience with message queues, pub-sub systems, and/or event streams\\nExperience working with AWS or other cloud providers\\nYou have built highly resilient, scalable REST-based services\\nYou are product focused and collaborate to find the best possible solutions\",\n",
       "  'Posting Details\\n\\nPosting Number\\n\\n044722\\n\\nPosition Title\\n\\nData Analyst\\n\\nNumber of Vacancies\\n\\n1\\n\\nSchool/College/Division/Institutes/Centers\\n\\n92 - Student Services\\n\\nPrimary Department\\n\\nH9243-Registrar/Records/Registration\\n\\nEssential Functions (Job Duties)\\n\\nPOSITION PURPOSE\\nProvide analysis and reports on data designed to improve the student recruitment, admissions and enrollment processes. Serve as a consultant to students and management on system use and data interpretation.\\n\\nESSENTIAL JOB FUNCTIONS\\nCollect, create, test and manipulate data in the student information systems and applications software to assure data integrity and enhance decisions. Coordinate external and internal business partners to obtain required data and documentation.\\nPrepare, verify and validate reports. Prepare and maintain documentation and make presentations regarding data analysis and process efficiency.\\nParticipate in the development and execution of project, recruitment and yield plans designed to increase student enrollment. Identify target markets and develop appropriate messaging.\\nServe as consultant to system and data users. Assist with technical problems, data interpretation, data fulfillment and application issues.\\nRespond to inquiries from applicants to ensure full utilization of data triggers required to complete enrollment processes.\\nDevelop and maintain collaborative working relationships with other university schools and colleges to maximize system utilization and efficiency.\\nMay be required to supervise a small staff of part time support personnel.\\nPerform related work as assigned.\\n\\n\\n\\nUnique Duties\\n\\nQualifications\\n\\nMINIMUM QUALIFICATIONS\\nGraduation from an accredited college or university in computer science, information science, business or an equivalent combination of education and/or experience. Knowledge of Banner Student Information System preferred.\\nExcellent written and oral communication skills.\\nExcellent customer service skills.\\nStrong organizational skills and attention to detail.\\nAbility to analyze quantitative data and communicate the information accurately to customers.\\nAbility to work effectively in a fast-paced environment.\\nProficient with Microsoft office suite including Excel, Word and Access.\\nPrevious experience in data management.\\n\\n\\n\\nPreferred Qualifications\\n\\nTesting Requirements\\n\\nNot Applicable\\n\\nTest Scheduling\\n\\nWorking Conditions\\n\\nJob Type\\n\\nFull-Time\\n\\nJob Category\\n\\nProfessional/Administration & Supervisory/Management',\n",
       "  \"Stefanini is looking for a Software Engineer in MI.\\n\\nWhile others are taking a technology-first approach, our Client is putting people first. They are building a self-driving business ecosystem that is purpose-designed from the outset. Clients AV strategy is different because they are designing, testing and operating an ecosystem today that is fit for the purpose of our AV business model in the future. Are you passionate about transportation-as-a-service (TaaS) served by Autonomous Vehicles? Are you motivate to apply your software skills to build the self-driving ecosystem? Our team has been tasked to develop scalable building block platforms based on mobility opportunities. Does it sound exciting? Then, join our team as a Software Engineer\\n\\nSkills Required:\\n2+ years of work experience in Object Oriented development in Java/J2ee / Ruby / Python / Golang\\n2+ years of experience with SQL.\\nExperience Preferred:\\nExceptional software engineering knowledge; OO Design Principles.\\nFamiliar with eXtreme Programming (XP) practices including: Pair/Mob programming &Test-first/Test Driven Development (TDD).\\nExperience with Spring Cloud and deploying to cloud platforms, preferably Pivotal Cloud Foundry or Cloud Foundry.\\nExperience with Spring, RESTFUL/SOAP Web services development.\\nExperience in development of microservices.\\nFamiliarity with GitHub or equivalent source control repositories and Build Tools like Gradle.\\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders.\\nCapable in Continuous Integration/Continuous Delivery tools and pipelines such as Jenkins, Maven, Gradle, etc.\\nExperience with JAVA development.\\nStrong analytical and problem-solving skills . Strong oral and written communication skills.\\nAbility to multi-task and manage changing priorities.\\nStrong teamwork and interpersonal skills . Ability to work independently and take initiative when solving unexpected problems.\\nExperience with Continuous Integration/Continuous Delivery tools and pipelines.\\nExperience understanding/deploying to cloud environments.\\nPCF/ Azure etc.\\nExperience with CA Agile Central (Rally), backlogs, iterations, user stories, or similar Agile Tools.\\nExperience with Lean Agile methodology.\\nEducation Required:\\nBachelor's degree in Computer Science or similar technical discipline or equivalent experience\",\n",
       "  'Company: iTech US, Inc.Title: Big Data Engineer\\n\\nLocation: Dearborn, MI\\n\\nDescription:\\n\\n• Strong experience working on Hadoop tools (like Sqoop, Hive, PIG, HBASE, etc)\\n• Strong on JAVA and spring boots\\n• Good Hands on Experience in SPARK and SCALA\\n• Should have Project lead experience and had worked on Hadoop architecture.\\n• Should be able to manage responsibility of developing and maintaining data ingestion framework developed using Java, Spark.\\n\\nRequired Education: At least a Bachelor of Computer Science, Software/Electronics/Electrical Engineering, Information Systems or closely related field is required.\\n\\nNecessary Skills: Hadoop, Spark, Scala, Java, Spring boots\\n\\nTo apply:\\n\\nSamuel\\n802-391-0368\\n\\niTech US, Inc. Texas/Vermont/NJ)\\nprovided by Dice',\n",
       "  \"Job Description\\nPosition Description:\\nWork closely with stakeholders throughout Operations/Enterprise to identify opportunities for leveraging company data to drive process optimization and efficiency.\\nLead, coordinate and communicate work across multiple teams.\\nOrganize large datasets and get actionable insights for business.\\nApply data mining, data transformation and develop statistical models to datasets.\\nUse algorithms and programming to efficiently explore large dataset.\\nCreate meaningful data visualization or outputs files to communicate findings.\\nCollect feedback from stakeholders. Work with IT to support implementation of algorithm or tools to production environment.\\nRelate analytical insights to business impact. Build analytic model work flows to solve business questions.\\nCreate dashboards to drive data driven insights.\\nCreate different analytic models to solve multiple problems.\\nTranslate model/analytic outputs into business insights.\\nSkills Required:\\nProven proficiency in developing analytic models, working in a team environment, supporting customer/end users.\\nExperience with R, SQL, Tableau, Alteryx, CPLEX, Python, Any Logic, Qlikview, Hadoop,\\nAble to understand and transform various data structures, excellent pattern recognition and predictive modeling skills.\\nProven ability to drive business results from data insights, algorithm, optimization and simulation\\nStrong problem solving skills, critical thinking, strong collaboration and communication skills\\nExperience Required:\\n2+ years of experience in data mining, statistical analysis, modeling, optimization\\n\\nCompany Description\\nRGBSI, a leader in the staffing industry, is dedicated to providing the highest quality of services to our clients and employees. We are proud to be the primary staffing partner to top companies in the automotive, aerospace, IT, media & entertainment, sports, energy, and finance industries. As a result of our clients' continued growth, we are looking for energetic, personable professionals to partner with these leading US companies.\",\n",
       "  'Detroit, Michigan\\nSkills : Account,Accounting,accuracy,ACH,Administrative,analysts,AS,basic,Cash,Check,clients,Close,Computer,Copy,coverage,create,Customer,Data,deposits,detail,Diploma,direction,discrepancies,electronic,entries,entry,escalating,estate,files,financial,functions,GED,General,Good,hard,Information,IS,Job,Key,knowledge,learn,Level,Load,mainframe,Maintain,maintenance,Manage,managers,Microsoft,Office,operations,ops,OR,order,organizational,Other,Pension,perform,personal,prioritize,Process,Processes,processing,research,resolve,Respond,Review,school,senior,Service,suite,Supervision,support,systems,Team,this,TRANSACTIONS,transfers,trust,under,with,work,working,Works\\nDescription :\\n2-3 years financial operations experience\\nExperience using Office products\\nA high school diploma or GED is required for this role.\\nAttention to detail\\nTime management\\nOrganizational Skills\\nInterview: Onsite interview with manager\\nResponsibilities include:\\nProcess all same day trades\\nProcess all next day trades\\nProcess Leveraged Loan trades\\nProcess derivative trades such as Options, Swaps\\nAccount,Accounting,accuracy,ACH,Administrative,analysts,AS,basic,Cash,Check,clients,Close,Computer,Copy,coverage,create,Customer,Data,deposits,detail,Diploma,direction,discrepancies,electronic,entries,entry,escalating,estate,files,financial,functions,GED,General,Good,hard,Information,IS,Job,Key,knowledge,learn,Level,Load,mainframe,Maintain,maintenance,Manage,managers,Microsoft,Office,operations,ops,OR,order,organizational,Other,Pension,perform,personal,prioritize,Process,Processes,processing,research,resolve,Respond,Review,school,senior,Service,suite,Supervision,support,systems,Team,this,TRANSACTIONS,transfers,trust,under,with,work,working,Works',\n",
       "  \"Duration: 12 months to start\\n\\nWork location: Detroit, MI (onsite)\\n\\n***W2 candidates only\\n\\nLooking for a Data Analyst with prior experience analyzing data for a healthcare payer, strong knowledge of claims, billing and enrollment data, as well as the associated business processes is required.\\n\\n-Strong knowledge of concepts and prior experience working within an Enterprise Data Warehouse environment with numerous data sources and data volumes in the terabytes.\\n\\n-Ability to perform analysis with little/no direction, proactively identify potential issues in the data and conduct research to determine the best course of action to correct the data\\n\\n-Strong Data Mapping and documentation skills\\n\\nREQUIRED SKILLS/EXPERIENCE\\n\\n-5+ years prior experience as a data analyst or data architect within healthcare, preferably with a payor, should have performed data analytics on daily basis as a primary role\\n\\n-Strong knowledge of Claims, Billing and Enrollment data and the associated business processes\\n\\n-Understanding of Enterprise Data Warehouse concepts and prior experience applying that understanding to query data in a Very Large Database (VLDB) environment\\n\\n-Demonstrated expertise in writing and analyzing complex SQL queries is required\\n\\n-Ability to work on multiple projects simultaneously and deliver within tight timelines while being flexible in adapting to new roles\\n\\n-Knowledge of data modeling concepts both in a multi-layer data warehouse (Normalized / Inmon) and data mart (Dimensional / Kimball)\\n\\n-Ability to work with multiple areas within organization to get business objectives, data requirements etc.\\n\\n-Identify problematic areas and conduct research to determine the best course of action to correct the data\\n\\n-Ability to interpret data that is not well defined or documented and develop recommendations based on findings\\n\\n-Excellent written and verbal communication skills with the demonstrated ability to present findings and recommendations and to business users or leadership\\n\\nPlus skill:\\n\\n-Previous experience with data modeling tools\\nKnowledge of Informatica, ability to read / analyze existing code and be able to propose innovative solutions and modifications.\\nEDUCATIONAL REQUIREMENTS\\n\\nBachelor s Degree in related field required. Master's Degree in related field preferred\\n\\n- provided by Dice\",\n",
       "  \"Position Overview/Description\\n\\nFord's Global Data, Insights and Analytics team is seeking a Data Scientist. The Data Scientist will discover and curate new and existing data sources to create solutions for the business. These key positions help bring data to life to solve business problems. At times, the problem is identified by a business unit, frequently wishing to optimize cost, revenue, or improve quality, and at times problems are formulated by the Data Scientists. Solutions are then envisioned that harness internal and external data and through visualizations, predictive analytics, and prescriptive methods, providing the ability for users to answer their key questions.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nThe Data Scientist position is responsible for:\\n\\nSupporting Material Planning & Logistics, Order to Delivery, and Complexity teams with insights gained from analytics and modeling\\nSupporting the development and delivery of analytic models using skills such as data acquisition and management, algorithm design, and model development & refinement\\nUsing large dataset to find opportunities of process optimization and improve efficiency in varieties of processes and systems\\nFormulating problems using variety of data mining/transformation methods, creating algorithm and simulations to introduce and verify effectiveness of different courses of action\\nAcquiring deep understanding of the business problems, define problem scope from business requests, and translate business requirements to analytical projects\\nCollaborate across teams, tracking project progress, and communicating between technical and business groups\\nCollecting feedback from business users and implementing with technical teams\\nTranslating data based insights into business actions and results\\nWorking effectively with a wide range of stakeholders and functional teams\\nDesigning visual interface for users to interact with the data\\nInterpreting results and communicating them to technical and non-technical audiences, cross-functional teams and executive leadership\\nWorking with business on change management, including new process implementations\\nProviding training and maintenance of implemented tools to stakeholders\\n\\nBasicQualifications\\nBachelor's degree in quantitative field such as Statistics, Economics, Mathematics, Data Science, Operations Research or related field\\n1+ years of experience with one or more of the following analytics tools: R, SQL, Tableau, Alteryx, MINITAB, CPLEX, Python, Any Logic, Qlikview, Hadoop, or SAP OR equivalent college course experience\\n\\nPreferred Qualifications\\nMaster's/PhD in quantitative field, such as Statistics, Economics, Mathematics, Data Science, Operations Research\\n3+ years of experience with R, SQL, Tableau, Alteryx, MINITAB, CPLEX, Python, Any Logic, Qlikview, Hadoop, SAP\\n3+ years of experience in data mining, statistical analysis, modeling, optimization\\nComfortable working in an environment where problems are not always well-defined\\nInquisitive, proactive, and interested in learning new tools and techniques\\nStrong oral, written and interpersonal communication skills\\nWell-organized, independent and ready to work with minimal supervision\\nHave a desire to excel and work with talented people\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nVisa sponsorship may be available for this position\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  'The Rock Family of Companies is made up of nearly 100 separate businesses spanning fintech, sports, entertainment, real estate, startups and more. We’re united by our culture – a drive to find a better way that fuels our commitment to our clients, our community and our team members. We believe in and build inclusive workplaces, where every voice is heard and diverse perspectives are welcomed. Working for a company in the Family is about more than just a job – it’s about having the opportunity to become the best version of yourself.\\n\\nWho We Are\\n\\nRocket Homes Real Estate LLC is a Detroit-based, tech-driven company with a passion for simplifying real estate. Our mission is to create a seamless home buying and selling experience by combining the process of searching for homes, connecting with a trusted real estate agent and getting a mortgage. Since 2006, we’ve partnered with our sister company, Rocket Mortgage® by Quicken Loans, and our nationwide network of top-rated real estate agents to help over 500,000 clients with their real estate needs.\\n\\nJob Summary\\n\\nThe Software Engineer works with a small team of dedicated developers building highly scalable data integration and data pipeline platforms to power multiple products, including our home search website. The primary focus is to build consumable data products by identifying, integrating, transforming and cleansing massive amounts of real estate and user data. This team member owns the designing, building and maintaining of data integration platforms that support our products.\\n\\nResponsibilities\\nWork in a fast-paced, agile environment with a team of inquisitive minds\\nDeliver timely recommendations to technical and nontechnical audiences\\nWrite reusable, testable and efficient code while working independently in these areas\\nDevelop and maintain a low-latency, high-availability and performant data pipeline platform\\nConsistently meet project deadlines and take ownership of delegated projects\\nRequirements\\n5 years of software engineering experience\\nExperience in C#, Java, Node.JS and Git\\nRelated experience in Agile methodology\\nUnderstanding of front-end technology\\nStrong communication skills\\nSuperb attention to detail and process-driven\\nAbility to collaborate within a small team and be comfortable working in a fluid, startup work environment\\nDemonstrated history of taking the initiative to solve tough problems while getting high-quality work done quickly\\nEnthusiasm and an eagerness to learn\\nAbility to respond professionally to feedback\\nThe Company is an Equal Employment Opportunity employer, and does not discriminate in any hiring or employment practices. The Company provides reasonable accommodations to qualified individuals with disabilities in accordance with state and federal law. Applicants requiring reasonable accommodation in completing the application and/or participating in the employment application process should notify a representative of the Human Resources Team, The Pulse, at 1-800-411-JOBS.',\n",
       "  'OverviewWho we are:\\n\\nMeridian, a WellCare Company, is part of a national network of passionate leaders, achievers, and innovators dedicated to making a difference in the lives of our members, our providers and in the healthcare industry.\\n\\nWe provide government-based health plans (Medicare, Medicaid, and the Health Insurance Marketplace) in Michigan, Illinois, Indiana, and Ohio. As a part of the WellCare Family of companies, we deliver healthcare excellence to millions of members nationwide.\\n\\nOur associates work hard, play hard, and give back. Meridian associates enjoy an exceptional experience and culture including special events, company sports teams, potlucks, Bagel Fridays, and volunteer opportunities.\\n\\nA Day in the Life of a Data Scientist:\\n\\nThe Data Scientist is part of a team that researches and investigates business problems through quantitative analysis using healthcare cost, utilization, clinical, assessment, and external data. This position also conducts the on-going presentation of analysis results and findings to support clinical programs, healthcare business operations, performance and improvement.\\n\\nResponsibilitiesEssential Functions:\\n\\nGather and prepare analyses based on information from internal and external sources to evaluate and demonstrate program effectiveness and efficiency (10%) Leverage knowledge of programming, mathematics and computer science to transform the way our company does business\\nTranslate business opportunities into data-driven big-data and modeling solutions using statistical, econometric or data-mining tools and methods\\n\\nAnalyze and extract data from the Enterprise Data Warehouse (EDW) to report to internal and external customers (5%) Working in big data environment with Oracle and Hadoop environment to bring actionable insights with R, SAS, Python or other statistical packages\\n\\nResearch complex functional processes and industry standards using a variety of resources (5%) Educate the organization both from IT and the business perspectives on new analytic methods and technologies\\n\\nDevelop and prepare highly complex reports and provide and/or interpret information and data across divisions and departments (15%) Communicate complex analysis clearly to all audience through use of data visualization tool such as Tableau\\nGain an understanding of how data integrates across multiple transaction systems to create complex reporting solutions for actionable insights\\n\\nAssume responsibility for data integrity among various internal groups and/or between internal and external sources (5%)\\nDesign integrated and clinically-based engineering, management, medical and environmental solutions aiming at providing person-centered care to members, households and their respective communities with emphasis on health, social, economic and environmental sustainability (15%) Using statistical methods such as linear models (linear regression, generalized linear regression, logistic regression) and nonlinear modeling techniques to build predictive models to help drive better member engagement and impactability\\nLead the research and development of predictive models and capabilities using machine learning and artificial intelligence\\nSegment target population using supervised (classification) and unsupervised (clustering) learning methods to identify most impactful members\\n\\nConduct economic/outcomes analyses to quantify savings obtained from integrated solutions.(10%) Evaluate return of investment (ROI), utilization and compare them with pre-defined goals to measure effectiveness of program\\nDevelop meaningful data visualizations in form of dashboards to communicate results and empower senior executives decision making process\\n\\nDevelop systematic and scalable reporting processes and procedures to ensure timely delivery of weekly, monthly, quarterly, annual and ad hoc reporting to management (10%) Develop and improve algorithms like risk stratification methodology, pregnancy identification, etc. that is applied to all member on a monthly basis\\n\\nRecommend and implement new or modified reporting methods and procedures to improve report content and completeness of information (15%) Analyze benchmarking data, reports, processes, and measurements for key managed care indicators to recognize trends and assist in developing population health initiatives\\n\\nServe as a mentor to junior staff (5%)\\nPerform all other related duties as assigned (5%)\\n\\nQualificationsJob Requirements:\\n\\nEducation:\\n\\nMasters Degree or higher in Data Science, Biostatistics/Statistics, Econometrics or related field\\n\\nExperience:\\n\\n5+ years of experience in advanced analytics experience, machine learning, statistics, or econometrics role and who has solid analytical skills using, SAS, R, or Python\\nAdvanced knowledge of statistical and mathematical principles\\nExperience in Regression Analysis, Predictive Modeling, Data Mining, Machine Learning and Forecasting\\nExperience in Propensity Modeling for control group and treatment group testing\\nExperience in designing research experiments and measuring their effectiveness.\\nExperience with healthcare claims data\\nExperience with peer-reviewed publications\\nExperience with Six Sigma, Triple Aim and/or other relevant Project Management tools\\nStrong SQL Scripting, Statistical Programming, Data Visualization and Communication skills\\nAbility to interpret and communicate statistical results to a non-technical audience\\nWorking knowledge of Predictive Modeling and Grouper tools such as John Hopkins ACG System , Optum Symmetry, or related population health platform system(s)\\n\\nKnowledge: Must be familiar with common database architectures and understand how to extract necessary data\\nMust have strong proficiency in MS Excel (pivot tables, macros, lookups, etc.) and common analytic tools such as SAS, SQL, SPSS (writing queries, troubleshooting queries, extracting data, etc.)\\nKnowledge about Medicare, Medicaid and Commercial Managed Care preferred\\nWorking knowledge of Healthcare Effectiveness Data Information Set (HEDIS) specifications and CAHPS survey requirements\\nKnowledge of health care quality improvement processes and documentation\\n\\nSkills/Tools:\\n\\nAt least 2 years of experience with SQL Server or Oracle\\nAt least 2 years of experience with SAS or R\\nAt least 2 years of experience with MS Excel\\nAt least 2 years of experience with Data Visualization in Excel, R or Tableau\\nExperience with Oracle BI Answers a plus, not required\\nExperience with MS Access a plus, not required\\nExcellent written, verbal, presentation skills and inter-personal skills\\nStrong clinical analytical and data interpretation skills\\nStrong project management skills required; must be very organized and detail oriented\\nStrong written and oral communication skills required; must be able to interact with all levels within an organization\\n\\nAbilities:\\n\\nMust be able to execute complex clinical analyses using sound epidemiological concepts and methodologies\\nMust be able to produce requested analyses on a production schedule\\nAbility to present analytical findings to leadership to enable data-driven decisions\\nMust be able to interact with a variety of clinical operations SMEs to organize analytic plans and requirements for clinical analytic requests\\nAbility to work in a collaborative, virtual team environment and analyze problems using a team approach\\n\\nMust be a team player and someone with ability to work with minimal guidance in fast-paced work environment.\\n\\nAbility to influence and manage a diverse set of stakeholders (clinical and nonclinical), both internally and externally\\nAbility to think creatively to resolve common barriers and issues\\n\\nAbility to deal with ambiguity of ad hoc reporting requests, ask questions to seek clarification, and deliver results\\n\\n#MSTR',\n",
       "  \"Detroit Labs OnSite is looking for experienced Data Engineers to join one of our existing development teams in Detroit. Successful applicants will be working at a client site within our clients’ teams and play an important role in the development of software used by the masses.\\n\\nAs a full time member of our OnSite team, you're part of Detroit Labs family first. You’ll have access to ongoing career development, mentorship, and be a part of a great group of people who are passionate about never settling for less than their best. We prioritize your career growth by providing consistent check-ins, learning activities, yearly retros, and support through out your time to ensure you are always growing, and working on something you are passionate about.\\n\\nThe Data Engineer for Big Data is responsible for the full life cycle of the back-end development of a data platform. As a Data Engineer, you will create new data pipelines, database architectures, and ETL processes, you will observe and suggest what the go-to methodology should be. You will be gathering requirements, performing vendor and product evaluations, deliver solutions, training, and documentation. You will also handle the design and development, tuning, deployment, and maintenance of information, advanced data analytics, and physical data persistence technologies.\\n\\nYou will establish analytic environments required for structured, semi-structured and unstructured data. You will implement the business requirements and business processes, build ETL configurations, create pipelines for the Data Lake and Data Warehouse, research on new technologies and build proofs-of-concept around them. You will carry out monitoring, tuning, and database performance analysis. You will perform the design and extension of data marts, meta data, and data models. You will also ensure all data platform architecture code is maintained in a version control system.\\n\\nYou will be responsible to share knowledge with fellow team members allowing the entire team to grow and become proficient to further build-out and enhance the data platform.\\n\\nRequirements\\nMinimum two years' experience with big data tools: Hadoop, Spark, Kafka, NiFi, Hive, Sqoop\\nMinimum two years' experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena, Glue\\nMinimum two years' experience with stream-processing systems: Spark-Streaming, Kafka Streams, Flink\\nMinimum three years' experience with object-oriented/object function scripting languages: Java (preferred), Python, Scala\\nExpertise in design / developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks.\\nMinimum two years' experience with relational SQL and NoSQL databases like Mysql, Postgres, Cassandra and Elasticsearch.\\nMinimum two years' experience working in a Linux environment\\nDemonstrated ability to performance-tune MapReduce jobs\\nDemonstrated ability to work independently as well as with a team\\nAbility to troubleshoot problems and quickly resolve issues\\nStrong communication skills\\nResponsibilities\\nFocus on scalability, performance, service robustness, and cost trade-offs\\nDesigning and implementing high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark\\nCreate prototypes and proof-of-concepts for iterative development\\nDevelop ETL processes to populate a Data Lake with large datasets from a variety of sources\\nCreate MapReduce programs in Java, and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large datasets\\nMonitor and troubleshoot performance issues on the enterprise data pipelines and the Data Lake\\nFollow the design principles and best practices defined by the team for data platform techniques and architecture\\nBenefits\\n\\nThis is a Full time salaried role complete with the following benefits:\\nFull medical, dental, vision\\n401k matching\\nCoin + Craft yearly personal improvement budget\\nOngoing mentorship and learning opportunities\\nPaid Vacation\\nMonthly outings and events\\nVolunteer opportunities\\nYearly Retro\",\n",
       "  \"Data Analyst II - REMOTE\\nCompany Overview:\\nCoventBridge Group is the leading worldwide full-service investigation solutions company providing: Surveillance, SIU and Compliance, Claims Investigation, Counter-Fraud Programs, Desktop Investigations, Social Media, Record Retrieval, Canvasses and Vendor Management programs. With offices in the UK and U.S. the company provides top tier data privacy and security practices, deploys robust case management technology customized to clients’ needs and delivers worldwide coverage via its 1000 employees and affiliates worldwide.\\nAbout the Opportunity:\\nThe Data Analyst II will primarily be responsible for performing in-depth evaluation and analysis of potential fraud cases and requests for information using claims information and other sources of data. In addition, it will support the development of complex cases that involve high dollar amounts, sensitive issues, or that otherwise meet criteria for referral to law enforcement, recoupment of overpayment, and/or administrative action based on reactive and proactive data analysis.\\nIn assuming this position, you will be a critical contributor to meeting CoventBridge Group's objective: To provide services to our clients that exceed their expectations and contribute to improved healthcare delivery by identifying and eliminating fraud, waste and abuse.\\n\\nThe Data Analyst II position will report to the Data Supervisor and will work in our Grove City, OH office or if not local, remotely from a home office. Ideal candidates will reside in one of the following areas: Chicago, Detroit, Des Moines, Kansas City, St. Louis, Indianapolis, Columbus, Cincinnati, Louisville.\\n\\nResponsibilities:\\nWork with local management, investigators, and analysts to provide reactive and proactive case development support and to fulfill law enforcement data requests.\\nCommunicate effectively with internal and external customers, including federal law enforcement officers.\\nValidate data analysis results and analytically identify potential fraud, waste and/or abuse situations in violation of Medicare/Medicaid laws, guidelines, policies, and regulations.\\nSupport management requests for CMS reporting requirements.\\nUtilize data analysis techniques to detect aberrancies in Medicare/Medicaid claims data and proactively seeks out and develops leads and cases received from a variety of sources including CMS and OIG, fraud alerts, and referrals from government and private sources.\\nWork with Statisticians and Data Analysts to provide proactive data analysis results with statistically high probabilities of producing case referrals to law enforcement, overpayments, and/or administrative actions.\\nPrepare, develop and participate in provider, beneficiary, law enforcement, or staff training as related to Medicare fraud, waste and/or abuse data analysis.\\nMaintain chain of custody on all documents and follow all confidentiality and security guidelines.\\nComply with and maintains various documentation and other reporting requirements as needed.\\nPerform other duties as assigned.\\nRequirements:\\n2 years’ experience in data analysis as well as demonstrated knowledge of health care and claims or a combination of education and equivalent work experience.\\nDemonstrated knowledge of various database management systems in order to input, extract or manipulate information.\\nDemonstrated experience and knowledge of health care information (health claims data; specifically, Medicare and Medicaid, ICD-9-CM and ICD-10-CM codes, physician specialty codes, pharmaceutical data including NCPDP file formats and codes, provider identifiers, etc.) is preferred.\\nHave high proficiency level with MS Access and MS Excel.\\nRequires a working knowledge of SAS and/or other applications to perform various types of data analysis.\\nKnowledge of Medicare and Medicaid rules and regulation is a plus.\\nEducational Qualifications:\\nBachelor’s degree in statistics or related discipline with preference given to MA or MS recipients, and at least (2) two years of experience in data analytics.\\nAssociate must have and maintain a valid driver’s license issued by his/her state of residence.\\n\\nBenefits:\\nMedical, Dental, Vision plans\\nLife, LTD and STD paid by the employer\\n401(k) with company match up to 4%\\nPaid Time Off and company paid holidays\\nTuition assistance after 1 year of service\\n*CoventBridge is proud to be an EEO-AA employer M/F/D/V and maintains a Drug-Free Workplace.*\\nIND123\",\n",
       "  'Description\\nYou’re passionate about a zero-emissions future and want to build something special. You want to own your space, but work with talented, like-minded people on important projects. The pace doesn’t scare you; it excites you. Quality and integrity matter to you.\\n\\nWe feel exactly the same way.\\n\\nDrive Forward. Introduce yourself to us and let’s start a conversation.\\n\\nWe’re currently on the lookout for a MEA Scientist to lead fuel cell performance and durability testing. In this role, you will be responsible for cell testing, data analysis, reporting, identifying failure mechanisms and developing mitigation strategies while working to meet an aggressive development schedule.\\n\\nYou will:\\nManage and report on test programs for internal/external qualification of vendor supplied MEA components\\nSupport test stand calibration, lab safety reviews and lab maintenance projects to meet EH&S regulations\\nCreate work instructions and internal documents (material safety, lab safety and process safety) for the area\\nClosely work with MEA processing and cell assembly groups to meet test schedule\\nFocus on innovation, IP generation and publication in peer reviewed high-impact journals to maintain Nikola’s competitive edge in FC and H2 technologies\\nMaintain database of testing results and provide clear and effective oral and written reports to management\\nCollaborate and communicate with technicians, scientists, engineers, and managers through the development cycles and participate in technical meetings\\nSeek opportunities to develop expertise and maintain professional relationships, including frequent and open effective communication with internal and external technical groups\\nComply with all EH&S procedures and support management initiatives to instill a culture of safety awareness in the laboratory\\nTo be successful in this role, you’ll need at least an MS Chemical Engineering or Chemistry with minimum 5 years of industry experience. Or, you’ll have a PhD in Chemical Engineering with at least 3 years of industry experience. In addition, you have a strong fundamental understanding of cell testing and interdisciplinary areas of electrochemistry, solid state chemistry, and energy technologies such as fuel cells and batteries. This position also requires strong analytical, interpersonal and communication skills (written and verbal) with demonstrated ability to articulate complex technical concepts to a wide audience of varying technical backgrounds.\\n\\nAdditionally, you’ll need\\nHands-on experience in the operation, maintenance and repair of test cell data acquisition systems and data analysis\\nWorking knowledge of statistical uncertainty analysis as applied to test and calibration data and to the specification of test cell instrumentation will be beneficial\\nExcellent problem-solving skills, ability to identify and develop new ideas and initiatives, able to turn ideas into practical working systems and procedures\\nCreative, hands-on mentality and skilled in design of experiments, advanced statistics and analysis software tools\\nGood organizational skills and ability to independently identify and prioritize tasks to meet company objectives and milestones is a must to be successful in this position',\n",
       "  \"The Data Scientist II mines and analyzes complex data sets using advanced statistical methods for use in data driven decision making. The Data Scientist II assists in the production of research and analysis to quantify the impact of internal and external environments on portfolio performance. This position is responsible for summarizing, reporting, and providing polished presentations of findings to a variety of internal clients as well as working with other departments to achieve the overall company objectives.\\n\\nPerforms research, analysis, and modeling on organizational data\\nAssists in analyzing key metrics and performing data analysis\\nBuilds technical knowledge to support research and analytic responsibilities\\nAssists in conducting research projects, incorporate project design, data collection and analysis, summarizing findings, developing recommendations and effectively communicating to leadership the impact to the business\\nDevelops and applies algorithms or models to key business metrics with the goal of improving operations or answering business questions\\nProvides findings and analysis for use in decision making\\nEnsures that the delivered products meet the business needs of the company\\nPartners with and provide recommendations to business leadership on the appropriate application of analytics to business strategies and effectively communicate analysis and implications to senior leadership\\nPerform other duties as assigned\\nConform with all corporate policies and procedures\\n\\nSkills & Knowledge:\\nAbility to identify and understand business issues and map these issues into quantitative questions\\nEfficiently work with large datasets\\nAbility to design and implement model documentation and monitoring protocols\\nDemonstrated understanding and experience with technical systems, datasets, data warehouses, and data analysis techniques\\nComprehensive knowledge and experience with logistic regression, decision trees, random forests, and neural network techniques\\nComprehensive knowledge and experience with evaluation of programs including experimental design, A/B testing, matching, propensity score matching\\nPreferred knowledge with synthetic controls, difference-in-difference, and regression discontinuity methods\\nPreferred knowledge of operations research, linear/non-linear programming, Spark, Hadoop, and text analytics\\nExperience:\\n3-5 years as Data Scientist or similar quantitative field\\nMS Office required\\nPython, R, SQL, and SAS programming required\\nStrong written and verbal presentation skills with an ability to communicate effectively with Senior Management\\nAbility to identify and seek needed information/research skills\\nAnalytical thinking skills\\nAbility to interact collaboratively with internal and external customers\\nCapable of managing multiple and varied projects, including the ability to coordinate and balance numerous tasks in a time-sensitive environment, under pressure\\nProblem/situation analysis\\nEducation:\\nRequired: Master's Degree in Computer Science, Data Science, Statistics, Applied Mathematics, Econometrics, Operations Research or similar quantitative field\\nPreferred: Ph D in Computer Science, Data Science, Statistics, Applied Mathematics, Econometrics, Operations Research or similar quantitative field\",\n",
       "  '_1) 4-5 years of Azure project. ETL pipelines. Pull data from pipelines. Azure SQL. Data skills. Building ETL pipelines_\\n_ 2) Azure SQL Database_\\n_ 3) Azure DevOps - uploading code _\\n\\nPlay a crucial part in a small team responsible for designing, implementing, deploying and testing an ETL pipeline using Azure Cloud services capable of migrating data from enterprise grade relational & non-relational on-prem/cloud datastores into Azure SQL DB and Azure Data Warehouse deployments in an effective, secure and performance optimized manner. Be responsible for implementing an end-to-end data analytics solution capable of indexing large data-sets for performing fast yet effective text-based search queries and generating results in the form of data reports and visualization graphs.\\n\\nRequirements:\\n5+ yrs of software development background building backend applications involving integration with database systems.\\n3+ yrs of experience using public cloud based data processing, analytics and storage services like Azure Data Factory, Azure Functions/Logic Apps, Azure SQL DB, Azure Search/Cognitive services and Azure Data Lake\\nAbility to define data models, ensure data consistency across replicas and tune the performance of database queries based on application use-cases.\\nExperience with designing and implementing ETL pipelines - API/Web based data extraction and collector frameworks, job schedulers, database related transforms and load workflows.\\nHabit of taking data driven decisions, proving through actions, being open minded and receptive to ideas from others.\\nAbility to work with remote team members and collaborate with them effectively.\\nComposure to work in a challenging, fast paced, dynamic but planned environment.\\n\\nLanguage Expertise: Python, C#, Transact-SQL, PL/SQL Databases: Azure SQL DB, Microsoft SQL, DB Migration/BI Tools: Azure Data Factory, SSIS, Azure Embedded Power BI Operating Systems: MS Server, Linux\\n\\nJob Type: Contract\\n\\nExperience:\\nrelevant: 1 year (Preferred)\\nETL: 1 year (Preferred)\\nWork Location:\\nOne location',\n",
       "  'Who we are:\\nMeridian, a WellCare Company, is part of a national network of passionate leaders, achievers, and innovators dedicated to making a difference in the lives of our members, our providers and in the healthcare industry.\\n\\nWe provide government-based health plans (Medicare, Medicaid, and the Health Insurance Marketplace) in Michigan, Illinois, Indiana, and Ohio. As a part of the WellCare Family of companies, we deliver healthcare excellence to millions of members nationwide.\\n\\nOur associates work hard, play hard, and give back. Meridian associates enjoy an exceptional experience and culture including special events, company sports teams, potlucks, Bagel Fridays, and volunteer opportunities.\\nA Day in the Life of a Data Scientist:\\nThe Data Scientist is part of a team that researches and investigates business problems through quantitative analysis using healthcare cost, utilization, clinical, assessment, and external data. This position also conducts the on-going presentation of analysis results and findings to support clinical programs, healthcare business operations, performance and improvement.\\n\\nEssential Functions:\\nGather and prepare analyses based on information from internal and external sources to evaluate and demonstrate program effectiveness and efficiency (10%)\\nLeverage knowledge of programming, mathematics and computer science to transform the way our company does business\\nTranslate business opportunities into data-driven big-data and modeling solutions using statistical, econometric or data-mining tools and methods\\nAnalyze and extract data from the Enterprise Data Warehouse (EDW) to report to internal and external customers (5%)\\nWorking in big data environment with Oracle and Hadoop environment to bring actionable insights with R, SAS, Python or other statistical packages\\nResearch complex functional processes and industry standards using a variety of resources (5%)\\nEducate the organization both from IT and the business perspectives on new analytic methods and technologies\\nDevelop and prepare highly complex reports and provide and/or interpret information and data across divisions and departments (15%)\\nCommunicate complex analysis clearly to all audience through use of data visualization tool such as Tableau\\nGain an understanding of how data integrates across multiple transaction systems to create complex reporting solutions for actionable insights\\nAssume responsibility for data integrity among various internal groups and/or between internal and external sources (5%)\\nDesign integrated and clinically-based engineering, management, medical and environmental solutions aiming at providing person-centered care to members, households and their respective communities with emphasis on health, social, economic and environmental sustainability (15%)\\nUsing statistical methods such as linear models (linear regression, generalized linear regression, logistic regression) and nonlinear modeling techniques to build predictive models to help drive better member engagement and impactability\\nLead the research and development of predictive models and capabilities using machine learning and artificial intelligence\\nSegment target population using supervised (classification) and unsupervised (clustering) learning methods to identify most impactful members\\nConduct economic/outcomes analyses to quantify savings obtained from integrated solutions.(10%)\\nEvaluate return of investment (ROI), utilization and compare them with pre-defined goals to measure effectiveness of program\\nDevelop meaningful data visualizations in form of dashboards to communicate results and empower senior executives’ decision making process\\nDevelop systematic and scalable reporting processes and procedures to ensure timely delivery of weekly, monthly, quarterly, annual and ad hoc reporting to management (10%)\\nDevelop and improve algorithms like risk stratification methodology, pregnancy identification, etc. that is applied to all member on a monthly basis\\nRecommend and implement new or modified reporting methods and procedures to improve report content and completeness of information (15%)\\nAnalyze benchmarking data, reports, processes, and measurements for key managed care indicators to recognize trends and assist in developing population health initiatives\\nServe as a mentor to junior staff (5%)\\nPerform all other related duties as assigned (5%)\\n\\nJob Requirements:\\nEducation:\\nMaster’s Degree or higher in Data Science, Biostatistics/Statistics, Econometrics or related field\\nExperience:\\n5+ years of experience in advanced analytics experience, machine learning, statistics, or econometrics role and who has solid analytical skills using, SAS, R, or Python\\nAdvanced knowledge of statistical and mathematical principles\\nExperience in Regression Analysis, Predictive Modeling, Data Mining, Machine Learning and Forecasting\\nExperience in Propensity Modeling for control group and treatment group testing\\nExperience in designing research experiments and measuring their effectiveness.\\nExperience with healthcare claims data\\nExperience with peer-reviewed publications\\nExperience with Six Sigma, Triple Aim and/or other relevant Project Management tools\\nStrong SQL Scripting, Statistical Programming, Data Visualization and Communication skills\\nAbility to interpret and communicate statistical results to a non-technical audience\\nWorking knowledge of Predictive Modeling and Grouper tools such as John Hopkins ACG System , Optum Symmetry, or related population health platform system(s)\\nKnowledge:\\nMust be familiar with common database architectures and understand how to extract necessary data\\nMust have strong proficiency in MS Excel (pivot tables, macros, lookups, etc.) and common analytic tools such as SAS, SQL, SPSS (writing queries, troubleshooting queries, extracting data, etc.)\\nKnowledge about Medicare, Medicaid and Commercial Managed Care preferred\\nWorking knowledge of Healthcare Effectiveness Data Information Set (HEDIS) specifications and CAHPS survey requirements\\nKnowledge of health care quality improvement processes and documentation\\nSkills/Tools:\\nAt least 2 years of experience with SQL Server or Oracle\\nAt least 2 years of experience with SAS or R\\nAt least 2 years of experience with MS Excel\\nAt least 2 years of experience with Data Visualization in Excel, R or Tableau\\nExperience with Oracle BI Answers a plus, not required\\nExperience with MS Access a plus, not required\\nExcellent written, verbal, presentation skills and inter-personal skills\\nStrong clinical analytical and data interpretation skills\\nStrong project management skills required; must be very organized and detail oriented\\nStrong written and oral communication skills required; must be able to interact with all levels within an organization\\nAbilities:\\nMust be able to execute complex clinical analyses using sound epidemiological concepts and methodologies\\nMust be able to produce requested analyses on a production schedule\\nAbility to present analytical findings to leadership to enable data-driven decisions\\nMust be able to interact with a variety of clinical operations SMEs to organize analytic plans and requirements for clinical analytic requests\\nAbility to work in a collaborative, virtual team environment and analyze problems using a team approach\\nMust be a team player and someone with ability to work with minimal guidance in fast-paced work environment.\\nAbility to influence and manage a diverse set of stakeholders (clinical and nonclinical), both internally and externally\\nAbility to think creatively to resolve common barriers and issues\\nAbility to deal with ambiguity of ad hoc reporting requests, ask questions to seek clarification, and deliver results\\n#MSTR',\n",
       "  \"While others are taking a technology-first approach, Ford AV LLC is putting people first. We are building a self-driving business ecosystem that is purpose-designed from the outset. Ford's AV strategy is different because we are designing, testing and operating an ecosystem today that is fit for the purpose of our AV business model in the future.\\n\\nAre you passionate about transportation-as-a-service (TaaS) served by Autonomous Vehicles? Are you motivate to apply your software skills to build the self-driving ecosystem? Our team has been tasked to develop scalable building block platforms based on mobility opportunities. Does it sound exciting? Then, join our team as a Software Engineer.\\n\\nOur focused on delivering software leveraging eXtreme Programming and cloud technologies. In this environment the Software Engineer is expected to work in a pair developing working, tested code based on proven Lean/Agile methods. Engineers on the team work across the full stack of technologies to enable the highest priority work to be delivered.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nAs a Software Engineer you are going to:Write production-quality code to deploy these solutions on the Transportation-as-a-Service solutions\\nMonitor and evaluate the performance of our solutions\\nSupport and develop code to enhance testing platform\\nCollaborate with teams working on autonomous vehicles software solutions\\n\\nBasic qualifications needed:Bachelor's Degree in Computer Science or similar technical discipline\\n2+ years of work experience in Object Oriented development in at least one of the following web or mobile technologies:Spring and/or Rails frameworks OR\\nJava/J2ee / Ruby / Python / Golang\\n\\n2+ years of experience with SQL.\\n\\nPreferred desirable qualifications:Exceptional software engineering knowledge; OO Design Principles\\nFamiliar with eXtreme Programming (XP) practices including:Pair/Mob programming\\nTest-first/Test Driven Development (TDD)\\n\\nExperience with Spring Cloud and deploying to cloud platforms, preferably Pivotal Cloud Foundry or Cloud Foundry\\nExperience with Spring, RESTFUL/SOAP Web services development.\\nFamiliarity with GitHub or equivalent source control repositories and Build Tools like Gradle\\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\\nCapable in Continuous Integration/Continuous Delivery tools and pipelines such as Jenkins, Maven, Gradle, etc.\\nExperience with JAVA development using Eclipse or similar tooling\\nStrong analytical and problem-solving skills\\nStrong oral and written communication skills\\nAbility to multi-task and manage changing priorities\\nStrong teamwork and interpersonal skills\\nAbility to work independently and take initiative when solving unexpected problems\\nExperience in development of microservices.\\nExperience with Continuous Integration/Continuous Delivery tools and pipelines\\nExperience understanding/deploying to cloud environments - PCF/ Azure etc\\nExperience with CA Agile Central (Rally), backlogs, iterations, user stories, or similar Agile Tools.\\nExperience with Lean Agile methodology.\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for the win.\\n\\nVisa may be available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  \"GoAhead Solutions has a Great opportunity in Detroit, MI for a Quality Data Analyst. This would be a Six month contract to start out with extension.Job Description:\\n\\nQuality Data Analyst will be working within our Healthcare company client to design, deploy, configure and manage enterprise data solutions.\\n\\nRequired skills and experience:\\n\\n-Minimum of 5 years experience working in Information Technology data quality or advanced analytics.\\n\\n-At least 5 years experience writing and executing SQL and various forms of queries to do data analysis.\\n\\n-Demonstrate 3 years minimum experience working developing and implementing reference data management applications\\n\\n-At least 3 years experience working with complex master data management(MDM) tools and concepts such as entity management, and relationship management.\\n\\n-Working knowledge of developing and integrating applications in a big data platform and a distributed\\n\\n-Experience working within a Healthcare domain and working with Master Data Management(MDM) is required.\\n\\nPreferred Skills/Experience Additional skill requirements include:\\n\\n-2+ years of supporting project's using Agile delivery.\\n\\n-2+ years experience working as data analyst in healthcare industry.\\n\\n-Must be a critical thinker capable of analyzing issues and creating potential solutions on one s own with limited guidance.\\n\\n-Must be able to communicate effectively using Microsoft Office products (EG. Outlook, Word, PowerPoint, and Excel).\\n\\n-Demonstrates the ability to lead work on complex enterprise-wide project/issues/enhancements\\n\\n-Working experience or familiarity with Ataccama software is desired\\n\\n- provided by Dice\",\n",
       "  \"As a Salesforce Engineer at StockX, you will use your Salesforce Engineering experience to lead the development of a customized CRM Salesforce solution, enabling our other internal teams to work smarter, better, and faster. You will not only be empowered and encouraged to take ownership of technology and implementations but as an early engineer, play a pivotal role in establishing a successful Engineering team culture and processes at a fast-growing company.\\n\\nThis is a great opportunity to leverage your existing skills, to build a world-class team and to have a huge impact on how marketplaces can be redefined. At StockX, we are just getting started.\\n\\nResponsibilities\\nLead development of the StockX integration with the Salesforce platform.\\nDesign and implement technical solutions on the Salesforce platform using the Apex programming language and the Lightning Platform.\\nDesign and implement integrations between StockX APIs and the Salesforce platform.\\nWork cross-functionally with other Engineering teams to resolve outages.\\nAutomate monitoring and alerting mechanisms to identify outages early.\\nWrite automated Unit and Integration tests.\\nWork with other StockX internal teams, including Product and Customer Service, to define future features and identify new initiatives.\\nParticipate in Agile ceremonies such as stand-ups, planning, grooming, etc.\\nStay up-to-date with any technology changes including changes to the Salesforce development language, tools, or platform.\\nPromote the use of Salesforce out-of-the-box solutions and know when to implement a custom solution.\\nRequirements\\n3+ years' Software Engineering work experience, 2+ years' Salesforce application development experience.\\nExperience with the Salesforce Developer tools, specifically the Apex language and the Lightning Platform.\\nExperience with developing and administrating projects on Salesforce platform.\\nExperience with the Salesforce Development Life Cycle as well as Salesforce Configurations/Customizations User Interface, Page Layouts, Tabs, Custom fields, Custom objects, Validation Rules, Triggers.\\nExperience with Sales Cloud, Service, Cloud, and Community.\\nWorking knowledge in Generating Reports, Dashboards, customized reports and analyzing the data in Salesforce.\\nExperience with JavaScript (Node.js).\\nExperience in service oriented and/or microservice architectures.\\nFamiliarity with different types of data storage and relational/non-relational databases.\\nAbility to write automated Unit and Integrations tests.\\nAbility to navigate through ambiguity, prioritize work, and have the ability to adapt to quickly changing priorities.\\nExperience working in an Agile setting, preferably using Agile project management tools such as Jira.\\nExperience working with monitoring and logging tools, preferably New Relic.\\nExperience with version control.\\nAbility to document new features, enhancements, and bug fixes clearly.\\nAbility to explain complex technical terms to non-technical users.\\nNice To Haves\\nEither Salesforce Admin certified or ability to complete certification within 2 months of starting.\\nExperience working with AWS or other cloud providers.\",\n",
       "  \"As an iOS Engineer at StockX, you'll not only be empowered and encouraged to take ownership of technology and implementation. As an engineer on the iOS team, you'll play a pivotal role in establishing a successful engineering culture and processes at a fast-growing company disrupting the e-commerce space for buying and reselling sneakers, watches, handbags and more to come!\\n\\nWhat You'll Be Working On\\nWork on a fast-paced team building new features on the StockX iOS app.\\nWork on projects from start to finish in a fast-paced, collaborative environment.\\nStay up-to-date with any technology changes including changes to the language (Swift), tools (Xcode, Mac OS, Instruments, etc.), platform (iOS), and devices.\\nImplement user-friendly interfaces using Swift.\\nAdhere to any coding and design standards, including code reviews.\\nSkills You Have\\n2+ years of thorough experience in iOS development, preferably with tools like Swift, Xcode.\\nExperience with version control.\\nAbility to understand tasks quickly and adapt to an ever-changing environment.\\nAbility to explain complex technical terms to non-technical users.\",\n",
       "  \"Our Software Engineers build products on a variety of platforms and digital mediums, including web and mobile apps, wearables, smart cars, AR/VR and connected home. As a Software Engineer based out of the Detroit area, you’ll be exposed to the entire product lifecycle, from idea generation, design, prototyping, planning, execution, and ultimately shipping the final product to market. We’re building a team that’s passionate about innovation, apprenticeship (learning by pairing), and building the best-connected experiences. We’re looking for a smart, kind, and reliable Software Engineer who is eager to make a big impact and build amazing products used by millions of users. If you’re ready to grab this amazing opportunity and help us build the future, we want to work with you!\\n\\nConnected is a rapidly growing product development firm headquartered in Toronto, with locations in Detroit and the Bay Area. We partner with the world’s leading organizations to build software-powered products that are enjoyed by millions of users globally. You likely have our clients' apps on your home screen, their smart products in your home, or wear their devices every day to stay connected. Connected is one of Canada’s Top Small and Medium Employers two years running and is one of LinkedIn’s Top Startups.\\n\\nQualifications:\\nSoftware development experience in one or a variety of programming languages and frameworks (Java, Node.js, ReactJS, React Native, Objective-C, Swift, C/C++, Python, PHP/Hack, Ruby, Go, NDK, Kotlin)\\nExperience working directly with clients, leading project teams, and mentoring engineers\\nSolid understanding of programming and computer science fundamentals\\nStrong interpersonal skills with the ability to collaborate across the team and work closely with clients\\nDemonstrated experience building consumer-facing products\\nStrong communication skills and an interest in a pair-programming environment where you will accelerate your knowledge and skills quickly\\nExperience working in a Continuous Integration and Delivery model\\nExperience and/or interest in Test Driven Development (TDD) and agile methodologies\\nExperience analyzing, designing, and developing large and complex code bases and software systems, including API design techniques, to improve the overall design and keep them clean and maintainable\\nA tenacious, entrepreneurial attitude with the drive to deliver high-quality products on time\\nAbility to travel as required by client and engagement needs\\nResponsibilities:\\nBuilding impactful products for Fortune 500 clients\\nCreating simple, clean code to power new user-facing product features\\nMaintaining a high level of performance quality and having a reputation for shipping quality product\\nCollaborating, teaching and learning with fellow Connected engineers, designers, product managers, and our clients\\nBeing reliable, taking ownership and making a big impact\\nProducing repeatable and predictable delivery of features\\nRefactoring and improving existing code when it gets brittle\\nEnsuring code is written to standards, has sufficient test coverage, and adheres to design requirements\\nDefining product and technical solution architecture\\nHelping us achieve our mission of building better products\\nContributing to growing Connected’s new location in Detroit\\nPerks:\\nWe’re proudly employee-owned, with stock option grants for all full-time permanent employees\\n4 weeks vacation\\n$1,000 annual Education Credit for conferences, courses, workshops, and textbooks\\n$200 annual Fitness Credit for a gym membership, running shoes, or classes\\nOur quarterly “Disconnect” events are a chance to get out of the office and unwind as a team\\nApply Today\\n\\nConnected welcomes applications from all candidates. We especially encourage candidates to apply who are Women, Indigenous, LGBTQ+, People of Colour, candidates in caregiving roles, immigrants, and persons living with disabilities. We are an equal opportunity employer who recognizes the value of every individual’s contribution to the success of our team, clients, employees and community. We are committed to equal employment opportunities in our recruiting, hiring, employee development and promotion practices.\\n\\nIf you have a disability or special need that requires accommodation, please let us know by emailing hr@connected.io or calling 647-478-7493. Please provide your name, preferred contact method, and a detailed description of the nature of any accommodation that you may require. Please include any materials or processes that may be used to ensure your equal participation.\",\n",
       "  'Position :Big Data Engineer\\n\\nJob Duties :\\n\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.Strong analytic skills related to working with unstructured datasets.Build processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Implement the use case with Bigdata open source tools and technologies like SQOOP, HADOOP Ecosystems, HIVE, Unix Scripting, SPARK, SCALA, PYTHON, PYSPARK and SCALA to achieve the requirements. Ability to utilize the bigdata tools and techniques to integrate traditional databases like Oracle, SQL Server and Mysql into Hadoop Datalake. Create the Hive databases and store the processed data into Hive tables and then perform the aggregate operations to generate the new datasets and analyze the data. Develop an Enterprise Data Lake Application using Bigdata tools like Hive, Spark Streaming, Kafka, Sqoop, Oozie, Spark, Shell Scripting, Python, SQL, HDFS, Streamsets and Tidal.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Provide automation solutions for the jobs which are run manually to generate frequent reports which are utilized by the business team for data analysis.\\n\\nWork Locations :\\n\\nVarious unanticipated work locations throughout the United States; relocation may be required. Must be willing to relocate.\\n\\nMinimum Qualifications Education :\\n\\nBachelor – Computer Science\\n\\nExperience : None\\n\\nJob Order No : 9534132\\n\\n***This position is eligible for a referral bonus through our employee referral program***\\n\\nPosition :Big Data Engineer\\n\\nJob Duties :\\n\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management. A successful history of manipulating, processing and extracting value from large disconnected datasets. Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores. Strong project management and organizational skills. Experience supporting and working with cross-functional teams in a dynamic environment. Implement the use case with Bigdata open source tools and technologies like SQOOP, HADOOP Ecosystems, HIVE, Unix Scripting, SPARK, SCALA, PYTHON, PYSPARK and SCALA to achieve the requirements. Ability to utilize the bigdata tools and techniques to integrate traditional databases like Oracle, SQL Server and Mysql into Hadoop Datalake. Create the Hive databases and store the processed data into Hive tables and then perform the aggregate operations to generate the new datasets and analyze the data. Develop an Enterprise Data Lake Application using Bigdata tools like Hive, Spark Streaming, Kafka, Sqoop, Oozie, Spark, Shell Scripting, Python, SQL, HDFS, Streamsets and Tidal.Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies. Provide automation solutions for the jobs which are run manually to generate frequent reports which are utilized by the business team for data analysis.\\n\\nWork Locations :\\n\\nVarious unanticipated work locations throughout the United States; relocation may be required. Must be willing to relocate.\\n\\nMinimum Qualifications Education :\\n\\nBachelor – Electronics Engineering or Telecommunications\\n\\nExperience : None\\n\\nJob Order No : 8753175\\n\\n***This position is eligible for a referral bonus through our employee referral program***\\n\\nPosition :\\n\\nBig Data Engineer\\n\\nJob Duties :\\n\\nResponsible for designing and building big data platforms, tools, and solutions that help manage, secure, and generate value from data. Mostly involved in creating scalable and reusable solutions for gathering, collecting, storing, processing, and serving data at scales. On a day-to-day basis his responsibilities include:Gathering requirements for the business use case and design an implementation plan for the application.Provide different prototypes for the Business use case using Bigdata tools and let client’s Analysts team to confirm on best model for Data Analysis.Implement the use case with Bigdata open source tools and technologies like SQOOP, HADOOP, HIVE, Unix Scripting, SPARK, SCALA, PYTHON, PYSPARK, SCALA, AWS and EMR to achieve the requirements.Create the Hive databases and store the processed data into Hive tables and then perform the aggregate operations to generate the new datasets and analyze the data.Provide Big Data Solutions in Cloudera Environment and act as a technical expert addressing problems related to system and application design, Data Lake, Data warehouse, performance optimization, code integration, Automation workflows, etc.Develop an Enterprise Data Lake Application using Bigdata tools like Hive, Spark Streaming, Kafka, Flume, Sqoop, Oozie, Spark, Shell Scripting, Python, SQL, HDFS, Streamsets.Develop the automation scripts for data visualization reports out of processed data to present the research for the end clients.Integrate and deploy the code into lower environments to check for the merge conflicts of the code.Involve in research on the latest open source big data tools to implement the client’s requirements at low cost and better performance.Implementing the code and integrating the code into the production servers.Contribute for knowledge share and transfer with the team members for better outputs and best implementation solutions.Provide automation solutions for the jobs which are run manually to generate frequent reports which are utilized by the business team for data analysis.\\n\\nWork Locations :\\n\\nVarious unanticipated work locations throughout the United States; relocation may be required. Must be willing to relocate.\\n\\nMinimum Qualifications Education :\\n\\nBachelor – Electronics Engineering or Telecommunications\\n\\nExperience : None\\n\\nJob Order No : 8746283\\n\\n***This position is eligible for a referral bonus through our employee referral program***',\n",
       "  'The Data Analyst focuses on working with business and IT teams to provide data insights and recommend solutions to deliver business value. They analyze large and complex transactional data and derive meaningful and actionable insights to help gain efficiency by process and systems improvement. The Data Analyst researches new data sources, dives deep into existing data, discovers patterns, provides recommendations and impacts assessments to enable data-driven decision making. They develop deep partnerships with business areas within the company and the Rock Family of Companies.\\n\\nResponsibilities\\nDevelop a thorough understanding of business processes and sub-processes\\nCondense large and complex data sets into clear, concise, easy-to-understand observations\\nConduct profiling on large volumes of data to look at patterns, quality and trending\\nApply descriptive and inferential statistical methods to perform analyses on internal and external data and processes to answer specific business questions, and identify points of interest and ways to improve the process based on those results\\nUnderstand and apply tools for gathering data and performing data analyses\\nAct as a liaison between teams to gather and communicate essential information for process improvement\\nFacilitate preliminary analysis for model development and support model implementation and maintenance\\nRequirements\\nMasters degree in statistics, applied statistics, mathematics, econometrics or data science or equivalent work experience\\nExperience with statistical tools such as R, SAS and SPSS\\nStrong understanding of statistical methods used in descriptive and inferential statistics\\nSelf-motivation and the ability to execute projects with speed and quality\\nAbility to identify business problems, find opportunities and offer solutions by applying advanced statistical analyses\\nAbility to clearly articulate and effectively write findings and solution recommendations\\nStrong written and verbal communication skills\\nExcellent problem-solving and analytical skills\\nAbility to seek feedback and a willingness to act on feedback\\nPositive attitude and an openness to change\\nWhatll Make You Special\\nExperience writing SQL queries with SQL Server, Oracle or similar\\nExperience with data manipulation tools such as Excel or Minitab\\nWho We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past 9 consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for 6 consecutive years, 2014 through 2019, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last 5 years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top 30 for the past 15 years.',\n",
       "  'POSITION PURPOSE\\nServes as a business intelligence consultant in analyzing data, particularly large sets of data, in order to assist Trinity Health with Population Health, Value Management and other strategic initiatives. Contributes and participates in projects with Data Governance, Data Warehouse and Advanced Analytics teams. Assists Trinity Health customers with various financial and clinical analyses and research, along with participating in various special projects. Develops an analytical skill set to solve multiple business problems, creating and implementing data driven strategies to support Population Health, Value Management and other key initiatives for Trinity Health. Position incumbents are expected to be comfortable working in the capacity of a software engineer and a quantitative researcher.\\n\\nESSENTIAL FUNCTIONS AND RESPONSIBILITIES\\nApplies statistical techniques and quantitative methods in reviewing Trinity Health data, to develop scorecards, population health analyses, benchmarking studies, and other clinical and financial analyses. Utilizes regression analysis, decision trees, segmentation, and simulation in support of statistical analysis. Utilizes quantitative techniques and models to identify improvement opportunities and quality measures. Participates in System-wide complex projects and lead small projects as needed. Conducts benchmarking studies and other research and provides value added analyses and summaries. Assists in performing clinical analyses including quality measures, improvement of the care experience, etc. Assists in development of strategic and tactical conclusions from data review and analyses in order to make recommendations to senior leadership in an easy to understand communication format. Assists in the research of existing methods and new approaches for statistical and computational analysis and modeling of clinical and financial data. Participates in leveraging selection procedures to distill through large datasets in order to identify crucial pieces of information. Collaborates with members of various TIS teams, including Advanced Analytics, Data Warehouse Data Governance, and Clinical etc., to provide them with the results of data analyses. Executes recurring data mining and process activities that provide the foundation for key analytic capabilities. Assists in the translation of data mining activities into actionable business information. Participates in the development and implementation of sophisticated financial models. Builds strong relationships internally and collaborate effectively on cross-functional teams. Develops a trusted relationship with customer contacts through effective communication and efficient, quality execution of analyses and projects. Maintains a working knowledge of applicable Federal, State and local laws/regulations; the Trinity Health Integrity and Compliance Program and Code of Conduct; as well as other policies and procedures in order to ensure adherence in a manner that reflects honest, ethical and professional behavior.\\n\\nMINIMUM QUALIFICATIONS\\nBachelors degree in Computer Science, Engineering, Statistics or other technical concentration or an equivalent combination of education and experience. Advanced degree in a relevant technical field preferred. Three (3) to five (5) years of experience in a relevant technical, analytical or statistical role. Some experience solving analytical problems using quantitative approaches. Familiarity manipulating and analyzing high-volume, high-dimensionality data from varying sources. Strong passion for empirical research and for answering hard questions with data. Ability to provide a flexible analytic approach that allows for results at varying levels of precision. Ability to communicate complex quantitative analysis in a clear, precise, and actionable manner, in a way that can be understood by customers. Familiarity with relational databases and SQL. Knowledge of an analysis tool such as R, Matlab, or SAS is desired. Experience working with large data sets, and/or experience working with distributed computing tools a plus (Map/Reduce, Hadoop, Hive, etc.) is desired. Excellent Microsoft Office skills, particularly in Excel, well organized and detail oriented. Strong written and verbal communication skills, ability to respond to all communications effectively and in a timely manner. Strong analytical and integrative skills including ability to formulate conclusions and identify trends from data in a logical and systematic manner. Ability to ask the right questions and seek help where appropriate. Flexibility and ability to diagnose and resolve issues; strong client service orientation. Leadership qualities preferred. Ability to work both independently and on client teams and enjoy a fast-paced environment. Self-starter and interest in continually challenging oneself and willingness to step outside of ones comfort zone. Interest and ability to think beyond the task at hand and understand how ones work fits into the broader landscape.\\n\\nPHYSICAL AND MENTAL REQUIREMENTS AND WORKING CONDITIONS\\nPosition operates in a typical office environment. The area is well-lit, temperature-controlled and free from hazards. Incumbent communicates frequently, in person and over the telephone, with people in a number of different locations on technical issues. Manual dexterity is needed in order to operate a keyboard. Hearing is needed for extensive telephone and in person communications. The environment in which the incumbent will work requires the ability to concentrate, meet deadlines, work on several projects at the same period and adapt to interruptions. The incumbent must be capable of traveling in the course of completing project assignments. Must be able to set and organize own work priorities, and adapt to them as they change frequently. Must be able to travel to the various Trinity Health sites as needed (may or may not apply).',\n",
       "  \"Revature is the fastest growing employer of emerging technology talent in the US and we are currently looking to hire over 100 new Software Engineers.\\n\\nOur Software Engineers design, analyze and build next-gen software systems, including business applications, games, computer applications, middleware, and network control systems across a variety of industries, including finance, insurance, retail, healthcare and government.\\n\\nRevature has been featured in the Wall Street Journal, Money, Time, on MSN, and was recently named as one of the 8 Cool Companies to Apply to With Awesome Benefits by Glassdoor.\\n\\nJoin us and be part of the next generation of Software Engineers. Interviews are starting now!\\n\\nWhat We Are Looking For:\\n0-3 years experience\\nSolid foundational knowledge of SQL\\nA natural problem solver\\nStrong communication and interpersonal skills\\nAbility to relocate\\nEligible to work in the US\\nRevature is not currently sponsoring work visas or transfers at this time.\\n\\nWhat We Offer:\\nCompetitive Salary\\nRelocation Assistance\\nCorporate Housing\\nHealth, Vision and Dental Insurance\\nPaid Time Off\\nEnterprise level development training\\nLife Insurance\\n401K\\nMentoring and on-going support throughout your entire Revature career\\nExperience with one of the world's largest and most reputable companies in the US\\nSuitable candidates are encouraged to apply immediately\\n\\nNot Mentioned\",\n",
       "  \"Position Overview/Description:\\n\\nFord Global Data Insight and Analytics (GDI&A) is a growing team that islooking foraData Scientist to jointheSmart Mobility Analytics team.We are looking for a high-potentialanalystto assistin all phases of analysis. This is an exciting role that will interface with a wide variety of teams both within GDI&A and across the enterprise. This role with offer a broad spectrum of opportunities and willrequirea successful candidate to havestrengths inboth strategy and analytics.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nResponsibilities:\\nStrategic Thinking: Able to influence the strategic direction of the company by identifying opportunities in large, rich data sets and creating and implementing data driven strategies that fuel growth including cost savings, revenue and profit\\nModeling: Experience with creating ETL processes to source and link data, feature design and selection design and implement statistical / predictive models using edge algorithms on diverse sources of data and testing and validation of models.\\nAnalytics: Utilize analytical applications like R, Python, SAS, Alteryx, Matlab to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical and machine learningfindings into business strategies.\\nVisualization: Create visualizations to connect disparate data, find patterns and tell engaging stories. This includes both scientific visualization as well as geographic (GIS) analysisusing applications such as Tableau and Qlik, Power BI or ArcGIS.\\nCommunications and Project Management: Capable of turning dry analysis into an exciting story that influences the direction of the business and communicating with diverse teams to take a project from start to finish. Collaborate with product teams to develop and support our internal data platform and to support ongoing analyses.\\nEngage inexciting and constantly evolving projects that will help to form the future of Ford Motor Company.\\n\\nBasic Qualifications:\\nBachelor's Degree\\n1+ years of experience with SAS, SPSS, Python, R, Java, MatLab, ArcGIS or other analytic /developmentplatforms.\\n\\nPreferred Qualifications:\\nMBA or Master's Degree in a science-based program with an emphasis on data analysis\\nFamiliarity with big data and machine learningtools and platforms.\\nProject management experience and strong leadership skills\\nComfortable working in a dynamic environment where problems are not always well-defined\\nInquisitive, proactive, and interested in learning new tools and techniques\\nStrong oral, written and interpersonal communication skills\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nVisa sponsorship may be available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  'The Senior Big Data Engineer is responsible for the full life cycle of the back-end development of a data platform. This team member creates new data pipelines, database architectures and ETL processes, and they observe and suggest what the go-to methodology should be. This person gathers requirements, performs vendor and product evaluations, delivers solutions, conducts trainings and maintains documentation. They also handle the design and development, tuning, deployment and maintenance of information, advanced data analytics and physical data persistence technologies.\\n\\nThis team member establishes analytic environments required for structured, semi-structured and unstructured data, and they implement the business requirements and business processes, build ETL configurations, create pipelines for the data lake and data warehouse, research new technologies and build proofs of concept around them. They carry out monitoring, tuning and database performance analysis and perform the design and extension of data marts, meta data and data models. This team member also ensures all data platform architecture code is maintained in a version control system.\\n\\nThe Senior Big Data Engineer is responsible for sharing knowledge with fellow team members, allowing the entire team to grow and become proficient to further build out and enhance the data platform.\\n\\nResponsibilities\\n\\nFocus on scalability, performance, service robustness and cost trade-offs\\nDesign and implement high-volume data ingestion and streaming pipelines using Apache Kafka and Apache Spark\\nCreate prototypes and proofs of concept for iterative development\\nLearn new technologies and apply the knowledge in production systems\\nDevelop ETL processes to populate a data lake with large data sets from a variety of sources\\nCreate MapReduce programs in Java and leverage tools like AWS Athena, AWS Glue and Hive to transform and query large data sets\\nMonitor and troubleshoot performance issues on the enterprise data pipelines and the data lake\\nFollow the design principles and best practices defined by the team for data platform techniques and architecture\\n\\nRequirements\\n\\nBachelors degree in computer science or equivalent experience\\n2 years of experience with big data tools: Hadoop, Spark, Kafka, NiFi, Hive and/or Sqoop\\n2 years of experience with AWS cloud services: EC2, S3, EMR, RDS, Redshift, Athena and/or Glue\\n2 years of experience with stream-processing systems: Spark-Streaming, Kafka Streams and/or Flink\\n3 years of experience with object-oriented/object function scripting languages: Java (preferred), Python and/or Scala\\n2 years of experience with relational SQL and NoSQL databases like MySQL, Postgres, Cassandra and Elasticsearch\\n2 years of experience working in a Linux environment\\nExpertise in designing/developing platform components like caching, messaging, event processing, automation, transformation and tooling frameworks\\nDemonstrated ability to performance-tune MapReduce jobs\\nStrong analytical and research skills\\nDemonstrated ability to work independently as well as with a team\\nAbility to troubleshoot problems and quickly resolve issues\\nStrong communication skills\\n\\nWhatll Make You Special\\n\\nExperience with managing real estate data\\nExperience leading a team of engineers on a large enterprise data platform build\\n\\nWho We Are\\n\\nRocket Homes Real Estate LLC is a Detroit-based, tech-driven company with a passion for simplifying real estate. Our mission is to create a seamless home buying and selling experience by combining the process of searching for homes, connecting with a trusted real estate agent and getting a mortgage. Since 2006, weve partnered with our sister company, Rocket Mortgage® by Quicken Loans, and our nationwide network of top-rated real estate agents to help over 500,000 clients with their real estate needs.',\n",
       "  \"Operations Analytics (OA) team within Ford's Global Data, Insights, and Analytics (GDI&A) organization is looking for a highly skilledquantitative analyst / data scientist to assist in model development, evaluation, and deployment. You will have the opportunity to work with some of the brightest global subject matter experts that are transforming the automobile industry.\\n\\nTechnical Excellence is a cross-functional research team within the OA group at Ford GDI&A that focuses on applying advanced quantitative methods tosolve a wide variety of challenging business problems. We focus on Computer Vision, Machine Learning, Optimization, Diagnostics/Prognostics, and IoT technologies to improve efficiency of Ford's business and reduce cost.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nResponsibilities\\nResearch and apply the latest quantitative techniques toward the solution of important business problems\\nReview academic / industry literature, develop and test models, and gain insights from research findings\\nPlay a key role in developing analytical solutions with the business engagement teams and business partners\\nIdentify new and novel data sources and explore their potential use in developing actionable business insights\\nTranslate ideas and theory into solutions the business can use to grow\\nHelp maintain and enhance existing models\\nProvide training courses / workshops on modeling skills, such as machine learning, optimization, statistics, simulation, etc.\\nEnforce quality assurance of analytic solutions through technical governance process\\nHave the intellectual curiosity to identify new modeling technologies/methodologies/software packages to improve the current modeling processes\\n\\nBasic Qualifications\\nMaster's degree in a quantitative field such as Operations Research, Computer Science, Engineering, Statistics, Mathematics, Physics, etc.\\n1+ years' of experience doing quantitative research using techniques from one or more of the following areas: 1) machine / deep learning, 2) statistics / statistical modeling, 3) optimization / mathematical programming\\n1+ years' of experience in data manipulation using software tools such as SQL or Alteryx\\n1+ years' of experience in at least two of the following languages: R, Python, MATLAB, Java, Scala, Julia, C/C++/C#\\n\\nPreferred QualificationsPh.D. degree in a quantitative field such as Operations Research, Computer Science, Engineering, Statistics, Mathematics, Physics, etc.\\n3+ years' of experience doing quantitative research using techniques from one or more of the following areas: 1) machine / deep learning, 2) statistics / statistical modeling, 3) optimization / mathematical programming\\n3+ years' of experience in data manipulation using software tools such as SQL or Alteryx\\n3+ years' of experience in at least two of the following languages: R, Python, MATLAB, Java, Scala, Julia, C/C++/C#\\nExperience in automotive and operations analytics\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nVisa sponsorship may be available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  \"Job Description & Qualifications\\n\\nPosition Overview/Description\\n\\nFord's Global Data, Insights and Analytics team is seeking a Systems and Simulation Data Scientist. Our team is charged with digitally transforming the business that has evolved over the last 116 years into a highly complex system. Given this level of complexity, our task is uniquely challenging. Optimizing any individual piece of the system often leads to sub-optimizing the overall system.\\n\\nThe Systems and Simulation Data Scientist role is critical to analytically connect systems that were historically thought of as individual silos as well as building out simulations to understand the cause and effect of proposed solutions. You will have the opportunity to work with some of the best and brightest in the field and be at the forefront of the data science movement that is transforming the automotive industry.\\n\\nThe Systems and Simulation Data Scientist position is responsible for:\\nSupporting Material Planning & Logistics, Order to Delivery, and Complexity teams with insights gained from analytics and modeling\\nAcquiring deep understanding of business processes and translate them into appropriate mathematical representations, system documentation such as stock/flow and causal loop diagrams\\nSupporting various analytic teams to aid in an enterprise view of problem formulation and solution design\\nAligning developed analytic solutions in the context of overall system to better understand effects and trade-offs\\nDesigning simulation interfaces for users to interact with solutions and perform what-if analysis\\nSupporting the development and delivery of analytic models using skills such as data acquisition and management, algorithm design, and model development & refinement\\nInterpreting results and communicating them to technical and non-technical audiences, cross-functional teams and executive leadership\\nWorking with business on change management, including new process implementations\\nBasic Qualifications\\nMaster's degree in Data Science, Systems Dynamics, Engineering, Mathematics, Operations Research, or related fields\\n2+ years of experience with mapping complex systems\\n2+ years of experience building and deploying analytic solutions (modelling, simulation, visualizations)\\nPreferred Qualifications\\nPhD degree with a strong background in system dynamics\\n3+ years of experience with mathematical programming, optimization techniques, data mining, or statistical analysis\\n3+ years of experience utilizing simulation software packages (SimPy, Anylogic, Arena, VisualSim, etc.)\\n3+ years of experience with data gathering skills/tools (SQL, Hadoop, Teradata, SAS, Alteryx, etc.)\\n3+ years of experience utilizing visualization skills/tools (e.g. QlikView, Tableau, WebFocus, etc.)\\nPossess a strong understanding of network science and ability to apply its principles to organizational processes\\nExperience with analyzing complex non-linear systems, performing discrete event simulation\\nStrong collaboration and communications skills\\nStrong problem formulation and problem solving skills\\nExperienced in communicating findings to make data analysis actionable and understandable by business partners\\nAbility to deep dive complex processes and break them down to understand the nuances of how they work and connect with other processes\\nProficiency in analytic languages and frameworks such as Python, SAS, R, CPLEX, MATLAB, or Java\\nComfortable working in an environment where problems are not always well-defined\\nInquisitive, proactive, and interested in learning new tools and techniques\\nStrong oral, written and interpersonal communication skills\\nWell-organized, independent and ready to work with minimal supervision\\nHave a desire to excel and work with talented people\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for the win.\\n\\nVisa sponsorship may be available for this position\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\\n\\nSDL2017\\n\\n</br>\",\n",
       "  \"Data Analyst:\\nLocation:Detroit,MI\\nDuration: 12+ Months\\n\\nREQUIRED SKILLS/EXPERIENCE:\\n- 5+ years prior experience as a data analyst or data architect within healthcare, preferably with a payor, should have performed data analytics on daily basis as a primary role\\nStrong knowledge of Claims, Billing and Enrollment data and the associated business processes\\nUnderstanding of Enterprise Data Warehouse concepts and prior experience applying that understanding to query data in a Very Large Database (VLDB) environment\\nDemonstrated expertise in writing and analyzing complex SQL queries is required\\nAbility to work on multiple projects simultaneously and deliver within tight timelines while being flexible in adapting to new roles\\nKnowledge of data modeling concepts both in a multi-layer data warehouse (Normalized / Inmon) and data mart (Dimensional / Kimball)\\nAbility to work with multiple areas within organization to get business objectives, data requirements etc.\\nIdentify problematic areas and conduct research to determine the best course of action to correct the data\\nAbility to interpret data that is not well defined or documented and develop recommendations based on findings\\nExcellent written and verbal communication skills with the demonstrated ability to present findings and recommendations and to business users or leadership\\nEDUCATIONAL REQUIREMENTS:\\n\\nBachelor s Degree in related field required. Master's Degree in related field preferred\\nContact\\nPraveen\\n248 793 9045\\npraveen(@)epromptus.com\\n- provided by Dice\",\n",
       "  'Position: Bigdata Engineer(Data Engineer)\\n\\nLocation: Taylor, Detroit and Dearborn, MI\\n\\nDuration: long term/Fulltime\\n\\nDo not go by Title Date Engineer, Fine with profiles mentioning Data Engineer, Database Developer with experience in Bigdata or Hadoop, Java Engineer with Bigdata, Hadoop Etc is also fine\\n\\nData Engineer will be part of the Data Supply Chain. The product team s objective is to replicate data from hundreds of database sources within the company to the Hadoop environment and do transformations to make it usable for data scientists. This position will require an individual who has a strong background with multiple database technologies, who is process oriented and has knowledge of Java and expertise in the Hadoop environment. Big Data Engineer will be responsible for designing & developing data ingestion and data transformation framework for modern data lake solutions. Will be working with structured/unstructured Data sets, building large scale Data processing platforms, implementing world class data governance and operational controls, solving complex performance challenges.\\n\\nSkills Required:\\nMust have experience working with big data tools: Hadoop, Spark, Kafka, etc.\\nData Ingestion using Kafka, CDC or similar technologies.\\nImplement and support streaming technologies such as Kafka, K-SQL and Spark.\\nImplement and support big data tools and frameworks such as HDFS, Hive, and Hbase or Cassandra\\nImplement end to end Data Lake solution in on premise as well as on a cloud environment\\nExperience working with relational SQL and NoSQL databases, including Postgres and Cassandra.\\nExperience working with object-oriented/object function scripting languages: Java, Scala, Python etc.\\nprovided by Dice',\n",
       "  'Greetings from Collaborate Solutions Inc !!\\n\\nCollaborate Solutions is an Software Development & IT Consulting company with having operations in USA, Canada & India. We have around 600+ IT associates working with our esteemed clients across several domains such as IT, Banking, Finance, Telecom, Insurance, Healthcare, Manufacturing and Government.\\n\\nWe have an urgent requirement for below position with our client.\\nRole: Data Analyst\\n\\nLocation: Detroit, MI\\n\\nDuration: Long Term\\n\\nskills and experience\\nexperience working in Information Technology data quality or advanced analytics.\\nexperience writing and executing SQL and various forms of queries to do data analysis.\\nDemonstrate 3 years minimum experience working developing and implementing reference data management applications\\nAt least 3 years experience working with complex master data management tools and concepts such as entity management, and relationship management.\\nWorking knowledge of developing and integrating applications in a big data platform and a distributed\\nTarun | Technical Recruiter\\n\\ntarun@collaboratesolutions.com ,\\n\\n508-619-9788\\n\\n- provided by Dice',\n",
       "  \"DLZ professionals are focused on protecting our water resources. Our staff of experienced engineers and environmental specialists are an integral part of the firm’s multidisciplinary approach to water projects. DLZ's water services team focuses on developing solutions-based approaches to water, sanitary, storm, and combined sewer overflow (CSO) conveyance and storage needs. We strive to build community partnerships and utilize our vast expertise to solve complex management, planning, design, and construction challenges.\\n\\nDLZ Corporation is an award-winning architecture and engineering firm serving public and private entities across the nation. As a multidisciplinary firm, DLZ provides engineering and architectural design services, construction management, surveying, right-of-way acquisition and materials testing. In 2016, DLZ was named Design Firm of the Year by Engineering News-Record (ENR) Midwest. In addition, DLZ is consistently ranked as one of ENR's Top 150 U.S. Design Firms.\\n\\nDLZ Corporation currently has a Water Engineer - Data Analyst opportunity in Program office in Detroit, MI. We are looking for a highly talented and motivated individual who will retrieve and analyze data from various sources to support condition assessment and capital improvement planning activities for a major water distribution and wastewater conveyance / treatment client.\\n\\nDuties & Responsibilities:\\nAcquire data from primary or secondary data sources and maintain databases using Innovyze data management software.\\nCollect and configure large datasets to meet project specifications.\\nDevelop, implement and/or optimize data collection and analytical processes to meet clients’ requirements.\\nDevelop and utilize data validation tools in Excel, Access and SQL to identify data quality issues.\\nIdentify trends or patterns in complex datasets and generate tables, charts, and or graphical output using visualization tools such as Tableau.\\nFilter and clean data by reviewing computer reports and performance indicators to locate and correct problems.\\nDefine process improvement requirements.\\nWork collaboratively with other engineering, GIS Analysts, Field Inspectors, and IT/IS team.\\nDocument work processes, compose training documents and conduct training sessions.\\nImplement, configure and test commercial-of-the-shelf data management related software to meet project specifications.\\nProvide technical support to field inspection teams.\\nPerform all work in a safe and responsible manner, in accordance with the Health and Safety Plan.\\nPerform other duties as assigned.\\nJob Requirements:\\nBachelor’s degree in Science, Engineering or Information Technology Information Systems/Management, Mathematics, Statistics.\\n2 or more years of experience in the environmental/engineering field with emphasis on environmental data management or an advanced degree\\nKnowledge of the latest principles, practices, and techniques of GIS. Ability to produce professional, high quality, GIS-based cartographic products with the ESRI Software Suite and relational databases\\nDemonstrated proficiency in writing and executing simple to complex SQL queries\\nDemonstrated proficiency in Excel for manipulating and displaying data, including the use of pivot tables and charts\\nExperience with Innovyze data management and modeling software including Innovyze InfoNet, and InfoWorks\\nEffective organization and project management skills to meet multiple deadlines and attain results\\nProven attention to detail and problem-solving skills\\nHigh initiative, superior interpersonal skills, and ability to work with minimal supervision and to manage multiple tasks simultaneously\\nExcellent verbal and written communications skills\\nMicrosoft Office experience is required\\nPreferred Qualifications:\\nExperience with ESRI based applications such as Azteca Cityworks\\nRelevant experience with AutoCAD and/or MicroStation\\nTechnical expertise in designing, implementing, and documenting data models\\nExperience with data analysis, data mining and segmentation techniques\\nKnowledge and familiarity with data visualization packages (Power BI, Tableau, etc.)\\nFamiliarity with Python, JavaScript, Ruby programming and report generation such as Crystal Reports is desirable\\nExperience in the planning, design and implementation of mobile applications for field data collection a plus\\n\\nAt DLZ we offer an excellent benefit package including health, dental, vision and life insurance, medical and dependent care flexible spending accounts, health savings account, and 401(k) with an Employee Stock Ownership Program. We also offer an innovative wellness program including gym reimbursement!\\nDLZ is an Equal Opportunity/Affirmative Action employer. DLZ fully supports and maintains compliance with all state, federal, and local regulations. All qualified applications will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, marital status, protected veteran status, genetic information, or any other category protected by federal, state, and local laws.\",\n",
       "  \".\\n\\nDo you dare to reinvent the future of education?\\n\\n\\nAt Cengage, we are harnessing the power of tech to build a future where all learners have the tools and confidence to achieve their goals.\\n\\nAs a Cengage employee, you will blaze a new trail to transform the way people learn. Collaborating with the best of the best, you will feel challenged and inspired to do breakthrough work. With the support of our united team, there is no limit to what you can imagine, create and set in motion.\\n\\nAre we right for you?\\n\\n\\nWe set the bar higher by bringing our unique talents and point of view to the table every day. We are curious and comfortable with change and are willing to take risks to transform education. Most importantly, with everything we do, we put learning first.\\n\\nWhat You'll Do Here:\\n\\n\\nWe’re looking for a Full Stack developer who will take a key role on our team. Our Full Stack developer must have knowledge in all stages of software development.\\n\\nResponsibilities:\\nDesign overall architecture of the web application.\\nMastery of modern backend web programming standards, including object-oriented design and application architecture.\\nIs a strong coder in any language. (we primarily use Java 8, JavaScript)\\nHighly experienced with using modern (ES6/Next) JavaScript languages and frameworks, particularly in a React/Redux stack. Familiarity with modern JS developer tooling such as Webpack\\nExcellent analytical, problem solving, and communication and collaboration skills.\\nSkills You Will Need Here:\\n\\n\\nExperience with Responsive Web Design\\nExperience with micro services with containers\\nExperience in building Cloud Native Applications\\nExperience with cloud technologies (AWS, PCF)\\nExperience with Splunk, Dynatrace\\n#LI-DC\",\n",
       "  \"Position Overview and Responsibilities:\\n\\nFord IT is growing an organization focused on delivering software leveraging eXtreme Programming and cloud technology. In this environment the Software Engineer is expected to work in a pair developing working, tested code based on proven Lean/Agile methods. Engineers on the team work across the full stack of technologies to enable the highest priority work to be delivered.\\n\\nBasic Qualifications:\\nHigh school diploma.\\n3+ years of work experience in Object Oriented development in at least one of the following web or mobile technologies:\\nAndroid OR\\nSwift/Objective-C OR\\nSpring and/or Rails frameworks OR\\nJava / Ruby / Python / Golang\\nPreferred Qualifications:\\nExceptional software engineering knowledge; OO Design Principles\\nPracticed in eXtreme Programming (XP) disciplines including:\\nPaired programming\\nTest-first/Test Driven Development (TDD)\\nExperience with Spring Cloud and deploying to cloud platforms, preferably Pivotal Cloud Foundry or Cloud Foundry\\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\\nDelivered products that include web front-end development; JavaScript, client-side MVC frameworks like Angular, React, etc.\\nCapable in Continuous Integration/Continuous Delivery tools and pipelines such as Jenkins, Maven, Gradle, etc.\\nA Bachelor's degree in Computer Science or similar technical discipline.\\nThe distance between imagination and ... creation. It can be measured in years of innovation, or in moments of brilliance. When you join the Ford team, you discover all the benefits, rewards and development opportunities you'd expect from a diverse global leader. You'll become part of a team that is already leading the way, with ingenious solutions and attainable products - and it is always ready to go further.\\n\\nCandidates for positions with Ford Motor Company must be legally authorized to work in the United States on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  \"The Global Data Insights and Analytics (GDI&A) Connected Vehicle Analytics team supports Ford Motor Company's various connected vehicle initiatives with analytical and machine learning solutions. We are looking for a data scientist with expertise in Machine Learning and Statistical Modeling. We are especially excited about candidates with prior experience in scalable technologies (Hadoop/Spark) and strong technical mindset who demonstrate a passion for applying state-of-the-art solutions to novel and challenging problems. As a member of this dynamic team, you will have the opportunity to work with some of the brightest global subject matter experts in product development and vehicle connectivity who are transforming the automobile industry. The job candidate should have great autonomy, exceptional collaboration and leadership skills, and self-discipline to conduct original research and choose appropriate methodologies to solve related problems.\\n\\\\n\\\\n\\\\n\\\\n\\n\\nResponsibilities:\\nExplore and analyze source data and data flows, working with both structured and unstructured data; Manipulate high-volume, high-dimensional data from a variety of sources to identify value-generating patterns, anomalies, relationships, and trends\\nStay current with advances in emerging technologies and analytic solutions for use in quantitative model development\\nUtilize analytical applications like Python/R and recent Big Data technologies (Hadoop/Spark) to identify trends and relationships between different pieces of Ford-owned and third-party data, draw insightful conclusions and translate those analytical findings into next generation control algorithms\\nDevelop scalable machine learning algorithm solutions to address real-world automotive and mobility challenges to enable an accelerated vehicle development process\\nWrite timely and high-quality work reports, external publications, and records of inventions to document new ideas and research results; Communicate and present model insights to business customers and executives\\n\\nBasic Qualifications:Master's degree in a quantitative field such as Computer Science, Computer Engineering, Statistics, Economics, Mathematics, or Physics\\n3+ years of experience with Machine Learning methods and tools, including but not limited to: deep neural networks, linear and nonlinear regression, classification, and clustering\\n1+ year of experience in Hadoop/Spark\\n3+ years of experience in the following languages: Python/R, Java/Scala\\n\\nPreferred Qualifications: PhD in a quantitative field such as Computer Science, Computer Engineering, Statistics, Economics, Mathematics, or Physics\\nDomain knowledge in vehicle architecture, mobility and connectivity\\nDemonstrated ability in the application of Machine Learning to real-world industrial settings with large scale data\\nExperience with high-performance Deep Learning frameworks such as TensorFlow, PyTorch, Theano, Caffe, Caffe2\\nExperience with version control systems such as Git\\nStrong oral, written, and interpersonal communication skills and an ability to work in a collaborative team setting; Comfortable working in a fast-paced and innovative environment where problems are not always well-defined\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nCandidates for positions with Ford Motor Company must be legally authorized to work in the United States on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position, TN visa holders may be considered.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  \"The technical expertise within this Software Engineering discipline is the backbone in executing a Configuration Management role in an Embedded Software Engineering environment on Military Combat Vehicles.\\n\\nJob Functions:\\n\\nEstablishing configuration management repositories utilizing tools such as Subversion (SVN), Bit-Bucket, Confluence, JIRA, DI2E, etc., for proper access control.\\n\\nDeveloping automated build scripts, executing official software builds, developing branching schemes, repository setup and access control, software configuration change control, and software release management.\\n\\nConducting Software Configuration Control Boards (SCCBs) to approve or disapprove Software Change Requests (SCRs), baselines and releases.\\n\\nDeveloping and supporting the Software Configuration Management Plan (SCMP) for assigned projects.\\n\\nRequired Skills:\\n\\nDue to the sensitivity of customer related requirements, U.S. citizenship is required.\\n\\nBachelor's degree in engineering (software, computer, or electrical) or computer science plus 5 years of experience is required.\\n\\nExperience in the development of build scripts to conduct official SCM software builds.\\n\\nExperience with Subversion (SVN), Bit-Bucket, Confluence, JIRA, DI2E, ClearCase / ClearQuest, and/or similar tools.\\n\\nExperience with software engineering languages such as C/C++, C#, Java, etc.\\n\\nExperience with developing software code branching schemes.\\n\\nExperience with Configuration Items (CIs), Change Requests (CRs), Software Configuration Control Boards (SCCBs), Software Trouble Reports (STRs).\\n\\nExperience with software industry best practices such as Capability Maturity Model Integrated (CMMI).\\n\\nMust have strong communication skills, both verbal and written, to interface with software engineers and project management.\\n\\nMust be able to obtain and maintain a DoD Secret clearance.\",\n",
       "  \"Position Overview/Description:\\n\\nICI (Intelligent Customer Interactions) Data Access Software Engineer positionis for a self - motivated, high performing individual who is passionate about building data environments which serve strategic advantage to ICI. The individual is expected to have a very strongdevelopment backgroundin Hadoop, DataStage, UNICA, and mustbe adeptwith general Software Engineering principles and tools. The candidate is also expected to have prior DevOps experience.\\n\\nResponsibilities:\\nInterface with Business and IT teams to acquire clarity on data requirements\\nPerform data analysis, data landing, transformation, business rule modeling, data modeling functions and launch data product solutions to various development environments using automated delivery pipelines\\nLead team of SW Engineers to come up with forward looking solutions to address business opportunities and problems.\\nGuide and execute Production Support related activities in immediate team as well as provide guidance to other teams in the portfolio as needed\\nEnsure data compliance to applicable regional norms and legal requirements.\\nDevelop and maintain accurate documentation\\n\\nJob Requirements:\\n\\nBasic Qualifications:\\nBachelor's Degree\\n3 + years of Hadoop design and development experience\\n10 + years of DataStage design and development experience\\n4 + years of UNICA Campaign Management design and development experience\\n10 + years of Application Development and Production Support\\n\\nPreferred Qualifications:\\nAbility to communicate effectively with business users and IT professionals including globally distributed teams; the ability to adjust communications appropriately for the audience; work with all levels of management and diverse work groups\\nStrong hands on design and development experience in DataStage and Hadoop (Oozie, HIVE, Ambari, HBase, Falcon, Spark, etc) technologies, Hadoop Developer Certification a MUST\\nStrong hands on technical expertise in implementing Continuous Integration/Continuous Delivery & Deployment pipeline by using technologies such as Jenkins, Cucumber, Gradle,Cucumber, Kubernetes, Putty, GITHUB, DataStage, and Hadoop\\nProficiency in managing and administering IBM UNICA Campaign Management application\\nStrong expertise in transmission layer scripting techniques and tools including SSL, SSH, HTTP, and Linux Shell Scripting\\nWorking level experience in ReSTFUL API/SOAP APIs\\nIBM WebSphere/Infosphere Certification\\nExperience with Teradata, Oracle, SQL, DB2 and Salesforce\\nKnowledge of Windows server and Windows Active Directory\\nExperience with VM servers\\n\\nThe distance between imagination and ... creation. It can be measured in years of innovation, or in moments of brilliance. When you join the Ford team, you discover all the benefits, rewards and development opportunities you'd expect from a diverse global leader. You'll become part of a team that is already leading the way, with ingenious solutions and attainable products - and it is always ready to go further.\\n\\nCandidates for positions with Ford Motor Company must be legally authorized to work in the United States on a permanent basis. Verification of employment eligibility will be required at the time of hire. Visa sponsorship is not available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  'We are looking for a Software Engineer to join our growing Engineering team and build out the next generation of our platform. The ideal candidate is a hands-on platform builder with significant experience in developing scalable data platforms. We’re looking for someone with experience in business intelligence, analytics, data science and data products. They must have strong, firsthand technical expertise in a variety of configuration management and big data technologies and the proven ability to fashion robust scalable solutions that can manage large data sets. They must be at ease working in an agile environment with little supervision. This person should embody a passion for continuous improvement and test-driven development.\\nResponsibilities\\nAnalyze, design and develop tests and test-automation suites.\\nDesign and develop a processing platform using various configuration management technologies.\\nTest software development methodology in an agile environment.\\nProvide ongoing maintenance, support and enhancements in existing systems and platforms.\\nCollaborate cross-functionally with data scientists, business users, project managers and other engineers to achieve elegant solutions.\\nProvide recommendations for continuous improvement.\\nWork alongside other engineers on the team to elevate technology and consistently apply best practices.\\nQualifications\\nHands-on experience working with technologies like Hadoop, Hive, Pig, Oozie, Map Reduce, Spark, Sqoop, Kafka, Flume, etc.\\nStrong DevOps focus and experience building and deploying infrastructure with cloud deployment technologies like ansible, chef, puppet, etc.\\nExperience with test-driven development and automated testing frameworks.\\nExperience with Scrum/Agile development methodologies.\\nCapable of delivering on multiple competing priorities with little supervision.\\nExcellent verbal and written communication skills.\\nBachelor’s Degree in computer science or equivalent experience.\\nWe’re looking for someone with 3-5 years of experience in B2B, has a BS degree in computer science or similar, and is familiar with the following software/tools:\\nExperience with infrastructure automation technologies like Docker, Vagrant, etc.\\nExperience with build automation technologies like Maven, Jenkins, etc.\\nExperience with monitoring technologies like Nagios, Ganglia, etc.\\nExperience with modern programming languages like Java, Python, etc.\\nExperience with building APIs and services using REST, SOAP, etc.\\nExperience with scripting languages like Perl, Shell, etc.',\n",
       "  \"Job Description\\nBusiness Analyst\\n\\nData mining, analytics. Focused on working with complex data and performing analysis.\\n\\nRequirements:\\n• Bachelor's degree or equivalent experience\\n• Proven and demonstrated experience with reporting on sales initiatives, ROI, and client metrics\\n• Strong reporting and data management experience / Proactive and positive “can-do” attitude\\n• Critical thinker with excellent problem solving skills\\n• Exceptional verbal and written communicator\\n• Strong business acumen and judgment\\n• Excellent written, oral, and presentation skills\\n• Proficient in Excel, Word and PowerPoint\\n\\nThe GENERAL JOB DESCRIPTION\\n\\nBusiness Analyst\\n\\nWe are currently seeking a Business Analyst with a focus on retail sales analytics. The Business Analyst is responsible for managing and leading all day-to-day operations functions for our GM program. He/she is responsible for all program systems, metrics, and technology. This position requires a combination of sales, operations, technology, and management experience for large-scale programs and teams; with a strong emphasis in sales scorecards and dashboards. This position focuses heavily on generating reports that show specific ROI for the client around field team execution and client metrics. This individual is responsible for maintaining a positive client relationship in the arena of operations and field execution. He/she is the Project Champion for all process improvements and technology related to the client’s evolving needs. This individual will be part of a business analyst team that produces all the client facing and internal use reports to manage the business on behalf of the customer. A strong background in Excel, PowerPoint and Word is necessary. SQL experience for generating reports preferred.\\n\\nResponsibilities:\\n• Support for Regional Director and Regional Managers\\n• Export Battery Credits\\n• Dealer Fixed Ops & ACDelco Sales Reports\\n• Dealer Fixed Ops & ACDelco Marketing Reports\\n• Assist with event/meeting planning\\n• Create and communicate Fixed Ops & ACDelco monthly Newsletters\\n• Create field training manuals\\n• Staff meeting presentations\\n• Regional Business Plan updates\\n• Create, communicate, track and analyze regional promotions for Dealers and ACDelco Wholesale Distributors\\n• Ongoing miscellaneous special projects\\n• Employee contact list\\n• Regional Phone List updates/submission\\n• Mileage Log Audits\\n• Office phone/voicemail system back-up\\n• Monthly Budgets\\n• Wholesale reporting\\n• Some travel may be required due to meeting/event planning (minimal)\\n• Misc Field & Staff Report Generation\\n\\nRequirements:\\n\\nMUST HAVE POWERBI experience.\\n• Bachelor's degree or equivalent experience\\n• Proven and demonstrated experience with reporting on sales initiatives, ROI, and client metrics\\n• Strong reporting and data management experience / Proactive and positive “can-do” attitude\\n• Critical thinker with excellent problem solving skills\\n• Ability to manage multiple demands and establish priorities\\n• Able to independently prioritize daily work assignments\\n• Ability to meet deadlines and work effectively with multiple deadlines\\n• Proven and demonstrated experience with strong relationship building skills\\n• Exceptional verbal and written communicator\\n• Strong business acumen and judgment\\n• Excellent written, oral, and presentation skills\\n• Proficient in Excel, Word and PowerPoint\\nCompany Description\\nSearch Current Career Opportunities: http://www.ektello.com/search-jobs\",\n",
       "  \"Job Description\\nRole: Health Care Analyst\\nDuration: 6+ months\\nLocation: Detroit, MI\\n\\nRequired Skills: SQL, Data, Analytics\\n\\nOur Fortune 500 client is looking for Health Care Analyst based out of Detroit, MI\\n\\nOverview:\\nPosition will involve working on ad hoc analytic requests which would include executing the methodology using SQL, analyze data to answer business questions and create deliverables for business areas.\\nResponsible for identifying, collecting, analyzing, and maintaining data to evaluate issues that support prospective business decisions\\nCoordinate projects for senior management\\nQUALIFICATIONS:\\nTop Skill Requirements:\\nSQL\\nSynthesize analytic findings or data\\nCreate presentations\\nMore Qualifications:\\nBachelor's Degree in Business Administration, Economics, Health Care, Information Systems, Statistics or other related field is required\\nMaster's Degree in related field preferred\\nFour (4) to Six (6) or more years of experience in related field required\\nOther related skills and/or abilities may be required to perform this job.\\nPlease send your updated word format resume along with your best contact details to Bhavya@bsc-us.com or call me at 248-270-8533\\n\\nBSC Solutions Inc., is a Global IT Solution provider headquartered in Troy, Michigan with operations in the US, India and the Middle East. BSC Solutions has over 15 years of IT and consulting experience to give cost effective solutions to many Fortune 1000 companies.\\n\\nThanks & Regards,\\nBhavya\\nSr. IT Recruiter\\nW:+1-248-270-8533\\nBhavya@bsc-us.com\\nBSC Solutions, Inc. USA | DUBAI | INDIA\\n1000 John R. Rd, Suite 203, Troy, MI 48083\\nwww.bscsolutionsinc.com\\nCompany Description\\nBSC Solutions is a Global IT Solution provider with headquarters in Troy, Michigan and operations in Dubai and India.\\nBSC has been providing cost effective solutions to many Fortune 1000 companies for the last 15 years.\\nBSC has over 15 years of IT and consulting experience.\\nBSC has expertise of different tools and in various industries which enables them to offer you the best solution for your needs.\",\n",
       "  \"As a Software Engineer, for embedded platforms you will have the unique opportunity to join a growing team focused on re-defining the transportation experience, as we know it today. You will be challenged with developing innovative solutions which solve difficult and ambiguous real world issues.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nResponsibilities:\\n\\nAs an Application Software Engineer, you will:Work with Product Managers and architects to understand the requirements of the project and develop solutions, which meet those requirements.\\nIdentify opportunities to increase the velocity of your efforts and the wider team.\\nParticipate in architectural and code reviews when necessary\\n\\nBasic Qualifications:Bachelor's degree in Computer Engineering, Electrical Engineering, Computer Science, or related\\n1+ years C/C++ software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n2+years' Java software development experienceon embedded, infotainment platforms, mobile, or consumer electronic platforms\\n2+ years experience software development experience for Android\\n1+ years experience with developing software leveraging Agile concepts such as SCRUM, SAFe, Kanban, etc.\\n\\nPreferred Qualifications:Master's degree in Computer Engineering, Electrical Engineering, Computer Science, or related\\n2+ years C/C++ software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n4+years' Java software development experienceon embedded, infotainment platforms, mobile, or consumer electronic platforms\\n3+ years experience software development experience for Android\\n2+ years experience with developing software leveraging Agile concepts\\n3+ years using Android Studio\\nFamiliar with eXtreme Programming (XP) practices including:Pair/Mob programming\\nTest-first/Test Driven Development (TDD)\\n\\nIn-depth knowledge of C/C++ language on Unix based systems and/or Java\\nProficiency with revision control including Git, Subversion, or equivalent\\nExperience with Jira, Confluence, or equivalent\\n1+ years Multi-site software project team experience\\nFamiliarity with GitHub or equivalent source control repositories and Build Tools\\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\\nCapable in Continuous Integration/Continuous Delivery tools and pipelines such as Jenkins, Maven, Gradle, etc.\\nExperience in development of microservices.\\nExperience with Continuous Integration/Continuous Delivery tools and pipelines\\nExperience with CA Agile Central (Rally), backlogs, iterations, user stories, or similar Agile Tools.\\nExcellent communication skills\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nVisa sponsorship may be available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  'Job Summary\\n\\nThe Enterprise Data Architect participates in the creation and evolution of a technology road map that defines current technical architectures and future architectures. They are a subject matter expert for architectural frameworks, methods, design patterns and tools. This team member leads strategic technological planning to achieve company goals, provides an architecture that meets the company\\'s business needs, and ensures the most efficient and secure IT environment possible. The Enterprise Data Architect drives common approaches and exposes information assets and processes across the enterprise. They document multiple architectural models or views that show how the current and future needs of the organization will be met in an efficient, sustainable, agile and adaptable manner.\\n\\nResponsibilities\\nIdentify commonalities between business teams and businesses to define and recommend common solutions\\nBuild for performance, fault tolerance, security and reusable patterns\\nIdentify and implement build vs. buy strategies\\nMonitor and identify performance trends and work with teams to improve processes and procedures\\nMentor and coach team members to expand their awareness of other aspects of the enterprise\\nDevelop a strategic vision for technology platforms that enables the broader vision for IT and the business\\nSeek self-driven development opportunities\\nPartner with the business and the Technology team to deliver road maps\\nWork with stakeholders to build a holistic view of the organization\\'s strategy, processes, information and information technology assets\\nResearch, analyze and promote technology standards to be used across the enterprise\\nFacilitate problem-solving among a strong team\\nRequirements\\nBachelors degree in information technology or equivalent work experience in technology\\n10 years of experience in technology, with an understanding of mortgage lending\\n3 years of experience providing technical leadership or guidance\\nExperience using architectural methodologies and frameworks like TOGAF, Zachman and ArchiMate\\nExperience with messaging systems and developing transactional systems\\nProven experience in rewriting, refactoring, scaling and the re-architecture of applications\\nIn-depth understanding of our internal applications and how our infrastructure works together\\nAbility to keep current on product trends and technology innovations\\nKnowledge of system architecture and normalization considerations\\nKnowledge of inter-system communication strategies, including ESB and web services\\nWho We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past nine consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for five consecutive years, 2014 through 2018, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top-30 for the past 15 years.',\n",
       "  'Key responsibilities and skills required include:\\n\\n• Industry experience in analyzing source system data and data flows, working with structured and\\n\\nunstructured data, and delivering data and solution architecture designs.\\n\\n• Experience building new data pipelines from various structured and unstructured sources into Hadoop.\\n\\n• Demonstrated abilities to perform Data profiling using Collibra and ETL (Talend, Informatica etc.) and\\n\\nRelational Databases (i.e. SQL, Teradata, Oracle etc.).\\n\\n• Knowledge of Data Warehousing best practices; modeling techniques and processes and complex data\\n\\nintegration pipelines.\\n\\n• Demonstrated abilities to create Data models ,create Data mapping and functional specifications\\n\\n• Experience with SQL, NoSQL, relational database design, and methods for efficiently retrieving data\\n\\n• Ability to collaborate with key stakeholders including architects, software engineering, and project managers\\n\\nto report out development progress and escalates issues requiring attention\\n\\n• Ability to understand, navigate and develop with data tools and technologies - data modeling, metadata\\n\\nmanagement tools, master data management tools, database technologies and design\\n\\n• Ability to lead resolution of end to end data issues, data design issues and end to end data test issues\\n\\nRequired Qualifications:\\n\\n• 2 to 3 Years of experience implementing Big Data solutions.\\n\\n• Recent and demonstrable coding, unit testing, automate testing processes, and debugging applications in\\n\\nvarious software languages, with software testing and quality assurance\\n\\n• Experience in troubleshooting highly technical issues on your own and within a collaborative team setting.\\n\\n• Experience in Horton Works Data platform hadoop distribution, Python, SQL & Hive QL programming skill\\n\\n• Ability to create mapreduce programs as well as Pyspark programs on HDP platform\\n\\n• Experience with industry leading Relational database platforms (i.e. Oracle, Teradata etc.)\\n\\n• Exposure to Data Governance technologies i.e. Collibra and InfoSphere.\\n\\n• Exposure to Data mining using tools like SAS\\n\\n• Experience in Visual Analytics using tools like Tableau or Qlikview\\n\\n• Exposure to campaign management system like IBM Unica Or Merkle\\n\\n• Proactively engage in the remediation of software issues related to code quality, security, and/or pattern/frameworks.\\n\\n• Experience with Text Analytics solutioning and delivery (either SOLR or ElasticSearch).\\n\\n• Knowledge of Oracle Golden Gate, heterogeneous data replication techniques\\n\\nMin Education: Bachelor Degree in Computer Science, Software Engineering or Information Systems, Electronic/Electrical Engineering, or a related field to the software is being developed or progressive experience.',\n",
       "  \"The future is bright at DTE Energy! We are one of the largest Fortune 500 diversified utilities in the United States with an aspiration to be the best-operated energy company in North America and a force for good in the communities we live and serve. We have businesses in 26 different states and are comprised of regulated utility and non-utility businesses. Our utility business provides electric and gas service to approximately 3 million customers. Our non-utility businesses include a diversified portfolio of energy related companies, ranging from gas storage and pipelines to renewable power development.\\n\\nDTE Energy’s utility and non-utility businesses are poised for significant growth. We look forward to working with highly motivated and team-oriented individuals to energize our efforts of growing economically and environmentally.\\n\\nRecently, DTE Energy has been recognized as an outstanding place to work and has received the following accolades:\\n\\n* Gallup Great Workplace Award for consecutive years\\nCivic 50 Award for corporate citizenship excellence\\nIndeed’s annual “50 Best Places to Work” award for two years running\\nMetropolitan Detroit’s 101 Best and Brightest Companies to work For\\nJ.D. Power Customer Satisfaction Award\\nProfessional Women’s Magazine/Black EOE Journal “Best of the Best”\\nComputerworld’s 100 Best Places to Work in IT\\nBest Employers for a Healthy Lifestyle Gold Award\\nDetroit Free Press Green Leaders Award\\n\\nDTE Energy is an equal opportunity employer and considers all qualified applicants without regard to race, color, sex, sexual orientation, gender identity, age, religion, disability, national origin, citizenship, height, weight, genetic information, marital status, pregnancy, protected veteran status or any other status protected by law.\\n\\nExternal Pre-Hire Assessment Required: Professional Pre-Hire Assessment\\nTesting Required: Not Applicable\\nOverview\\nDTE Electric is undergoing a major technological transformation to improve power grid reliability. We are seeking electrical engineers who will help us plan, design and deploy solutions and technologies that will help deliver higher quality power to our customers. As an Electrical Engineer assigned to this incredible project, you may work within one or more focus areas: substations, sub-transmission, system protection and automation, or distribution planning. We look forward to working with highly motivated and team-oriented electrical engineers to energize our efforts of growing economically and environmentally. The location for these roles will depend on the group and availability at the time of hire.\\nJob Summary\\nIntermediate level engineer responsible for independently evaluating, selecting and applying standard engineering techniques, procedures and criteria. Uses judgement in applying or recommending adaptions and modifications. Typical assignments are in the areas of engineering or scientific design, problem solving, and technical support for the optimum operation and maintenance of company assets. Participates on teams of field employees, technicians, engineers and other personnel.\\nKey Accountabilities\\nPerforms engineering assignments, proposing solutions to engineering related problems.\\nWorks with experienced professionals to recommend and implement timely solutions involving routine engineering, design, operation, maintenance of company assets and market optimization.\\nCollaborates with more experienced engineers to provide follow-up and engineering assistance on defined problems within time and budget constraints.\\nDevelops models and analyzes data in order to recommend solutions and/or improvements in processes and programs; develops and presents presentations to all levels of management and/or external parties.\\nMay provide field engineering assistance or technical support.\\nKeeps up to date on current technology and market/technology trends.\\nMinimum Education & Experience Requirements\\nBachelor's Degree in Engineering or Engineering Technology and a minimum of 2 years of job relevant experience\\nOther Qualifications\\nPreferred\\nBachelor's degree in Electrical Engineering\\n2+ years of utility experience\\nWork Lead experience or experience in the management of engineer related projects and/or initiatives\\nExperience in one or more of the following: Distribution Engineering & Planning, Equipment Engineering, Substation Design, Sub Transmission & Industrial Power Engineering & Planning, Transmission Engineering & Planning, CYME & PSSE Modeling & Solution Development, and/or Smartgrid\\nOther Requirements\\nProficiency in Microsoft Office programs, primarily Excel and Word\\nIntermediate analytical and problem solving skills\\nStrong time management, prioritization, and organization skills\\nProficient communication and interpersonal skills\\nAdditional Information\\nIncumbents may engage in all or some combination of the activities and accountabilities, and utilize a variety of the competencies cited in this description depending upon the organization and role to which they are assigned. This description is intended to describe the general nature and level of work performed by incumbents in this job. It is not intended as an all-inclusive list of accountabilities or responsibilities, nor is it intended to limit the rights of supervisors or management representatives to assign, direct and control the work of employees under their supervision.\",\n",
       "  \"Contract (Part-Time) - REMOTE POSITION\\n\\nCompany: Lantana Consulting Group provides services and software for standards-based health information exchange. We have built our expertise through more than a decade of development and deployment of technical specifications and interoperability solutions. As a distributed company with no single brick and mortar office, Lantana can hire the best available talent and offer a flexible work schedule. We are a small, growing business on a path towards employee ownership.\\n\\nPrimary purpose: Support the development of Trifolia-on-FHIR, Trifolia Workbench and other internal tools developed by the company. The candidate will work with product stakeholders to establish and/or clarify requirements and implement solutions for features, enhancements and defects in the product backlogs.\\n\\nA successful candidate will:\\nHave experience with Agile and Kanban software development methodologies.\\nBe familiar with HL7 Fast Healthcare Interoperability Resource (FHIR). HL7 Clinical Document Architecture (CDA) experience is a plus.\\nHave good communication skills to support working in a remote environment.\\nQualifications:\\nBachelor’s degree or equivalent experience is required.\\nFive or more years of experience in software development.\\nExperience with FHIR is required, including the FHIR REST API. Experience with CDA is preferred.\\nExperience with Enterprise JavaScript technology stack, including Node.JS, Angular 2 and Bootstrap is required.\\nIdeally familiar with the FHIR profiling and the FHIR implementation guide publishing process.\\nIdeally have experience with C# technologies (ASP.NET MVC, Visual Studio, SQL Server, Nuget).\\nIdeally have experience with Java technologies (Eclipse/IntelliJ, JPA, Maven, HAPI FHIR Server).\\nRuby development experience is a plus.\\nAdditional Job Information:\\nThis position is 100% work-from-home/remote.\\nLantana is an equal opportunity employer. All qualified applicants for current openings will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, or natural origin.\\nJob Types: Part-time, Contract\\n\\nExperience:\\nsoftware engineering: 4 years (Required)\\nHL7 Fast Healthcare Interoperability Resource (FHIR): 2 years (Preferred)\\nEducation:\\nBachelor's (Preferred)\",\n",
       "  'Role-Big Data Engineer\\nLocation-Dearborn - MI\\nExperince-6 to 10\\nJD:--\\nSelecting and integrating any Big Data tools and frameworks required to provide requested capabilities\\nImplementing ETL process {{if importing data from existing data sources is relevant}}\\nMonitoring performance and advising any necessary infrastructure changes\\nDefining data retention policies\\n{{Add any other responsibility that is relevant}}\\nSkills and Qualifications\\nProficient understanding of distributed computing principles\\nManagement of Hadoop cluster, with all included services {{unless you are going to have specific Big Data DevOps roles for this}}\\nAbility to solve any ongoing issues with operating the cluster {{unless you are going to have specific Big Data DevOps roles for this}}\\nProficiency with Hadoop v2, MapReduce, HDFS\\nExperience with building stream-processing systems, using solutions such as Storm or Spark-Streaming {{if stream-processing is relevant for the role}}\\nGood knowledg\\n\\nDiverse Lynx LLC is an Equal Employment Opportunity employer. All qualified applicants will receive due consideration for employment without any discrimination. All applicants will be evaluated solely on the basis of their ability, competence and their proven capability to perform the functions outlined in the corresponding role. We promote and support a diverse workforce across all levels in the company.',\n",
       "  'Data Scientist\\n\\nAutomotive\\n\\nDearborn, MI\\n\\nPosition Objective\\n\\nOur organization is launching a Pass Rewards credit card, and FNBO (First National Bank of Omaha) is the credit card issuer. Data needs to be shared between our organization and FNBO to administer this program. Data ingestion, curation, transformation, and governance will need to be executed to support this project.\\n\\nJob Duties and Responsibilities\\nFacilitate ingestion of bank data to our organization’s Hadoop environment\\nPerform data transformations that allow for reliable and consistent interpretation of the data in addition to compatibility with current data applications\\nValidate and test customer bank data\\nCreate data products with existing data in combination with bank data\\nAct as initial data steward of the incoming data to ensure proper protection / controls\\nTechnical Requirements\\nEffective communication and presentation skills\\nInformatica\\nAlteryx\\nExperience with Hadoop\\nSQL / HQL\\nMS Office\\nSynergy Solutions is an equal opportunity employer. In keeping with the values of Synergy, we make all employment decisions without regard to race, religion, color, sex, age, national origin, ancestry, sexual orientation, physical handicap, mental disability, medical condition, disability, gender or identity or expression, pregnancy or pregnancy-related condition, marital status, height and/or weight. The list of duties and responsibilities outlined above is representative and may not be a complete and detailed list of tasks. Employee must perform any and all other duties assigned by his or her supervisor. Additionally, this job description does not constitute a contract of employment and that the company may exercise its employment-at-will rights at any time.\\n\\nJob Type: Full-time\\n\\nSalary: $70,000.00 to $90,000.00 /year\\n\\nExperience:\\nrelevant: 1 year (Preferred)\\nData Analytics: 1 year (Preferred)\\nAlteryx: 1 year (Preferred)\\nInformatica: 1 year (Preferred)\\nHadoop: 1 year (Preferred)\\nWork Location:\\nOne location\\nBenefits:\\nDental insurance\\nHealth insurance\\nVision insurance\\nRetirement plan\\nFlexible schedule\\nPaid time off\\nSchedule::\\nMonday to Friday',\n",
       "  'Job Description\\nJOB DESCRIPTION\\n\\nThe candidate will create, plan, and perform a variety of software analysis, design, development, code, documentation, integration, test, and product assurance tasks. He or she will contribute to the development of engineering design models and develop software to implement and support project or system functionality for the assigned technical effort. Ensure technical outcomes are consistent with established engineering principles and practices and that customer and company technical, system, performance, and quality requirements are met. Perform basic engineering design, development, analysis, experimentation, test and/or product assurance tasks of limited complexity for assigned portions of a project. Ensure assigned tasks achieve cost, quality, schedule, and performance requirements.\\n\\nSkills Required\\n\\n• Exceptional verbal and written communication\\n\\n• Ability to work independently and as part of a team\\n\\n• Understanding of software design, development, and testing principles\\n\\n• Working knowledge of the software development and maintenance life cycle\\n\\n• C++ and/or Java programming languages\\n\\n• Linux operating system\\n\\n• Bash, Perl, Python scripting languages\\n\\n• JIRA, Bitbucket, Git, Confluence configuration management and problem reporting tools\\n\\nSkills Preferred\\n\\n• Experience with scripting languages (i.e. Bash, Perl, Python)\\n\\n• Experience testing software or working test scripts\\n\\n• Experience with the use of scripting tools to develop and maintain test scripts\\n\\n• Experience with Software configuration management tools (i.e. Git, SVN, etc.)\\n\\n• Experience with Software issues tracking software (i.e. JIRA, Bugzilla, etc.)\\n\\n• Experience with Software requirements management tools such as DOORS\\n\\n• Knowledge of vehicle/computing communication busses (Gigabit Ethernet, RS-232, CAN) and their protocols\\n\\n• Systems and Software requirements analysis (including requirement writing, and specification generation)\\n\\n• Experience generating and writing supporting documents such as test plans, test procedures, test reports, system software requirements, user manuals / tools documentation, and software validation documents\\n\\nExperience Required\\n\\n• Experience with embedded software and hardware development and testing\\n\\nEducation Required\\n\\n• Bachelor’s degree in Electrical/Computer Engineering, Computer Science, or related field\\nCompany Description\\nAt Amtec, we care about you and your career. Since 1959, we have changed the lives of thousands of people for the better - people just like you. It is our goal to help you find meaningful work by matching your character, competence, and culture with an organization that truly values what you have to offer. Whether you want a contract assignment, a temp-to-perm job, or a regular full-time position, we are here to be your partner throughout your whole career.',\n",
       "  'The Principal Software Engineer mentors other engineers, leads small development teams and acts as an application owner, in addition to developing, improving and maintaining innovative software applications as part of a team or independently. This team member takes a lead role in design and code reviews, assists with delivery estimates and provides feedback about all aspects of the process all while working with a variety of team members across the entire organization.\\n\\nResponsibilities\\nDevelop progressive web apps using MVC/web API and C# through the full software development life cycle process\\nUnit-test all developed code\\nWork directly with business partners to determine technical solutions based on business needs\\nMentor other Software Engineers to grow their technical skills\\nUnderstand general, undeveloped concepts and explain them concisely to others\\nWork with or without complete business requirements or specifications\\nFoster a collaborative environment on a cross-functional team\\nImprove standards and best practices\\nRequirements\\n8 years of programming or related experience in object-oriented programming languages (C# or Java)\\n8 years of experience with .NET 4.0+ or related frameworks\\n8 years of experience in software testing and design\\n8 years of database-related experience (SQL, MySQL, Oracle, etc.)\\n8 years of experience working in a team environment\\nExperience with JavaScript and affiliated frameworks (Angular, Vue, React, etc.)\\nExperience with Git/Github\\nExperience as the lead engineer on multiple projects\\nExperience with web APIs (REST, SOAP)\\nExperience with design patterns\\nExperience mentoring others\\nUndergraduate degree in computer science or equivalent relevant experience\\nExceptional verbal and written communication skills\\nWhatll Make You Special\\nSome graduate coursework\\nExperience with .NET Core\\nExperience with cloud technologies (AWS, Azure, Google Cloud, etc.)\\nExperience with architectural patterns such as MVVM, MVC and MVP\\nWho We Are\\n\\nWere Americas largest mortgage lender, closing loans in all 50 states. J.D. Power ranked Quicken Loans Highest in Customer Satisfaction in Primary Mortgage Origination for the past nine consecutive years, 2010 2018. The company was also ranked highest in the nation for client satisfaction among mortgage servicers by J.D. Power for five consecutive years, 2014 through 2018, each year the company was eligible. Theres a simple reason weve been so successful: We care about the people we work with.\\n\\nIf youre tired of stuffy, bureaucratic workplaces, then youll be delighted to find something different here. We strive to make a creative, fun and collaborative environment you simply wont find anywhere else. Quicken Loans was named #1 in ESSENCE Magazines first ever list of Best Places to Work for African Americans in 2015. We\\'ve been on Computerworld\\'s \"Best Places to Work in IT\" list for 13 years running, hitting #1 the last five years. We were also ranked #14 in FORTUNE Magazines list of \"100 Best Companies to Work For\" in 2018, remaining in the top-30 for the past 15 years.',\n",
       "  \"Permanent position: Electrical/Software Engineer sought at a strong Fortune 500 company in greater Detroit MI. This role is a mix of electrical engineering and computer science, as it combines level I automation (PLCs, HMIs, etc) with level II programming (C#, Java, etc). Join a thriving company with multiple locations, where you can take ownership of your work, feel the impact of your work, and grow!\\n\\nResponsibilities for the Software Engineer\\nInstallation, maintenance, and troubleshooting software systems, hardware servers, devices, and network switches\\nSupport C software as well as embedded SQL development\\nIT in a process manufacturing environment, including automation, control, and systems integration\\nMaintaining and improving level II computer systems\\nSupport signal processing, Data/Failure analysis, System commissioning, Model development/support\\nC#, Java, etc\\nRequirements for the Software Engineer\\nBachelor's degree or higher required in one of the following: Computer Science, Computer Science Engineering, or Computer Science Electrical Engineering(CSEE)\\nKnowledge of the C programming language and SQL a plus\\n1+ years experience from a manufacturing environment strongly desired\\nBenefits\\nGood benefits including a generous vacation policy and strong relocation package\",\n",
       "  \"As a Software Engineer, for embedded platforms you will have the unique opportunity to join a growing team focused on re-defining the transportation experience, as we know it today. You will be challenged with developing innovative solutions that solve difficult and ambiguous real world issues.\\n\\n\\\\n\\\\n\\\\n\\\\n\\n\\nResponsibilities:\\n\\nAs a (Middleware / OS/ BSP) Software Engineer, you will:Work with Product Managers and architects to understand the requirements of the project and develop solutions that meet those requirements.\\nIdentify opportunities to increase the velocity of your efforts and the wider team.\\nParticipate in architectural and code reviews when necessary\\n\\nBasic Qualifications:Bachelor's degree in Computer Engineering, Electrical Engineering, Computer Science, or related\\n3+ years C/C++ software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n2+years' Java software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n2+ years experience software development experience for Android\\n1+ years experience with developing software leveraging Agile concepts such as SCRUM, SAFe, Kanban, etc.\\n\\nPreferred Qualifications:Master's degree in Computer Engineering, Electrical Engineering, Computer Science, or related\\n4+ years C/C++ software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n3+years' Java software development experience on embedded, infotainment platforms, mobile, or consumer electronic platforms\\n3+ years experience software development experience for Android\\n3+ years using Android Studio\\n2+ years experience with developing software leveraging Agile concepts\\nFamiliar with eXtreme Programming (XP) practices including:Pair/Mob programming\\nTest-first/Test Driven Development (TDD)\\n\\nIn-depth knowledge of C/C++ language on Unix based systems and/or Java\\nProficiency with revision control including Git, Subversion, or equivalent\\nExperience with Jira, Confluence, or equivalent\\n1+ years Multi-site software project team experience\\nFamiliarity with GitHub or equivalent source control repositories and Build Tools\\nHighly effective in working with other technical experts, Product Managers, UI/UX Designers and business stakeholders\\nCapable in Continuous Integration/Continuous Delivery tools and pipelines such as Jenkins, Maven, Gradle, etc.\\nExperience in development of microservices.\\nExperience with Continuous Integration/Continuous Delivery tools and pipelines\\nExperience with Jira, Confluence, or equivalent\\nExcellent communication skills\\n\\nJoin our team as we create tomorrow! We believe in putting people first, working together, and facing challenges head-on, because we're Built Ford Tough. We're one team striving to make people's lives better while creating value, delivering excellence and ultimately going for thewin.\\n\\nVisa sponsorship may be available for this position.\\n\\nFord Motor Company is an equal opportunity employer committed to a culturally diverse workforce. All qualified applicants will receive consideration for employment without regard to race, religion, color, age, sex, national origin, sexual orientation, gender identity, disability status or protected veteran status.\",\n",
       "  'Detroit, Michigan\\nSkills : .NET,Architecture,Java\\nDescription :\\n\\nOur client is looking for people that have been playing senior software design and software development roles.\\n\\n• Participate in architecture definition, product selection, and application design.\\n• Play key roles in large projects. Senior Developer. Mentor. Technical Leader. Technical Project Manager.\\n• Participate and provide leadership in all phases of a project from discovery and planning through implementation and delivery.\\n• Design and develop core business applications for insurance, healthcare and other clients.\\n• Wear many hats and gain experience with tools, technologies and platforms across many technology stacks.\\n• Work closely and share ideas with fellow Developers and Architects.\\n• Learn new things, and grow rapidly from constant exposure to innovative ideas, concepts, and patterns.\\n\\nQualifications:\\n\\n• Design and develop enterprise class business applications and integration solutions using Java/J2EE, .NET, and Javascript.\\n• Degree in computer science preferred\\n• Knowledge and experience using enterprise application, integration and design patterns\\n• Strong problem-solving skills\\n• Ability to conceptualize and articulate ideas clearly and concisely\\n• Excellent communication and interpersonal skills\\n• Willingness to travel on an occasional basis, as local and possibly out-of-state travel is required\\n\\n.NET,Architecture,Java',\n",
       "  'Are you ready to step up to the New and take your technology expertise to the next level?\\n\\nJoin Accenture\\nand help transform leading organizations and communities around the\\nworld. The sheer scale of our capabilities and client engagements, in\\nconjunction with the way we collaborate, operate and deliver value, provides an\\nunparalleled opportunity to grow and advance. Choose Accenture and make\\ndelivering innovative work part of your extraordinary career.\\n\\nPeople in our Client\\nDelivery & Operations career track drive delivery and capability\\nexcellence through the design, development and/ or delivery of a solution,\\nservice, capability or offering. They grow into delivery-focused roles, and can\\nprogress within their current role, laterally or upward. We partner with our\\nclients to help transform their data into an \\'Appreciating Business Asset.\\'\\n\\nAs\\npart of our Data Business Group, you will lead technology innovation for\\nour clients through robust delivery of world-class solutions. You will\\nbuild better software better! There will never be a typical day and\\nthat\\'s why people love it here. The opportunities to make a difference within\\nexciting client initiatives are unlimited in the ever-changing technology\\nlandscape. You will be part of a highly collaborative and growing network of\\ntechnology and data experts, who are taking on today\\'s biggest, most complex\\nbusiness challenges using the latest data and analytics technologies. We\\nwill nurture your talent in an inclusive culture that values diversity.\\nYou will have an opportunity to work in roles such as Data Scientist, Data\\nEngineer, or Chief Data Officer covering all aspects of Data including Data\\nManagement, Data Governance, Data Intelligence, Knowledge Graphs, and\\nIoT. Come grow your career in Technology at\\nAccenture!\\n\\nThe Azure Technical Architect Delivery is\\nresponsible for delivering Data On Cloud projects for Azure based deals. The\\nideal candidate would also be responsible for developing and delivering Azure cloud\\nsolutions to meet todays high demand in areas such as AIML, IoT, advanced\\nanalytics, open source, enterprise collaboration, microservices, serverless,\\netc. The Azure Data Engineer is a highly performant engineer responsible for\\ndelivering Cloud based Big Data and Analytical Solutions at our clients.\\nResponsibilities include evangelizing data on cloud solutions with customers,\\nleading Business and IT stakeholders through designing a robust, secure and\\noptimized Azure architectures and ability to be hands-on delivering the target\\nsolution. This role will work with customers and leading internal engineering\\nteams in delivering big data soltuions on cloud. Using Azure public cloud\\ntechnologies, our Data Engineer professionals implement state of the art,\\nscalable, high performance Data On Cloud solutions that meet the need of todays\\ncorporate and emerging digital applications\\n\\nRole & Responsibilities:\\nProvide subject matter expertise and hands on delivery of data capture, curation and consumption pipelines on Azure and Hadoop- Ability to build cloud data solutions and provide domain perspective on storage, big data platform services, serverless architectures, hadoop ecosystem, vendor products, RDBMS, DW/DM, NoSQL databases and security.- Participate in deep architectural discussions to build confidence and ensure customer success when building new solutions and migrating existing data applications on the Azure platform.- Conduct full technical discovery, identifying pain points, business and technical requirements, \"as is\" and \"to be\" scenarios.- Build full technology stack of services required including PaaS (Platform as-a-service), IaaS (Infrastructure as-a-service), SaaS (software as-a-service), operations, management and automation.- Apply Accenture methodology, Accenture reusable assets, and previous work experience to delivery consistently high-quality work.- Stay educated on new and emerging technologies/patterns/methodologies and market offerings that may be of interest to our clients.- Adapt to existing methods and procedures to create possible alternative solutions to moderately complex problems.- Understand the strategic direction set by senior management as it relates to team goals.- Use considerable judgment to define solution and seeks guidance on complex problems.- Primary upward interaction is with direct supervisor. May interact with peers and/or management levels at a client and/or within Accenture. Establish methods and procedures\\non new assignments with guidance.\\nManage small teams of deliver engineers\\nsuccessfully delivering work efforts\\n\\n(if in an independent contributor role) at\\na client or within Accenture.\\nExtensive travel may be required\\nBasic Qualifications\\nAt least 5 years of consulting or client service\\ndelivery experience on Azure\\nAt least 5 years of experience in developing data\\ningestion, data processing and analytical pipelines for big data, relational databases,\\nNoSQL and data warehouse solutions\\nExtensive experience providing practical direction within the Azure Native and Hadoop - Minimum of 5 years of hands-on experience in Azure and Big Data technologies such as Powershell, C#, Java, Node.js, Python, SQL, ADLS/Blob, Spark/SparkSQL, Hive/MR, Pig, Oozie and streaming technologies such as Kafka, EventHub, NiFI etc.\\nExtensive hands-on experience implementing data\\nmigration and data processing using Azure\\nservices: Networking, Windows/Linux virtual machines,\\nContainer, Storage, ELB, AutoScaling, Azure Functions, Serverless Architecture,\\nARM Templates, Azure SQL DB/DW, Data Factory, Azure Stream Analytics, Azure\\nAnalysis Service, HDInsight, Databricks Azure Data Catalog, Cosmo Db, ML\\nStudio, AI/ML, etc.\\nCloud\\nmigration methodologies and processes including tools like Azure Data Factory,\\nEvent Hub, etc.\\n5+ years of hands on experience in programming\\nlanguages such as Java, c#, node.js, python, pyspark, spark, SQL, Unix\\nshell/Perl scripting etc.\\nMinimum of 5 years of RDBMS experience\\nExperience in using Hadoop File Formats and compression techniques- Experience working with Developer tools such as Visual Studio, GitLabs, Jenkins, etc.\\nExperience with private and public cloud\\narchitectures, pros/cons, and migration considerations.\\nBachelors or higher degree in Computer Science or\\na related discipline.\\n\\nCandidate Must Have Completed The Following\\nCertifications\\nMCSA Cloud Platform (Azure) Training & Certification\\nMCSE Cloud Platform & Infratsructiure Training & Certification\\nMCSD Azure Solutions Architect Training & Certification\\nNice-to-Have Skills/Qualifications:\\nDevOps on an Azure platform\\nExperience developing and deploying ETL solutions\\non Azure\\nIoT, event-driven, microservices,\\ncontainers/Kubernetes in the cloud\\nFamiliarity with the technology stack available in\\nthe industry for metadata management: Data\\nGovernance, Data Quality, MDM, Lineage, Data Catalog etc.\\nFamiliarity with the Technology stack available in\\nthe industry for data management, data ingestion, capture, processing and\\ncuration: Kafka, StreamSets, Attunity,\\nGoldenGate, Map Reduce, Hadoop, Hive, Hbase, Cassandra, Spark, Flume, Hive,\\nImpala, etc.\\nMulti-cloud experience a plus - Azure, AWS, Google\\nProfessional Skill Requirements\\nProven ability to build, manage and foster a team-oriented\\nenvironment\\nProven ability to work creatively and analytically in a\\nproblem-solving environment\\nDesire to work in an information systems environment\\nExcellent communication (written and oral) and interpersonal\\nskills\\nExcellent leadership and management skills\\nExcellent organizational,\\nmulti-tasking, and time-management skills\\nProven ability to work\\nindependently\\n\\nAll of our professionals receive comprehensive training\\ncovering business acumen, technical and professional skills development. You\\'ll\\nalso have opportunities to hone your functional skills and expertise in an area\\nof specialization. We offer a variety of formal and informal training programs\\nat every level to help you acquire and build specialized skills faster.\\nLearning takes place both on the job and through formal training conducted\\nonline, in the classroom, or in collaboration with teammates. The sheer variety\\nof work we do, and the experience it offers, provide an unbeatable platform\\nfrom which to build a career.\\n\\nApplicants for employment in the US must have work\\nauthorization that does not now or in the future require sponsorship of a visa\\nfor employment authorization in the United States and with Accenture (i.e.,\\nH1-B visa, F-1 visa (OPT), TN visa or any other non-immigrant status).\\n\\nCandidates who are currently employed by a client of\\nAccenture or an affiliated Accenture business may not be eligible for\\nconsideration.\\n\\nAccenture is a Federal Contractor and an EEO and\\nAffirmative Action Employer of Females/Minorities/Veterans/Individuals with\\nDisabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to\\nage, race, creed, color, religion, sex, national origin, ancestry, disability\\nstatus, veteran status, sexual orientation, gender identity or expression,\\ngenetic information, marital status, citizenship status or any other basis as\\nprotected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed\\nor expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment\\nopportunities to our service men and women.',\n",
       "  'Description\\nSENIOR iOS SOFTWARE ENGINEER\\n\\nYou’re passionate about a zero-emissions future and want to build something special. You want to own your space, and partner with like-minded people on important projects. The pace doesn’t scare you, it EXCITES you.\\n\\nYOU WILL HAVE ACCOMPLISHED:\\n3+ years of Swift experience required\\nExperience and expertise with iOS and Swift from conceptual design, development, test, documentation, debug, validation, deployment and maintenance\\nPrefer some experience with Objective-C, Kotlin, Android, Ruby, HTML, JavaScript, CSS\\nYOU BRING:\\nA minimum of 3+ years Swift & some Objective-C experience\\nA college degree or equivalent experience in software-related fields\\nYou will be a Full Time Employee @ our new Phoenix Headquarters.\\nSoirry, No remote employees.\\n\\nMy name is PT, SR. Tech Recruiter, and if you apply, I will reply back to you! Thx…\\nDrive Forward! Introduce yourself to us and let’s start a conversation!',\n",
       "  'Description\\nWHAT WE’RE LOOKING FOR\\n\\nPeople who push themselves, work quickly and get things done, consider how their work impacts others, and treat the company and its resources as their own.\\n\\nTHE JOB AND ITS REQUIREMENTS\\nWork closely with our Chief Software Architect to improve performance and UX of Nikola Software.\\nExperience and expertise with Sass, Vanilla JS, ES6, CSS and HTML are required.\\nExposure to Ruby/Rails, Crystal, C, C++ and Python would be fantastic.\\nA college degree or equivalent experience in a software-related field and 4+ years of work experience.',\n",
       "  'SUMMARY:\\nThis position is responsible for providing test and turn-up expertise for all VoIP and Data products. To be considered for this position, a candidate must demonstrate a consistent ability to adhere to and promote the core competencies listed below:\\n\\nESSENTIAL DUTIES AND RESPONSIBILITIES:\\n\\nProvides service installation management for all Digital Voice customers.\\n\\nInteracts daily with complex SIP and Ethernet packet analysis to detect and fix call processing/registration issues.\\n\\nCreates, maintains, and updates internal documentation as necessary, including but not limited to process documentation, troubleshooting guides, Technician Rating Forms, modem report Card, etc.\\n\\nUtilizes internal systems to process trouble tickets with external carriers and internal resources.\\n\\nMaintains the acceptable standards of efficiency and quality established by the company in order to ensure customer satisfaction. Identifies opportunities for process improvement that will result in demonstrated increases in production quality, efficiency, or quantity.\\n\\nAdheres to all policies, practices, and standards of the Company and our clients to ensure good order and discipline on the production floor.\\n\\nAvailability to provide after-hours support via a rotating On-call program.\\n\\nIn addition to these essential duties and responsibilities, Activations Engineers should be able to handle large projects, escalations, process improvement ideas and be a resource development for the company as a whole. This also includes:\\nINSTALLATION SUPPORT:\\nManages the installation and configuration of CPE for VoIP and WAN connectivity to customers LAN including but not limited to:\\nSIP\\nINTEGRATED VOICE\\nHOSTED IP / VoIP INSTALLATION\\nDMARC EXTENSION\\nINSIDE WIRING\\nTROUBLE SHOOTING\\nIP PRI\\nLAN TOPOLOGY DISCOVERY\\nADVANCED BROADBAND INSTALLATIONS\\nFailover Devices (Cradlepoint, Skyus)\\nRouting Devices (Business grade LAN/WAN routers)\\nSecurity Hardware appliances and cloud-based Software (Managed Firewall\\nWireless Access Points\\nSD-WAN\\nGuide 3rd party Field Technicians through various work order activities.\\nCompletes VoIP Readiness & Service Verification Testing\\nBroadband Circuit Testing and Analysis\\nDOCSIS 3.0 Cable Modem transmission and receive level analysis\\nDSL Modem transmission and receive level analysis\\nRate Shaping and QOS prioritization\\nRemediation Execution\\nVoIP Testing Procedures\\nTest Wi-Fi networks as well as DECT wireless systems\\nProvides internal activations for “coordinated” circuit installs.\\nPerforms MACD type updates/changes within the BroadSoft switch platform\\nHandles the Self-Service Response Queue by answering incoming customer calls related to various self-installation issues such as Installation Walk-Through & Troubleshooting.\\nTrouble Resolution & Reporting:\\nPerforms Basic Trouble Shooting\\nApplies logical troubleshooting procedures to network services including logical fault isolation on network equipment, carrier circuitry, Customer Premise Equipment, customer wiring, and customer hardware / software applications.\\nRetrieves and analyzes SIP captures obtained from internal systems and/or Wireshark.\\nIdentifies and escalates “next level” problems when appropriate including but limited to:\\nNetworking and IP issues\\nTechnician Issues\\nWorks within a queue at a high rate of speed while building quality relationships with technicians.\\nWorks closely with various internal teams to understand & implement requests\\nCUSTOMER PREMISE EQUIPMENT CONFIGURATION\\nPerforms portal-based equipment configuration such as but not limited to:\\nCradlepoint\\nUbiquiti\\nAdtran\\nSophos\\nRemotely provisions customer premise equipment\\nObiHai\\nPanasonic\\nPolycom\\nNetgear\\nYealink\\nSYSTEM KNOWLEDGE\\nAbility to provision, test, and work in the following systems: “list is subject to change”\\n\\nQuickBase\\nBroadsoft/Metaswitch\\nTeam Viewer\\nField Nation\\nUbiquiti\\nPolycom ZTP\\nPanasonic ZTP\\nBullsEye Telecom is an Equal Opportunity Employer',\n",
       "  \"Position:\\nSoftware Engineer\\n\\nJob Type:\\nFull-time, Long Term Contract, W2\\nNo C2C\\n\\nLocation:\\nDetroit, MI\\n\\nAbout the Role:\\nOur Automotive Client in Detroit, MI is seeking a Software Engineer.\\nThey are looking for a Java Developer with experience with Spring Boot, Test Automation, and CI/CD experience.\\nYou will get to work on an Agile team at a Fortune 100 company and received exposure to exciting technologies like Kafka, ActiveMQ and Jenkins.\\n\\nTop 3 Skills:\\n• 3+ Years' relevant experience\\n• Familiar with CI/CD process implementation\\n• Java / Spring Boot debugging\\n• Able to work on test automation\\n\\nResponsibilities:\\n• Work with an agile team to develop, test, and maintain rest based applications.\\n• Assist in the development of user stories, and estimates.\\n• Develop and test applications in accordance with established standards.\\n• Develop functional tests for micro service systems.\\n• Package and support deployment of releases.\\n• Analyze and resolve technical and application issues.\\n• Work on automation tasks that help constantly improve on CI/CD processes\\n• Follow direction to integrate new technology where required\\n• Debug/triage spring-boot applications and cloud based services\\n• Setup builds with scheduler tools like Jenkins\\n• Triage on JMS systems like Kafka, ActiveMq as needed\\n• Develop utility applications that will facilitate automation processes\\n\\nSoft Skills:\\n• Energetic, team player\\n• Ability to take direction\\n• Willing to learn and continuously improve on existing process\\n\\nInterview Process:\\n• Video Interview - First round\\n• Face to face or Video Interview - Second round\",\n",
       "  'Job Description\\nMy client, a global auto manufacturer in Detroit, is seeking a Reporting/Data Analyst for a long-term contract opportunity. The Reporting/Data Analyst will gather and analyze business performance requirements as well as to develop monitors for key performance indicators.\\n\\nResponsibilities\\n\\n· Generate and publish reports for general use using approved BI Visualization Tools\\n\\n· Work with the Data teams to set up appropriate data structures/content for use in developing BI Reports\\n\\n· Create and maintain User Guide/Training Manuals\\n\\nQualifications\\n\\n· 2 - 4 years of overall experience in business/statistical analytics or reporting\\n\\n· Advanced communication skills with demonstrated ability to communicate clearly with charts, graphs, text, and oral presentations\\n\\n· Technically savvy; experience with SQL, Power BI, Tableau, Cognos, and/or other reporting/analytics software is required\\n\\nEmail resumes to cheryl.fisher@bluestonestaffing.com\\n\\nFind out more about blueStone Staffing at: https://bluestonestaffing.com.',\n",
       "  'About GameChanger:\\n\\nSports matter because they inspire leadership, teamwork, commitment, and confidence—critical life lessons that have the power to propel young athletes toward meaningful futures. GameChanger recognizes that without coaches, parents, and volunteers, organized youth sports could not exist. We celebrate those tireless heroes and make it our mission to help them do what they do best.\\n\\nGameChanger is a technology company providing team management, scorekeeping, and live fan experiences that keep athletes connected with the team community. With GameChanger, coaches and parents can organize and follow their athlete\\'s journey in a simple and powerful way.\\n\\nWe\\'re headquartered in downtown Manhattan and are proud to be part of the DICK\\'S Sporting Goods family.\\n\\nThe position:\\n\\nAs a Data Engineer you will work together in a cross-disciplinary team of Data Scientists, Production Engineers, and Data Engineers to evolve our data pipeline to be more resilient, extensible, and maintainable. You will also design and extend data models for a normalized data warehouse with terabytes of data to be turned into sports and customer insights. Our solutions benefit everyone including Product Managers, Developers, Data Scientists, Marketing, Executives, Coaches, Players, and Parents.\\n\\nThe role:\\nPartner with the data science team to provide the data and tools they need to solve complex problems\\nCollaborate across the engineering organization to develop database designs and queries\\nWork with product teams to get feature data emitted into the data pipeline, and engage with them to leverage insights to inform their roadmap\\nIdentify improvements in the flow of our data\\nEvaluate new technologies (both self-managed and services) for moving, transforming, modeling, and storing data and make recommendations\\nImprove ETL efficiency\\nWork with Engineers in other parts of DICK\\'S Sporting Goods to design data flows across businesses\\nDesign tools for compliance with data protection laws\\nWhat we\\'re looking for:\\nBachelor\\'s degree (or equivalent) in computer science or a related field\\nSoftware development experience, preferably as a data or backend engineer\\nExperience with Scala, Python, or Typescript\\nFamiliarity with Apache Kafka\\nComfort and excitement working across multiple languages, databases, paradigms, etc\\nEagerness to learn about a range of technologies.Our teams use a variety of languages, databases, and technologies to accomplish their objectives, so we\\'re always learning and sharing\\nThe Perks:\\nBe part of a diverse team of genuinely kind and interesting individuals dedicated to improving the lives of our customers.\\nBenefits include medical, vision, prescription, dental, FSA/HRA, and coverage for family/dependents.\\nGenerous maternity / paternity leave policy\\nWell-furnished, modern office\\nCommuter Benefits\\nEndless supply of snacks and drinks\\nLearning tools including Safari Books Online, opportunities to join clubs, hack days, and conferences\\nOrganized lunches, happy hours, and team outings\\nThe Culture:\\n\\nAt GameChanger we are…\\n\\nCustomer-Obsessed\\nWe put our customers first, always asking, \"Will this enhance their youth sports experience?\"\\nWe deeply understand our customers and work hard to anticipate their needs.\\nWe celebrate and support our customers\\' contributions to their communities.\\nTeam Players\\nWe listen actively, seeking to understand others before making ourselves heard.\\nWe challenge each other directly while caring about each other personally.\\nWe do unglamorous work in service of the team.\\nAmbitious\\nWe set aggressive goals designed to drive meaningful change.\\nWe are eager teachers and curious students who invest in learning.\\nWe are resilient, thriving in change and adversity.\\nEmpowered\\nWe value creative ideas and productivity over seniority and hours worked.\\nWe entrust decisions to those best positioned to make them.',\n",
       "  \"A Better company\\n\\nWe believe homeownership should be an option for every American. The way the mortgage industry operates today makes the journey towards homeownership extremely complex – so we set out to change that. The traditional mortgage process is designed to confuse, riddled with unnecessary fees, takes too much time, and is built on a foundation of misaligned incentive structures. It's time Americans had a Better option.\\n\\nA Better option is one that puts customers in control of the largest financial transaction of their lives. It's built with best-in-class technology, supported by non-commissioned staff, and offers affordable financing options that meet customers where they are.\\n\\nSince 2016, we've already funded over $1B in loans, raised $75M in capital, and won the NerdWallet Best Online Mortgage Lender for Customer Service and we're just getting started.\\n\\nTrying to modernize a decades old industry isn't easy, but it is supremely rewarding. Become part of a Better team.\\n\\nA Better opportunity\\n\\nHelp us hack a thirteen trillion dollar industry by building a product that will allow more people than the status quo to own a home and build wealth rather than rent for life. Our tech team is small, and you will be a big part of defining the technical direction and culture. We encourage proposals for projects off the beaten path, experimentation with different frameworks and libraries, and doing as you see fit to solve problems. We also offer above-market compensation and equity, as well as full benefits.\\n\\nSome projects you could be working on\\nWork closely with our product team to understand funnel drop off and come up with product ideas\\nWork closely with the marketing team to optimize our acquisition funnel\\nPresent conclusions to the executive team that can impact the strategic direction of the company\\nBuild a lead scoring model to help our customer support team prioritize.\\nModel the time-lag of conversions using fun math like Gamma distributions\\nDesign an experiment to understand the causal impact of an outbound phone call on conversion rates\\nBuild web scrapers to track price data for other mortgage lenders\\nMigrate our data warehouse to Redshift\\nWork on our underwriting engine, which turns out to be NP-complete and can be posed as a mixed integer programming problem\\nTranscribe all our phone calls using speech-to-text and figure out ways to optimize customer support.\\nBetter Technology\\nWe do continuous deployment and we ship code 50-100 times every day\\nThe data stack is all in Python 3.6\\nWe use Node.js, Python and Scala for services\\nPostgres for the database\\nKubernetes, for deployment and devops\\nAWS for infrastructure, leveraging EC2, S3, SWF, CloudFront, Route53, and much more\\nThe team\\nThe tech team is currently 30 engineers but growing quickly\\nErik Bernhardsson (CTO) used to run the data team and the music recommendation team at Spotify. He is the open source author of a few popular projects like Annoy and Luigi and writes a blog about (mostly) data\",\n",
       "  \"Come and be part of a great Audible team on the East Coast! We take learning, innovation, and teamwork to the next level to accomplish superior results. We are a team that cares about your work-life balance and harmony, while challenging you to solve problems at Amazon scale. You will be part of a strong agile team where innovation and inventions are encouraged. You will have broad visibility and direct access to product, business, and tech leadership teams. You will build software that will improve peoples lives and experience with Audible, where inspiring voices bring stories, ideas, and characters to life.\\n\\nWe have opportunities available on teams doing front end engineering, web or mobile apps development, services and data engineering, software engineering for machine learning and more. Come and talk to us to know more about this exciting opportunity and see if we're indeed a good match for you.\\n\\nABOUT YOU\\nYou are an innovative and passionate software engineer looking to make a positive impact on customer experience. You like to own deliverables end-to-end and have a meaningful influence on the final product. You are a builder. You like to work with fellow engineers and product people to share knowledge, imagine, design, develop, test, and launch software that wows our community and inspires our peers. You're curious and love to learn unprompted as you stay up to date with tools, trends, technologies and best practices in the industry. Please don't hesitate to apply!\\n\\nHOW DOES AMAZON FIT IN?\\nWe're a a strategic and growing part of Amazon, our parent company. Audible's built on Amazon technology and you'll have insight into the inner workings of the world's leading ecommerce experience. You'll get to play with all of Amazon's and AWS technologies. There's a LOT to learn! Your career will benefit from working with teams like Kindle, A9, P13N and many more.\\nIf you want to own and solve problems, work with a creative dynamic team, fail fast in a supportive environment whilst growing your career and working on high volume (2000+ tps) services that support millions of customers worldwide, we want to hear from you.\",\n",
       "  \"WHO WE ARE\\nLooker is on a mission to bring better insights and data-driven decisions to every business. Everything we do is aimed at making sure our customers love every aspect of Looker, from our products and technologies to our ease of doing business and our support. We are looking for curiously brilliant individuals to join our team as we reinvent data analytics. Get data-driven and see yourself at Looker.\\n\\nWHAT WE'VE GOT GOING ON\\nLooker is searching for a Data Engineer with experience in implementing modern data architectures. With an exciting new round of funding and a new team, you will be a key owner in bringing our infrastructure to the next level. You will be working on one of the biggest greenfield opportunities at Looker: A major overhaul of our data, pipeline, and security architecture, as well as our modeling layer. Looker’s amazing success is just getting started and you will be working for a company that truly values the power of data. Your first projects will include helping to scale our data infrastructure and optimizing our warehouse performance and footprint. You will be working with other engineers and analysts on the Data & Analytics team to support both our internal warehousing efforts and prototyping new best practices for our Professional Services and Customer Love department.\\n\\nWHAT WE NEED YOU TO DO\\nPartner on the design and implementation of our secure, global data architecture.\\nExecute engineering tasks with maturity in variety of languages including JVM-based, ruby, and other scripting languages.\\nOwn medium-to-large size data engineering projects.\\nMentor junior staff by providing opportunities to execute on your engineering agenda.\\nOwn optimization and monitoring projects for our Massively Parallel Processing (MPP) Database and Pipeline technologies.\\nSet standards and create documentation for self-serve data pipeline services supporting core engineering and professional services use cases.\\nWork with a variety of AWS, GCP, and Azure technologies.\\nWork with stream and queue-based solutions.\\nWHAT YOU BRING TO LOOKER\\n5+ years experience with Data Warehouse Systems.\\nJava and Python experience in production.\\nFamiliarity with Spark and/or MapReduce.\\nStrong familiarity with batch processing and workflow tools such as Airflow, NiFi, Azkaban.\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done.\\nFamiliarity with modern BI and exploration tools.\\nFamiliarity with Linux systems.\\nSome familiarity with streaming approaches preferred.\\nCS Degree preferred.\\nSome experience preferred with Node, Ruby, or Scala.\\n#LI-SK1\\n\\nA LITTLE MORE ABOUT LOOKER\\nLooker is a unified Platform for Data that delivers actionable business insights to every employee at the point of decision. Looker integrates data into the daily workflows of users to allow organizations to extract value from data at web scale. Over 1700 industry-leading and innovative companies such as Sony, Amazon, The Economist, IBM, Spotify, Etsy, Lyft and Kickstarter have trusted Looker to power their data-driven cultures. The company is headquartered in Santa Cruz, California, with offices in San Francisco, New York, Chicago, Boulder, London, Tokyo and Dublin, Ireland. Investors include CapitalG, Kleiner Perkins Caufield & Byers, Meritech Capital Partners, Redpoint Ventures and Goldman Sachs. For more information, connect with us on LinkedIn, Twitter, Facebook and YouTube or visit looker.com.\\n\\nLooker aspires to be a workplace that is not only free of discrimination, but one that fosters inclusion and belonging. We strongly believe that diversity of experience, perspective, and background lead to a better environment for our employees and a better product for our users. We encourage you to join us in changing the way businesses use data.\\n\\nFor information on how Looker uses your information, visit Looker's Privacy Policy.\",\n",
       "  'When was the last time you were planning a business trip and really tried to save your company money? If your company allowed you to stay in a fancy hotel, would you ever volunteer to stay at an Airbnb or at a friend\\'s house? How about flying coach instead of business class? The vast majority of employees optimize for comfort and convenience, spending at the high end of their company policy limits, because, well, why not? So how can a company get its employees to care about expenses without implementing draconian policies, creating friction and frustrating employees? How can a company motivate its employees to save?\\n\\nThe answer is Rocketrip. We\\'re a NYC-based startup that rewards business travelers for cost-sensitive behavior. It\\'s a win-win: companies save, while employees cash in with real rewards.\\n\\nThe Role:\\n\\n\\nWe are seeking a Data Engineer who can operate within our engineering organization and help take our data strategy to the next level. The Data Engineer will be able to design, code and provide architecture solutions for the team, including but not limited to ETL, data warehousing, and data integration. The right candidate for this role is someone who is passionate about technology and interacting with product owners, thrives in ambiguity, and is focused on delivering exceptional results with great teamwork skills in a scrappy, startup environment. The candidate will have the opportunity to influence and interact with fellow engineers beyond their team.\\n\\nResponsibilities:\\n\\n\\nDesign and development of ETL and data pipeline solutions for complex business problems to load Data Warehouse\\nData Stewardship - own or support the data definitions and lineage across our organization.\\nCreate a data integration plan and build data integrations between systems.\\nFigure out the best way to share information and build the tech needed to execute.\\nMentoring - help teach other team members about data architecture and also be a consultant for developers who need help with data.\\n\\nRequirements:\\n\\n\\nAt least 3 years of relevant experience.\\nExperience working with data warehouses, including data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools and environments like RDS, MySQL, Python, Pyspark, Airtable, Talend.\\nExperience developing, deploying, and testing in AWS\\n\\nHere at Rocketrip, we...\\nAre in growth mode where all work has impact.\\nOffer great benefits, including medical, dental and optical.\\nGive all employees free membership to One Medical.\\nProvide access to a 401k plan and offer matching.\\nBelieve it\\'s important to rejuvenate and offer a \"take what you need\" vacation policy.\\nEncourage employees to spend the holidays exploring, relaxing, or with loved ones by closing our offices during the last week of December\\nRegularly huddle up as a company to share goals, learnings and celebrate!\\nHave a dog-friendly office.\\nProvide access to gym membership and Citibike discounts.\\n\\nFounded in 2013 and headquartered in New York City, Rocketrip is aiming to revolutionize business travel by introducing the motivation to save. We\\'re a group of tech innovators who looked at the current state of business travel, became frustrated by the antiquated employee and employer experiences, and decided to do something big about it. Our team is focused on utilizing technology, design and data to align employee and company interests.\\n\\nRocketrip is backed by a renowned set of investors and advisors, including Google Ventures, Bessemer Venture Partners, Canaan Partners, Genacast Ventures, and Y Combinator.',\n",
       "  \"MongoDB is growing rapidly and seeking a Data Engineer to be a key contributor to the overall internal data platform at MongoDB. You will build data driven solutions to help drive MongoDBs growth as a product and as a company. You will take on complex data-related problems using very diverse data sets.\\n\\nWho?\\n\\nYou have experience with:\\nseveral programming languages (Python, Scala, Java, etc..)\\ndata processing frameworks like Spark\\nstreaming data processing frameworks like Kafka, KSQ, and Spark Streaming\\na diverse set of databases like MongoDB, Cassandra, Redshift, Postgres, etc.\\ndifferent storage format like Parquet, Avro, Arrow, and JSON\\nAWS services such as EMR, Lambda, S3, Athena, Glue, IAM, RDS, etc.\\norchestration tools such as Airflow, Luiji, Azkaban, Cask, etc.\\nGit and Github\\nCI/CD Pipelines\\nAlso\\nEnjoy wrangling huge amounts of data and exploring new data sets\\nValue code simplicity and performance\\nObsess over data: everything needs to be accounted for and be thoroughly tested\\nPlan effective data storage, security, sharing and publishing within the organization\\nAre constantly thinking of ways to squeeze better performance out of the pipelines\\nBonus Points\\nYou are deeply familiar with Spark and/or Hive\\nYou have expert experience with Airflow\\nUnderstand the differences between different storage format like Parquet, Avro, Arrow, and JSON\\nUnderstand the tradeoffs between different schema designs like normalization vs denormalization\\nIn addition to data pipelines, you're also quite good with Kubernetes, Drone, and Terraform\\nYou've built end to end production grade data solutions that run on AWS\\nHave experience building ML pipelines using tools likeSparkML, Tensorflow, Scikit-Learn, etc.\\nWhat?\\n\\nAs a Data Engineer, you will:\\nBuild large-scale batch and real-time data pipelines with data processing frameworks like Spark on AWS\\nHelp drive best practices in continuous integration and delivery\\nHelp drive optimization, testing and tooling to improve data quality\\nCollaborate with other software engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day\\n*MongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws*\",\n",
       "  \"FanDuel Group is an innovative sports-tech entertainment company that is changing the way consumers engage with their favorite sports, teams, and leagues. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media, including FanDuel, Betfair US, DRAFT, and TVG. FanDuel Group has a presence across 45 states and 8 million customers. The company is based in New York with offices in California, New Jersey, Florida, Oregon, and Scotland.\\n\\nOur competitive edge comes from making decisions based on accurate and timely data. As a Data Engineer, you will help us build scalable systems to provide access to that data across the company.\\n\\nWhat we're looking for\\n\\nWe are looking for an experienced Data Engineer, ideally well versed in Python, with a deep understanding of large scale data handling and processing best practices in a cloud environment. You should be comfortable building complex yet performant SQL queries on large data sets. Our current stack is built on AWS with Spark and Hive on Amazon EMR for batch processing and Redshift for the data warehouse. Experience working with and tuning these for large scale workloads would be a plus.\\n\\nData is a key component of the business used by almost every facet of the company including product development, marketing, operations and finance. It is vital that we deliver robust solutions that ensure reliable access to data with a focus on quality and availability. We operate a rigorous code review process, so you need to be able to continuously give and take feedback and act on it.\\n\\nAs our data is always growing it is important that we have a cost effective data warehouse with data that is modelled to suit our users needs.\\n\\nLooking ahead to the next phase of our data platform we are keen to do more more with real time data processing and working with our data scientists to create machine learning pipelines. We would love to hear how you have tackled these before.\\n\\nWhat you get in return\\n\\nBeyond working with such a great team?\\nAn exciting environment with real growth\\nContribute to exciting products used by a highly passionate user base\\nPersonal learning and development opportunities\\nFlexible holiday allowance\\n401K plan with company match\\nAttractive health insurance premiums\\nThere's more, but we don't want to go on and on.\\n\\nFanDuel is an equal opportunities employer. Diversity and inclusion in FanDuel means that we respect and value everyone as individuals. We don't tolerate bias, judgment or harassment. Our focus is on developing employees so that they reach their full potential.\",\n",
       "  \"Lancer Insurance Company is looking for a Data Engineer to develop, maintain, test and evaluate data solutions in support of business goals. The person will also develop data models, corresponding data architecture documents and API’s. The right candidate should be an excellent communicator and strategic thinker.\\nDuties and Responsibilities\\nCreate, design and maintain reusable datasets for analysis by data scientists.\\nAssess new data sources to better understand availability and quality of data.\\nProvide governance and best practices of data structures, data integrity, and querying.\\nInterpret business needs from requests, and rapidly implement effective technical solutions.\\nDesign, implement and enhance ETL (extract, transform and load) processes.\\nWrite SQL queries to answer questions from stakeholders.\\nMaintain source code repository of scripts (SQL, Python, R) and other data products (dashboards, reports, etc.).\\nWork with technology teams (BA,QA, Dev and Admin) to understand data capture and testing needs.\\nAutomate and improve creation/maintenance of reports and dashboards.\\nSkills & Experience\\nBA/BS or Master's degree with emphasis on coursework of a quantitative nature (e.g., Statistics, Computer Science, Engineering, Mathematics, Data Sciences).\\nAdvanced SQL and relational databases including queries, database definition and schema design.\\nPython or R experience required.\\nWriting and maintaining ETL on a variety of structured and unstructured sources.\\nExperience sourcing data via REST web services.\\nExcellent written and verbal communication skills.\\nMicrosoft SQL Server, SQL Server Integration Services (SSIS), Business Intelligence Development Studio (BIDS), Excel (pivot tables).\\nInsurance experience a plus.\",\n",
       "  \"We're seeking an experienced Engineer to join our growing data team. In this role you'll build, scale, maintain and improve the core of our data pipeline application systems. You will be instrumental in designing a sustainable and efficient architecture for our event tracking and data warehouse. In addition you will be working closely with our product engineers and analytics team to productionalize our data infrastructure and integrity. We have a ton of interesting data to explore and are looking for someone who's excited to help us ask the right questions, dig deep into the answers, and build solid systems that address them.\\n\\nA little about us\\n\\nThe Paperless Post Data team plays a crucial role in our product's success. We get to dive into advanced data work—from machine learning to visualization, classifier creation to ETL pipelines—that solve complex business problems and increase the functional capabilities of everyone throughout the company. On the reporting side, we help ensure that all teams have the information they need to make sound business decisions. Above all, we're a team that is excited about what we do.\\n\\nWhat you'll do here\\nDesign, build, and maintain production code that automates data collection with our applications. Languages used include Ruby, Javascript, Go, Node, and Python.\\nWork closely with other engineers and stakeholders to organize and prioritize your efforts for performance, stability, and clean code.\\nStay current on technology trends and participate in internal tech talks, hack days, and learning sessions.\\nBecome the best child/spouse/sibling/friend because you'll send more greeting cards than you ever thought possible.\\nAbout you\\n\\nThe ideal candidate will have a good blend of programming, analysis, and communication skills. We're not expecting you to have everything on this list, but here are some things that spark our interest:\\nKnowledge of Ruby and one (or more!) of: JavaScript, Golang, Node, Python, React, AWS, Google Cloud.\\nA solid understanding of both relational and NoSQL database technologies.\\nKnowledge of systems integration using HTTP and Queues; processing high throughput streams of near real-time data is something that you have already done or are very interested in.\\nExperience building, integrating, and productionizing event measuring tools at scale.\\nAn eagerness to embrace an engineering culture that values testing, code review, and leaving things better than the way you found them.\\nPassion for product quality and attention to detail.\\nThe ability to be opinionated but not dogmatic.\\nComfort backing ideas up with logic and data.\\nCompany-wide we enjoy an amazing ecosystem of an even gender split and healthy balance of engineers and designers. Because Paperless Post isn't supported by ad revenue, we get to spend our days focused on creating and improving on the ideal version of our platform, product, content, and partnerships for our users.\\n\\nWe are proud that Paperless Post helped over 30 million people connect in the real world last year. Our product is global, and we are committed to being a company where everyone belongs. We encourage people of all backgrounds, races, genders, and abilities to apply!\",\n",
       "  \"Our Vibe:\\n\\nIf you have a passion for travel & hospitality, you’ve come to the right place! Domio is a technology platform focused on the travel sector, encompassing real estate, hospitality, and design with an emphasis on social connections. Delighting our customers is in our DNA and we’re upping the ante on group travel with curated, end-to-end consumer experiences. We’re all about the hustle AND having fun. The Domio culture embraces innovative, bright, and talented professionals eager to make their mark on these ever-changing industries. Excited yet? Keep on reading.\\n\\nWhat We're Looking For:\\n\\nDomio's focus is to provide our guests with an incredible experience in dynamic locations powered by world class engineering and data science.\\n\\nAs a Data Engineer you will help develop the underlying data infrastructure for collecting, processing, and generating insights from millions of properties across the world and using those insights to automate a next generation hospitality experience.. You will have a lot of autonomy and influence on the technical direction and the ability to work on greenfield projects.\\nAt Domio You Will:\\nIntegrate with different providers to pull data from millions of listings across the world in order to create a realtime view of the rental space\\nBuild out the data warehouses and pipelines to support the expansive growth of our organization\\nImplement and test data mining / data science algorithms in a variety of hosted settings, such as AWS\\nTranslate business analytics problems into technical approaches that yield actionable recommendations for pricing and investment decisions\\nCommunicate results and mentor junior engineers\\nWho You Are:\\nBS/MS/PhD in Computer Science or related field\\n2+ years industry experience\\nFluency in Python, SQL, and relational database design\\nAbility to code, test, and document an ETL process from source data through to a warehouse\\nExperience with building algorithms and machine learning models\\nExperience with AWS, Google Cloud Platform, Redshift, and/or BigQuery\\nExperience with distributed computing frameworks (Hadoop or Spark)\\nExperience developing stream-based data processing pipelines (e.g. Kafka) or with messaging systems such as RabbitMQ, Celery\\nStrong initiative; ability to drive forward projects independently\\nDesire to work with a team and mentor junior engineers\\nTech Stack:\\nPython / Flask\\nNode\\nReact\\nPostgres\\nBigQuery\\nKubernetes\\nGoogle Cloud Platform\\nOur Perks:\\n\\n• Competitive salary\\n• Medical, dental, vision, life insurance and more\\n• 401(k)\\n• Unlimited vacation and flexible work-from-home policy\\n• Travel discount when booking Domio properties\\n• Learning & education budget, free books, and classes\\n• Fitness stipend\\n• Catered lunches every Friday\\n• Team bonding and offsite events\\n• Dog-friendly office\\n\\nDomio is an equal opportunity employer!\",\n",
       "  'Company Description\\nSagence is a management advisory firm dedicated to helping our clients optimize the value of their data assets. From thinking to doing, Sagence works with leading institutions in the acquisition, evaluation, development and management of their critical data assets and in the application of analytics to discover new insights, shorten time-to-value, and drive competitive advantage.\\nJob Overview\\nSagence is looking for experienced, client-facing Data Engineers to help us build and sustain our client’s data capabilities and our competitive advantage. Along with the requirements below, candidates must possess deep, practical experience in one or more of the following Data Management competencies; data warehousing, data lakes, reference data or master data management, data architecture, data modeling, data governance, data analysis, business intelligence.\\nSkills & Requirements:\\nMUST BE HANDS ON. Candidates should have system development experience and proficiency with system development methodologies and possess the following skills and knowledge:\\nMust have significant hands-on experience with various IT concepts of data management/engineering (ETL, data modeling, data warehousing, etc.)\\nMust have significant hands-on experience with SQL, data profiling, and data discovery\\nExperience building business intelligence, analytics, or reporting solutions - either front-end consumption mechanisms (e.g., Microsoft, Tableau & Qlik) or supply of data for these purposes\\nFamiliarity with data architecture principles/approaches, data environment infrastructure considerations, and data modeling principles/approaches\\nAbility to drive out technical requirements with business and IT stakeholders for implementations of data solutions\\nHands-on experience with Agile delivery methodology\\nPrior professional experience in an IT management, management consulting, or client facing role is preferred\\nKnowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred\\nDemonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions\\nTools and Technology:\\nProficient at leveraging tools and technology to drive value for clients. Examples include the following;\\nDatabase Management Tools:\\nRelational – e.g. Oracle, MySQL, Microsoft SQL Server, PostgreSQL, DB, or similar\\nNoSQL – e.g. MongoDB, Couchbase, DataStax, Redix, MarkLogic, or similar\\nCloud – e.g. AWS, Azure, xxx, xxx, xxx\\nETL Tools - e.g. Informatica, Talend, Microsoft SSIS, or similar\\nData Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar\\nIndustry Leading BI tools - e.g., Business Objects, Microsoft, Cognos, Tableau, OBIEE, Qlickview, or similar\\nGeneral:\\nMust be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas\\n3+ years of professional experience working in a related role\\nMust be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude\\nStrong desire to work on interesting projects with smart and creative people\\nWillingness to travel to client sites as needed\\nChicago or New York area candidates preferred, but will consider candidates in other parts of U.S.\\nOur Culture\\nPassionate, diverse, creative, genuine, flexible, hands-on…these are just a few of the words that describe our culture. Our Partners are deeply involved in the client work on a daily basis. We have a high-energy workplace with a focus on producing high-quality, impactful results. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We build teamwork through small, dedicated teams who continuously teach each other and learn from one another. We strongly believe these characteristics enable our employees to develop to their fullest potential. To learn more, please visit us at www.sagenceconsulting.com',\n",
       "  \"Medidata: Conquering Diseases Together\\n\\nWHAT WE'RE LOOKING FOR\\n\\nMedidata’s Data and Analytics Group is seeking an engineer that is passionate about working with clinical data. We’re looking for someone who is excited by data and the value that can be found in it, and who has the skills and drive to deliver value from it. You will be a key contributor in growing and expanding our clinical data platform by making our data assets usable and accessible to platform consumers.\\n\\nWe work with a variety of cutting edge as well as industry standard techniques and tools. We are looking for candidates that can both support existing technologies and apply new techniques to solve the next generation of data issues that we will encounter as we extend and enlarge data platform.\\n\\nWHAT YOU’LL DO\\n\\nIn this role, you will be responsible for data acquisition from source systems, transformation, standardization, and delivery to enterprise repositories and systems. You will work with product teams to understand business requirements. Using your understanding of clinical data, you will define the acquisition, transformation and delivery needs, will lead efforts to understand and design for production data workloads and shape, and will work with our architecture team to deliver solutions that are aligned with enterprise architecture plans.\\n\\nYou will deliver solutions that work within a DevOps delivery pipeline using infrastructure designs that are scalable, resilient and highly performant. You will develop mapping and testing artifacts that enable data movement solutions to be verified as meeting functional, non-functional and business requirements. Your mission will be to deliver efficient and error free data movement systems and processes.\\n\\nWHO YOU ARE\\n\\nRequired Skills:\\nYou hold at least a bachelor’s degree in Computer Science or a related discipline\\nYou have at least 3 years’ experience working with large and complex data sets\\nYou have at least 6 years experience working with clinical data (e.g. clinical trial data, lab data, EMR data, etc.)\\nYou are a proficient Java developer\\nYou are a proficient SQL developer\\nYou have experience writing ETL/ELT code\\nYou have experience with data profiling tools and concepts\\nNice to Have:\\nExperience with build frameworks such as Maven, Gradle or Ant\\nAgile experience\\nExperience working with Message Bus technologies\\nAny combination of these AWS Technologies:\\nEC2 w/ EBS\\nStep Functions\\nAurora\\nRedshift\\nKinesis\\nEMR\\nECS/ECR\\nPrior experience working with ETL systems (e.g. Pentaho, Talend, Informatica, etc.)\\nOracle PL/SQL experience\\nMedidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or status as a veteran. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.\\n\\n#LI-AS1\",\n",
       "  \"About Foursquare:\\n\\nFoursquare is the leading independent location technology platform, powering business solutions and consumer products through a deep understanding of location. Foursquare's business solutions include Pilgrim SDK, Places API, Analytics, Placed powered by Foursquare, and Pinpoint. Together, these products empower brands to analyze trends; measure foot traffic lift; optimize advertising campaigns; and drive deeper engagement via Foursquare's industry-leading developer tools, which have been selected by 150,000 developers including AccuWeather, Apple, Samsung, Microsoft, Snapchat, Tinder, TripAdvisor, Twitter and Uber. Our toolkit also includes our consumer apps Foursquare City Guide and Swarm. Over the past 10 years, we've counted more 13 billion verified signals from people around the world, helping us to keep our dynamic map and models fresh and up-to-date.\\n\\nAbout our Engineering Team:\\n\\nAs a member of Foursquare's engineering team, we want you to bring experience building real products from the ground up. We're passionate about tackling tough challenges in the location space and look for others who like to dive deep into code and help solve hard problems. You should be comfortable running with your own ideas and eager to learn new skills on a bleeding edge platform. We use a variety of tools, technologies, and languages to build software (Scala, Thrift, MongoDB, Memcached, JS/jQuery, Kafka, Pants, Hadoop, MR, Spark) but experience with equivalent ones will do just fine.\\n\\nJoin us and help bring our ideas (and your own!) off the whiteboard and into reality. You'll be a key member of our Attribution team, building a system that builds hundreds of machine learning models per day at scale to drive marketing decisions for many well-known companies. You'll build resilient services and tooling which drive all of our processing of petabytes of data\\n\\nResponsibilities:\\nDevelop and maintain our data pipelines using Hadoop, Scalding, Luigi, Spark, Mongo and more\\nPartner with the Data Science team to investigate and implement advanced statistical models and machine learning pipelines\\nIdentify and implement performance improvements across all pipelines\\nData investigations to validate assumptions or find the source of a problem\\nAssist client support and sales with client integrations\\nQualifications:\\n3+ years of experience working with Hadoop MapReduce and/or other big data technologies and pipelines\\nYou have a solid foundation in computer science fundamentals with particular expertise in data structures, algorithms, and design\\nYou obsess over data: everything needs to be accounted for and be thoroughly tested\\nYou are constantly thinking of ways to squeeze better performance out of the pipelines\\nStrong Java or other object-oriented programming experience or, even better, experience and/or interest in functional languages (we use Scala!)\\nExperience with Scala, Scalding, Luigi,Hive, machine learning pipelines and model training is a plus\\nBachelors Degree or higher in Computer Science, Electrical Engineering or related field\\nFoursquare is proud to foster an inclusive environment that is free from discrimination. We strongly believe in order to build the best products, we need a diversity of perspectives and backgrounds. This leads to a more delightful experience for our users and team members. We value listening to every voice and we encourage everyone to come be a part of building a company and products we love.\\n\\nFoursquare is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected Veteran status, or any other characteristic protected by law.\",\n",
       "  'The Role:\\nBy market definition, a data engineer is responsible for creating, maintaining and understanding data and the resulting delivery infrastructure. They are the connection between smart business users and not-so-smart data repositories. They are capable of taking any source of data and performing an EVL(ST) Extract, Validate, Load, Standardize, transforming to the correct data store, in the form agreed upon by the Data Engineer and the end-user. Data Engineers are often responsible for the efficacy, quality and elegance of their solutions. They are business savvy and understand the importance of the data they are piping in.\\nResponsibilities\\nContribute to the design and development of our Python data workflow management platform\\nDesign and develop tools to wrangle datasets of small and large volumes of data into cleaned, normalized, and enriched datasets\\nBuild and enhance a large, scalable Big Data platform (Spark, Hadoop)\\nRefine processes for normalization and performance-tuning analytics\\nAbout you\\nYou love building elegant solutions that scale\\nYou bring deep experience in the architecture and development of quality backend production systems, specifically in Java\\nYou love working on high-performing teams, collaborating with team members, and improving our ability to deliver delightful experiences to our clients\\nYou are excited by the opportunity to solve challenging technical problems, and you find learning about data fascinating\\n\\nMust Have\\n5+ years of full-time experience in a professional environment\\nExpertise in core Java, some Python experience is an advantage\\nExperience with data pipeline development, ETL and/or other big data processes\\nExperience working in a cloud-based environment, such as GCP or AWS\\nExperience with at least 2 popular big data / distributed computing frameworks, eg. Spark, Hive, Kafka, Map Reduce, Flink\\nExperience working independently, or with minimal guidance\\nStrong problem solving and troubleshooting skills\\nAbility to exercise judgment to make sound decisions\\nProficiency in multiple programming languages\\nStrong communications skills, interpersonal skills, and a sense of humor\\nBS degree or higher in a technical discipline\\n\\nEven Better\\nData skills: RDBMS SQL and NOSQL, structured and unstructured data, BigQuery\\nProficiency in Jupyter, C24; familiarity with ETL, CDC, and workflow tools\\nAbout Crux:\\nCrux is a data delivery and operations company that takes on the critical, yet commoditized, tasks of ingesting data and getting it ready for analysis by financial institutions. We are experiencing rapid growth from financial firms turning to Crux to process and onboard the data they need.\\n\\nCrux is transforming how the financial services industry works with data. For years, data discovery, operations, and delivery have been expensive, time consuming, and frustrating for financial firms. We are solving this challenge by creating a new technology solution: the go-to platform for data delivery. Crux is in high demand and we have a clear path to success.\\n\\nFounded in 2017 by a seasoned team of data professionals, Crux has grown to a team of 70 employees in San Francisco and New York. It is backed by leading industry players, including Citi, Goldman Sachs, and Two Sigma.\\n\\nAt Crux, diversity is valued and and treatment of employees and applicants are based on merit, talent and qualification. We encourage people from underrepresented groups to apply. We believe the key to success is bringing together unique perspectives and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. For qualified applicants with criminal histories, consideration will be consistent with the requirements of the San Francisco Fair Chance Ordinance. All your information will be kept confidential according to EEO guidelines.',\n",
       "  'About Us:\\n\\nRent the Runway is transforming the way modern women get dressed and disrupting the $2.4 trillion global fashion industry by enabling women to rent, versus buy, clothing. Founded in 2009 with a vision to build the world\\'s first living closet, RTR believes that women everywhere will soon have a subscription to fashion. Rent the Runway has pioneered a new industry by transforming the traditional model of clothing consumption, making apparel rental an indispensable utility while also powering women to feel their best every day. RTR offers apparel, accessories and home decor from over 650 designer partners and has built in-house proprietary technology and a one-of-a-kind reverse logistics operation. Under CEO and Co-Founder Jennifer Hyman\\'s leadership, Rent the Runway has been named to CNBC\\'s \"Disruptor 50\" five times in ten years, and has been placed on Fast Company\\'s Most Innovative Companies list multiple times. RTR has five retail stores of the future, 11 million community members and employs nearly 1,800 people, all of whom have equal benefits across the organization, 93% of whom are female and/or non-white, with 80% of leaders who are women.\\n\\nAbout the Team:\\n\\nWe work on unique questions at the intersection of tech, reverse logistics and the sharing economy and are obsessed with improving our customer experience every day. Finding solutions often requires pushing the boundaries of data science and finding novel and creative solutions. This is where we hope you come in.\\n\\nAbout the Role:\\n\\nData platform engineers at Rent the Runway design and implement core components of our data platform. You will work collaboratively with the data science and analytics teams to ship scalable tools that dramatically enhance their productivity and impact on the business. You will iterate on our platform for machine learning model training, build a framework for scheduling and execution of large batch compute jobs, implement continuous monitoring, automate the deployment of python services as first-class citizens in a microservice architecture, etc.\\n\\nWhat You\\'ll Do:\\nWork on a team of peers in an environment that will keep you constantly challenged and learning new things every day.\\nDevelop and deploy data platform tools that enable our data science and analytics teams to move faster and build better products.\\nCollaborate closely in an equal partnership with our data science, dev ops, and engineering teams.\\nHave a significant voice both in what you work on and how the work is carried out.\\nAbout You:\\nExperience building data platform tools with python in collaboration with data science or machine learning teams.\\nFamiliarity with AWS, Azure, or GCP. Prior experience with Airflow is a plus.\\nPassion for building stable, scalable, and secure infrastructure.\\nGreat communication skills.\\nBenefits\\n\\n\\nAt Rent the Runway, we\\'re committed to the happiness and wellbeing of our employees, and aim to create a workplace that fosters both personal and professional growth. Our benefits include, but are not limited to:\\nGenerous Paid Time Off including vacation, paid bereavement, and family sick leave - every employee needs time to take care of themselves and their family.\\nUniversal Paid Parental Leave for both parents + flexible return to work program - because we know your newest family member(s) deserve your undivided attention.\\nPaid Sabbatical after 5 years of continuous service - Unplug, recharge, and have some fun!\\nExclusive employee subscription and rental discounts - to ensure you experience the magic of renting the runway (and give us valued feedback!).\\nComprehensive health, vision, dental, FSA and dependent care from day 1 of employment - Your health comes first and we\\'ve got you covered.\\nIndustry leading 401k match - an investment in your future.\\nCompany wide events and outings - our team spirit is no joke - we know how to have fun!\\nRent the Runway is an Equal Opportunity Employer. Rent the Runway does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need',\n",
       "  \"Job Summary:Disney Streaming Services is a place for the creative and the bold. Whether New York City, San Francisco, Manchester or Amsterdam, we provide opportunities to elevate your career and transform the industry.\\n\\nSoftware Engineers at Disney Streaming Services develop premium digital media products for Major League Baseball and our partners. The products we build, such as ESPN , MLB.TV and NHL.TV are paving the way for the next-generation media and sport technologies, including the upcoming Disney offering. Our Engineering team for Disney Streaming Services is headquartered in the Chelsea area of New York City. Other office locations also include the SoMo area of San Francisco, CA and several international locations.\\n\\nAt Disney Streaming Services, data is central to measuring all aspects of the business, and critical to its operations and growth. The data engineering team is responsible for collecting, analyzing and distributing data using public cloud and open source technologies and offers transparency into customer behavior and business performance.\\n\\nIf you are interested in joining Disney Streaming Services in the pursuit of not only crafting new media products but enjoying the products you build, we are interested in hearing from you.\\n\\nResponsibilities:\\n\\n• Collaborate with product teams, data analysts and data scientists to design and build data-forward solutions\\n• Build and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably\\n• Integrate with a variety of data providers ranging from marketing, web analytics, and consumer devices metrics\\n• Build and maintain dimensional data warehouses in support of business intelligence tools\\n• Develop data catalogs and data validations to ensure clarity and correctness of key business metrics\\n• Drive and maintain a culture of quality, innovation and experimentation\\n\\nBasic Qualifications:\\n\\n• 2-3 years of experience developing in object oriented Python\\n• Familiar with big-data solutions using technologies like EMR, S3, Spark\\n• Loading and querying cloud-hosted databases such as Redshift and Snowflake\\n• Building data pipelines using Kinesis, Kafka, Spark, or Flink\\n\\nPreferred Qualifications:\\n\\n• Familiarity with binary data serialization formats such as Parquet, Avro, and Thrift\\n• Experience deploying and using data notebook and analytic environments such as Jupyter and Databricks\\n• Knowledge of the Python data ecosystem using pandas and numpy\\n• Experience with graph-based data workflows using Apache Airflow\\n\\nRequired Education\\n\\nBachelor's degree in Computer Science or related field or equivalent work experience\\n\\nAbout Disney Streaming Services:\\n\\nDisney Streaming Services is responsible for developing and operating The Walt Disney Company's direct-to-consumer video businesses globally, including the ESPN premium sports streaming service; the upcoming Disney subscription video service; and BAMTECH Media, a global leader in direct-to-consumer video streaming products and solutions. Our core mission is to deliver global audiences the freedom to access content on their terms across any connected device, time or location. We serve consumers by bringing the world's most beloved characters, timeless stories, legendary athletes, and epic sporting events to global audiences through best-in-class direct-to-consumer video services. We strive daily to imaginatively challenge convention with innovative technology that gives consumers the freedom to access content on their terms across any connected device, time or location.\\n\\nThis position is with Disney Streaming Technology LLC , which is part of a business segment we call Disney Streaming Services .\\n\\nDisney Streaming Technology LLC is an equal opportunity employer. Applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. Disney fosters a business culture where ideas and decisions from all people help us grow, innovate, create the best stories and be relevant in a rapidly changing world.\",\n",
       "  \"We're looking for a developer experienced with data engineering and interested in expanding into new technologies to join our team in New York. You’ll work with our clients to build great products that delight users while using regular investment time to improve yourself, the company, and our community.\\n\\nAbout thoughtbot\\n\\n\\nthoughtbot works with companies in every step of the process to help identify and solve problems. We lead and participate in product design sprints, build high-quality apps, and then deploy them. We use emerging and effective technologies and methods on both internal and client projects. We believe there is always a better way to do our work, and we want to find it and share it with as many people as possible.\\n\\nAdditionally, we maintain an inclusive work environment where everyone can thrive professionally, as well as have full lives outside of work. thoughtbot does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity, or any other reason prohibited by law in provision of employment opportunities and benefits. We welcome you to apply and let us know if you need any reasonable accommodations during the interview process.\\n\\nWant to dig deeper? Read more about our Purpose and Values, how we work in our Playbook, or check out this video to hear from our team.\\n\\nRequirements\\n\\nthoughtbot data engineers are able to build high-quality, high-throughput data pipelines. Well-qualified candidates have an excellent knowledge of data engineering, including streaming data, distributed data processing, and big data. Experience with Scala and common data stores like Kafka, Cassandra, and ElasticSearch are a plus. Data pipelines will power user interfaces written in Ruby on Rails, Django, and React. Interest and familiarity with those languages is a plus, as is knowledge of building APIs using GraphQL. Being able to contribute to both sides of the API is a huge plus.\\n\\nBenefits\\n\\nOur team works in a relaxed and educational environment to develop excellent products for our clients. We work at a sustainable pace of 40 hours per week, consulting for clients four days each week. We dedicate our non-client time to improving ourselves, our communities, and thoughtbot. Everything we do is predicated on having a great team and a culture of growing. We use the latest technologies and are willing to try new methods on both internal and client projects.\\n\\nINVESTMENT DAYS\\n\\nWe have an investment day each Friday where we learn new tools and techniques, work on open source, create new products, write blog posts, and try to make ourselves, each other, and the community better. If you’ve used our open source libraries, read our blog, attended the local events we host, or seen us speak at conferences then you’ve seen the fruits of investment time.\\n\\nALWAYS LEARNING\\n\\nWe have a culture of continuous improvement. Investment days are a critical component in this, but we also offer training and conference benefits. We will cover 100% of all expenses incurred when you speak at a conference and a minimum of 50% of your expenses for any conference or training you attend.\\n\\nYou’ll also enjoy working with and learning from a diverse set of teammates across your client projects and during investment time. We dedicate time each week towards working collaboratively on exciting investment projects, wrapping the year up with a two-day event which our teammates often use to build brand new products in exciting new technologies.\\n\\nSTAY FRESH\\n\\nWe offer 25 paid vacation days and 11 paid holidays per year in addition to 10 paid sick days. New parents receive at least 6 weeks paid parental leave, as well as the ability to take up to 6 months off.\\n\\nHEALTHCARE + FINANCIAL\\n\\nWe offer a competitive salary and excellent benefits. We pay 100% of medical, dental, vision, and life insurance premiums for full time employees and 90% of medical premiums for dependents. We also offer a comprehensive 401k plan.\",\n",
       "  'Data Engineer\\nRef #9911 Manhattan, NY\\nJob Description\\nThe team is looking to expand their data models to multiple business units. As a result they require to add a couple more Data Engineers to their team.',\n",
       "  'What we\\'re looking for\\n\\nWe are looking for a motivated data engineer who is passionate about building reliable and scalable data pipelines to make data sets available for analysis. The ideal candidate is entrepreneurial, motivated to grow, and has a passion for Python development.\\nOur Engineering Values\\nCollaboration: We believe that engineers do their best work when working together in cohesive teams.\\nExcellence: We believe in doing things the \"right way\" rather than the \"fast way\", and holding ourselves to a high standard of excellence.\\nGrowth: We believe engineers do their best work when they are constantly growing, learning, and changing.\\nCommunication: We believe in combining empathy with openness and honesty to set clear expectations and hold each other accountable.\\nImpact: We believe we\\'re making the world a better place by empowering marketers to really help their customers rather than just sell stuff.\\n\\nWhat you\\'ll be doing\\nDeveloping and enhancing multiple ETL pipelines\\nDesigning and implementing data storage structures and ETL pipelines, keeping long-term impacts in mind\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nWorking knowledge of message queuing, stream processing, and highly scalable \\'big data\\' data stores.\\nStrong project management and organizational skills.\\nIdentifying and improving upon current internal processes through automation and optimization\\n\\nWho you are:\\n\\n\\nBachelor\\'s degree in Computer Science, Mathematics, or related field/equivalent experience\\n3+ years of experience with Python, Java, Scala, R, Go or similar language\\n3+ years of experience in working with cloud computing technology (AWS, Google Cloud Platform, etc.)\\n3+ years experience working on data warehouse systems such as Snowflake\\nHave developed systems based on key principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)\\nComfortable using version control and working in a collaborative environment\\nExperienced with CI/CD tools for testing and deployment\\nSelf-starter with the ability to work independently or in a team\\nAble to manage one\\'s schedule and prioritize tasks independently\\nWhy You Should Join\\n\\n\\nAt Conductor, we are looking for engaged and passionate engineers that can raise the bar. There is a tremendous opportunity here to have immediate impact in the day-to-day and affect the company\\'s success and growth. We all share in the same values and push each other to meet these standards.\\n\\nWe are made up of a diverse group of people from all backgrounds and include a team of exceptional engineers in Kyiv as well. Wherever you are from, you will find a common ground here for continuing to push forward your career and make a difference in this industry.\\n\\nConductor, Inc. is an equal opportunity employer. We celebrate diversity and are committed to creating an inclusive environment for all employees.\\n\\nAbout Conductor\\n\\n\\nConductor\\'s search and content intelligence platform helps marketers create and optimize content to improve visibility online.\\n\\nThe technology generates customer intent insights that lead to compelling content, increased traffic, and higher organic marketing ROI. Customizable dashboards and workflows guide marketers through the content creation process, empowering them to measure, refine, and demonstrate the effectiveness of their SEO and content marketing efforts.\\n\\nIn addition to its SaaS platform, Conductor offers a suite of services and support including site audits, site migrations, and managed services that empower in-house marketing teams and digital marketing agencies to drive results and put their customers\\' needs first.\\n\\nConductor\\'s forward-thinking customers include global and emerging enterprise brands like Citibank, Salesforce, ClassPass, and WeWork.',\n",
       "  'Sony Music Entertainment is a global recorded music company with a roster of current artists that includes a broad array of both local artists and international superstars, as well as a vast catalog that comprises some of the most important recordings in history. Sony Music Entertainment is a wholly owned subsidiary of Sony Corporation of America.\\nSony Music is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender, gender identity or expression, pregnancy, veteran or military status, marital and civil partnership/union status, alienage or citizenship status, creed, genetic information or any other status protected by applicable federal, state, or local law.\\nThe Data Engineer will report to the Head of A&R Research. This role will explore and build products using the latest and greatest in Data Analytics, Cognitive Services, Machine Learning and more. This role is based at Sony Music’s offices in New York\\n\\nCreate and maintain systems to load and transform very large data sets from digital media retailers (iTunes, Spotify, YouTube, etc) as well as social media sources.\\nWork with a cross-functional team to create data-driven insights and reports for business stakeholders.\\nWork with other members of the team to create customer-facing analytics tools and visualizations.\\nProcess millions of rows of data daily to provide analytics to our end users.\\nTake advantage of our continuous integration and deployment.\\nParticipate in technical design and peer review for new projects.\\n\\nExperience using Snowflake and Google Cloud Platform preferred.\\nExperience with AWS ecosystem. Some preferred services are Redshift, RDS, S3, and SWF.\\nProven experience with ETL frameworks (Airflow, Luigi, or our own open sourced garcon ).\\nExpertise with at least one distributed data stores (Redshift, Cassandra, Snowflake).\\nFamiliarity with noSQL technologies (mongoDB, DynamoDB).\\nProficient in scripting language of choice. Python is strongly preferred, PHP a plus.\\nHighly proficient in writing SQL for a relational datastore (MySQL, PostgreSQL).\\nKnowledge of technologies that can deal with Big Data is a Big Plus (Kafka, Spark, Hive, Hadoop/MapReduce).\\nAbility to write automated tests (unit, functional, and integration) to ensure code works as expected.\\nDesire to collaborate with other engineers through peer code reviews.\\nDeep understanding of data structures and schema design.\\nDetail-oriented, proactive problem solving skills.',\n",
       "  \"Job Title:\\nData Engineer\\n\\nReq Number:\\n30- VAN\\n\\nJob Description:\\n\\nEPIX, an MGM Company, is seeking a Data Engineer work closely with the digital team, data stewards, product managers, as well as marketing, sales and business development teams to turn data into critical information and knowledge to make sound product and organizational decisions. In this role, it is vital to be a creative thinker and propose innovative ways to extract/combine/manipulate data that feed many systems in near real-time.\\n\\nPRINCIPAL RESPONSIBILITIES:\\nDesign, develop and maintain data software pipelines\\nManage EPIX's data lake in test and production environments\\nCollaborate in team discussions that require data science and machine learning computation\\nData mine multiple sets of data in different formats from multiple internal and external sources\\nCombine structured and unstructured data sets to be used for analysis in production systems such as recommendation engines and tools like Tableau\\nSolve and enhance bottlenecks in the software stack by working closely with our DevOps and API engineering team\\nCreate feedback loops within multiple channels: dashboards, alert systems and real time data integration for use in production applications\\nSKILL REQUIREMENTS, EXPERIENCE AND EDUCATION:\\nBS in Computer Science, Data Science or Engineering\\n3-4 years of data-related software development\\nKnowledge in facility extracting, mapping and scripting data from various in-house and 3rd party tools\\nExperience with big data as well as sparse datasets\\nStrong competency in SQL, JQL, NoSQL, Python\\nInterest in data classification strategies and their uses (DBSHIFT, KMC, EM etc.) and machine learning techniques (Tensorflow, MS ML Suite)\\nFamiliarity with scrum based environments\\nPassionate about learning emerging techniques and the state of the art in data processing, analytics and machine learning -- you use data to inform and make decisions\\nPride in your craft and enjoy building informative, reliable products\\nEnthusiastic about data's impact on the world and are curious, generous, and can hustle in a competitive environment\\nAbout EPIX\\nEPIX®, an MGM company, is a premium television network delivering a broad line-up of quality original series and documentaries, the latest movie releases and classic film franchises -- all available on TV, on demand, online and across devices. EPIX® has tripled the amount of original programming on the network and has become a destination for original premium content with series including Godfather of Harlem, starring and executive produced by Forest Whitaker; Pennyworth, the origin story of Batman's butler Alfred; Perpetual Grace, LTD, starring Sir Ben Kingsley; Get Shorty, featuring Chris O'Dowd and Ray Romano; spy thriller Deep State; and docuseries NFL: The Grind, from NFL Films and hosted by Rich Eisen; and PUNK from John Varvatos and Iggy Pop; as well as upcoming premieres of new series Belgravia, from Julian Fellowes; docuseries Slow Burn, based on the hit podcast, and Laurel Canyon. Launched in October 2009, EPIX® is available nationwide through cable, telco, satellite and emerging digital distribution platforms as well as through its EPIX NOW app, providing more movies than any other network with thousands of titles available for streaming.\\n\\nFor more information about EPIX, go to www.EPIX.com. Follow EPIX on Twitter @EpixHd (http://www.twitter.com/EpixHD) and on Facebook (http://www.facebook.com/EPIX), YouTube (http://youtube.com/EPIX), Instagram (http://instagram.com/EPIX) and Snapchat @EPIXTV.\",\n",
       "  'How will this role have an impact?\\n\\nAs a member of one of the infrastructure teams, our new Data Engineer will be given the freedom to make meaningful and measurable improvements impacting millions of people. This role will report to our VP, Technology Security & Infrastructure.\\n\\nWhat will you do?\\nQuery data to include aggregations, calculations, and producing metrics from data.\\nCreate data pipelines to include ETL and streaming data such as log data or tool/sensor data to indexes. Experience with Splunk / Sumo forwarders, ELK (Elasticsearch, Kafka, Logstash), or ES/Splunk python libraries preferred.\\nProvide novice level expertise with hands on configuration, tuning and operating of distributed data storage stacks. Open source tools including Kafka, Logstash, Beats, Elasticsearch, Kibana, or Splunk / Sumo preferred.\\nScript languages. Python preferred\\nDesign and implement data visualizations. Experience with Kibana or Sumo preferred.\\nWe are looking for someone with:\\nComputer science, engineering, information science or a related technical discipline plus 1+ years of relevant experience or Master’s degree\\nExperience working on an Agile development team\\nExperience with code repositories, esp Git/GitHub\\nExperience with ALM tools, esp Jira\\nExperience with Elastic Common Schema or Splunk / Sumo Common Information Model\\nExperience with tools in both Linux and Windows environments\\nUnderstand current cyber exploits, attack methodology, and detection techniques using a wide variety of security products.\\nCloud workload experience (Amazon Web Services, Azure)\\nUnderstanding of machine learning, and it’s use in anomaly detection\\nExperience evaluating new methodologies and technologies to meet requirements and deliver capabilities\\nOur benefits include but are not limited to:\\nPaid Time Off\\n401(k) Savings Plan with match\\nMedical, Vision & Dental\\nPre-Tax Commuter Benefits\\nParental Leave\\nGym Reimbursement\\nTuition Reimbursement',\n",
       "  'Data Engineer - Product\\n\\n\\nApply\\n\\nRef#: 35132\\n\\nCBS Business Unit: Showtime\\n\\nJob Type: Full-Time Staff\\n\\nJob Schedule: Full-Time\\n\\nJob Location: New York, NY, US\\n\\nDescription:\\nThe Showtime Product team is looking for a curious and creative Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.\\n\\nThe big data platform at Showtime is relatively new, but is now being used across the company for critical functions and features like: recommendations, analyzing customer experience, understanding programming consumption’s effect on subscriber lifetime, building churn/retention prediction models, and more. In this role, you will work our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team and our in-house engineering team, and you will architect and enable technologies, systems and workflows that enable our analysts and data scientists to focus more on algorithms and analyses than on the associated engineering.\\n\\nIdeal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.\\n\\nKey Technologies:\\n\\nJava, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA etc\\n\\nResponsibilities\\nDevelop understanding of key business, product and user questions.\\nCollaborate with other Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.\\nEvolve data-driven feature prototypes into production features that scale; streamline feature engineering, so that the underlying data is efficiently extracted.\\nBuild flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.\\nDevelop and enhance data warehouse in AWS S3.\\nEmploy data mining, segmentation, and other analytical techniques to capture important trends in our user base.\\n\\nQualifications:\\n3+ years of relevant experience in a comparable data engineering role\\n\\nExpert-level knowledge of SQL/Spark SQL\\n\\nExperience in pursuing and applying data-backed decisions, such as recommendations, trends etc. to make the core product better\\n\\nYou like to dive-deep on data analysis or technical issues to come up with effective solutions and are comfortable summarizing key insights graphically when needed\\n\\nYou believe in writing code that is easy to understand, test and maintain\\n\\nYou enjoy a workplace that values autonomy, applauds ideas and a enjoys a sense of humor\\n\\nAbout Us:\\nSHOWTIME continues to make its mark across the cultural landscape with one of the most successful programming lineups in television. The SHOWTIME programming slate features original series including Emmy® nominated limited series ESCAPE AT DANNEMORA, BILLIONS, HOMELAND, SHAMELESS, THE CHI, RAY DONOVAN, THE AFFAIR, KIDDING, BLACK MONDAY, THE LOUDEST VOICE, CITY ON A HILL and ON BECOMING A GOD IN CENTRAL FLORIDA. SHOWTIME continues to raise the bar with fresh content including upcoming series THE L WORD: GENERATION Q, BACK TO LIFE, THE GOOD LORD BIRD and WORK IN PROGRESS. The network’s eclectic, brand-defining programming is further distinguished by the captivating offerings of SHOWTIME Documentary Films, including docuseries THE CIRCUS: INSIDE THE WILDEST POLITICAL SHOW ON EARTH, Emmy-nominated WU-TANG CLAN: OF MICS AND MEN and THE FOURTH ESTATE and upcoming documentary films THE KINGMAKER and READY FOR WAR. SHOWTIME Sports continues to dominate with its flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at www.showtime.com and www.showtimeanytime.com.\\n\\nEEO Statement:\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled\\n\\nApply',\n",
       "  'As a Senior Data Engineer, here\\'s what we\\'ll be looking for you to bring:\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nDeep understanding of relational database technologies and database development techniques\\nUnderstanding of how to architect solutions for data science and analytics\\nData management for reporting and BI experience is a plus\\nUnderstanding of \"Agility\", including core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nAny experience building and leading an offshore/outsourcing function would be highly beneficial.\\nThere\\'s no typical day or engagement for our Senior Engineers. Here\\'s what you\\'ll do:\\nBe the SME. Develop Big Data architectural approach to meet key business objectives and provide end to end development solution\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that Big Data has to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you\\'re equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You\\'ll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nRegardless of what you do at ThoughtWorks, you\\'ll always have the opportunity to:\\nThink through hard problems, and work with a team to make them reality.\\nLearn something new every day.\\nWork in a dynamic, collaborative, transparent, non-hierarchal, and ego-free culture where your talent is valued over a role title\\nTravel the world.\\nSpeak at conferences.\\nWrite blogs and books.\\nDevelop your career outside of the confinements of a traditional career path by focusing on what you\\'re passionate about rather than a predetermined one-size-fits-all plan\\nBe part of a company with Social and Economic Justice at the heart of its mission.\\nA few important things to know:\\n\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn\\'t the right role for you? That\\'s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA',\n",
       "  \"PromoteIQ delivers intelligent vendor marketing solutions built for the next generation of e-commerce. Our solutions help retailers implement, automate, and scale brand-funded marketing programs on e-commerce sites. PromoteIQ is a New York City-based technology company that works with the US's largest e-commerce retailers. Learn more about us at https://www.promoteiq.com.\\n\\nWho we’re looking for\\n\\nAt PromoteIQ, data plays an integral role in our product, and software engineers on our data engineering team build the pipelines that power reporting and analytics for our e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.\\n\\nResponsibilities\\nShip high-quality, well-tested, secure, and maintainable code\\nDesign, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions\\nManage automated unit and integration test suites\\nWork collaboratively and communicate effectively with a small, motivated team of engineers and product managers\\nExperiment with and recommend new technologies that simplify or improve PromoteIQ's stack\\nParticipate in an on-call rotation and work occasional off-hours\\nQualifications\\nBS/MS in Computer Science or a related technical field\\nSeeking candidates with 1+ years of experience in:\\nArchitecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services\\nDesigning data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)\\nDesigning efficient data structures and database schemas\\nWorking with distributed systems architecture\\nIncorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)\\nUsing profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements\\nDeveloping for continuous integration and automated deployments\\nUtilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)\\nWrangling large-scale data sets\\n#MicrosoftAdvertising #PromoteIQ\\n\\nMicrosoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request via the Accommodation request form.\\n\\nBenefits/perks listed below may vary depending on the nature of your employment with Microsoft and the country where you work.\",\n",
       "  'Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.\\n\\nFounded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 6,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.\\n\\nJob Title:\\n\\nData Engineer\\n\\nData is served to hundreds of our clients and their customers. There is a big demand for analytics and data-freshness, so if you are not afraid of complex business problems, and you think traditional tools are slow, and you won’t stop looking for new approaches and technologies, then we have a position for you!\\n\\nAs a Data Engineer, you should have the expertise in the design, creation, management, and business use of large data sets to drive practical insights. You know and love working with analytics tools and can use your technical skills and creative approaches to help clients solve their most critical business challenges. This individual will be an integral part of New York’s Data and Analytics Practice and, in addition to working with clients, collaborate closely with members of the Slalom team who are focused on data visualization, advanced analytics, data science, and data strategy.\\n\\nResponsibilities:\\nCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions\\nConduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations\\nDesign and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python)\\nDesign, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets\\nExperience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies\\nHighly self-motivated to deliver both independently and with strong team collaboration\\nAbility to creatively take on new challenges and work outside comfort zone\\nStrong written and oral communications along with presentation and interpersonal skills\\nDeliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space\\nQualifications:\\n5+ years of experience in using SQL and databases in a business environment\\n5+ years of experience in custom ETL/ELT design, implementation, and maintenance\\n3+ years of experience with schema design and data modeling\\n3+ years of experience applying data architecture or engineering to solve real-world business problems\\n2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi)\\nManipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle)\\nSQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL)\\nPrior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.\\nNice to have:\\nImplementation experience with various technologies under Hadoop eco-system (HDFS, Hive, HBase, Sqoop, Pig, Presto etc.) and Spark (PySpark preferred)\\nExperience with designing digital data platforms leveraging clickstream data from Adobe Analytics (Omniture/SiteCatalyst) or Google Analytics\\nExperience in languages such as Python and Spark\\nExperience working in Agile Scrum teams\\nLinux and Windows proficiency\\nManagement consulting experience\\nProject management experience\\nExperience working with various verticals (i.e. insurance, retail, healthcare, financial services, technology)\\nSlalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.',\n",
       "  \"What is Slice?\\n\\nSlice is the leading technology and marketing platform made exclusively for local pizzerias, making it super easy to order delicious, authentic local pizza anywhere, anytime. We serve the $45 billion U.S. pizzeria market in two ways: by providing a pizza-centric mobile and web ordering experience for consumers, and by empowering local restaurants with the technology, tools, and marketing to grow their business, while helping them compete with Big Pizza. Can you imagine what a small mom and pop pizza shop could achieve with the resources of Domino’s?\\n\\nThe Role\\n\\nWe are looking for a business savvy Data Engineer to join our growing team of analytics experts. You will be responsible for expanding and optimizing our data and data pipeline architecture, as well as optimizing data flow and collection for cross-functional teams. The ideal candidate is an experienced data pipeline builder and data wrangler who enjoys optimizing data systems and building them from the ground up. You will support our software developers, business-intelligence and data scientists on data initiatives and will ensure optimal data delivery. They must be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right candidate will be excited by the prospect of optimizing or even building a new component of our company’s data architecture to support our next generation of products and data initiatives. We believe in tested automation of pipelines. The member of our team should not only be agile in producing the data pipelines, but able to test those pipelines to ensure a robust and stable platform, without data leakage and with understanding of the expected outcome.\\n\\nWhat you'll do:\\nCreate and maintain optimal data pipeline architecture\\nAssemble large, complex data sets that meet functional / non-functional business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild and support the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS ‘big data’ technologies.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with stakeholders including the Executive, Product, Data and DevOps teams to assist with data-related technical issues and support their data infrastructure needs.\\nKeep our data separated and secure\\nCreate data tools for analytics and data scientist team members which assist them in building and optimizing our product into an innovative industry leader.\\nWork with data and analytics experts to strive for greater functionality in our data systems.\\nWhat we're looking for:\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nA successful history of manipulating, processing and extracting value from large disconnected datasets.\\nWorking knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nWe are looking for a candidate with 2+ years of experience in a Data Engineer role.\\nThey should also have experience with the types of technology represented below, but do not need to be the exact named-software:\\nExperience with big data tools: Hadoop, Spark, Kafka, etc.\\nHistory of Slice:\\n\\nSlice was born in 2010 and has quietly bootstrapped its way to building a network of more than 11,000 pizzerias nationwide. Ilir Sela, our founder and CEO, started the company as a passion project to help his friends and family in the pizza business, but he quickly saw a massive opportunity to champion these small businesses by bringing their craft to the masses. Ilir has since built an amazing team of operators, marketers, technologists, and investors — all dedicated to making it easier for people to enjoy their favorite local pizza while helping local shops succeed.\\n\\nOur Pizza Philosophy\\n\\nSlice is on a journey to be the most valuable pizza brand on the planet. We connect makers and eaters to enrich their lives through the power of specialization and pizza expertise. Backed by generations of knowledge and cutting-edge technology, we give makers the platform and voice to take charge of their industry, expand their coverage, and fill the world with authentic cuisine. When passionate makers turn their craft into their livelihood, we all live happier, fuller lives. People stop accepting the homogenous, mass-produced pies of big chains and robotic trucks because the quality, variety, and authenticity of Slice is both hyper-convenient and known worldwide. We believe pizza isn’t just food. It’s a slice of community, a slice of culture, a slice of life.\\n\\nWe’re growing our family every day — so, if you’ve got a passion for local, authentic pizza and the drive to help share it with the world, we’d love to have you on the team! Check out a few awards we’ve recently won for our workplace and culture: Inc., Crain's, BuiltinNYC\\n\\nSlice is an Equal Opportunity Employer and is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to apply. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by applicable national, federal, state, or local law.\",\n",
       "  \"Entera, where residential real estate investing is made simple\\n\\nAt Entera, we are on a mission to transform the way investors find and buy properties. Powered by machine-learning, Entera's end-to-end residential real estate platform modernizes the real estate buying process. Entera's property source aggregation platform, discovery algorithms, intelligent tools and expert real estate service team help our clients access and evaluate more properties, make data-driven investment decisions, and win more - 100% online.\\n\\nEntera is based in San Francisco, New York & Houston, with satellite service offices in 12 additional markets across the US. We're always looking for talented, creative and passionate people to join our team. If you're interested in opportunities at Entera, we'd love to hear from you!\\n\\nJob Description\\n\\nAs a Data Engineer at Entera, you'll contribute to our best-in-class data pipeline and data-driven culture. You'll work with multi-discipline experts with hard-science backgrounds in a tight knit team to deliver on our efforts around data curation and management. You'll work with modern frameworks to ETL and massage data for preparation, and then utilize BI tools to develop visualizations and deliver data to both our internal business users and customers as you surface brand new depths of our vast dataset. You'll write Python, SQL, and R in shared notebooks that you and the data science team have ownership of. Within our team, you'll be able to further develop your skills and work with a team of experts to deliver on massive improvements to our data pipe and associated systems.\\n\\nSuccessful candidates will thrive in Entera's unique operating environment and culture: high-growth, innovative, lean, and values-driven. As such, successful candidates must be highly capable in each of the following dimensions (among others): adaptability, curiosity, resourcefulness, analytical thinking/problem solving, pro-activity, collaboration, technological savvy, and operating in a dynamic environment.\\n\\nJob Responsibilities\\nUse Python, SQL, and R to improve upon a best-in-class data pipeline and develop our workflows\\nContribute to cloud-first services that improve our reporting, analysis, and metrics collection efforts\\nUse agile software development processes to iteratively make improvements to our back-end systems\\nMold front-end and back-end data sources to help draw a more comprehensive picture of user flows throughout our system\\nDeliver on detailed specifications for business intelligence and reporting needs\\nContribute and further develop our data-driven culture\\nWork with product and engineering in cross-functional teams to deliver on improvements to our systems\\nPreferred Qualifications:\\nMS or PhD in Computer Science, Mathematics, Statistics, Physics, Economics, or similar hard-science\\n3+ years hands-on experience in Data + Analytics at growing product-driven tech companies\\nProficiency in cloud services and modern ETL workflows\\nAdvanced capabilities across Python, R, and SQL\\nUnderstanding of Spark\\nStrong analytical and problem solving skills\\nWorking knowledge of Python web frameworks like Flask\\nSoftware development background\",\n",
       "  'Job Description\\nWarby Parker is on the lookout for a motivated Data Engineer to help every department across our organization monitor, track, and improve their work. Our team’s reach is wide, and our efforts to collect and share data impact nearly every employee. In this role, we’ll tailor your day-to-day responsibilities to your skillset. You may support our work by developing integrations between our rapidly growing tech systems and our data warehouse. Or maybe you’ll design sophisticated BI data models, advocate data governance best practices, and own the tools used by analysts across Warby Parker. Either way, our ideal candidate is equipped with analytical thinking, communication, and collaboration skills, as well as a fluency in SQL/relational modeling. The ability to write code with razor-sharp attention to detail will also set you up for success here.\\n\\nAt the end of the day, we’re out to prove that businesses can scale, be profitable, and do good in the world. As you contribute to the company’s success, you’re also supporting our Buy a Pair, Give a Pair program and Pupil’s Project. Interested in joining us? Read on!\\n\\nWhat you’ll do:\\nAsk thoughtful questions and listen carefully to get to know your business partners’ vocabulary, problems, goals, and constraints\\nWork with a Product Manager and other Engineers to scope out and prioritize potential solutions to complex data problems\\nDevelop data models that meet business partners’ needs\\nDream up ways to help our stakeholders solve problems they may not have even thought of yet\\nImplement tested, production-ready ETL code in a mix of database environments\\nProvide thorough and helpful documentation to ensure that our decisions can be shared and understood in the future\\nWho you are:\\nBacked by + years of software engineering experience with data pipelines or other data-intensive applications\\nComfortable working with the tools of modern software engineering, like working at the command line, using version control, writing tests, performing code review, and more\\nExperience working in an iterative, Agile environment\\nAble to write SQL\\nExcited to help bring the rigor of modern software development practices to the business intelligence space\\nAn analytical thinker who can understand the needs of an analyst or business expert\\nAble to exercise judgment on all aspects of project execution, while also soliciting input from colleagues and stakeholders\\nA kind, empathetic listener and proactive, effective communicator (in writing and in person!)\\nA team player who’s able to collaborate with people of various skills and backgrounds\\nNot on the Office of Inspector General’s List of Excluded Individuals/Entities (LEIE)\\nExtra credit—definitely not required, but if you have these skills we can put them to good use!\\nThe ability to develop transformations in modern cloud databases such as BigQuery, Snowflake, or Redshift\\nExperience applying the techniques of Kimball-style dimensional data modeling in an Agile environment\\nExperience developing, debugging, and deploying infrastructure within AWS or a similar cloud environment\\nExperience with streaming event services, such as Kinesis or Kafka\\nThe ability to visualize data and teach others to, too\\nExperience with data governance, data dictionaries, and other processes or techniques to ensure that our work is maximally useful to the company\\nA background in teaching, mentoring, tutoring, or lecturing\\nAbout us:\\nWarby Parker was founded with a lofty objective: to offer designer eyewear at a revolutionary price while leading the way for socially conscious businesses. By circumventing traditional channels and designing our frames in-house, we’re able to offer top-quality glasses and sunglasses (plus an uncommonly delightful shopping experience) at a fraction of the traditional going price.\\nSince starting out in 2010, we’ve set up headquarters in New York City and Nashville, built our own optical lab, and opened retail locations all around the U.S. and Canada. As we grow, we’re committed to proving that businesses can scale and be profitable while doing good in the world. For every pair of glasses we sell, a pair is distributed to someone in need—to date, that’s over five million pairs.\\nOf course, all work and no play makes a dull workplace. Who likes that? At Warby Parker, you can look forward to company outings, volunteering and learning opportunities, and just great company. Teammates can also connect around common interests, backgrounds, and identities, no matter their home base, through our various employee resource groups. (We’re happy to say that the Human Rights Campaign has named us a Best Place to Work for LGBTQ+ employees!) That sense of community keeps us excited to walk through the door every day. Good work, good people.\\n\\nSome benefits and perks of working at Warby Parker:\\nHealth, vision, and dental insurance\\nFlexible “My Time” vacation policy\\nRetirement savings plan with a company match\\nParental leave (non-birthing parents included)\\nCell phone plan reimbursement\\nA health-and-wellness stipend\\nFree eyewear, plus discounts for friends and family\\nAnd more—just ask!',\n",
       "  \"Company Description\\n\\nDailymotion is the leading video discovery destination & technology that learns about your tastes over time, constantly surfacing the best, most relevant content on the web. Our mission is to provide the best video user experience for consumers on the market, connecting publishers and advertisers to engaged viewers who turn to Dailymotion for their daily fix of the most compelling music, entertainment, news and sports content around.\\n\\nThrough partnerships with the world's leading publishers and content creators, including CBS, CNN, Fox Sports, GQ, Mashable, Universal Music Group, VICE and more, Dailymotion commands 4 billion monthly pageviews across its mobile app, desktop and connected TV experiences. Dailymotion is owned by Vivendi, one of the largest mass-media corporations in the world.\\n\\nJob Description\\n\\nMake a huge impact for Dailymotions by joining our team as a Data Scientist within our AdTech team. You will be responsible for building models within a complex video advertsiing ecosystem to make data-driven decisions and shape the future experience for all Dailymotion viewers.\\nWork with fast-paced initiatives, overseeing projects from idea conception and strategy development to deployment, production, and operations\\nLeverage petabytes of video, audience engagement and advertising data to build customized experiences for users and advertisers at scale\\n· Responsible for full-cycle development of machine learning models from data collection and cleansing to feature engineering to model selection, prototyping and validation to A/B testing to deployment on managed cloud platforms to documentation and maintenance\\nDevelop data collection, forecasting, and reporting procedures that instantly highlight business opportunities, flag potential issues, and ease reporting to all business stakeholders\\nWork with a global team on data science challenges related to online advertising\\nQualifications\\nMasters or PhD in computer science, statistics, or a related field\\nHave a deep understanding of online advertising technologies and ecosystem, with a focus on revenue forecasting and inventory management\\n>3 years experience building end-to-end models for real-world internet scale systems\\nExcellent software engineering fundamentals (programming, data structures, complexity analysis, testing, etc.)\\nExperience with data science tool kits (Pandas, NumPy, Scipy, NLTK, Spark, Weka, Docker) and platforms (Google Cloud, AWS, etc.)\\nProficient in query languages (SQL, Hive, Pig) and in MySQL or NoSQL databases\\nExcellent business sense and communication skills\\nAdditional Information\\n\\nLocation: New York\\nType of contract: Full-time\\nStart Date: ASAP\\n\\n• Flexible time off, vacation, holidays, sick-leave so you can take time off when you need to\\n• Fitness club membership to NY Health & Racquet Club\\n• 100% healthcare coverage starting on day 1\\n• Commuter benefits\\n• 401k Contribution\\n• Paid parental leave\\n• Fully stocked kitchens with free snacks and drinks\\n\\nIf you want to explore Dailymotion culture a little further please check out:\\n\\n1./ Our BuiltIn page: https://www.builtinnyc.com/company/dailymotion\\n\\n2./ Our Recent Global Hackathon in November 2018. https://www.dailymotion.com/video/x70val9\\n\\n3./ Welcome to the Jungle page: https://www.welcometothejungle.co/companies/dailymotion/team\\n\\nAll your information will be kept confidential according to EEO guidelines.\",\n",
       "  \"Hinge is hiring a Data Scientist to join the team. As the newest member of the data science team and reporting into our Director of Data Science, you will glean insight from massive amounts of data to empower product and business decision-making.\\n\\nWhat You'll Do\\nTranslate product and company goals to data science problems and vice versa.\\nCreate essential performance metrics.\\nDevelop methodologies in sampling, A/B testing, and survey design.\\nApply statistical inference to draw conclusions from data.\\nUse a combination of exploratory analysis and data mining techniques to identify and interpret trends and anomalies.\\nCommunicate results and make applicable recommendations to team members in Product, Marketing, and Community.\\nAbout You\\n2+ years of experience as a data scientist or quantitative researcher\\nYou are proficient in SQL\\nProficiency in Python or R\\nYou have a strong theoretical and practical knowledge of statistics\\nSolid understanding of machine learning principles and models\\nExperience with product analytics and A/B testing\\nIntellectual curiosity and comfort with ambiguity\\nExceptional communication skills\\nYou can balance thoughtfulness with practicality\\nBonus Points:\\nFamiliarity with Looker\\nWorking knowledge of Linux (command line)\\nObjectives in the first three months\\nLearn to work with the main data sources and tools at Hinge.\\nFamiliarize with the Data Science team's ongoing projects and propose some ideas for iteration.\\nComplete one major data science project for Product, Marketing, or Community.\\nOur Company\\n\\n\\nHinge is the dating app for people who want to get off dating apps. In today's digital world, singles are so busy matching that they're not actually connecting, in person, where it counts. Hinge is on a mission to change that. So we built an app that's designed to be deleted. On Hinge, there are no rules, timers, or games. Instead, you'll meet your most compatible matches and you'll have unique conversations over what you've shared on your detailed profile. It's a natural way to find a great first date. Currently, 3 out of 4 first dates lead to second dates, we're the #1 mobile-first dating app mentioned in the New York Times wedding section, and we're the fastest growing dating app in the US, UK, Canada, and Australia.\\n\\nOur Culture\\nAuthenticity: Share your genuine thoughts and opinions directly.\\nCourage: Invite and deeply consider challenges and criticism.\\nEmpathy: Be empathetic, communitarian and trustworthy.\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\",\n",
       "  'JOB Description: Big Data Analyst / Engineer\\nLocation: New York, NY (preferred) OR San Jose, CA\\nDepartment: Data Engineering and Insights\\nHours/Shift: Full Time\\nReports To: Director, FI Analytics\\nJob Description:\\nAffinity Solutions is looking for a hands-on and self-driven Big Data Analyst/Engineer, preferably with experience in the bank card loyalty/fin-tech/advertising/marketing space, to enhance and automate its data and analytics infrastructure to support its growing customer base and expanding partner ecosystem. The position is based in New York, NY, and reports into the Data Engineering and Insights division. The demand for advanced analytical solutions continues to grow exponentially, and this is your opportunity to grow with us in a fast-paced, collaborative environment.\\nThe ideal candidate is a passionate and highly skilled individual, who can utilize programming and analytics tools such as SQL, Python, and Tableau to query and process large data sets to produce high quality customer facing data deliverables and insights. If you have experience designing and building business intelligence/analytics applications, especially around credit/debit card transactions, unified consumer behavioral and profile data, and excited about leveraging your experience to catapult a venture-backed company into hyper-growth, this job is for you.\\nResponsibilities:\\nDesign and maintain analytical database structures and requirements to manage and process various data sets (Affinity in-house and 3rd party data)\\nConvert OLTP data into OLAP for reporting/analytics/modeling purpose\\nDevelop analytical data assets with an eye towards process efficiency and automation through scripting.\\nCommunicate and present data to both internal and external customers by developing reports/dashboards/charts using BI tools such as Tableau\\nWork closely with a dynamic and growing team of account managers, data engineers and data scientists to perform quantitative analysis of customer data, including gathering data requirements and validate data, applying judgement and statistical analysis to assist with planning and decision making.\\nOther responsibilities include but not limited to - data validation, troubleshooting issues, process documentation, campaign analysis, Quarterly Business Reviews.\\nQualifications:\\nBachelor’s or Master’s degree in Computer Science or related field such as Mathematics and Statistics, preferably with focus on Data Analytics.\\nAt least 1 year of hands-on experience in designing and building analytical data applications and BI Reporting\\nProficient in SQL and Tableau, familiar with at least one coding language in Python/Shell scripting. Nice to have R / SAS Programming experience.\\nExperience in using Cloud based managed services and Hadoop for data warehousing/analytics is a big plus – e.g. Amazon RedShift, Hive, MapR.\\nVery strong written and verbal communication skills; Ability to tell a story with the data\\nAnalytical thinker, with an ability to evaluate multiple products/technologies to address various aspects of a big data platform.\\nExperience working on UNIX / Linux development and production environments\\nExperience working in Agile software development environments\\nStrong organization skills with attention to detail is a must.\\nAbility to manage multiple conflicting priorities, take proactive ownership of problems and outcomes, think outside the box\\nKnowledge of Retail and Financial verticals is useful but not required.\\nExperience in the Bank loyalty/FinTech/AdTech industry, especially knowledge of Bank card loyalty, Email and Digital Marketing Campaigns, DMPs, programmatic advertising is a big plus.',\n",
       "  \"The Role:\\n\\nThis is an exciting opportunity to help build the next generation data warehouse for a mission-based company, growing 50% year over year. As a Data Engineer, you will be part of making insights accessible and timely by working closely with our data modelers to populate our data warehouse and transition it to incremental load processing. In your daily work, you'll be responsible for optimizing data sourcing and transformation that create an efficient, effective data organization. You will focus on executing the data warehouse strategy to ensure resiliency, fault tolerance, and scalability, while also establishing best practices. You will also take a hands-on approach to collaborating with analysts and business intelligence to optimize the sourcing and consistency of KPIs, which will ultimately drive informed business decisions across the organization.\\n\\nWhy You'll Love This Role:\\n\\nThis is a rare opportunity to make an impact on how Newsela thinks about data during a critical time in the company's evolution. You will work closely with the Manager of Data Warehouse and other key stakeholders to optimize the most critical enterprise-wide KPIs that drive business growth and engagement. Using cutting-edge cloud technology, your work will combine deep analytics with analysis of user behavior, and will serve as the backbone of business intelligence and decision-making for Sales, Marketing, Product, and Customer Success. These insights will also be leveraged for C-level and Board reporting across Newsela. You will be joining the Data team as it doubles in size over the next year, and will help build the foundation for a product that transforms education through reading engagement.\\n\\nWhy We'll Love You:\\n\\nYou have 3+ years of demonstrated success in sourcing, transforming, and optimizing data from divergent sources. You are an expert in creating an ETL/ELT foundation leveraging best practices for continuous improvement, in addition to possessing expert SQL, DDL, and DML knowledge. You are highly skilled in scheduling, dependency management, and optimization. You have used python or other scripting languages in order to help data analysts and data scientists, among other partners, optimize data models. You will have an acute understanding of data warehouse modeling techniques and some experience using cloud-based data warehouse technologies. Through your work in data engineering, you have expanded and improved either the data warehouse set, utilization, and/or business performance. You do all this, while also contributing to an engaging data team culture.\\n\\nAbout Newsela\\n\\nNewsela is an Instructional Content Platform that combines engaging, leveled content with integrated formative assessments and insights to supercharge engagement and learning in every subject. Students and teachers use Newsela to find digital content from 100+ of the best sources—from National Geographic to NASA, Biography.com to Encyclopedia Britannica, the Washington Post to the Wichita Eagle. Content is instructionalized to meet students where they are, with interactive tools and analytics to take them where they want to go. Newsela has become an essential solution for schools and districts, with a presence in over 90% of U.S. K-12 schools. Newsela is the content platform for the connected classroom.\",\n",
       "  \"Dashlane is a password manager and online security app for everyone who lives, works, and plays on the internet. With a simple, intuitive design and patented security technology, Dashlane keeps passwords, personal data, and payment info at users' fingertips, so they can stop guessing passwords and wasting time filling out forms. Dashlane has helped over 11 million users in 180 countries manage and secure their digital identities and has enabled over $17 billion in e-commerce transactions. Our team in New York, Paris and Lisbon is united by our passion for password security and the belief that our success is built on the diverse backgrounds of every member.\\n\\nYou will be based in New York.\\n\\nDashlane is looking for a highly talented data engineer to join the Data team. Optimizing our data pipelines and data warehouses will be an essential part to helping our company scale in the coming years. You have several years of experience and a proven track record in building, deploying and keeping such applications up 24x7. You will work on a daily basis with your teammates in New York City.\\n\\nAt Dashlane you will:\\nWork on the development and maintenance of messaging services, BI tools, data warehouses\\nImplement custom ETL/ELT processes in distributed computing environments\\nWork on improving in-house tracking systems for all of our applications\\nDesign and implement data pipelines capable of modeling data from many sources and store it in such a way that users can self-serve\\nWork on server applications and APIs that are used by our Data Team\\nHandle the challenges that come with managing terabytes of data\\nDevelop automated reporting for API and system health (process, memory, response time)\\nRequirements:\\n3+ years experience in software development\\n3+ years experience designing SQL tables, choosing indexes, tuning queries and understanding the intricacies required to optimize a table in different environments\\nWe're also looking for:\\nHaving experience in architecting, implementing and testing data processing pipelines (e.g. Spark, Beam, ...) and data mining / data science algorithms either on-premise or on a cloud environment\\nHaving experience in administrating and ingesting data into standard data warehouses (e.g. Amazon Redshift, Microsoft SQL Server, Google BigQuery or Snowflake)\\nHaving strong experience in improving performance of queries, data jobs and scaling systems for exponential growth in data\\nBeing able to communicate and understand complex technical issues in English\\nNice-To-Haves:\\nHave prior knowledge of Python and Node.js\\nHaving experience with data lakes and expertise with designing and maintaining a BI solution\\nEnjoy writing clean code that is easy to maintain and understand\\nHave a security background\\nA true international company, founded in Paris and currently split between Paris, Lisbon and New York, we thrive off diverse perspectives. We recognize that diversity has different aspects: gender, sexual orientation, ability, ethnic origin, social, age, lifestyle, and more. We're committed to finding diverse talent and fostering a culture where everyone is heard and feels a sense of belonging.\",\n",
       "  \"Cherre provides investors, insurers, brokers and other large enterprises with a platform to collect, resolve, and augment real estate data from thousands of public, private, and internal sources. By providing a “single source of truth,” we empower companies to evaluate opportunities and trends faster and more accurately, while saving them millions of dollars in manual data collection and analytics costs.\\n\\nWe are looking for an enthusiastic data engineer who is interested in working with a fast-growing team in building industry-leading real estate data services. You will be part of designing and implementing server side services to ingest, organize, analyze, and display real estate data and insight. You will be working in a small team and be a real partner in the design and implementation of all aspects of our product.\\nYou will\\nDevelop and implement ETL processes\\nDesign data warehouse solutions to support ETL processes and data analytics applications\\nWrite SQL/NoSQL database queries, stored procedures, triggers, user defined functions, analytic functions, etc.\\nOwn features that you develop end to end, develop and test your code, implement new processes in production, and maintain and support them over time\\nDrive our data platform and help evolve our technology stack and development best practices\\nDevelop and unit test assigned features to meet product requirements\\nRequired\\nBS/MS/PhD in CS or related field\\nExperience in database technologies and data warehousing\\nExperience in Javascript or Python\\nHands on experience with MongoDB, PostgreSQL, and large-scale distributed storage and database systems\\nAbility to deal with ambiguity and communicate well with both technical and non-technical teams\\nWe'd love the following experience\\nReact\\nGraphQL\\nElasticsearch\\nBI tools (i.e. Looker)\\nHands on experience developing APIs and SDKs\\nExperience with service oriented architecture and good understanding of distributed systems, data stores, data modeling, and indexing (experience with Event Sourcing and/or CQRS preferred)\\nBenefits\\nCompetitive Base Salary\\nEquity\\nRange of Healthcare Plans that start day one\\nPaid Parental Leave\\nEducational Credit\\nUnlimited Vacation\\nFlexible Work Schedule\\n\\n\\nIf this opportunity sounds interesting, apply or reach out to our internal talent team. We are happy to tell you more about Cherre: the technology we work with, the problems we solve, the team we are assembling, and the culture we all contribute to. We are excited you are considering working with us and look forward to hearing from you!\\n\\nCherre is an equal opportunity employer. We pride ourselves on hiring the best people for the job no matter their race, sex, orientation, nationality, religion, disability, or age.\",\n",
       "  'Job Description\\nAbout Caserta:\\n\\nAt Caserta, we work with leading organizations to deliver innovative Data & Analytics solutions. We specialize in Cloud Computing, Big Data, AI/ML, Business Intelligence, Data Warehousing, Modern Data Architecture, and Enterprise Data Management. We are looking for creative, entrepreneurial, and highly-motivated people to carry out our mission of designing, architecting, and implementing the most innovative, forward-looking Data-Driven solutions available to our clients.\\n\\nDATA ENGINEER\\n\\nAs a Data Engineer at Caserta, you’ll work in small teams to deliver innovative solutions using core cloud data warehouse tools and Spark, Event Stream platforms, and other Big Data related technologies. In addition to building the next generation of data platforms, you’ll be working with some of the most forward-thinking organizations in data and analytics.\\n\\nLife as a Data Engineer at Caserta:\\nWork as part of a team to develop modern Data-Pipelines in Python, Spark and PySpark for real-time data streaming\\nParticipate in the development of cloud data warehouses, business intelligence and analytics solutions across multiple industries and technology ecosystems\\nExposure to the Latest & Greatest Cloud technologies in the market place, including but not limited to: AWS, EMR, S2, EC2, Glue, Kinesis, Redshift, Snowflake; Google Cloud, GCS, DataPrac, DataFlow, BigQuery, PubSub; Azure, Blob Storage, Data Factory, SQL DW\\nWork with other highly motived individuals who have a passion for solving complex data & analytics challenges and learning new technologies\\nDesign, Build, Migrate complex ETL pipelines, extracting and combining data from various heterogeneous data sources\\nHow to thrive at Caserta:\\nBe Agile: Data Analysis and Data Profiling are key, map data across the organization to build robust and efficient pipelines\\nBe Insightful: Clients come to Caserta to gain Strategic & Actionable Insights from their data, you will guide them along their journey to realizing the value of their data assets\\nBe Entrepreneurial: Bring ideas and new ways of Innovating in and through data\\nBe Consultative: Direct interaction with clients requires the ability to explain complex technical issues in non-technical terms, leveraging the business value of data\\nBe Resourceful: Imagination is a skill at Caserta; you will learn many new technologies, and conceive new ways of efficiently and optimally implementing new tools and products\\nBe Knowledgable: of Database Structures, Theories, Principles, and Best Practices',\n",
       "  \"Sapphire Digital seeks a dynamic and driven mid-level Data Engineer to join our growing New Jersey team who will be responsible for operating and enhancing the performance and functionality of existing data management systems, as well as participating in the design and delivery of new database solutions.\\nIn this position, you'll be responsible for:\\nManaging data activities such as data requirements gathering, data analysis/modelling, and data issues resolution using standard approved technology\\nManaging standardization, migration, transformation, validation, and quality assurance of data within multi-database platforms\\nLeveraging internal and external ETL tools for data processing and publishing Identifies and maintains company databases, including data sources, data structures, data organization, and data optimization\\nIdentifying complex issues proactively and is responsible to see them through resolution, including identifying trends through data analysis and manipulation\\nSpecific client data life-cycles from discovery to implementation to maintenance\\nFormulating and monitors policies, procedures, and standards relating to database management\\nResponding to production defects and relays information back to the Operations Manager to communicate to clients\\nContributing in all phases of the data and software development lifecycle when needed\\nCreating and maintaining code through GitHub repository for change control\\nSupporting off hours data processing and emergency requests as needed\\n\\nYou might be a good fit if you have:\\nBachelor's degree (B. A. / B. S.) from four-year college or university; and two to four years related experience and/or training; or equivalent combination of education and experience.\\n5+ years of experience with SQL, database design, optimization, and tuning\\n5+ years of experience with Postgresql\\n4+ years of experience using Github\\n4+ years of experience in Shell Scripting and one other object oriented language such as Python, or PhP.\\n3+ years of experience in continuous integration and development methodologies tools such as Jenkins\\n5+ years of experience in an Agile development environment\\nTime management skills\\nProgramming skills particularly SQL, Shell Scripting, and Python\",\n",
       "  'Our client is a leading hedge fund looking to hire a Data Engineer for their Macro Strategies business unit.\\n\\nResponsibilities:\\nDriving innovation through product and platform development\\nHelping to facilitate bespoke custom basket trades for clients in a scalable infrastructure\\nDeveloping infrastructure and tools to administer basket rebalances for external clients and internal trading teams\\nAutomation of corporate action adjustments and improvement of work-flow\\nProviding metrics for basket trades, to drive sales and trading decisions and to grow the business\\nBoth independent and collaborative work, involving several sales/strat/trading teams globally\\n\\nRequirements:\\nExpertise in Python\\nStrong SQL skills\\nWeb scraping experience\\nExperience with Linux and Windows platform\\nStrong communication skills, both written and verbal\\nExposure to non-relational databases\\nExposure to web UI technologies\\nData Warehousing and Modeling expertise\\nFinancial knowledge\\n\\n\\nIf you would like to be considered for the position of Data Engineer or wish to discuss the role further then please leave your details below. Your resume will be held in confidence until you connect with a member of our team\\nEmail: info@njfsearch.com or call London (0207 604 4444,) New York (212 400 4845) or Chicago (312 204 72176) to speak to a member of our team. Thank you',\n",
       "  'Beeswax is looking for a Data Engineer to join our growing team. We were recently recognized on the Inc. 5000 list as #46 in the fastest growing companies and #5 in the top software companies. In 2018, we were also named by Business Insider as the \"fastest growing company in AdTech\"\\n\\nBeeswax is a high scale, high availability digital advertising platform founded by executives from Google and funded by leading VCs including RRE and Foundry Group. We aim to offer the most extensible and transparent advertising system, servicing technology enabled clients and executing and processing billions of events every day.\\n\\nThe Beeswax engineering team is a top-notch group with backgrounds from Google, Amazon, Oracle and other premier technical teams. Because digital advertising operates at an extremely high scale (millions of transactions per second) and low latency, the opportunity to learn about web-scale distributed systems and hard scaling problems is a great advantage of our engineering culture and work.\\n\\nWe are looking for a Data Engineer to build clean pipelines and maintain data products that our customers rely on.\\n\\nOur products are built on a variety of technologies including C++, Java, Python and LEMP stack (PHP, MySQL, nginx), and while our engineers typically specialize in one area, they need to coordinate and integrate with a wide variety of systems, including homegrown and AWS-native services.\\n\\nResponsibilities:\\nCode in a variety of languages, primarily Python, Java and/or C++\\nDesign and implement data pipelines, building scalable and optimized enterprise level data systems\\nWork cross functionally with Product, Ops and Engineering counterparts\\nParticipation and collaboration from inception to deployment\\nRequirements:\\nMS in Computer Science, Math, related technical field or equivalent practical experience\\n4+ years of general software programming experience in Java, C/C++, Python and SQL\\nLarge systems software design and development experience, with knowledge of Unix/Linux\\nKnowledge of database technology, schema design, and query optimization techniques\\nSolid foundation in data structures, algorithms and software design with strong analytical and debugging skills\\nPreferred Qualifications:\\nPhD in Computer Science, Mathematics, or related technical field\\nFamiliarity with open source cloud and application platforms, AWS development experience\\nExperience with big data technologies such as Spark, Hive, Presto and Impala.\\nExperience working with MPP databases such as Redshift, Snowflake, Vertica and Netezza\\nHands-on experience working in SOA and high throughput environments\\nAbout You:\\nYou are passionate about learning, mentoring and building a world class team and culture, while constantly empowering others around you\\nYou take a second to step back and look at the big picture before diving in head first\\nYou care about the quality of the data flowing through your code as much as about the quality of the code\\nNot afraid to take risks, voice opinions or ideas that help build the next generation of data platforms all within a massive distributed system\\nYou\\'re the type to peel back the layers and use non-conventional means to solve the task at hand.',\n",
       "  'Want to build a product that uses data to see and make sense of the future?\\n\\nIf you are a coding fanatic and passionate about programming, we want you to help us make a huge impact. Our clients love our product and are thirsty for more!\\n\\nAt CB Insights we build products that help clients make sense of the future and drive their businesses forward using data. Our system retrieves large amounts of structured and unstructured data and uses scientific methods to extract knowledge and insights from that data. We present those analytics through a sophisticated, dynamic user interface which enables our clients to find answers to their most important questions.\\n\\nAs a Software Engineer at CB Insights you will be part of a cross-disciplinary, self-motivated team with clear ownership and passion to form the future. Our crew uses state-of-the-art technologies and writes quality code that ships often.\\n\\nOur data software engineers build scalable data pipelines and big data processing systems that runs on AWS cloud We focus on modularity and reuse where it makes sense, while ensuring that there are no constraints to delivering world-class software continuously.\\n\\nMuch of our software team has been with us for several years, despite a white-hot tech market with options galore. We attribute this to our collaborative, \"teach and learn\" culture where the role evolves with your interests.\\n\\nIf this sounds interesting to you, reach out and join CB Insights now!\\n\\nKey Responsibilities:\\nDevelop features for our next generation market intelligence platform on a small Agile team.\\nConsistently and frequently deliver solutions that are well-engineered, maintainable and tested within the agreed upon time frame.\\nDesign and build efficient ETL infrastructures for unstructured textual data sets and various other types of data sources\\nTake a prototype of a data product built with NLP and/or machine learning models and make it run reliably in production.\\nDesign and implement internal tools to make this data processing infrastructure easily accessible to and usable by data scientists and software developers\\nParticipate in code reviews and sprint planning, help identify opportunities, work through challenges and share knowledge with colleagues.\\nCollaborate with the product team to understand business requirements.\\nRequirements and Qualifications:\\nBS in Computer Science or similar; advanced degrees also acceptable\\n4+ years professional experience in Python. Familiarity with Go is a plus\\nRelational database proficiency (e.g. Redshift, MySQL, Aurora)\\nExperience using Spark, Sqoop, AWS services (Spectrum, Glue) and other related tools in the big data ecosystem\\nKnowledgeable of data modeling, data storage techniques, data warehousing and general data architecture\\nExperience with engineering data pipelines to capture, store and process unstructured data\\nProficiency developing in a Mac/Linux environment\\nBeliever in Lean and Agile values and principles for building software\\nExcellent written and verbal communication skills\\nExcellent problem solving and analytical skills\\nHelpful, Humble Human\\nLove for experimentation\\nWe know that diversity makes for the best problem-solving and creative thinking. We are dedicated to adding new perspectives to the team and encourage everyone to apply if your experience is close to what we are looking for.\\n\\nPerks and Benefits:\\nSubsidized health, dental, and vision insurance\\n401k with up to 4% match\\n$1,000 yearly continuing education stipend\\nDaily lunch stipend\\nHappy, Helpful, Humble, and Hungry: Check out more about our company culture here.\\n\\nEqual Opportunity Employer: CB Insights is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\\n\\nIf you know someone who\\'d be perfect for the role,\\nsubmit here and you\\'ll be eligible for $5,000!\\n\\n#LI-RR1',\n",
       "  'Founded in 2016 with only a handful of individuals, Quantexa purpose was built that through a greater understanding of context, better decisions can be made. 3 years, 6 locations and 180+ employees later we still believe that today. Working within industries such as Finance, Insurance, Energy and Government, we connect the dots within our Customers data using dynamic entity resolution and advanced network analytics to create context, empowering businesses to see the bigger picture and drive real value from their data.\\n\\nOur success is driven by the talent of our staff and our commitment to quality. We are looking for Data Engineers to join us in tackling some of the industry’s most challenging problems.\\n\\nWhat does a Data Engineer role at Quantexa look like?\\n\\nIn order to be a successful data Engineer at Quantexa, you’ll need to be comfortable dealing with both internal and external stakeholders You will be managing, transforming and cleansing high volume data, helping our Tier 1 clients solve business problems in the area of fraud, compliance and financial crime.\\n\\nBeing Agile is an integral part to the success we have at Quantexa and having regular team sprints and Scrum meetings with your Projects team is essential. You’ll be working closely with Data Scientists, Business Analysts, Technical Leads, Project Managers and Solutions Architects, with everyone following the same goal of meeting our Clients expectations and delivering a first-class service.\\n\\nWe want our employees to use the latest and leading open source big-data technology possible. You will be using tools such as Spark, Hadoop, Scala and Elasticsearch, with our platform being hosted on Google cloud (GCP). Our primary language is written in Scala, but don’t worry If that’s not your strongest language or if you haven’t used it before, we make sure that every Quantexan goes through our training academy so they’re comfortable and confident with using our platform.\\n\\nRequirements\\n\\nWhat do I need to have?\\nWe’re looking for individuals who have proven big data experience, either from an implementation or a data science prospective.\\nThe desire to learn and code in Scala\\nExperience in working in an Agile environment\\nExpert knowledge of at least one big data technology such as Spark, Hadoop, or Elasticsearch.\\nA strong coding background in either Java, Python or Scala\\nExperience of building data processing pipelines for use in production “hands off” batch systems, including either traditional ETL pipelines and/or analytics pipelines.\\nPassion and drive to grow within one of the UK’s fastest growing Start-ups\\nBenefits\\n\\nWhy join Quantexa?\\n\\nWe know that just having an excellent glass door rating isn’t enough, so we’ve put together a competitive package as a way of saying “thank you” for all your hard work!\\nCompetitive Salary\\nCompany Bonus\\nExcellent private healthcare, Dental and Optic coverage, Life assurance, LTD and STD coverage\\n401k where we’ll match up to 5%\\nOnline training customized to your personal preferences\\nGenerous annual leave\\nAmazing working environment - Ranging from regular social events, free beverages',\n",
       "  'Knotel is seeking an experienced Data Engineer to build data products that will help inform better business decisions. Data is at the core of what we do. It is how we get insights into the effectiveness of our physical and digital products. In this role, the Data Engineer will set the vision and build our data infrastructure that will scale with us as we continue to grow. He/she will own back-end systems and will be responsible for design, development, and evolution of architecture going forward.\\n\\nWhat You’ll Do\\nDesign and develop dimensional data models and schema designs (OLTP, OLAP)\\nDesign and develop robust data processing pipelines\\nBuild APIs that allow us to access the right information faster across the Knotel infrastructure\\nBuild and maintain large-scale data infrastructure\\nCreate data tools for Analytics and Data Science\\nPartner closely with product to contribute to the Data Product roadmap\\nWork across multiple team in high visibility roles and own solutions end to end\\nWho You Are\\n\\nYou’re passionate about data. You apply quantitative information to solve business challenges. You know how to design, build, and manage modern back-end architecture (microservices, etc.). Listening and collaboration are core competencies, and you excel when working closely with Product Management, Analytics, and other internal stakeholders. In addition to maintaining a robust/resilient system, you ship high quality code on time. You also have the following:\\nBS/MS in Computer Science, Computer Engineering, or a related discipline\\n5+ years developing data processing infrastructure in an Agile environment\\nDeep understanding of distributed systems concepts and principles\\nExperience developing design rationale of databases through conceptual, logical, and physical modeling\\nDiverse development experience across database types (relational, NoSQL, graph)\\nExpert proficiency in SQL and Python and/or Scala; significant experience in ETL design, implementation, and maintenance (Airflow, DBT, Luigi)\\nExperience developing data warehouses (Redshift, Snowflake)\\nExperience developing internal and external APIs (REST, GraphQL)\\nFamiliarity with front-end development frameworks (React)\\nFamiliarity with IOT, streaming frameworks (Spark, Kafka, MQTT), and time series databases\\nAbout Knotel\\n\\nKnotel is the world’s leading flexible workspace provider that gets, fits, fills and manages space for customers. Knotel caters to established and growing companies, giving them the freedom to focus on their business, culture, and people. With over 4 million square feet across 200 locations in three continents, Knotel is transforming commercial real estate and moving companies forward.\\n\\nKnotel was founded in 2016 by Amol Sarva and Edward Shenderovich to give businesses the flexibility and speed to scale on their own terms. All Knotel spaces are tailored to the needs of each individual company by an in-house team of architects, interior designers, and workplace strategists. For more information, please visit www.knotel.com.\\n\\nOur Core Values\\nFly your Flag - Don’t check your identity at the door. Bring it inside - we are better for it.\\nDon’t Look Away - Care for the details no one told you about. Make every space a home and give the gift of belonging.\\nOutcomes, Not Processes - Time is precious. Save it. Start with the goal and back out the journey\\nGet Uncomfortable - Innovation starts at the edge of the unknown. Embrace the adventure and sign up for the hairy challenges.\\n“Where is it Engraved…” - Question orthodoxy.\\nShare in the Victory Dance - No one summits alone. Leverage the best in others and offer the most in yourself.\\nDiversity & Inclusion at Knotel\\n\\nAt Knotel, we know that a diverse workforce fosters our individual and collective success. We are committed to building an inclusive, collaborative culture where people of all races, genders, sexual orientations, and religious backgrounds can do their best work. This is all fueled by our employees, who drive the efforts and initiatives outlined here.',\n",
       "  'Our client, based in Manhattan, manages a $1 Billion regional health insurance plan whose goal is to improve the lives of its members and their families by providing comprehensive and affordable health care. They are part of a larger organization that provides a host of benefits – from training to retirement benefits – to their membership. This role combines Data Engineering with a public health focus.\\nAs a Data Engineer you will get to play a key role in the delivery of powerful data-driven products that support a self-insured health fund. The Data Engineer is responsible for data collection, movement, transformation processing, and storage of large data sets. This individual works with both current ETL/Data Warehousing and future Data/Streaming/Pipeline architectures. The focus is on choosing optimal solutions to use for these purposes, then implementing, maintaining, and monitoring them.\\n\\nEssential Duties and Responsibilities:\\nProgram in a variety of languages and platforms to automate the processing of patient-level healthcare transactions, third party data sources and aggregated public health data;\\nServe as the data wrangler and ETL expert for the company; Ingest, transform, cleanse and augment internal and external data assets;\\nBuild algorithms for fuzzy matching, de-duplication and rule-based de-identification; Fully indulge your love for math, statistics and logical problem solving;\\nLeverage the main toolsets: Python, Anaconda stack (Jupyter Notebook, NumPy, Pandas, MatPlotLib/Bokeh, SciPy, Scikit-Learn), SQL, Stata, Qlikview;\\nLead data modeling, database design and performance optimization; Write SQL for defining database objects and performing manipulations;\\nContinuously learn by investigating and adopting new technologies;\\nFacilitates data collection from a variety of different sources, getting it in the right formats, assuring that it adheres to data quality standards, and assuring that downstream users can get that data quickly and with a common standard interface;\\nEnsures that data streams/pipelines are scalable, repeatable, and secure, and can serve multiple users within the Health Fund;\\nDevelops as a core member of an Agile team, using Agile tools and methodology; Work closely with other team members including Application Developers, Database Developers, and Data Scientists;\\nResponsible for creating the infrastructure that provides insight from raw data and handles diverse sources of data seamlessly;\\nEnables big data and batch/real-time analytical solutions that leverage emerging technologies;\\nAdditional responsibilities include developing prototypes and proof of concepts for the selected solutions, and implementing complex big data projects with a focus on collecting, parsing, and managing large sets of data using multiple platforms to allow for research and data science initiatives;\\nTranslates business requirements into modern data pipeline solutions; Create centralized documents and diagrams of all solutions;\\nCreates a data catalog store of all metadata;\\nApproaches all relationships with a world-class customer service approach; Maintains a customer- focused approach with users to provide solutions that are science/research-driven;\\nResponsible for the integrity and security of data in all forms of storage throughout the Data Architecture;\\nWorks with other professionals throughout the Funds effectively; Comply with HIPAA to follow all applicable policies and procedures;\\nAssists in the development of standards and procedures affecting data management, design and maintenance; Documents all standards and procedures;\\nProvides presentations and training to other team members in the above;\\nPossesses an extremely flexible attitude; Willing to work with multiple types of technologies and languages with an open mind and without technology bias; Continuous interest in updating skill sets and knowledge of trends in the Data Technology space; and\\nOther duties as assigned.\\nQualifications:\\nBachelor’s degree in Computer Science or a related discipline; Advanced degree preferred;\\n5+ years of full-time experience or demonstrated accomplishments in relevant subject areas;\\n2 years of relevant professional development experience\\nDemonstrated mastery with Python and its data science ecosystem;\\nStata is a must; Knowledge of other mathematics environments such as R,SAS, SPSS or Matlab is a plus;\\nHigh level of expertise in SQL, relational database optimization, stored procedures and data modeling;\\nFamiliarity with Subversion or other Version Control Management Tools;\\nUnderstanding of methods to ingest and process non-relational JSON and XML formatted data.\\nStrong SQL and NoSQL Database Knowledge: Oracle, PostgreSQL/MYSQL, and Mongo DB (or similar);\\nExperience with micro-services and SOA;\\nKnowledge of Hadoop, Spark, Kafka and other big data technology stacks and streaming tools is a plus;\\nFamiliarity with and the ability to leverage a wide variety of open source technologies and tools;',\n",
       "  'ThoughtWorks is a global software consultancy, made up of around 4,500 passionate technologists across 15 countries. We specialize in strategy, portfolio management and product design, combined with digital engineering excellence.\\n\\nAs a Lead Data Engineer, here\\'s what we\\'ll be looking for you to bring:\\nHands-on Engineering Leadership\\nProven track record of Innovation and expertise in Data Engineering\\nTenure in coding, architecting and delivering complex projects\\nDeep understanding and application of modern data processing technology stacks. For example Spark, Hadoop ecosystem technologies, and others\\nDeep understanding of streaming data architectures and technologies for real-time and low-latency data processing\\nDeep understanding of NoSQL technologies including column family, graph, document, and key-value data storage technologies\\nUnderstanding of how to architect solutions for data science and analytics such as productionizing machine learning models and collaborating with data scientists\\nUnderstanding of agile development methods including: core values, guiding principles, and key agile practices\\nUnderstanding of the theory and application of Continuous Integration/Delivery\\nPassion for software craftsmanship\\nA rich breadth of industry experience and background working across different organizations, ranging in size, from start-ups to large corporations..\\nStrong stakeholder management and interaction experience at different levels\\nThere\\'s no typical day or engagement for our Senior Data Engineers. Here\\'s what you\\'ll do:\\nBe the SME. Develop modern data architectural approaches to meet key business objectives and provide end to end data solutions\\nYou might spend a few weeks with a new client on a deep technical review or a complete organizational review, helping them to understand the potential that data brings to solve their most pressing problems.\\nOn other projects, you might be acting as the architect, leading the design of technical solutions, or perhaps overseeing a program inception to build a new product.\\nIt could be much more about getting stuck into a delivery project where you\\'re equally happy coding and tech leading the team to implement the solution.\\nWhatever your role, the team always look to draw on your experience when things get tough and you often handle the difficult client conversations allowing the team to continue delivering without undue pressure.\\nYou have great relationships with our new business team and work collaboratively to support pre-sales, meet prospective clients and ultimately influence and shape our portfolio of work.\\nYou recognize that building your network with a client is absolutely key to enable you to perform in your role. You\\'ll be drawing on all of your passion for technology, hands-on experience and knowledge of latest Big Data and Engineering best practices to help you gain the respect and credibility of those around you.\\nA few important things to know:\\n\\n\\nProjects are almost exclusively on customer site, so candidates should be flexible and open to extensive travel.\\n\\nCandidates must possess work authorization that does not require H-1B visa sponsorship by ThoughtWorks or dependent EAD.\\n\\nNot quite ready to apply? Or maybe this isn\\'t the right role for you? That\\'s OK, you can stay in touch with AccessThoughtWorks, our learning community (click \"contact me about recruitment opportunities\" to hear about jobs in the future).\\n\\nIt is the policy of ThoughtWorks, Inc. to provide a work environment free of discrimination. The Company will take affirmative action to ensure applicants and ThoughtWorks employees are treated without regard to race, color, religion, sex, national origin, ethnic origin, veteran status, family status, disability, sexual orientation, gender expression or gender identity. This also includes individuals who are perceived to have any of the aforementioned attributes. ThoughtWorks will adhere to all federal, state, and municipal laws and regulations governing employment.\\n\\n#LI-NA',\n",
       "  \"(We are unable to sponsor for this role or in the future )\\n\\nAt Fareportal, we create the technology that is driving innovation in the travel industry - one of the world's fastest-growing sectors. Our employees are the core of our organization and together we're revolutionizing the way people book travel.\\n\\nOur portfolio of brands including CheapOair and OneTravel receive over 100 million visitors annually and drive over $4 billion in annual revenue.\\n\\nIn addition to competitive pay and benefits, generous time off, and frequent company-wide social events, Fareportal provides employees with an environment that nurtures diversity, creativity, and success. Our open and Agile workspace gives our employees the time and space for collaboration, brainstorming, and research and development. At Fareportal, you'll be challenged, rewarded, and motivated to work effectively day in and day out.\\nWe are looking for a software engineer to join our Data Science team. You will be handling hundreds of millions of events per day, responsible for building out the data tooling and processes to support the creation of machine learning models and data science insights that will drive our business.\\n\\nThe ideal candidate will participate in the design and implementation of the entire data pipeline, from capturing and storing data to streaming and processing the data and making it available to the organization.\\n\\nWe are passionate about making data-driven decisions and you will have the opportunity to shape the team's direction and create large impact.\\n\\nOur team loves Python and Scala (and is not afraid of Functional Programming) and we strongly encourage DevOps approaches.\\n\\nResponsibilities:\\nCreate and maintain data pipeline architectures for providing a real time and batch processing platform for all models to run on\\nCreate and maintain APIs for our machine learning models\\nCoordinating the movement of data between data sources in cloud environments (streaming and batch) Assemble large, complex data sets that meet functional / non-functional business requirements\\nOur ideal candidate:\\n\\nWho You Are\\nYou are smart and love to build systems that are well tested as well as flexible You like being around smart people who will challenge you on a daily basis.\\nYou love to ramp up on new technologies to build awesome things with us!\\nRequirements\\nBachelor in Computer Science, Software engineering or related discipline (we are open to exceptions) Only May 2020 Undergraduates\\nGood knowledge of one of: Python or Scala\\nSolid understanding of object oriented or functional programming concepts Familiarity with version control concepts\\nNice to have\\nExperience with big data processing: Flink, Spark, Kafka, etc.\\nExperience with different databases, such as Redis, Elasticsearch, Postgres or Cassandra.\",\n",
       "  \"What You'll Be Doing:\\nWorking on problems that affect the lives of real people. Our users depend on us to make positive changes to their health and their lives.\\nBasing your work on scientifically-proven, peer-reviewed methodologies that are designed by medical professionals\\nCollaborating with a team both onsite and offsite -- about 50% of our engineering team is fully remote; we worry about results, not time spent in seats.\\nWhat We're Looking For:\\nExperience dealing with data at scale, processing and transforming hundreds of millions of data points per day.\\nFirst-rate SQL skills, but are aware of its limits. You know when to use it, and when it's better to find a different solution.\\nFamiliarity with ETL tools and problems -- we use Airflow, Redshift, Glue, and many other systems.\\nA team player able to work alongside data analysts and data scientists\\nWhat Makes This Job Amazing\\nYou'll be helping millions of people lead healthier lives every day\\nYou'll be a part of Noom's rocketship-- revenue has grown 20x in the last 2 years and our team is growing fast.\\nYou'll have wonderful benefits including healthcare/dental, wellness budget, daily gourmet meals prepared by our onsite chefs, and onsite yoga\\nYou'll experience huge learning & professional growth opportunities. Noom believes in supporting your professional and personal growth: we'll cover the cost of books, courses, conferences… you name it!\\nYou'll add to our transparent, high-performing, and close-knit culture\\nNoom Inc. is a leader in mobile health coaching. We combine the power of technology with the empathy of real human coaches to deliver successful behavior change. Our direct-to-consumer mobile applications have reached more than 45 million users worldwide-- and counting. We've leveraged our behavior change platform to incorporate the CDC's Diabetes Prevention Program (DPP), and have expanded to programs for diabetes management, pre-hypertension, and hypertension. Our Engineering team is at the forefront of this challenge, solving complex technical problems that center around habits, behavior, and lifestyle.\\n\\nWe are looking for a Data Engineer to join our Data team and help us improve and maintain our Data Warehouse. If you like to work with billions of rows of data and be at the center of data-driven decisions, we'll love working with you.\",\n",
       "  'If interested, please email most updated resume to jenna.trinh@mondo.com\\n\\nTitle: Data Engineer\\nDuration: 9 month contract - potential to hire\\nLocation: Remote\\nStart Date: ASAP\\nWork Hours: 10-6pm EST\\n\\nWe are seeking a Senior Big Data Engineer for the Analytics department. As players perform actions in-game, the platform collects telemetry in real-time. The data collected is used to provide insights for guiding game development, marketing, and monetization.\\n\\nThe Senior Big Data Engineer is a key contributor to the design and development of the analytics platform.\\nCode development.\\nProduction support.\\nMentoring of junior team members.\\nAutomation.\\nArchitecture and design.\\nRequirements:\\nBachelors in computer science or closely related discipline.\\nMinimum 3 years experience with a focus on big data.\\nExperience in AWS (Amazon Cloud).\\nExperience building data processing pipeline.\\nExperience with any combination of Python, Java, Hadoop and Apache Spark.\\nOther skills needed to perform the job (e.g. presentation skills, negotiation skills, etc.).\\nAmazon Cloud.\\nDeep understanding of the concepts involved with managing resources in AWS.\\nApache Spark.\\nKafka.\\nAble to lead new and complex projects having multiple dependencies from design phase all the way to production. This may involve facilitating across multiple teams.',\n",
       "  \"(We are unable to sponsor for this role or in the future)\\n\\nAt Fareportal, we create the technology that is driving innovation in the travel industry - one of the world's fastest-growing sectors. Our employees are the core of our organization and together we're revolutionizing the way people book travel.\\n\\nOur portfolio of brands including CheapOair and OneTravel receive over 100 million visitors annually and drive over $4 billion in annual revenue.\\n\\nIn addition to competitive pay and benefits, generous time off, and frequent company-wide social events, Fareportal provides employees with an environment that nurtures diversity, creativity, and success. Our open and Agile workspace gives our employees the time and space for collaboration, brainstorming, and research and development. At Fareportal, you'll be challenged, rewarded, and motivated to work effectively day in and day out.\\n\\nWe are looking for a machine learning engineer to join our team. You will be handling hundreds of millions of events per day, responsible for creating and supporting machine learning models that will drive our business.\\n\\nMachine Learning Engineers is at the heart of how Fareportal works and they are part software engineer and part machine learning / data scientist. You will focus on creating and supporting large scale models that we deploy to power our recommendation, pricing or other systems.\\n\\nThe ideal candidate will participate in the design and implementation of the entire model pipeline, from project ideation, figuring out which data to capture and store, coming up with features, to creating the final model.\\n\\nWe are passionate about making data-driven decisions and you will have the opportunity to shape the team's direction and create large impact.\\n\\nOur team loves Python and Scala (and is not afraid of Functional Programming) and we strongly encourage DevOps approaches.\\n\\nResponsibilities:\\nSupport our data modeling efforts to ensure we are capturing the data needed to improve our modeling\\ncapabilities.\\nCreate features for our feature store\\nBuild machine learning models\\nUse a variety of techniques including predictive modeling, recommendation engines, revenue\\nmanagement, conversion rate optimization, and site and user experience optimization.\\nOur ideal candidate:\\n\\nWho You Are\\nYou are smart and love to build systems that are well tested as well as flexible\\nYou like being around smart people who will challenge you on a daily basis.\\nYou love to ramp up on new technologies to build awesome things with us!\\nPassionate about working with large unstructured and structured data sets and developing new\\napproaches to relevance problems\\nRequirements\\nBachelor in Computer Science, Software engineering, Data Science or related disciplines (we are open to exceptions) - Only May 2020 Undergraduates\\nGood knowledge of one of: Python or Scala\\nSolid understanding of object oriented or functional programming concepts\\nFamiliarity with version control concepts\\nGood knowledge of machine learning\\nGood knowledge of Pyspark\\nGood knowledge of software engineering best practices\",\n",
       "  'Position Summary\\n\\nOur Data Engineer will be responsible for assisting with the implementation and maintenance of the enterprise wide data management solution for certified analytics and reporting rectangles (CARRs) and data assets. This includes the analysis, design, development, testing, implementation, and initial maintenance of the solution and acting as interface with SME/Product owners and enterprise data analytics team, by taking a holistic approach for delivering reusable CARRs and data assets enabling data solutions.\\n\\nYour Responsibilities\\n\\n· Support the design, build and execution of post source system extraction and data lake ingestion and business transformation, CARR creation framework and development processes in production.\\n\\n· Enhance analytic environments and platform required for structured, semi-structured and unstructured data.\\n\\n· Develop data quality metrics that identify gaps and ensure compliance with enterprise wide standards.\\n\\n· Provide technical support for projects and team members, along with SMEs and product owners.\\n\\n· Build data pipelines to feed descriptive and predictive analytics use cases, KPI and enterprise wide reporting.\\n\\n· Interface with architects, product managers/SMEs and product analysts to understand data needs and support the implementation of the business rules into transformation.\\n\\n· Document the data blending process along with the specifications and workflow/data lineage.\\n\\n· Maintain/add to existing data dictionaries and work with UI team for creating the profiling platform and data profiling for already created CARRs.\\n\\n· Execute project-based data governance processes.\\n\\n· Work with our team in India\\n\\nReporting Relationships\\n\\nAs our Data Engineer, you will report to our Data Scientist who reports to our Vice President, Predictive Analytics.',\n",
       "  'Job Summary\\nData Engineer\\n\\nDeveloping a data mart using Azure sequel data warehouse. Looking for key skills in Azure Sequel Data Warehouse, SQL, data warehousing, Python, Spark (PySpark), cloud (Azure).\\n\\nJob Type: Contract\\n\\nExperience:\\nrelevant: 4 years (Preferred)\\nContract Renewal:\\nLikely',\n",
       "  'Do you like working with big data? Are you passionate about using Artificial Intelligence, Machine Learning and Deep Learning to influence product & business decisions? Do you enjoy helping customers in building solutions leveraging the state-of-the-art AI/ML/DL tools? If Yes, we want to talk to you.\\n\\nDigilant is seeking a Data Engineer passionate about using Machine Learning & Data Science to help design and build analytics and optimization tools to support ongoing delivery of key insights to drive business growth and overall impact. The most exciting part about working at Digilant is the enormous potential for personal and professional growth. You will be part of a small but growing team and help shape the way we develop, deploy, and operate production quality analytics systems and processes and have an impact on how Digilant uses data in the years to come.\\n\\nThis is a full time position based in US and you will work from our offices in New York, Boston or work remotely.\\n\\nWhat you’ll be doing:\\n\\nHave a lot of fun:\\nCollecting, processing and cleansing data from a wide variety of sources.\\nExploring datasets in notebooks, revealing trends and patterns, communicating insights to business users.\\nDeveloping and prototyping predictive models.\\nBuilding, maintaining and owning scalable data pipelines to support our platform and business needs.\\nWorking with Analytics to understand and leverage our data assets to solve client problems and needs.\\nBeing a team player, and bring the team and company forward by solving team and company priorities.\\nTechnical Qualifications:\\n2+ years experience working with data\\nExperienced in writing readable, re-usable code SQL and Python\\nExperienced with distributed system technologies, Hadoop, HiveQL, and Spark SQL/PySpark\\nProficiency in linux\\nSelf-driven, with a hunger to learn and spread knowledge by teaching others\\nExcellent communication skills – ability to synthesize and communicate technical concepts, limitations, and requirements to client-facing teams and stakeholders\\nBonus points:\\nWorked as a data scientist.\\nProduction experience with machine learning and deep learning models.\\nSolid grounding in statistics, probability theory, data modeling, machine learning algorithms and software development techniques and languages used to implement analytics solutions.\\nExperience with data modeling and Big Data solution stacks.\\nExperience with one or more deep learning frameworks.\\nKnowledge of digital marketing & programmatic space.\\nTechnologies we use:\\nPython\\nDjango\\ndocker\\nAWS\\nPostgres & Redshift\\nApache Spark & Hadoop ecosystem\\nWhat you’ll get:\\nCompetitive salary commensurate with experience and bonus opportunities.\\nDigilant offers an excellent benefits package including 401K with a matching contribution, Medical/Dental/Vision effective Day 1, Group Life Insurance, AD&D, Long and Short-‐Term Disability, Flexible Spending Accounts, Pre-‐tax Commuter Benefit Programs and an Uncapped Paid Time Off Policy!\\nTo apply for this job email your details to eng-jobs@digilant.com',\n",
       "  \"About Birchbox:\\n\\n\\nWe started Birchbox in 2010 to redefine the way consumers discover and shop for beauty and grooming. The company quickly grew from an exciting idea to a business that has materially shaped the beauty industry: we've activated an enormous group of underserved, untapped consumers, awakening their relationship with beauty by making the experience relevant, easy and fun. Our innovation isn't the simple concept of delivering a box of samples - it's understanding that although not everyone is passionate about beauty and grooming, everyone still deserves to have a great experience finding, trying and buying it.\\n\\nBirchbox operates in six countries, reaching more than 2.5 million active customers with a portfolio of 500 best-in-class prestige brand partners. We have retail locations within select Walgreens stores across the US as well as a flagship Birchbox store in Paris.\\n\\nAbout the role:\\n\\n\\nBirchbox is seeking an ambitious and experienced Data Engineer to help evolve our data systems as we continue to grow. In addition to fueling our recommender system and beauty box optimization toolkit, our data services enable and support decision-making company-wide. This software engineering role will be very impactful as we build out the next generation of our data services, and will have considerable leeway in leading architectural direction.\\n\\nYou'll be joining a lean Agile team supporting Data Infrastructure and Machine Learning. Primary upcoming initiatives include refreshing our algorithmically-driven subscription box optimization system, iterating on our e-commerce recommender services, retooling our event data collection systems, and supporting the next generation of our A/B testing machinery. Each of these projects will involve some DevOps work and some amount of data extraction and loading, but will also involve substantial backend software development.\\n\\nResponsibilities\\n\\n\\nAs a Birchbox Engineer, you will:\\nBuild fault-tolerant, scalable batch and real-time distributed data processing systems to drive our personalization and BI systems.\\nDesign and configure hosted and cloud-based data and machine learning infrastructure (Hadoop, Spark, AWS EMR).\\nImplement fault tolerant data integrations between internal systems and with third-party APIs as needed, supporting product and marketing needs.\\nBuild and support internal A/B testing tooling for the engineering and product teams.\\nSupport the Business Intelligence team as needed, advising them on data provenance and reliability.\\nDesign and build components of internal tooling used by our subscription operations team (Python, MILP, CP).\\nTool our systems for observability, including logging, metrics monitoring, and dashboarding (Datadog).\\nContribute to maintaining an open, empowering, responsible, and proactive engineering culture.\\nAbout You\\n\\n\\nIdeally, we're seeking somebody with:\\n3+ years of professional software experience (or equivalent).\\nBachelor's degree in Computer Science or related field (or equivalent experience).\\nExpert in Python, and comfortable with at least two other languages (e.g., Java, Ruby, PHP).\\nStrong command line skills for working within virtualized machines (bash, tmux / screen, vim / emacs).\\nExperience orchestrating data infrastructure (e.g., Pig/Hive, HBase, Spark, S3, Kafka/Kinesis, Redshift).\\nExperience with modern workflow management systems (e.g., Airflow, Luigi, Azkaban).\\nAdvanced SQL skills (MySQL and/or Postgres), familiarity with data warehousing and dimensional modeling.\\nFamiliarity with non-relational data stores and/or indexes (e.g., MongoDB, HDFS, Cassandra, Solr, ElasticSearch).\\nExperience working on teams using distributed version control (e.g., Git, Mercurial).\\nInsatiable curiosity, a commitment to precision and excellence, and a desire to work collaboratively within and between teams.\\nBonus Points\\n\\n\\nWe'll definitely want to have a conversation if:\\nYou're committed to enabling DevOps culture with development, deployment, and observability / monitoring tooling.\\nYou've read more IETF RFCs than you'd admit in polite company.\\nYou're opinionated about when and how to leverage different algorithms, data manipulation techniques, and frameworks.\\nYou're passionate about idiomatic code style, readability, and when to use which software design patterns.\\n\\nAt Birchbox you will...\\nTake your ideas to the next level right away. We experiment, iterate, learn, and repeat.\\nBuild things people love. Our goal is to surprise and delight our customers as much as possible. Great design, simple user experience, and access to data to make smart decision help us to achieve it.\\nCollaborate with purpose. You'll work in small groups with other talented thinkers and figure out how to make Birchbox's software even better.\\nWork with people who care. We're a group of talented professionals who pride ourselves on what we do. We're smart, innovative, energetic, and lots of fun.\\nTech @ Birchbox\\n\\n\\nThe technology team at Birchbox spans software engineering, technical operations, product, and business intelligence. We are responsible for developing the company's customer-facing sites in six countries, managing hosted and cloud infrastructure, and closely supporting other teams (logistics, marketing, et al.) in our 230+-person global company. Our service-oriented platform is built on a wide variety of open-source technologies: AWS, Apache Mesos, Marathon, and Docker; Salt, Jenkins, and Shippable for automation and CI/CD; Apache Spark and Hadoop ecosystem tools; Ruby on Rails, Java, Python, and PHP backing our React-based customer sites; Datadog, Kibana, and PagerDuty for observability and alerting. We rely on engineers to be self-motivated and quickly follow through on tasks without requiring close supervision. In return, engineers are given substantial freedom to use their own initiative and make their own decisions.\\n\\nOur challenges include:\\nEvolving our software and systems architecture to support a rapidly growing customer base across multiple countries and languages.\\nDesigning and implementing the best user experience for our customers. We strive to revolutionize online retail.\\nUsing data, complex algorithms, and statistics, to personalize the Birchbox experience for our customers, both offline and online.\\nAs an engineer at Birchbox you:\\nAre self-sufficient: you've learned how to navigate projects you're unfamiliar with.\\nExercise wisdom: you're able to identify opportunities to apply lessons learned, and do so with confidence and care.\\nTake humble initiative: you're always on the lookout to improve technology and processes, but understand how and when to best present this to your peers.\\nCultivate passion: not only do you continue to improve your craft, you share your discoveries with your colleagues.\\nAre given empowering responsibility: you're trusted with tools and processes to implement your vision, and you create this chance for others.\\nDocument kindly: your code and processes have documentation that will help those who come after you, regardless of skill level.\\nCollaborate eagerly: you understand that helping others isn't a distraction, but a vital part of what we do.\",\n",
       "  'Data Engineer\\n\\nMunich Re, New York, United States\\n\\nEntry level\\n\\nProfessional\\n\\nType of job\\n\\nFull-time\\n\\nArea of Expertise\\n\\nMiscellaneous\\n\\nYour job\\n\\n\\nWe’re adding to our diverse team of experts and are looking to hire those who are committed to building a culture that enables the creation of innovative solutions for our business units and clients. The Insurance OperationsMunich Re US is launching ...\\nApply now\\nOpen job offer\\n\\nPublished on 10/18/2019',\n",
       "  \"About Better.com:\\n\\nWe're one of the fastest growing homeownership companies in America. Why? Because we're building a better way to get home and our customers love it. By combining smarter technology with an award-winning team of mortgage experts, we're making homeownership so simple it feels magical.\\n\\nSo far, we've:\\nHelped more than 10,000 families get home\\nFinanced over $3B in loans\\nSaved families an average of $32,000 in fees over the life of their loans\\nGrew our geographic coverage to 36 states, up from 15 states from a year ago\\nAnd we're not slowing down. We continue to outpace the rest of the industry at every turn.\\n\\nOur backers have helped build some of the most transformative tech and finance companies in history. Kleiner Perkins, Goldman Sachs, IA Ventures, Ally Bank, American Express, and others have invested over $150MM in Better and our vision of making homeownership magical.\\n\\nA Better opportunity:\\n\\nHelp us hack a thirteen trillion dollar industry by building a product that will allow more people than the status quo to own a home and build wealth rather than rent for life. Our tech team is small, and you will be a big part of defining the technical direction and culture. We encourage proposals for projects off the beaten path, experimentation with different frameworks and libraries, and doing as you see fit to solve problems. We also offer above-market compensation and equity, as well as full benefits.\\n\\nSome projects you could be working on:\\nWork closely with our product team to understand funnel drop off and come up with product ideas\\nWork closely with the marketing team to optimize our acquisition funnel\\nPresent conclusions to the executive team that can impact the strategic direction of the company\\nBuild a lead scoring model to help our customer support team prioritize.\\nModel the time-lag of conversions using fun math like Gamma distributions\\nDesign an experiment to understand the causal impact of an outbound phone call on conversion rates\\nBuild web scrapers to track price data for other mortgage lenders\\nMigrate our data warehouse to Redshift\\nWork on our underwriting engine, which turns out to be NP-complete and can be posed as a mixed integer programming problem\\nTranscribe all our phone calls using speech-to-text and figure out ways to optimize customer support.\\nBetter Technology:\\nWe do continuous deployment and we ship code 50-100 times every day\\nThe data stack is all in Python 3.6\\nWe use Node.js, Python and Scala for services\\nPostgres for the database\\nKubernetes, for deployment and devops\\nAWS for infrastructure, leveraging EC2, S3, SWF, CloudFront, Route53, and much more\\nThe team:\\nThe tech team is currently 30 engineers but growing quickly\\nErik Bernhardsson (CTO) used to run the data team and the music recommendation team at Spotify. He is the open source author of a few popular projects like Annoy and Luigi and writes a blog about (mostly) data\\nThings we value:\\nCuriosity. Why? How? Repeat.\\nNerdiness. Financial news and trends are fascinating. Seriously.\\nRelentlessness. No one here gives up. We try. We fail. We try again.\\nPassion. If you don't get excited about homeownership, mortgages, and real estate, it simply won't work.\\nSmarts: book and street. We have to use all the tools at our disposal to build Better.\\nEmpathy and Compassion. You understand that people's biggest dreams are in your hands.\\nCommunication. Can you ask for help or put your hand up when you don't understand?\\nBuilding. Doing. Making. Yes, we have to do a lot of thinking and talking to figure this stuff out, but you can't wait to leave the conversation and build it\",\n",
       "  \"28-Aug-2019 (CST)\\nBI\\nNew York, NY, USA\\nFull Time\\n\\nEmail Me Similar Jobs Email Me This Job\\n\\nGroup One Trading is one of the largest proprietary options market making firms in the US-listed market. The Business Intelligence team supports the various functional groups by providing analytics and infrastructure to decision-makers across the firm. Interfacing with trading, risk, finance, and operations the group's tasks are myriad and require drive, attention to detail, and a commitment to improving the quality of the product every day.\\n\\nDescription:\\n\\nA data engineer at Group One will be responsible for designing and maintaining data pipelines used throughout the firm. Their technical leadership will be used to make the existing systems more efficient and accessible while also developing new methods that can accommodate a growing problem space. By working collaboratively with different functional groups throughout the firm, they will be able to apply their technical expertise and creativity to identify data sets that can be used to research new opportunities, audit processes, and identify areas for improvement.\\n\\nThe ideal candidate will have experience integrating data consumption from multiple sources and liaising across teams. They will have expertise in making design decisions for consuming, processing, and reporting on data. While designing data pipelines that reduce maintenance costs and increase automation, they will ensure that the systems are extensible and flexible for changing markets. They will establish and maintain database standards and provide data solutions to both Business Intelligence and other data consumers in the firm.\\n\\nDesired Skills:\\nStrong communication skills, including explaining technical problems and solutions to a non-technical audience;\\nTroubleshooting problem reports in a dynamic and live production environment;\\nExperience gathering functional requirements from a varied user base;\\nDeveloping complex T-SQL queries across multiple datasets;\\nImplementing scheduled jobs and stored procedures;\\nUsing SSIS and SSAS to manage ETL and data warehousing;\\nGenerating reports and building validation checks for data integrity, logic and quality;\\nDesigning infrastructure and implementation of database management best practices;\\nData validation and schema design;\\nData analysis with Excel, SQL Server, SSRS Microsoft Report Builder, and Power BI;\\nTime series analysis of financial data.\\nExperience:\\n\\nEducation: Master's degree in Information Systems, Computer Engineering or related OR Bachelor's degree in the same with two additional years of professional experience\\n\\nProfessional: Two years of experience in a position developing and maintaining information or computer systems in the financial or equities trading industry preferred.\\n\\nGroup One Trading\\nhttps://group1.applicantpro.com\",\n",
       "  'Title: Hadoop Developer/Data EngineerLocation: Hoboken, NJ\\nDuration: 6 months\\n\\nResponsibilities\\nDesigns and develops data-ingestion frameworks, real-time processing solutions,\\nand data processing and transformation frameworks.\\nsupport for deployed data applications and analytical models.\\nDeploys application codes and analytical models. Provides\\nExperience in Data Platform Administration/Engineering\\nExperience with Python and Strong SQL\\n\\nDesired Skills:\\nData analysis or business intelligence expertise\\nExperience with Apache NiFi Kafka, HDFS, Hive, Cloud computing, machine learning,\\ntext analysis, NLP & Web development experience is a plus but not mandatory\\nNoSQL experience a plus\\n\\nPlease click below to apply or feel free to call at 732-993-612\\nprovided by Dice',\n",
       "  'Data-driven decision making has always been important at SeatGeek. Our Analytics team helps drive strategic decision-making by providing meaningful business insights from data.\\n\\n\\nWe\\'re looking for a Data Analyst to join our growing team who can ensure the most relevant information is surfaced to each team to allow for well informed decisions. If you\\'re a well-rounded, business-savvy, technically capable analyst with SQL chops and a knack for figuring out complex systems fast, we\\'d love to talk with you.\\n\\nWhat You\\'ll Do\\nhelp drive strategic decision-making through detailed analysis, insights and reporting on key performance indicators (KPIs) and financial results\\ninvestigate positive and negative trends in a wide array of business processes\\ncollaborate with departments across the company to understand needs while effectively prioritizing incoming requests\\nwork with engineers and other source data experts to clean up messy data sets\\ncontinually improve visibility into benchmarks across the company\\ncreate and maintain world class dashboards and data models in our BI platform, Looker\\nWho You Are\\n\\n\\nYou love digging through mounds of data, but your understanding of what is relevant and your ability to communicate results in a concise way separates you from the pack. You look forward to open discussions around the \"gray\" area of business decisions, but aim to narrow that area with data driven insights as much as possible. You take pride in proactively providing data, analysis and findings in a collaborative manner. You also have many of the following:\\n2 to 4 years of relevant experience analyzing large and complex data sets, working with relational databases and writing SQL\\na degree in Mathematics, Economics, Statistics or another quantitative field\\nadvanced working knowledge of SQL, perhaps with a side of Python or R\\na keen sense of design, both in data architecture and dashboard presentation.\\nknowledge of the transactional marketplace business model or consumer facing e-commerce products (bonus for mobile-first!)\\nPerks\\n\\nA culture that places the product first. We are a technology company at heart, and are proud of the idea that great technology drives great user experience\\nA laid-back, fun workplace designed to facilitate collaboration and company wide events\\n$120/mo to spend on tickets to live events\\nA superb benefits package, including full health/dental/vision\\nA focus on transparency. We have regular team lunches and Q&A panels where employees can chat openly with teams across SeatGeek, our co-founders, and external guests from the industry\\nHackathons: scheduled times when everyone drops what they\\'re doing and builds cool stuff in small groups\\n\\nSeatGeek is committed to providing equal employment opportunities to all employees and applicants for employment regardless of race, color, religion, creed, age, national origin or ancestry, ethnicity, sex, sexual orientation, gender identity or expression, disability, military or veteran status, or any other category protected by federal, state, or local law. As an equal opportunities employer, we recognize that diversity is a positive attribute and we welcome the differences and benefits that a diverse culture brings. Come join us!\\n\\n#LI-DNI',\n",
       "  \"Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs and companies where they are most likely to succeed. We are leading the charge in an evolving industry, and growing our amazing team to support the mission of using data to unleash one's full potential.\\n\\nWe are looking for a talented Data Engineer to help scale all aspects of our business. You will have the opportunity to work with multiple teams to design and build data driven solutions. Therefore the ideal candidate will be able to wear multiple hats to solve a wide variety of data related challenges while working alongside both technical and non-technical team members.\\n\\nWHAT YOU’LL DO\\nBuild batch and real-time data pipelines\\nCollaborate with data scientist and machine learning experts to optimize and scale the predictive modeling process\\nDesign well-structured APIs, services, and architectures for new project objectives\\nHelp improve and expand internal tools for data delivery, access, and analysis\\nAnalyze and visualize data for both internal and client facing use cases\\nData quality control\\nWork with team leads to prioritize business and technology needs\\nFind and use the right tool for the job and integrate with existing architecture\\nCURRENT TECHNOLOGIES\\nPython/Django/Flask\\nJavascript/Angular/React\\nJupyter\\nAWS/Azure\\nMySQL/PostgresSQL/Redshift/MongoDB/CosmosDB\\nRabbitMQ/Celery\\nAirflow\\nDocker/Packer/Terraform\\nRequirements\\nHighly proficient in Python\\nProficient working with data and distributed systems\\nExperience with Python scientific libraries such as SciPy, Scikit, Pandas, and NumPy\\nExperience conducting methods using any of the following: machine learning, predictive modeling, statistical inference, experimental design, data mining, and optimization\\nExperience in Linux/Unix environment and shell scripting\\nExperience with ETL\\nDeep understanding of data structures and schema design\\nFamiliarity with both SQL and NoSQL technologies\\nExperience with AWS and Azure platforms a plus\\nKnowledgeable about data modeling, data access, and data storage techniques\\nCritical thinking: ability to track down complex data and engineering issues, evaluate different algorithmic approaches, and analyze data to solve problems\\nCreativity: conceive of new data driven products, features, and technologies\\nBenefits\\nHealth Care Plan (Medical, Dental & Vision)\\nPaid Time Off (Vacation, Sick & Public Holidays)\\nFamily Leave (Maternity, Paternity)\\nTraining & Development\\nWellness Resources\\nStock Option Plan\\nTransportation Reimbursement\",\n",
       "  'Responsibilities\\nInteract with external / internal data providers to understand the nature of data.\\nGather information on general data delivery schedule for all sources.\\nBuild databases and schemas for data warehousing and analysis.\\nDevelop ETLs to move data into the warehouse and analysis layers.\\nDevelop logical checks in ETL process to check for duplicates, obvious data errors etc.\\nJudge whether an ETL needs to be a batch process or stream process and use appropriate libraries.\\nSchedule and maintain ETLs for different sources.\\nStructure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.\\nWork with data scientists to ensure the data is formatted in a way that is optimal for their model.\\nCreate and maintain schemas to store MMM results.\\nServe as a go to person for ad hoc queries, schemas, views etc.\\nTechnical Skills\\nFamiliar with big data processing and concepts like MapReduce, spark RDDs etc.\\nKnowledge of cloud storage platforms on AWS, Azure etc.\\nJava, Python and Scala are required skill sets.\\nGood command over SQL, ETL Best practices and data warehousing concepts.\\nFamiliarity with ELT.\\nFamiliarity with various modes of file storage like Parquet, ORC, Avro etc.\\nPrior experience in working with Redshift/Snowflake a plus.',\n",
       "  \"Who You Are:\\n\\nGetty Images is looking for a Data Engineer who enjoys working across the entire lifecycle of Machine Learning projects and takes pride in deploying high-quality ML and data workflows.\\n\\nThe mission of the Data Science team at Getty Images Inc. is to leverage internal and third-party data to inform other groups on how to interact with its customer base. We achieve this goal by 1) building automated solutions that apply best-in-class Machine Learning and Engineering practices and 2) continuous interactions with stakeholders to identify critical needs that deliver results relevant to the business.\\n\\nAs a Data Engineer on the Data Science team, you will have end-to-end autonomy and ownership of your projects, and will work closely with other business units to develop creative solutions to a variety of problems.\\n\\n\\nYour Next Challenge:\\n\\nYou will join a team of highly-collaborative and curious Data Scientists and Data Analysts that are comfortable working with a diverse set of tools, and willing to take initiative on their ideas. As a member of the team, you will have the chance to define the technical architecture that serves as the foundation for upstream analytical projects, and accelerate the delivery of a robust portfolio of Data Science models.\\n\\nYour primary goal will be to catalyze the development and deployment of full-stack Machine Learning pipelines. You will have the opportunity to continuously develop and ship code in our production environment, and will be empowered to implement a variety of data-centric architectures that support critical operational initiatives.\\n\\nYou will also interact with the entirety of Getty Images Inc. technology stack, and collaborate with data infrastructure, platform and cloud engineers to design and build a production-level data ecosystem that aligns with business function requirements and capable of handling large-scale structured and unstructured data. You will also have the opportunity to continuously evaluate and provide guidance on the use of new technologies.\\n\\nWe value learning and development, and you will be given every opportunity to work on projects that excite you. You will get to lead and innovate as a thought-leader within Getty Images, and will sit at the intersection of Engineering, Marketing and Leadership to inform, influence, support, and execute on our decisions.\\n\\nWhat You'll Need:\\n\\nYou have prior experience working as a Data Engineer, preferably in a product or customer-focused organization.\\n\\nYou are extremely comfortable working with Python and have a working knowledge of Cloud services and Tools, as well as standard engineering tools such as Git, Linux and SQL.\\n\\nYou have experience building streaming and batch data pipelines and are comfortable working within a large-scale distributed environment with open source tools such as Hadoop, Hive, Airflow and Spark.\\n\\nYou can independently execute on a project, from ideation to delivery to stakeholders, and can pro-actively interact with other engineers at Getty Images to access necessary resources or data.\\n\\nYou understand, or have interest learning about, the real-world advantages and drawbacks of various Machine Learning techniques, and have applied those to a variety of datasets.\\n\\nNice to Have:\\n\\nA M.S. or Ph.D. in computer science, statistics, economics/econometrics, natural science or any other equivalent quantitative project is preferred. If you are self-taught and believe you are a good fit for this role, or have significant work experience, we would love to hear from you as well.\\n\\nPrevious experience in an analytical role, or experience working with teams of Data Scientists and Data Analysts.\\n\\nExperience having managed or contributed to the use of Business Intelligence platforms.\\n\\n#LI-MM1\\n\\nWho We Are:\\n\\nGetty Images is the most trusted and esteemed source of visual content, with over 200 million assets available through its industry-leading sites www.gettyimages.com and www.istockphoto.com. The Getty Images website serves creative, business and media customers in almost every country in the world and is the first place people turn to discover, purchase and share powerful content from the world's best photographers and videographers. Getty Images works with over 200,000 contributors and hundreds of image partners to provide comprehensive coverage of more than 160,000 news, sport and entertainment events, impactful creative imagery to communicate any commercial concept and the world's deepest digital archive of historic photography.\\n\\nFor company news and announcements, visit our Press Room. Find iStock on Facebook, Twitter, Instagram and LinkedIn, or download the iStock app where you can easily search, save and share superior images to create standout visual communications.\\n\\nGetty Images is an equal opportunity employer and strongly supports diversity in the workplace.\",\n",
       "  'Button’s mission is to be \"the better way\" in mobile commerce. Today, we work with some of the largest and most interesting businesses in the world to connect consumers with what they want at the tap of a button. We build with the consumer experience in mind, and we have a reputation for paving the future of mobile and enjoying the road to get there. Data will be the foundation for building our next generation of products—and that\\'s where you come in.\\n\\nAs a Data Engineer, you will help build the systems responsible for ingesting, processing, and analyzing the data streams that drive key decisions we and our customers make. You will work on integrating machine learning and ETL systems that process billions of events, built on Redshift, SQS, and Airflow, and written in Python and Go. You will be a close partner to Button’s Data Science team to craft and refine the reusable components of our data platform\\nAS A DATA ENGINEER YOU WILL:\\nWork with data scientists and data analysts to design, build, and deploy reusable components for data storage, data processing, machine learning, and data accessibility.\\nExpand our pipeline instrumentation and tooling with monitoring, alerting, logging and tracing for our critical business tasks.\\nSupport new feature development as the data partner. You’ll be involved end-to-end, helping lean, cross-disciplinary product engineering teams define and execute on the product’s goals including measurement and embedding ML frameworks.\\nWE ARE LOOKING FOR TEAMMATES WHO HAVE:\\nExperience with and a desire to continue learning about data infrastructure and frameworks. Right now, we make heavy use of ECS, Redshift, Airflow, and BigQuery — but we are eager to learn more.\\nStrong fundamentals including SQL, databases and ETL best practices.\\nProgramming experience in Python or Go. Experience writing production code that others can understand. Bonus points for experience with the Python scientific stack.\\nAn awareness of the landscape of available data storage and processing tools.\\nWHO YOU WILL WORK WITH:\\nBrad Groff, Director of Data: Brad is the head of the Data team at Button. Before joining us, he ran data science and data engineering at Ordergroove, and managed data science teams at Capital One and Booz Allen Hamilton. In his free time, he can be found engaging in lengthy discussions with his daughter on topics such as which colors are his favorite colors and why, how some monsters can be nice and others can be mean (but sometimes just because they\\'re hungry), and which things are the silliest things\\nJames (or Jimmy) McGill, VP of Engineering: Hailing formerly from both Australia and Google, he spends his weekends hiking, surfing and designing and building furniture. See James for all of your Vegemite needs.\\nMijail Gomez, Data Science Engineer: has worked on event ingestion, data processing frameworks, data standardization, and experimentation frameworks. He loves having discussions on data architecture, systems and tech. Before Button he was in SF working in early stage startups. Outside of work he enjoys soccer, hiking, and exploring new burger places in NYC.\\nMORE ABOUT US:\\nButton was founded in 2014. We’ve raised $65M in funding. Most recently we announced our $30M Series C in June 2019, led by Icon Ventures. Our investors include Norwest, Redpoint, Greycroft, DCM, and Capital One Ventures. We’re approximately 40 engineers and 120 people overall.\\nWe believe and invest in personal growth, and we’ve got the results to back it up. We’ve been recognized multiple times by Fortune, Inc., and Crain’s magazines as one of the best places to work in the US.\\nWe value diversity and come from all sorts of different backgrounds. Some of our teammates hail from big tech companies like Google; from financial giants like Bloomberg and MasterCard; and from previous startups like Chartbeat, Compass, and Blue Apron. We were especially proud the first time we hired a coding boot camp graduate.\\nButton is committed to being a welcoming and inclusive workplace for everyone, and we are intentional about making sure people feel respected, supported and connected at work—regardless of who you are or where you come from. We value and celebrate our differences and we believe being open about who we are allows us to do the best work of our lives.\\n\\nButton is an Equal Opportunity Employer. We do not discriminate against qualified applicants or employees on the basis of race, color, religion, gender identity, sex, sexual preference, sexual identity, pregnancy, national origin, ancestry, citizenship, age, marital status, physical disability, mental disability, medical condition, military status, or any other characteristic protected by federal, state, or local law, rule, or regulation.',\n",
       "  'Kinetix Trading is seeking experienced Data Engineers to join our team in New York! This is the perfect opportunity for an experienced Engineer well versed in Big Data Technologies to build on top of a greenfield and deliver valuable solutions to our clients.\\n\\nBasic Qualifications:\\nBachelors Degree\\nAt least 8 years of experience in application development\\nAt least 3 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)\\nPreferred Qualifications:\\nBachelors degree in Computer Science or Electrical Engineering\\n8+ years of experience in application development\\n3+ year experience working with Hadoop, Spark, Flume, HDFS, Zookeeper, and/or MongoDB\\n2+ years of experience with Agile engineering practices\\n3+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)\\n2+ years of experience developing Java based software solutions\\n2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)\\n2+ years of experience developing software solutions to solve complex business problems',\n",
       "  \"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By creating a single source of truth, Reonomy products empower individuals, teams and companies to share information, unlock insights and discover new opportunities.\\n\\nHeadquartered in New York, Reonomy has raised ~$70 million from top investors, including Sapphire Ventures, Bain Capital, Softbank and Primary Ventures. Our clients represent the biggest names in CRE, including Newmark Knight Frank, Cushman & Wakefield, Tishman Speyer and WeWork.\\n\\nIf you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning!\\n\\nABOUT THE ROLE:\\n\\nAs a Data Engineer at Reonomy, you will tackle hard challenges everyday! We are working to build a data infrastructure that can manage the complexities of commercial real estate as well as scale to support the disparate datasets required to build a first-of-its-kind data product. Using a mix of the latest-and-greatest tech as well as some proven tools, we are pioneering the use of data discovery, pipelining, extraction, importation, sanitization, and visualization of massive datasets to an industry that is eager to utilize an Enterprise CRE SaaS solution like ours in their daily workflow. We take on tough problems in machine learning and scalable data processing using technologies like Scala, Spark, Postgres, ElasticSearch and Docker.\\nResponsibilities include:\\nCollaborating with the Engineering team to design, build and improve Reonomy’s complex data layer\\nCreating data systems that ensure quality and consistency on our data platform\\nSolving real challenges around creating systems that import, cleanse, structure, and display huge volumes of data\\nPlaying a major role in the future architecture of our rapidly expanding backend platform\\nWriting high quality code, participating actively in code reviews, and consistently helping to ship software\\nABOUT YOU:\\n6+ years of experience in a Data Engineering capacity\\nExpertise in building and scaling ETL/batch processing systems that organize data and manage complex rulesets\\nProven ability leveraging database technologies to solve non-trivial, large-scale problems\\nAdvanced/Expert knowledge in SQL and data analysis\\nExperience programming in both typed (Scala, Java, etc..) and non-type (Python, Ruby, etc..) languages on production projects\\nExperience building modern, data-driven, web applications with emphasis on strong software design methodologies\\nA serious passion for data\\nHistory of excellence and responsibility in previous engineering positions\\nBENEFITS:\\nCompetitive salary\\nCompany stock options\\n100% coverage on medical, vision and dental health plans\\nUnlimited Vacation\\n401k plan and commuter benefits\\nOffice perks: catered lunches 3x/week, catered breakfast 2x/week, unlimited snacks, team happy hours, Free Citi Bike membership, fitness discounts, Free Spotify membership, dedicated wellness rooms in the office!\\nWe do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means.\",\n",
       "  'Do you love working with data and analytic platforms?\\n\\nWere a V1 startup that leverages interactive streaming video to create new shopping experiences for customers. Our group offers a creative and fast-paced work environment where youll invent and launch scalable solutions that directly impact Amazon customers. Data is at the center of every product we develop as we create new features and products that serve the needs of our growing base of consumers, brands and advertisers.\\n\\nThe ideal candidate has a strong bias toward data driven decision making, be a self-starter, comfortable with ambiguity, able to think big and be creative (while paying careful attention to detail), and will enjoy working in a fast-paced dynamic environment. If you are excited about using experimentation, data and machine learning to improve customer and brand experience, and want to join a growing team within Amazon - this role is for you.\\n\\n\\n\\nBasic Qualifications\\n\\n· Bachelors of Science degree in Computer Science or related field.\\n· 3+ years of industry experience in software development, data engineering, business intelligence, data science, or related field with a track record of manipulating, processing, and extracting value from large datasets.\\n· Demonstrated strength in data modeling, ETL development, and data warehousing.\\n\\nPreferred Qualifications\\n\\n· Advanced degree in Engineering, Computer Science or related field\\n· Understanding of Big Data technologies and solutions (Spark, EMR, Hive, S3, Redshift, etc.)\\n· Understanding of Amazon Web Services (AWS) technologies\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment.\\n· Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences.\\n\\n\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.',\n",
       "  \"114 5th Ave (22114), United States of America, New York, New York\\n\\nAt Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Engineer\\n\\nBeing Capital One Tech:\\n\\nAt Capital One, we consider ourselves the bank a technology company would build. Were delivering best-in-class innovation so that our 65 million customers - and counting - can manage their finances with ease. Our reality and vision empower our engineers to use artificial intelligence and machine learning to transform real-time data, software, and algorithms into financial clarity.\\n\\nWere all-in on the cloud and a leader in the adoption of open source, RESTful APIs, microservices, and containers. We build our own products and release them with a speed and agility that allows us to get new customer experiences to market quickly. Were going boldly where no bank has gone before. And, as a founder-led company, were inspired and empowered to make, break, do, and do good. So, lets do something great together.\\n\\nYour #LifeatCapitalOne\\n\\nLooking to work somewhere with the flexibility of a start-up but the financial muscle of a Top-10 bank? Youre in the right place! And heres what that means for you\\n\\nYou'll have a flexible work schedulewe want to understand where and when you're at your best so you have a healthy work-life balance. Diversity and Inclusion are cultural norms hereyoull have access to active local chapters of Women in Tech, Blacks in Tech, and Hispanics in Tech and more. Plus, youll be given time to support the next generation of technologists by volunteering with youth programs like Capital One Coders - our engineer-led experience that teaches middle school students in underserved communities how to code. Want to learn more? See what our associates are up to at #LifeatCapitalOne!\\n\\nCalling All Data Engineers:\\n\\nA hub for innovation, our New York presence is expanding, and we need Data Engineers who know their stuff to join our team. As a Capital One Data Engineer, youll have the opportunity to be on the forefront of driving a major transformation within Capital One. Youll work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems. Youll collaborate with digital product managers, and deliver robust cloud-based solutions to drive powerful experiences that help millions of Americans achieve financial empowerment. Want to learn more? Check out the low-down on our high-tech.\\n\\nWho You Are:\\nYou are fun to work with youre excited by a team environment\\nYou are curious. You like to learn new technologies, and you adapt well to change\\nYou are passionate about current state-of-the-art software technologies and tools, with experience implementing them effectively\\nYou are excited about working with cloud-native stack, building on AWS using technologies like Kubernetes and Serverless\\nYou possess a sense of intellectual curiosity and a burning desire to learn\\nYou are motivated and actively looking for ways to contribute\\nYou are passionately focused on the customer and the details that make their experience exceptional\\nYou value data and truth over ego\\nYou possess a strong sense of engineering craftsmanship, take pride in your code\\nYoure pragmatic - you make the best use of time and resources to find the simplest workable solution\\nYou think and act like an owner, taking personal responsibility for both team and product success\\nYou possess great communication and reasoning skills, including the ability to influence and make a strong case for technology choices\\nYou thrive in collaborativeagile teams and are ready to take on new and unexpected challenges while building the next wave of engineering solutions\\nWhat Youll Own:\\nCollaborating with and across Agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies\\nLeading the craftsmanship, security, availability, resilience, and scalability of your solutions\\nBringing a passion to stay on top of current trends, experiment with and learn new technologies, participate in internal & external technology communities, and mentor other members of the engineering community\\nEncouraging innovation, implementation of cutting-edge technologies, outside-of-the-box thinking, teamwork, and self-organization\\nAssisting in the hiring of top engineering talent and maintaining our commitment to diversity and inclusion\\nBasic Qualifications:\\nBachelors Degree\\nAt least 2 years of experience in application development\\nAt least 1 years of experience in big data technologies (Cassandra, Accumulo, HBase, Spark, Hadoop, HDFS, AVRO, MongoDB, or Zookeeper)\\nPreferred Qualifications:\\nMaster's Degree\\n3+ years of experience in application development\\n1+ year experience working on streaming data applications (Spark Streaming, Kafka, Kinesis, and Flink\\n1+ years of experience with Amazon Web Services (AWS), Microsoft Azure or another public cloud service\\n1+ years of experience with Ansible / Terraform\\n2+ years of experience with Agile engineering practices\\n1+ years in-depth experience with the Hadoop stack (MapReduce, Pig, Hive, Hbase)\\n1+ years of experience with NoSQL implementation (Mongo, Cassandra)\\n2+ years of experience developing Java based software solutions\\n2+ years of experience in at least one scripting language (Python, Perl, JavaScript, Shell)\\n2+ years of experience developing software solutions to solve complex business problems\\n2+ years of experience with UNIX/Linux including basic commands and shell scripting\\nAt this time, Capital One will not sponsor a new applicant for employment authorization for this position.\",\n",
       "  \"Team\\n\\nParsley Health is an ambitious, mission-driven team that’s reinventing primary care to help people live healthier lives.\\n\\nYou\\n\\nYou appreciate the challenge of building reliable, secure, complex systems. You have more than three years of data engineering experience working with complicated datasets. You prefer working with typed languages for the multitude of benefits they offer, but are a Python expert. You understand how to modularize systems to enable software teams to scale their efforts. You can't live without CI and have strong devops knowledge. You love writing tests, care about code quality and enjoy keeping up-to-date on industry best practices. You understand the many ways in which distributed systems may fail. You're always thinking beyond the scope of the current project and about the larger product vision which data engineering underpins.\\n\\nRole\\n\\nHelp evolve our existing data pipelines, including Airflow on Kubernetes, Google Pub/Sub and Google BigQuery.\\nBe a technical leader in data engineering within the broader engineering team, mentoring more junior data engineers.\\nEnsure high code-quality and high availability for our data infrastructure.\\nMaintain HIPAA compliant logging and security.\\nCreate new data APIs where needed in strongly typed programming languages.\\nExplore promising new technologies to differentiate our care and products.\\n\\n\\nQualifications\\n\\nExperience with typed languages such as Scala, Kotlin, Rust, Golang, Java.\\nExpert Python skills. Ideally experienced using Airflow, Spark, Kafka.\\nStrong preference to work with typed languages and build tooling.\\nGoogle Cloud / AWS / cloud-centric architecture and design patterns.\\nExperience developing APIs in modern applications.\\nContainerization, CI tools, testing frameworks and other code quality tools are crucial.\\nWork with modern SQL databases, particularly Postgres and BigQuery.\\nExperience with comprehensive testing and iterative continual deployment.\\n\\n\\nBenefits\\n\\nCompetitive salary\\nEquity stake\\nGenerously subsidized medical, vision and dental insurance\\nJoining a diverse, friendly product team\\nWeWork membership and benefits\\n4 weeks paid vacation time\\nFree Parsley Health membership!\\n\\n\\nAt Parsley Health we believe in celebrating everything that makes us human and are proud to be an equal opportunity workplace. We embrace diversity and are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We believe that the more inclusive we are, the better we are able to serve our members.\",\n",
       "  'About Capgemini\\nA global leader in consulting, technology services and digital transformation, Capgemini is at the forefront of innovation to address the entire breadth of clients’ opportunities in the evolving world of cloud, digital and platforms. Building on its strong 50-year heritage and deep industry-specific expertise, Capgemini enables organizations to realize their business ambitions through an array of services from strategy to operations. Capgemini is driven by the conviction that the business value of technology comes from and through people. It is a multicultural company of over 200,000 team members in more than 40 countries. The Group reported 2018 global revenues of EUR 13.2 billion (about $15.6 billion USD at 2018 average rate).\\n\\nVisit us at www.capgemini.com. People matter, results count.\\nJob Responsibilities\\nTitle: Data Engineer\\n\\nLocation: Malvern, PA\\n\\nDuration: Fulltime\\n\\nJob Description:\\nData engineer(s) to create data pipeline for data exploration & feature selection\\nStrong tech skills experience of Python, SPARK & Data Prep tools in AWS-Cloud environment\\nHaving some working experience of data science / modeling is a plus\\nExpertise in adding efficiency to models\\nKnowledge of writing a modular code\\nAdvanced Python (R not necessary):\\nVery strong test-driven programming skills\\nStrong object oriented programming knowledge\\nSoftware integration: API design and integration, DB connection performance tuning, exception handling\\nSQL, Hive, Presto and Advanced Spark ETL (PySpark API rather than Scala)\\nAWS packages (boto, troposphere) to be able to script / automated full workflows / pipelines\\nPlease click on below link to apply for these role:\\n\\nhttps://career5.successfactors.eu/sfcareer/jobreqcareerpvt?jobId=341388&company=C0001123183P&username=&st=6A5BBD13DC96E8C05CF34952C220B2EA43D48768\\n\\nCapgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.\\n\\nThis is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.\\n\\nClick the following link for more information on your rights as an Applicant: http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law',\n",
       "  \"Regular\\n\\nLocation: New York, NY\\n\\nPosition Summary:\\n\\nThe Data Engineer is responsible for building and deploying streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably. You will be responsible for ingesting and integrating large volumes of disparate data from a variety of sources, including but not limited to subscriber & listener data, customer journey data, vehicle data, video and 2nd/3rd party data. This will involve rapid innovation in large scale data pipeline design and development to ensure critical data sets are made available to our users and predictive models in a timely manner. We are looking for someone with hands on experience in all layers of the full stack involving data. The Data Engineer plays a significant role as both an enabler and practitioner of the data and analytics driven culture at SiriusXM.\\n\\nDuties and Responsibilities:\\nBuild and deploy streaming and batch data pipelines capable of processing and storing petabytes of data quickly and reliably.\\nCollaborate with product teams, data analysts and data scientists to design and build data-forward solutions.\\nGather and process all types of data including raw, structured, semi-structured, and unstructured data.\\nIntegrate with a variety of data providers ranging from marketing, web analytics, and consumer devices including IoT and Telematics.\\nBuild and maintain dimensional data warehouses in support of business intelligence tools.\\nDevelop data catalogs and data validations to ensure clarity and correctness of key business metrics.\\nDesign, code, test, correct and document programs and scripts using agreed standards and tools to achieve a well-engineered result.\\nDerive an overall strategy of data management, within an established information architecture (including both structured and unstructured data), that supports the development and secure operation of existing and new information and digital services.\\nPlan effective data storage, security, sharing and publishing within the organization.\\nEnsure data quality and implement tools and frameworks for automating the identification of data quality issues.\\nCollaborate with internal and external data providers on data validation providing feedback and making customized changes to data feeds and data mappings.\\nProvide ongoing support, monitoring, and maintenance of deployed products.\\nDrive and maintain a culture of quality, innovation and experimentation.\\nSupervisory Responsibilities:\\nNone\\nMinimum Qualifications:\\nAdvanced degree in relevant field of study strongly desirable, particularly in computer science or engineering level programs.\\nMinimum 2 years professional experience working with data extract/manipulation logic.\\nMinimum 2 years professional experience with object-oriented programming, functional programming, and data design.\\n1-3 years working with a public cloud big data ecosystem (certification in AWS a plus).\\n1-3 years working with MPP databases, distributed databases, and/or Hadoop.\\nRequirements and General Skills:\\nPassion for data engineering, able to excite and lead by example.\\nHungry and eager to learn new systems and technologies.\\nSelf-directed and enjoys the challenge and freedom of deciding what is the most impactful thing to work on next.\\nAbility to deliver exceptional results through iterative improvement rather than initial perfection.\\nExcellent communication and presentation skills and ability to interact appropriately with all levels of the organization, including: business users, technical staff, senior level colleagues, vendors, and partners.\\nAn extensive track record that demonstrates effectiveness in driving business results through data and analytics.\\nThe ability to develop and articulate a compelling vision and generate necessary consensus.\\nA successful history of translating business objectives and problems into analytic problems, and analytic solutions into actionable business solutions.\\nA proven ability to influence decision making across large organizations.\\nA proven ability to hire, develop, and effectively lead deeply technical resources.\\nDemonstrate and foster a sense of urgency, strong commitment, and accountability while making sound decisions and achieving goals.\\nArticulate, inspire, and engage commitment to a plan of action aligned with organizational mission and goals.\\nCreate an environment where people from diverse cultures and backgrounds work together effectively.\\nTechnical Skills:\\nExperience deploying and running AWS-based data solutions and familiar with tools such as Cloud Formation, IAM, Athena, and Kinesis.\\nExperience engineering big-data solutions using technologies like EMR, S3, Spark and an in-depth understanding of data partitioning and sharding techniques.\\nExperience loading and querying both on premise and cloud-hosted databases such as Teradata and Redshift.\\nBuilding streaming data pipelines using Kafka, Spark, or Flink.\\nFamiliarity with binary data serialization formats such as Parquet, Avro, and Thrift.\\nExperience deploying data notebook and analytic environments such as Jupyter and Databricks.\\nKnowledge of the Python data ecosystem using pandas and numpy.\\nExperience building and deploying ML pipelines: training models, feature development, regression testing.\\nExperience with graph-based data workflows using Apache Airflow a plus.\\nKnowledge of data profiling, data modeling, and data pipeline development.\\nStrong knowledge with high volume heterogeneous data, preferably with distributed systems.\\nStrong knowledge writing distributed, high-volume services in Python, Java or Scala.\\nFamiliar with metadata management, data lineage, and principles of data governance.\\nKnowledge of data modeling, data access, and data storage techniques.\\nAppreciation of agile software processes, data-driven development, reliability, and responsible experimentation.\\nMinimum 2 years' experience with the following:\\nprofessional role in one or more of the following: Development, Engineering, R&D or Information Technology\\nStrong and thorough knowledge of the following:\\nETL/ELT Tools\\nBI tools\\nMDM / Reference Data\\nRDBMS, NoSQL and NewSQL\\nMS Office Suite\\nSiriusXM is an equal opportunity employer that does not discriminate on the basis of sex, race, color, age, national origin, religion, creed, physical or mental disability, medical condition, marital status, sexual orientation, gender identity or expression, citizenship, pregnancy, military or veteran status or any other status protected by applicable law.\\n\\nThe requirements and duties described above may be modified or waived by the Company in its sole discretion without notice.\\n\\nSDL2017\\n\\n</br>\",\n",
       "  \"Reonomy leverages big data, partnerships and machine learning to connect the fragmented, disparate world of commercial real estate. By creating a single source of truth, Reonomy products empower individuals, teams and companies to share information, unlock insights and discover new opportunities.\\n\\nHeadquartered in New York, Reonomy has raised ~$70 million from top investors, including Sapphire Ventures, Bain Capital, Softbank and Primary Ventures. Our clients represent the biggest names in CRE, including Newmark Knight Frank, Cushman & Wakefield, Tishman Speyer and WeWork.\\n\\nIf you're excited by growth, innovation and the ability to shape one of the biggest markets, join us as our journey is just beginning!\\n\\nABOUT THE ROLE:\\n\\nWe’re looking for an accomplished Data Analyst to create key information that will drive what, how, and when we build out our products. Your work will allow other departments across the business to make better, faster, and more data-rich decisions. As the first Data Analyst, you will play a vital part in defining our analytics strategy and methodology.\\n\\nAs a rapidly growing company, we are looking for someone who is enthusiastic about data analytics and delivering excellent work that will impact crucial business decisions!\\n\\nResponsibilities include:\\nCreate engaging analytical deliverables that that will drive product and engineering strategy\\nCompile, transform, and interpret data from multiple sources using SQL\\nCreate statistics and metrics for our product offerings\\nReview and maintain data quality components and identify solutions\\nCollect, document, and systematically handle data issues\\nExecute quarterly analytics work for online publications\\nProvide actionable insights to help product managers understand trends, marketplace dynamics, and user behaviors\\nABOUT YOU:\\n2 years direct data analysis and reporting experience\\nAdvanced/Expert knowledge in SQL\\nA self-starter with a knack for organization\\nStrong verbal and written communications\\nA detail oriented mind with inclination for investigative data work\\nProactive and persistent\\nBENEFITS:\\nCompetitive salary\\nCompany stock options\\n100% coverage on medical, vision and dental health plans\\nUnlimited Vacation\\n401k plan and commuter benefits\\nOffice perks: catered lunches 3x/week, catered breakfast 2x/week, unlimited snacks, team happy hours, Free Citi Bike membership, fitness discounts, Free Spotify membership!\\nWe do not accept unsolicited resumes from outside recruiters/placement agencies. Reonomy will not pay fees associated with resumes presented through unsolicited means.\",\n",
       "  \"We're working to change the way wealth is created.\\nYieldStreet is disrupting the trillion-dollar lending and investment industry. We recently raised $113M in financing from world-class venture capitalists to help us scale. Our investor demand is soaring, and our team is expected to double in size this year. Do you have what it takes to join a booming startup that is changing the investment industry?\\n\\nAbout YieldStreet\\n\\nYieldStreet connects investors to opportunities in alternative asset classes like real estate and litigation finance that were historically unavailable to most. YieldStreet was created because our founders were frustrated by the lack of options regular investors had to participate in asset backed investment opportunities that exist outside of the stock market.\\nWe believe top notch technology and data can help us build the worlds most robust alternative investment platform and create financial independence for all. Join us.\\n\\nAbout this role\\n\\nYieldStreet is looking to expand our data science organization by adding an additional Data Engineer dedicated to our platform. In this role you will be responsible for designing, building and scaling our data infrastructure. You will be the bridge between our engineering, data science and business teams - owning the data stack as well as aggregating, analyzing, predicting, and visualizing data and performance. If you are looking to expand your skillset in one of NYC's fastest growing startups and work with an extraordinary team, apply below!\\n\\nWhat you'll do\\n\\nPartner with our engineering team to build and optimize data pipelines from our database, website analytics, marketing analytics, and other financial data sources\\nBuild a data warehouse that is easily accessible and understandable for business teams to run saved reports and ad-hoc analysis\\nPartner with our data science team to uncover investor and underwriting insights\\nPartner with our marketing, product and investments team with special projects and ad-hoc analysis\\nBuild processes for ensuring data integrity\\n\\nRequirements\\n\\nBS/MS degree or track record of success in a heavily quantitative field\\n2+ years of experience, including experience designing and scaling data infrastructure, models, and pipelines\\nAdvanced fluency in SQL, NoSQL & Excel\\nExperience with a major coding language (e.g: Python, R)\\nDeep knowledge around a variety of data tools and technologies including data warehouse and data visualization\\n\\nWhat Makes You Successful\\n\\nOwn and Execute:\\nGets things done, with both a short and long-term view in mind\\nBe able to thrive in a fast-paced, agile environment with exceptional organizational skills and the ability to re-prioritize on a consistent basis\\nExemplary planning and organizational skills, along with a high attention to detail\\nDecide with Data:\\nBe pragmatic and outcome-oriented, leveraging data to make decisions\\nDetect new or missed opportunities that will help YieldStreet grow and improve regularly\\nBe able to package data so it can be easily digested, analyzed and reacted to\\nBe Passionate:\\nBring a focus of YieldStreet’s vision to your day to day tasks\\nBet on your strengths; allow these skills to be absorbed by leaders on your team to make an impact\\nBe known for authenticity, honesty, and integrity\\nStay Curious:\\nNever take no for an answer, be able to dissect an issue and come up with creative solutions\\nAsk questions. At YieldStreet we believe that progress stops when you stop asking questions\\nFocus on team collaboration and communication\\n\\nBenefits\\n\\nWe offer an attractive market salary, stock option plan, health & dental benefits, life insurance, 401(k), paid vacation and holidays and that's before you even step in the office!\\nThis is an opportunity to work with a group of diverse, smart, and friendly people from 8 different countries who speak a total of 17 different languages. Our team is comprised of successful entrepreneurs with combined exits of over $1B, and we get social with each other during happy hours in our work hard/play hard culture. Make sure you get to know Diver the fish, and help yourself to a variety of top tier scotch and whiskies at the company bar.\\nWe're located in a beautiful new office in Midtown, our building is close to most major subway lines.\\n*** Please, no solicitors or recruiters. We're saving our dollars to pay the right candidate!\",\n",
       "  'Company Overview\\n\\n\\nAt Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. For the 28th year, MSK has been named a top hospital for cancer by U.S. News & World Report. We are proud to be on Becker’s Healthcare list as one of the 150 Great Places to Work in Healthcare in 2018, as well as one of Glassdoor’s Employees’ Choice Best Place to Work for 2018. We’re treating cancer, one patient at a time. Join us and make a difference every day.\\n\\nJob Description\\n\\n\\nAre you passionate about contributing meaningfully to battling cancer? Then join us here at MSK, where we can provide you with the opportunity to make a difference with your career. We believe this is a very exciting opportunity for someone who has the right skillset and drive to make an impact to support our mission.\\n\\nThe Computational Oncology Program in the Department of Epidemiology and Biostatistics is seeking a talented, highly skilled Data Engineer to join their team. We are motivated by contributing meaningfully to contemporary progress in cancer research driven by advances in computing and data. The right person will work in close collaboration with researchers and software engineers, and be responsible for managing data from leading edge, large scale research efforts in computational biology including genomics, imaging and clinical data analysis and interpretation. The Data Engineer will have experience managing data utilizing robust, enterprise level contemporary software systems.\\n\\nYou Will:\\nManage data from high-throughput next-generation sequencing and imaging\\nContribute to the design of databases as part of bioinformatics data processing and analysis systems\\nMaintain and monitor streaming and batch ETLs operating on structured and unstructured sources\\nMaintain a data lake with hundreds of terabytes of data\\nDevelop workflows and integrate systems with REST APIs\\nCompile datasets and verify data consistency\\nCommunicate with stakeholders of the data and upon request, conduct data query tracking and resolution\\nIdentify inefficiencies and work with software engineers to simplify processes, debug systems and automate routine tasks\\nYou Need:\\n\\n\\nBachelor’s Degree in Computer Science, Information Systems, or Database Management (or equivalent experience), Master’s degree is preferred\\n3+ years of experience, preferably with bioinformatics lab information management systems\\nExperience designing databases and defining system requirements for data collection\\nStrong software engineering skills in Python, and working with SQL and NoSQL data\\nSolid experience in Linux systems, and shell scripting\\n#LI-POST\\n\\nClosing\\n\\n\\nMSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.\\n\\nFederal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.',\n",
       "  'We’re looking for great engineers with diverse backgrounds. We need people that can fill hybrid roles with different emphases on the wide context of product development. We don’t have predefined positions: instead, we invite you to come talk to us about your skills, experience, ambitions, and dream role. We hope to find exceptional people to do exceptional work with us, and we believe it’s important that we define your role and path to our mutual growth together.\\nReaktor is a hybrid partner for all things future. We reimagine businesses, and we design and build tomorrow’s digital products. Our full range of consultancy and agency services include expertise from high-level business consulting, to hands-on service and product design and development, to branding and marketing.\\nWe believe in technology-agnostic product thinking, defined only by finding meaningful solutions to challenging problems. Your passion should be the same as ours: unmatched execution of those solutions. We structure, create, iterate, and reinvent, delivering tangible value faster than anyone in the world.\\nWhat we can offer you:\\nHybrid, completely autonomous teams, filled with experts in everything from design, to data science, to business consulting, to content.\\nA community of amazing people who you will learn from and will learn from you.\\nExperimentation and continuous improvement of our skills and ways of working.\\nA dedication to constant evolution (and the occasional revolution).\\nA world without rigid hierarchy—big ideas can come from anyone and anywhere.\\nEndless fun!\\nWhat you’ll bring:\\nExceptional technological competence.\\nAbility to make things, and to make things happen.\\nCompassion for people—both those around you, and those who will use what we build.\\nDrive to work on complex problems at any part of the project, from type system nuances to organizational dynamics.\\nA vision and commitment to the future of work: being truly lean and agile and human-centered, and leaving yesterday’s pointless rituals behind.\\nReadiness to be responsible for your work, development, and career path. Don’t worry: we’ll have your back in whatever you want to achieve, but the end goal is yours to define.\\n6+ years of real-world experience in building digital products and services.\\nAuthorization to work in the United States.\\nThink Reaktor sounds like home? Great. Introduce yourself via our careers page.\\nWe invite you to share details of any open source projects, your GitHub profile, and most importantly, your personality. Be original, not official.\\nWe are an Equal Employment Opportunity (EEO) employer and do not discriminate on the basis of race, color, national origin, religion, gender, age, veteran status, political affiliation, sexual orientation, marital status or disability.',\n",
       "  \"About Teampay\\n\\n\\nAt Teampay, we believe in a world where finance teams create a more efficient business with less friction and happier employees. Our distributed spend management software modernizes how companies manage spending by delivering intelligent automation, robust integrations, and a delightful user experience.\\n\\nTop-performing finance teams leverage Teampay to gain control and visibility into spending, while empowering employees to move at the speed of business. Led by an experienced team with prior exits, Teampay has raised double-digit millions of capital from prominent venture investors including CrossCut Ventures, Tribe Capital, Precursor Ventures, and CoVenture.\\n\\nWe're an agile team building software that's revolutionizing how companies spend their money. Joining Teampay at this early stage is an opportunity to grow your skill set, build a company, and get paid to do it.\\n\\nAbout the role\\n\\n\\nAs a data engineer at Teampay, you will architect a data infrastructure that is stable and scalable to support data analytics, reporting, and visualization. Your input and contribution will have a direct impact on our data strategy and technology roadmap.\\n\\nWhat you'll do...\\nCombine and analyze data from various sources to help drive business insights\\nWork with various team and executive stakeholders to define mission-critical metrics and key performance indicators\\nDevelop and maintain a scalable data infrastructure\\nDevelop tools supporting self-service data pipeline management\\nWork closely with teams on design and implementation of data solutions\\nOwn and provide BI development tools for internal use\\nCommon Candidate Qualifications include\\n3+ years functional experience in a data engineering or business intelligence role\\nProficient in data modeling and systems design skills\\nProficient in SQL\\nProficient in at least one scripting language (ideally Python)\\nExperience building and maintaining robust ETL pipelines\\nExperience with version control systems\\nExperience with BI platforms\\nNice to Have...\\nExperience with AWS\\nMachine learning experience\\nYou enjoy telling a compelling story with data\\nYou would be the first Data Engineer in a seed stage, fast-growing company. As a core member of the engineering team, you will have a strong impact on the future of Teampay. This is an unusual opportunity to join a business with traction at the ground floor.\\n\\nApply at Teampay if...\\n\\n\\nYou're a builder. You're passionate about crafting things that matter. You're curious and agile in thought and action. You value authenticity and possess a strong work ethic. You're empathetic and look forward to learning from people unlike yourself. You want to make an impact with a strong team. You look for challenges that force you to grow. You rarely miss a detail and always learn from your mistakes. You have diverse interests outside of work, but are ready to pitch in and be responsive when the pressure is on.\\n\\nInterested?\\n\\n\\nYou can learn more about the product at www.teampay.co and if you'd like to apply, please include a cover note and tell us about something you started, whether it's a club, a team, business or blog.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\",\n",
       "  \"ABOUT THE ROLE\\n\\nAnalytics works with numerous departments across UncommonGoods to turn data into insights. We are building out our data infrastructure and building out the team to take advantage of it. This role will play a key part in spreading data culture throughout the company, allowing other departments to make better, faster, and more data-rich decisions.\\n\\nYou will:\\nWork closely with our analytics and technology teams to improve our new data warehouse.\\nAdd new data sources to the data warehouse pipeline, such as Google Analytics 360, Google Search Console, Listrak, and many other third-parties.\\nTransform data by creating views in the data warehouse to simplify data retrieval.\\nManage and improve our third-party web site tags with assistance from developers and assist in our migration from Google Analytics Classic to Universal.\\nManage our third-party data feeds (Facebook, Google Shopping, Listrak, etc.) and make changes when needed.\\nTake on other technical and analytical projects as you demonstrate your abilities and interests.\\nABOUT YOU\\n\\nThis is an intermediate-level position, and we're looking for someone with the ambition to help the company get more accurate and easily accessible data. You'll be working with a team of experienced data scientists and technologists that will help shape your development. You'll initially be focusing on our new data warehouse that we started building this year.\\n\\nWe also want a great collaborator; someone whose written and verbal skills match their technical ones.\\n\\nLastly, we want someone who shares a belief in our mission, and who wants to work at a company where doing good is as valued as doing well.\\n\\nABOUT US\\n\\nUncommonGoods offers remarkable designs by independent makers, and we do it with a positive impact on people and our planet. Learn more about our products, B Corp certification, Better To Giveprogram, and cool team members you might be working with.\\n\\nSKILLS & QUALIFICATIONS\\n\\nRequired\\n• Ability to communicate with both technical and non-technical colleagues\\n• Proficiency with SQL and databases (we use Postgres, Redshift, and Oracle)\\n• Experience with one or more programming languages such as Python, R, or Java\\n• Experience with ETL tools such as Airflow, AWS DMS, and FiveTran\\n\\nBonus\\n• Experience with ecommerce and retail\\n• Experience with AWS Redshift\\n• Experience with Google Analytics\\n• Experience with version control such as Git\\n• Experience with BI platforms such as Tableau, Power BI, or Looker (we use Tableau)\\n• Knowledge of software development best practices\\n\\nCOMPANY PERKS\\n• Robust benefits package with generous PTO, 401(k) with company match, and paid family leave\\n• Stock options\\n• Casual work atmosphere, company snacks, lunches, and team events\\n• 40% discount on our products\\n\\nTo apply, please submit the following\\n• Resume\\n• Cover Letter (Make it Uncommon!)\\n• (optional) Examples of past work such as: papers, blog posts, and other publications; links to Github or other code repositories.\\n\\nUncommonGoods is an Equal Employment Opportunity Employer\",\n",
       "  'HelloFresh delivers \"cook from scratch\" meal plans straight to your door, with easy to follow recipe cards and high quality, pre-portioned fresh ingredients—saving time and money that you can instead spend with your loved ones!\\n\\nHelloFresh is the world\\'s leading meal kit company and is expanding rapidly. Founded in November 2011, HelloFresh now operates in the U.S., the United Kingdom, Germany, the Netherlands, Belgium, Luxembourg, Australia, Austria, Switzerland, and Canada. HelloFresh delivers millions of meals to millions of customers across the globe. and listed in November 2017 at the German Stock Exchange in Frankfurt.\\n\\nData Science Department:\\n\\nEmbedded in the NYC Tech Hub, we are building a cross-functional team of data scientists, analysts, and engineers with the mission to bring the modeling capabilities of our marketing organization to the next level. At HelloFresh we believe that automation and Machine Learning can take our marketing efforts to the next level by marrying the right customer with the proper advertisement. We will accomplish this using technology on a large scale instead of using manual processes. We are actively investing in the right people, data, and infrastructure to support this vision.\\n\\nJOB DESCRIPTION:\\n\\nWe are hiring a Data Engineer to work alongside marketing. Our vision is to build an automated platform directing our marketing initiatives on delivering the right food box at the desired price point to the front door of all our customers.\\n\\nWe are currently developing 2 major projects:\\n\\n1.Optimization of audiences and targeting across our foodtech platforms\\n\\n2. A data-driven and ROI based, campaign testing machine\\n\\nResponsibilities\\nDesign and build a world-class self-service data platform\\nTaking end-to-end responsibility for design, build and maintenance of batch and real-time data pipelines.\\nUnderstand and bring solutions for complex business problems, enabling insights that can empower better decision-making\\nWork closely with data scientist in bringing their models in production\\nBasic Requirements\\nDegree in Computer Science, Software Engineering, or equivalent experience\\n3 + years\\' related experience\\nAdvanced Python skills\\nExperience with Apache Spark\\nExperience with Amazon AWS, DevOps and Automation\\nThe ability to design, implement and deliver maintainable and high-quality code\\nExperience with data modeling, design patterns, building highly scalable and secured solutions\\nExperience with RDBMS such as PostgreSQL or MySQL\\nExperience with job orchestration tools like Airflow, Luigi or similar\\nExperience with end-to-end testing of data pipelines\\nKnowledge of visualization tools (Tableau, Grafana)\\nExperience with Data Quality checks and tools\\nYou\\'ll get\\nCompetitive Salary & 401k company match that vests immediately upon participation\\nGenerous parental leave of 16 weeks & PTO policy\\n$0 monthly premium and other flexible health plans\\n75% discount on your subscription to HelloFresh (as well as other product initiatives)\\nSnacks, cold brew on tap & monthly catered lunches\\nCompany sponsored outings & Employee Resource Groups\\nCollaborative, dynamic work environment within a fast-paced, mission-driven company\\nIt is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran.',\n",
       "  'Data Engineer\\nAt Hospital for Special Surgery our clinicians and scientists collaborate to deliver the most innovative care. Our specialized focus on orthopedics and rheumatology enables us to help patients get back to what they need and love to do reliably and efficiently. Our patients are overwhelmingly satisfied with the care they receive at our facilities. When you join us, you will become part of this legacy of commitment to the most cutting-edge research and coordinated care.\\nThe ideal candidate must be passionate about the field of analytics and well versed in all aspects of an enterprise-wide analytics architecture. We are looking for Data Engineers who have a strong understanding of data pipelines, ready to work hard but most importantly curious. They will work collaboratively across the organization with clinical and operational leaders on a diverse range of projects to find insights that support our mission of providing world class musculoskeletal health.\\nAs a Data Engineer You Will\\nDesign, implement, and automate data flows\\nAssist in the implementation and administration of the analytics data lake\\nWork with partners and vendors on data integration projects\\nDesign and create data models for new or existing data sources\\nDesign and implement ETLs\\n\\nYour Technical Skills and Experience\\n3+ years’ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool\\n3+ years’ experience with SQL Server, strong expertise in T-SQL required\\n3+ years’ experience with Web Services: RESTful APIs, .NET WCF and SOAP\\n1+ years’ experience with .NET C# or other OO language required\\nExperience with Windows Server Operating Systems required\\nExperience with SSAS and building cubes required\\nExperience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)\\nExperience with public cloud preferred: AWS or Azure\\nExperience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.\\nExperience in healthcare a plus\\nAbout You\\nYou are passionate about data and exploring new technologies\\nYou are a critical thinker who loves solving difficult problems\\nYou are flexible and a natural team player\\nEducation and Certifications\\nMS/BS in Computer Science or related field',\n",
       "  'At White Ops, we\\'re all hackers. This doesn\\'t mean you have to hold a key to the internet (although one of our founders does!), but it does require approaching problems in unconventional ways. We are attacking criminal botnets through the ad fraud that finances their operations. Our proprietary technology detects and prevents sophisticated online fraud. By reducing the incentives for crime, we are making the internet a better place. Our mighty team of humans have several things in common:\\n\\nWe are hackers.\\nWe create tau for ourselves and each other.\\nWe think big and move fast.\\nWe practice humility.\\nWe \"pwn it\".\\n\\nWhat we do:\\n\\nWe keep the internet safe by fighting organized cybercrime. Our products accurately detect and prevent even the most sophisticated bots and our customers include some of the web\\'s largest and most forward-thinking companies.\\n\\nOur team of engineers is friendly, curious, driven, and highly collaborative. We support one another and regularly hold code reviews to facilitate learning and reduce friction during our 2-week releases. We strive for clean and readable code. Our engineers also work closely with other teams in the company, including creative, product, and customer support.\\n\\nThe Data Platform team processes terabytes of data a day as the backbone for our analytical systems. We are looking for a hands on developer to design, develop and test our data platform and continuously improve the performance, availability and scalability of our products. Experience with fast and scalable distributed systems is a must as well experience with structured and unstructured data sets.\\n\\nWho you are:\\nYou have a strong sense of ownership which drives you to find ways to do things better, faster, and cheaper\\nYou look to find new and innovative ways to solve complex problems through rigorous experimentation\\nYou are open and transparent and work in tight collaboration with the rest of White Ops\\nOpen to tasks beyond software development, for example, there will be occasional 2nd level support for our processes on a rotational basis\\nWhat you\\'ve done:\\nExperience building data pipelines using Java or Python\\nExperience with distributed data storage systems/formats using parallel processes and/or columnar data stores such as Snowflake, Redshift, Druid, Cassandra, Parquet, Kudu, or HBase\\nSQL knowledge is required\\nLinux/Unix experience preferred\\nExperience working with batch processing/ real-time systems using various open source technologies like Spark, MapReduce, NoSQL, Hive, etc.\\nExperience working with messaging frameworks such as Kafka or Kinesis\\nHave worked with a major cloud provider such as AWS or Google Cloud\\nKnowledge in data modeling, data access, and data storage techniques for big data platforms\\nExposure to Continuous Integration/Continuous Deployment & Test Driven Development preferred\\nWe understand it takes a diverse team of highly intelligent, passionate, curious, and creative people to solve the challenges involved in protecting the Internet. Our dynamic team of humans have incredible perspectives to share, just as we know you do, and we take great pride in being an equal opportunity workplace.\\n\\nAbout White Ops:\\n\\nOur mission is to protect the internet by verifying the humanity of every online transaction. Founded in 2012 in a Brooklyn sci-fi bookstore, we now serve trillions of monthly decisions for some of the web\\'s largest platforms. Our technology protects digital advertisers, publishers, ad tech, and enterprise businesses globally by detecting and blocking bots. It\\'s an ongoing war that we happily fight every day.\\n\\nLife at White Ops:\\n\\nOur HQ office is centrally located in NYC in the heart of the Flatiron District. We had approximately 100% headcount growth in the past year and we are growing the company deliberately, with a keen eye towards maintaining a culture that values diversity, lifestyle, and career growth. We are doing meaningful work and we need people to join our mighty team. We are proud of our overwhelmingly positive presence on Glassdoor and Built in NYC. We have offices located in NYC, DC, Victoria, and London.\\n\\nWe have many world class customers who use our platform • We\\'re focused and propelled by a substantive mission • We\\'re building a very sophisticated product that fights a real threat to humanity • We\\'re committed to building a product companies need • We have created an environment that optimizes your skills and brings out your best work •',\n",
       "  \"About Us\\n\\nBorn out of frustration with the traditional cash register business, ShopKeep was designed by a retailer with a noble aim: to rescue independent business owners from the nightmare of archaic point of sale systems, and replace them with something beautiful, simple, and affordable. It turned out that by doing this, we were giving our fellow merchants a fighting chance against the big guys.\\n\\nToday, our mission is simple: empower independent business owners to dream big and to fight smart. We're doing this through our cloud-based architecture, amazing customer care, and intuitive software that delivers the data small business owners need to run smarter businesses.\\n\\nAt ShopKeep, we've been successful because of our awesome team that believes small businesses make up the heart of our communities.\\n\\nAbout This Role\\n\\nWe have terabytes of data and want you to put it to use. You'll work with us on the Data Engineering team to help others throughout the company get the most out of our data. A typical day might include taking a meeting with Customer Care about tracking agent productivity followed by running down a bug with Product's event analytics models in the afternoon. In this role you'll be wearing different hats but mostly building out our python ETL processes or writing excellent SQL, so you should be comfortable jumping in and out of projects.\\n\\nWhat You'll Do\\nYou are passionate about working with data to solve business problems and will build and maintain the infrastructure to answer questions with data.\\nWe have a big warehouse with data ranging from Google AdWords to Zuora billing to Product usage. You'll model new constructs and improve existing ones.\\nHelp streamline our data science workflows, adding value to our product offering and building out our customer life cycle and and retention models.\\nAll of our teams are hungry for insights. You'll work with stakeholders across the company, fielding abstract requests. This may involve pointing someone to the answer, teaching them how to fish, or ticketing a large project for later.\\nWhat We'd Like to See\\nSQL (3 or more years of experience)\\nPython (3 or more years of experience)\\nData visualization / exploration tools (1 or more years of experience)\\nCommunications skills, especially of technical concepts to non-technical business leaders\\nAttention to detail\\nFamiliarity with the AWS ecosystem specifically RedShift and RDS\\nBonus Points For\\nLooker experience\\nBuilding or maintaining ETL processes\\nAbility to juggle multiple projects\\nExperience with Git\\nBenefits\\n\\nWe provide the essentials...\\nHealth, Life and Disability Benefits\\nEmployee Assistance Program (EAP)\\nGenerous Referral Bonus Program for Technology Roles\\nFlexible Paid Time Off (PTO)\\n401(k) Match\\n...and the fun-damentals:\\nLively and enriching Engineering culture\\nRegularly scheduled hackathons, meetups and Tech Talks\\nOpportunity to attend Engineering Conferences, thanks to our generous company conference budget\\nNewsletters produced by Engineering teams\\nCross-office collaboration between our NYC and Belfast teams with the opportunity to travel to our different offices\\nRegular team events, including Happy Hours and Game Nights\\nCatered lunches\\nBreak area to play and relax\\nStanding desks\\nShopKeep is an Equal Opportunity Employer\\n\\nWe are an Equal Opportunities Employer. We don't discriminate based on gender, gender identity, sexual orientation, race, nationality, or any other individual characteristics. We practice equality of opportunity in employment and select the best person for the job.\",\n",
       "  \"Who is likely to renew their season tickets? Or churn? Which fans are likely to upgrade to a package? Or buy again? How do we best communicate with fans and encourage them to attend another game? Which team is a fan's favorite? How do we quantify the strength of that relationship?\\n\\nMLB is looking for a Data Scientist to join its Analytics team in our New York City office. The Analytics team aggregates data from hundreds of online and offline sources and leverages it to provide insight, better serve fans, and drive growth for the league and its 30 Clubs.\\n\\nIn this crucial role, you will be responsible for manipulating and analyzing complex, high-volume, high-dimensionality data from a myriad of sources. You will help drive a deeper understanding of MLB's fans, develop and validate predictive models, partner with Data Engineering to deploy them at scale, and collaborate with the Marketing and Product teams and Clubs to drive ticket sales, fan engagement and business impact. You will also have the opportunity to present and share insights with business leads at MLB and the 30 Clubs.\\n\\nIf you love Data, Baseball, and delivering work that makes an impact, you've come to the right place.\\n\\nCore Responsibilities\\nTranslate business challenges into data pipelines & model framework.\\nTrain, validate, and tune predictive models around fans' behavior.\\nAnalyze, interpret & communicate the results of experiments.\\nDeliver business value by translating complex data into meaningful insights.\\nEffectively present and explain complex concepts to a non-technical audience.\\nQualifications\\n2-4 years of relevant experience in Data Science\\nBachelor's degree or higher in a quantitative field such as mathematics, statistics, computer science, etc.\\nA strong foundation in probability, statistics, and algorithm development\\nProgramming skills that allow you to be self-sufficient in handling large datasets (Python, SQL, Scala, Java)\\nExperience with statistical tools or packages: Python (strongly preferred; specifically with scipy, numpy, scikit-learn etc), R, SAS, SPSS, or Matlab\\nExperience programmatically structuring and cleaning data, and not just analyzing highly cleaned data sets\\nYou're a self-starter that's highly accountable and will take ownership of delivering your work\\nStrong understanding of building models, ability to validate and measure efficacy of models via various metrics, and the knowledge to know what metrics make sense for which situation\\nComfort manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources\\nExperience with cloud services platforms (Amazon Web Services, Google Cloud, etc.)\",\n",
       "  'Jobtitle: Data Engineer\\nLocation: New York, NY\\nDuration: Long Term Contract\\nAmazon Web Services , MongoDB\\nGood Data platform development experience using Python, Spark, AWS services.\\nETL experience in Talend Big Data is a good add on.',\n",
       "  \"Vroom.com is a venture-backed, fast-growing start-up focused on revolutionizing the car buying experience. Our approach is unique in that we recondition pre-owned vehicles to a high standard, sell online, and deliver anywhere across the US. We have experienced tremendous growth in our first 5 years of operation and have become a disruptive force in the automotive industry. Vroom is an exciting, accelerating workplace, and there's no better time to join the team than right now.\\n\\nWe are building a team of experienced Data Engineers to ensure that our data infrastructure supports and helps drive the dramatic growth that we expect! We're looking for an exceptional data engineer to help us organize, test, and operationalize our data. Our business depends on putting the right data in front of the right people at the right time. As a Data Engineer in this dynamic environment, you will be instrumental in pulling data from multiple sources, performing extensive analysis, and applying a variety of data science models to provide our internal customers with recommendations and feedback.\\nResponsibilities\\nEstablish and maintain best practices for our data infrastructure\\nDevelop next-gen data pipelining and ETL based on open source data pipeline tools and cloud-based ecosystems that can deal with varied data types from disparate sources\\nDevelop and tune data storage and processing systems at scale\\nBuild real-time data processing systems\\nBuild automated systems to continually monitor data quality and integrity\\nWork closely with stakeholders across Vroom to ensure data is accurate, timely, and useful\\nQualifications\\n3+ years experience in data/software engineering or related field\\nFluency in Python and SQL, experience with Golang, C, C++, Java, or Scala is a plus\\nDemonstrated experience with distributed computing (Kafka, Storm, Spark, Hadoop, etc.)\\nExperience with NoSQL databases\\nProficiency in using and managing cloud infrastructure, preferably AWS\\nLinux and Bash competence\\nExperience integrating with Salesforce a plus\\nBenefits\\n\\nThis full-time role offers competitive compensation; health, dental, and vision insurance through United Healthcare; a 401k plan; fully company-paid short term disability, long term disability, and life insurance; access to a healthcare concierge service with virtual visits; and 15 annualized days of paid vacation.\\n\\nBut our biggest benefit is being part of a low-ego, high performing team that's transforming the used car market into a modern, online and data-driven industry. We are looking for people who want to be a part of a contemporary startup culture. What gets us out of bed is working with talented people on a mission that matters.\\n\\nTo Apply\\n\\nIf you think you might be who we’re looking for, apply below with your resume and a cover letter telling us why you think you’d be a great addition to the team.\",\n",
       "  \"Data Engineer\\n\\nLOCATION: NYC\\n\\nAt Ippon, We take our work seriously, but not ourselves. We turn large amounts of coffee into beautiful Code. We are always learning, and always improving on the latest technologies. We empower our people because they are our biggest asset. We put a high value on our culture, and continue to see success because of the great people that make us who we are.\\n\\nIppon values talented individuals who are fun to work with. We also value diversity and the different perspectives our employees bring to Ippon Technologies everyday. We are an Advanced AWS Consulting Partner. Do you love Data, Cloud and Open Source Technologies? Are you driven by providing solutions and value to clients? Are you the one everyone comes to for all things Tech? Then Ippon USA is where you belong! And It’s your lucky day, because we are currently seeking a Data Engineer.\\n\\nTake your next career step with Ippon Technologies, a Global Data leader!\\n\\nWhat you will do:\\n\\n\\nDesign and implement Data Architectures\\nCreate and optimize data pipelines\\nEnhance real-time data processing\\nWork with Data Scientists and Data Owners to understand use cases and hypothesis to test with the data\\nBuild complete data platforms by assembling major Big Data solutions such as: EMR, Kafka, Spark, Airflow, Flink, Snowflake, Elasticsearch, etc.\\nScala / Java / Python coding\\n\\n\\nWhat we are looking for:\\n\\nBachelor's degree in Computer Science or a related field\\nAt least three (3+) years of software development experience\\nData Streaming techniques: Apache Kafka, Amazon Kinesis, Flink, Spark Streaming, etc.\\nExperience with AWS technologies: DynamoDB, Glue, Athena, Redshift, Kinesis, EMR…\\nAWS Certifications: Solution Architect Associate or Pro, Big Data Specialty\\nGood knowledge of NoSQL solutions, must be able to recommend the best fit for NoSQL solutions (MongoDB and/or Cassandra is a plus)\\nSnowflake experience desirable\\nCI/CD\\nAgile development\\nJudo optional\\nMust love croissants\\n\\n\\nIppon values talented people who are fun to work with. We also value diversity and the different perspectives our employees bring to Ippon Technologies.\\n\\nSo, do YOU speak Ippon?\\n\\nIppon USA is an Equal Opportunity Employer\",\n",
       "  'The Requirements\\nMinimum of a degree in a quantitative subject, ideally containing computer science, software engineering, database, or data analysis modules\\n3 to 5 years experience working as a data engineer or software engineer with a strong focus on data, ideally in financial services (insurance, banking, instech, fintech)\\nStrong and proven capability in dimensional data modelling: to design, implement and use dimensional database / datamart structures to ensure data is organised, aggregated and indexed effectively and ready for downstream reporting, analysis and statistical modelling\\nStrong and proven capability in data acquisition: to design data capture systems, collect or stream data from a variety of systems, to creatively transform, manipulate, and methodically describe it\\nStrong and proven capability with database administration: to maintain transactional and reference data, with migration alongside product development to ensure logical consistency\\nStrong and proven capability with data validation and verification: to understand the pitfalls in different data sources and to perform necessary provenance and lineage checks, and implement various data testing\\nA passion for quality software coding, data and analytics and some knowledge of statistics\\nExperience working in a fast-paced environment and desire for good documentation, exceptional delivery, effective organisation, and high-quality communication with team and customers\\nStrong and proven technical skills in MS SQL Server, SSIS, SSRS, Python, Git, Bash / Powershell\\nDesirable technical skills in MS Azure (inc DataFactory), Apache Airflow, PowerBI, Linux (Redhat / CentOS)\\nEqual Opportunity Employer/Vet/DisabilityInnovisk is a global underwriting platform with a strategy of growth and diversification. We are building market leading underwriting businesses through the application of modern technology, high quality data management, advanced analytics, and by attracting industry leading underwriting talent.\\n\\nThe Data Science & Actuarial team is part of an international core of integrated advisory, technology and analysis services supporting speciality & commercial insurance underwriters in a wide variety of lines including Commercial EL/PI, D&O, Energy, Environmental, Financial Lines, Inland Marine, M&A, Renewables and Surety.\\nWe are looking for a Data Engineer to join our team of experienced data engineers, reporting engineers, data scientists and actuaries to help our underwriting businesses to evaluate, price and manage their risks & business operations.\\nThe successful candidate will have a passion for creating and managing innovative data assets, and will work closely with the tech team, the underwriters and the operations team. You will also enjoy direct interaction with highly skilled and experienced underwriters, brokers, and insurance experts the people who are pushing the industry forward.\\n\\nThe Role\\nHelp to acquire, create and manage innovative data assets for the team. These assets draw from various sources - internal underwriting systems, external claims systems, flat files, databases, external APIs etc.\\nAlongside your UK-based counterpart, you will manage the pipelining (acquisition, ETL, storage, access), engineering (cleansing, feature creation / selection, warehousing, data marts, cubing), and understanding (documentation, exploration, communication) of this data\\nWork closely with the whole Data Science & Actuarial team to identify patterns, trends, outliers, exceptions in the underwriting businesses operational and performance data: help to understand root causes, predict future patterns and recommend management actions\\nWork closely with the Tech team to integrate new in-house underwriting systems and data assets, and advise on systems architecture and transactional database design\\nHelp us develop streamlined external-facing commercial data products\\nAs a new venture we have very little technical debt, so you will also have an opportunity to help shape our frameworks and methodologies.\\nSupport the team in stakeholder engagement and business process',\n",
       "  'Job Description\\nData Engineer, AI Start-Up, New York City\\n\\nOur client, a fast-growing AI-based Software Start-Up, is hiring for a Data Engineer to join their team in New York which handles data integration and data integrity the massive amounts of data coming into their platform from their clients.\\n\\nThis is an exciting opportunity to join an office which doubled in size last year and is expecting to do so again this year, with long-term opportunity to help lead the build out the data team in New York, lead special projects, and grow your hands-on technical and analytical skills working with data.\\n\\nCandidates should have at least 3 years of hands-on technical experience working with SQL or NoSQL, as well as ETL and R. Experience with Python, project management, and client-facing skills are highly advantageous.\\n\\nDoes this sound like a fit for you or someone you know? If so, please get in touch. Matt.Kerwin@HarringtonStarr.com.\\nCompany Description\\nHarrington Starr deliver high quality IT professionals on a permanent, retained, contract, and interim basis to the global financial services and commodity trading sectors. Specialising in Financial IT, Commodities IT, Buy and Sell Side Trading Systems and Enterprise Technology, we provide a full solution to all of our clients and candidates.\\n\\nOur experienced consultants are passionate about our core belief of excellence through understanding and we strive to work with both our clients and candidates to ensure that we consistently deliver to their needs. We believe that success in the search for talent comes from developing a deep understanding of your situation and having a malleable approach to presenting the best solutions to exceed expectations.\\n\\nWe are meticulous in our attention to detail, seeking to ensure we consistently fulfill a thorough and comprehensive service that leads to complete customer satisfaction. Through this detail, we will tailor solutions that hold strong, long term benefits as well as short term wins. We pride ourselves on getting the basics right, ensuring that we achieve excellence by mastering the fundamentals.\\n\\nWe see honesty as integral to our relationships and will advise on what the best solutions are for the customer. We don’t look to force decisions, we look to present and advise on options. We understand our markets and have built impressive networks throughout the industry for candidates and clients alike.\\n\\nWe are highly committed to the details. We deal in facts, not opinions. We seek to master the basics. We are focused in our markets. We understand the needs of our customers. We consistently deliver.\\n\\nAll of these combine to ensure Harrington Starr consistently makes it happen for our clients and candidates.',\n",
       "  'C3.ai is a leading enterprise AI software provider for accelerating digital transformation. The comprehensive and proven C3 AI Suite uses a model-driven abstraction layer to enable organizations to develop, deploy, and operate enterprise scale AI applications 40x to 100x faster than alternative approaches. www.c3.ai\\n\\nAs a Data Scientist, you will participate in the definition of new analytics capabilities able to provide our customers with the information they need to make proper decisions to support our customers in operating the internet of things (IoT). In addition, you will help find the appropriate machine learning / data mining algorithms to answer these questions. Finally, you will be responsible for implementing this into the product and making it available to our customers.\\n\\nQualified candidates will have an in-depth knowledge of most common machine learning techniques and their application. You will also understand the limitations of these algorithms and how to tweak them or derive from them to achieve similar results at large-scale.\\n\\nYour Responsibilities:\\nDriving adoption of Deep Learning systems into next-generation of C3.ai products.\\nDesigning and deploying Machine Learning algorithms for industrial applications such as fraud detection and predictive maintenance.\\nCollaborating with data and subject matter experts from C3.ai and its customer teams to seek, understand, validate, interpret, and correctly use new data elements.\\nRequirements:\\nMS or PhD in Computer Science, Electrical Engineering, Statistics, or equivalent fields.\\nApplied Machine Learning experience (regression and classification, supervised, and unsupervised learning).\\nStrong mathematical background (linear algebra, calculus, probability and statistics).\\nExperience with scalable ML (MapReduce, streaming).\\nAbility to drive a project and work both independently and in a team.\\nSmart, motivated, can do attitude, and seeks to make a difference.\\nExcellent verbal and written communication.\\nPreferred\\nExperience with JavaScript and prototyping languages such as Python and R. Experience with Java and Scala is a plus.\\nKnowledge in electrical engineering and cyber-physical systems is a plus.\\nA portfolio of projects (GitHub, papers, etc.) is a plus.\\nC3.ai provides a competitive compensation package and excellent benefits including:\\nCompetitive salary, generous stock options, 401K, medical, dental, and vision benefits. At the office, we offer a fully stocked kitchen with catered breakfast and lunch, table tennis and pool table, free membership at our on-site gym, Friday evening social hours with food, drink and music and a fun team of great people.\\nC3.ai is proud to be an Equal Opportunity and Affirmative Action Employer. We do not discriminate on the basis of any legally protected characteristics, including disabled and veteran status.',\n",
       "  'Job Details\\nLevel\\nEntry\\nJob Location\\nNew York (Home Office) - New York, NY\\nPosition Type\\nFull Time\\nEducation Level\\n4 Year Degree\\nSalary Range\\nUndisclosed\\nTravel Percentage\\nNone\\nJob Shift\\nUndisclosed\\nJob Category\\nInformation Technology\\nDescription\\nGreater New York Mutual Insurance Company (\"GNY\") is an A+ rated, financially stable and growing property casualty insurance company with locations throughout the Northeast. We are currently looking for a dynamic and highly motivated Data Engineer Trainee for our New York office.\\n\\nResponsibilities:\\nDevelop, construct, test, and maintain architectures, such as databases\\nCreate data sets for data scientists used in data mining and modeling\\nProvide recommendations and implement ways to improve data reliability, efficiency, and quality\\nQualifications\\nHave obtained or close to completing a Bachelor\\'s degree from an accredited institution preferably in Computer Science, Computer Engineering, and Software Engineering, or a related field\\nUnderstand database management and have good knowledge of SQL\\nData warehousing and ETL experience is a plus\\nProficient in R, Python, and VBA or be comfortable quickly learning new programming languages\\nBe proficient in the latest version of Excel and Word\\nHave excellent oral and written communication skills\\n\\nApply Now!',\n",
       "  'Description\\n\\nThe Data Engineer will design and develop highly robust, repeatable and scalable workflow patterns to ingest, integrate and publish a wide variety of data from internal and external sources. They will be relentless in their focus on delivery and end ensuring that our data workflows and pipelines are enterprise-grade – reliable, scalable and secure.\\n\\nResponsibilities include:\\nDesigning, implementing, maintaining highly robust, scalable and flexible ETL jobs and data workflows\\nAutomating workflows leveraging DevOps framework where applicable\\nEnsuring quality of technical solutions as data moves across Healthfirst environments\\nWorking with solution and data architects to develop, construct, test and maintain architectures, prototypes and design solutions\\nHelping to maintain the integrity and security of the company data\\nProviding insight into the changing data environment, data processing, data storage and utilization requirements for the company and offer suggestions for solutions\\nArticulating both the technical implications as well as data usage implications of proposed solutions or solution options to a wide variety of stakeholders.\\nWorking directly with business users to align solutions with business requirements\\nRelentless focus on continuous improvement of solutions in to improve data reliability, efficiency, quality and more\\nCreate data monitoring capabilities for each business process and work with data consumers on updates\\nCollaborating with cross functional teams comprised of technical and business colleagues\\nMinimum Qualifications:\\nBachelor’s Degree in Computer Engineering or related field or equivalent experience\\n3 + years’ experience in a data engineering\\nProficiency in SQL a must - knowledge of SQL and multiple programming languages in order to optimize data processes and retrieval\\nStrong experience in data programing languages such as Python or Spark\\nStrong experience working in a Big Data ecosystem processing data; includes file systems, data structures/databases, automation, security, messaging, movement, etc.\\nExperience working in a production cloud infrastructure\\nPrior backgrounds with AWS services (e.g. lambda, S3, Aurora, Redshift);\\nExposure to CI/CD for software systems\\nMust be able to develop creative solutions to problems\\nDemonstrates critical thinking skills with ability to communicate across functional departments to achieve desired outcomes\\nAbility to work independently and as part of a team\\nSkilled in Microsoft Office including Project, PowerPoint, Word, Excel and Visio\\nPreferred Qualifications:\\nHands on experience in leading healthcare data transformation initiatives from on-premise to cloud deployment\\nDemonstrated experience working in an Agile environment as a Data Engineer\\nExperience with other Cloud Data Warehouses (Big Query, Snowflake);\\nKnowledge of provider-sponsored health insurance systems/processes and the Healthcare industry\\nProficiency with graph and noSQL databases\\nWE ARE AN EQUAL OPPORTUNITY EMPLOYER. Applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected Federal, State/Province or Local status unrelated to the performance of the work involved.\\n\\nIf you have a disability under the Americans with Disability Act or a similar law, and want a reasonable accommodation to assist with your job search or application for employment, please contact us by sending an email to careers@Healthfirst.org or calling 212-519-1798 . In your email please include a description of the accommodation you are requesting and a description of the position for which you are applying. Only reasonable accommodation requests related to applying for a position within Healthfirst Management Services will be reviewed at the e-mail address and phone number supplied. Thank you for considering a career with Healthfirst Management Services.\\n\\nEEO Law Poster and Supplement\\n\\n]]>',\n",
       "  \"Vettery is changing the way people hire and get hired. We use machine learning and real-time data to match talented job-seekers with inspiring companies. Our goal is to enrich and automate the recruiting process, make hiring more rewarding for everyone, and create a happier and more accountable working world.\\n\\nSince launching in 2015, we've made thousands of matches on our marketplace. We're currently working with over 45,000 job-seekers and 20,000 companies, from Fortune 500 giants to startups based out of co-working spaces. We've built powerful machine learning capabilities, and our matching algorithm is becoming more intelligent with each passing day. With an eye on the future, we're expanding our reach across major cities in the US, and around the globe.\\n\\nVettery Engineering is working to build a highly innovative platform enabling an efficient experience for our users so that we can continue to make hiring more rewarding for all. As a Data Engineer, you'll help productize data science models to more effectively match candidates and companies quickly.\\n\\nKey responsibilities and expectations:\\nWork with senior engineers to develop and maintain end-to-end data pipelines across multiple data sources and systems of record\\nDevelop data models, data structures and ETL jobs for data science consumption\\nManage and maintain cloud based data and analytics platform\\nWork in a highly agile development environment and practice Agile/Scrum methodology\\nParticipate in hackathons, team outings, lunches and other team building events\\nCandidate Qualifications:\\nSelf-starter who can work in a highly demanding environment and maintains a positive attitude\\n1+ years of data engineering experience, including ETL pipelines\\nExperience with one general purpose programming language, including but not limited to: Java, Scala, Python\\nInterest in or familiarity with Big Data technologies like AWS EMR, SQOOP, Hadoop, Spark, and HDFS\\nA degree in computer science, engineering or a related field is a plus\\nVettery has five key values that are the foundation of our company culture, which every employee embodies:\\nPositivity - We're positive when things get difficult so we can stay motivated and lift each other up. We're very team focused in everything we do so contagious positive energy is extremely impactful.\\nOwnership - We take pride in our work and take on a lot of responsibility from day one. All of us have the ability to see how our work and performance impact the success of the business.\\nGrit - We love getting in the trenches and building from the ground up. Even though we've significantly grown since our tiny startup days, we still have that scrappy mentality and love that there's still a lot to accomplish.\\nAwareness - We're focused, strategic, and constantly learning from our experiences. Each of us knows what's expected of us as a corporate citizen and within our teams.\\nCollaboration - We learn from one another and are constantly working with each other, within and across teams. Every team has an impact on others and we take pride in clear lines of communication.\\nWhy you'll love working at Vettery:\\n\\nWe love coming to work on Monday. It's easy to love the work you do when you see the positive impact it has, and helping someone find their dream job can change their life forever. We believe in our mission, love the work, and have fun doing it together. Plus, coming to work in our sunny Flatiron office is easy when there are so many things to look forward to: Flag Football games, Thursday Game Night, Cross-Team lunches, company happy hours, volunteer events, adorable pups, ping-pong, and your favorite snacks.\\n\\nWe know life is about more than just work. We have an open vacation policy so you can take the time you need to relax and rejuvenate, contribute to the cost of insurance coverage (health, vision, and dental), and offer a fully paid 12-week parental leave, 401k, commuter benefits, and gym membership discounts.\\n\\nWe invest in your development. A company is only as strong as its team, and we want to help strengthen every member of our team. We give everyone the opportunities and support they need to reach that next professional level through company-sponsored General Assembly classes and conferences, in-house training, a culture of continuous feedback, and the chance to run with projects.\\n\\nWe're consistently recognized for our culture. We're listed #5 on Fortune's 60 Best Companies to Work for in New York City this year, and have been previously honored at Crain's Best Places to Work Awards and included in Inc Best Workplaces.\\n\\nVettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.\",\n",
       "  'We are:\\n\\nApplied Intelligence, the people who love using data to tell a story. Were also the worlds largest team of data scientists, data engineers, and experts in machine learning and AI. A great day for us? Solving big problems using the latest tech, serious brain power, and deep knowledge of just about every industry. We believe a mix of data, analytics, automation, and responsible AI can do almost anythingspark digital metamorphoses, widen the range of what humans can do, and breathe life into smart products and services. Want to join our crew of sharp analytical minds? Visit us here to find out more about Applied Intelligence.\\n\\nYou are:\\n\\nAn expert engineer with an eye for AI. You want to change how the world works and lives by taking AI out of the lab and into everyday life.\\n\\nThe work:\\n\\nYoull be part of a team with incredible end-to-end digital transformation capabilities that shares your passion for digital technology and takes pride in making a tangible difference. If you want to contribute on an incredible array of the biggest and most complex projects in the digital space, consider a career with Accenture Digital.\\n\\nHeres what you need:\\nMinimum 2+ years of experience in designing, implementing large scale data pipelines for data curation and analysis, operating in production environments using Spark, pySpark, SparkSQL, with Java, Scala or Python on premise or on Cloud (AWS, Google or Azure)\\nMinimum 1 year of designing and building performant data tiers (or refactoring existing ones), that supports scaled AI and Analytics, using different Cloud native data stores on AWS (Kinesis, S3. GLUE, DynamoDB etc.) or Azure (HDInsights, AzureData Factory) or GCP (DataProc, PubSub, BigQuery) as well as using NoSQL and Graph Stores.\\nMinimum 1 year of designing and building streaming data ingestion, analysis and processing pipelines using Kafka, Kafka Streams, Spark Streaming and similar cloud native technologies\\nMinimum 1-year performance engineering, profiling, debugging very large big data and ML production solutions on Spark and native Cloud technologies\\nBonus points if:\\nMinimum 6 months of experience in implementation with Databricks.\\nMinimum 1 year of designing and building secured and governed Big Data ETL pipelines, using Talend or Informatica technologies; for data curation and analysis of large le production deployed solutions.\\nExperience implementing smart data preparation tools such as Palate, Trifacta, Tamr for enhancing analytics solutions.\\nMinimum 1 year of building Business Data Catalogs or Data Marketplaces for powering business analytics using technologies such as Alation, Collibra, Informatica or custom solutions\\nImportant information\\n\\nApplicants for employment in the US must have work authorization that does not now or in the future require sponsorship of a visa for employment authorization in the United States and with Accenture.\\n\\nAccenture is an EEO and Affirmative Action Employer of Females/Minorities/Veterans/Individuals with Disabilities.\\n\\nEqual Employment Opportunity\\n\\nAll employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law.\\n\\nJob candidates will not be obligated to disclose sealed or expunged records of conviction or arrest as part of the hiring process.\\n\\nAccenture is committed to providing veteran employment opportunities to our service men and women.\\n\\nCandidates who are currently employed by a client of Accenture or an affiliated Accenture business may not be eligible for consideration.',\n",
       "  'Advanced Python & PySpark\\nKnowledge of Regex\\nApache Spark (Databricks)\\nCloud and Distributed computing\\nOOP\\nWeb Scraping\\nData Quality\\nStructured and Unstructured data\\n- provided by Dice',\n",
       "  \"Job Title: Sr. Data EngineerLocation: Northern, NJ\\nJob Type: Contract to Hire\\nDuration: 3-6 months to Conversion\\nContact Info: Matt O'Brien - - 201-786-2415\\n\\nPosition Summary:\\n\\n• You will be part of a growing Data Engineering team that handles client data for Internal and External Users.\\n• The IT department services over 10 internal groups and the Data Engineer will be a seasoned Technology leader comfortable with a variety of data technologies. Data Engineering Group handles a Datawarehouse that sources data out of over 15 sources and services over 10 internal groups.\\n• The current data technology stack consists primarily of Datawarehouses using SQL Technologies, Neo4J based Graph DBs along with the use of a variety of NoSQL and AI/Client frameworks and languages like R, Python etc..\\n• The ideal candidate will be a technologist who is able to balance the rapid pace of technology change with an authoritative ability to handle client relationships - including working closely with the business and technical teams/vendors.\\n• Assists in the ongoing support of applications.\\n• We're looking for someone who is laser-focused on operational excellence and customer satisfaction.\\n• You'll need to wear many hats, so flexibility and a can-do attitude are critical!\\n• We are looking for a dynamic, collaborative personality that can champion the cause of Agile within the organization.\\n• The individual needs to have a consistent track record of successfully delivering value for their organization in a fast-paced environment along with successful management of customer expectations.\\n• A passionate engineer who strives for automation would be ideal for this position.\\n• Experience mentoring and leading other staff, both onsite and offshore would be required for this role to be successful.\\n• As a, Data Engineer, you will lead client's data efforts across all products and lines of business.\\n• You are a pioneer, building new capabilities that will help unlock new possibilities for our businesses.\\n• You will coordinate with external and internal resources to be a part of a Data Engineering practice.\\n• You will play a fundamental role in achieving our ambitious growth objectives.\\n• You must be comfortable switching between multiple projects, both leading as a manager and contributing as an individual and working with both business teams and technology teams to translate business requirements into finished product.\\n• You possess strategic vision and tactical mastery and combine it with an entrepreneurial spirit to get it done. You will collaborate closely with stakeholders across the company to design innovative solutions and balance challenging priorities and resource demands.\\n• We're looking for someone who welcomes challenges and is hyper-focused on delivering exceptional results to internal business customers while creating a rewarding team environment.\\n\\nMajor Responsibilities:\\n\\n• Understands business needs and develop solutions that delight consumers and customers\\n• Understands Agile artifacts and develops applications based upon business priority.\\n• Collaborate with project partners to ensure all requirements are met.\\n• Handles relationships with end user communities.\\n• Interacts regularly with users to gather feedback, listen to their issues and concerns, recommend solutions.\\n• Demonstrate your technical abilities and contribute to our overall architecture\\n• Help define and implement the Enterprise Data Architecture for client and help implement it in multi-functional alignment with the Data teams that exist across functions like Marketing, Finance, HR etc.\\n• Provide leadership during application design and development for highly complex or critical machine learning projects across numerous lines of business and shared technology.\\n• Evaluate readiness of new big/fast data capabilities to be used to support critical operational processes\\n• Build, Maintain and Demonstrate partnership with 3rd parties like Stanford Labs or Google Labs to be at the cutting edge of Data and Client technologies and foster data driven decision-making\\n• Ensure cross collaboration with sister teams in other geographies\\n• Ensure alignment to enterprise architecture and usage of enterprise platforms when delivering projects\\n• Provide oversight for budgets and project plans, and handle technology risks and issues\\n• Partner with your peers within the organization and build multi-functional alignment\\n\\nSkills/Knowledge:\\n\\n• Master's Degree in Computer Science, Engineering, or Management of Info Systems/Technology preferred\\n• Advanced Education in Statistics or Mathematics would be a plus\\n• 5+ years of experience in developing BigData and/or machine learning solutions\\n• 5+ years of experience in a highly regulated industry\\n• 2+ Years of experience defining data architectures\\n• 2+ Years of experience leading product engineering teams\\n• Experience with the full AWS stack\\n• Experience with SQL, NoSQL, BigData and Graph Technologies along with Programming languages like R, Python, Kafka, Storm etc.\\n• Experience building microservices\\n• Background in agile SW development and Scaled Agile Frameworks\\n• A true believer in measuring success based on working software and in quick prototyping\\n• Someone who is a passionate coder and can spin up a snippet of code quickly\\n• Strategic thinker with the ability to build and execute innovative digital product, combined with tactical ability to execute simultaneously against multiple contending priorities\\n• Someone with an iterative approach, drive to move fast and think big\\n• Experience managing internal and external teams at the same time, working with multiple brands and digital properties of varying maturities\\n• Demonstrated ability to partner and communicate effectively with non-technical team members, resolving contending or contradictory objectives, and unifying disparate ideas into a homogenized solution\\n• Ability to be versatile and handle multiple projects and re-prioritizations\\n• Possess the ability to influence others, implement change, and standardize processes in a complex business environment\\n• A passion for mentoring and growing the potential of others\\n• Ability to effectively and appropriately interview technical candidates\\n• Exposure to AWS and other cloud technologies is a plus\\n• Passion for Automation and Hunger for Acceleration; Keen knowledge of Devops as well as RPA is a big plus\\n• Experience with Architecting Applications (e.g. Design Patterns, distributed applications etc.) with the aim of reuse\\n• Superb communication skills (both written and verbal)\\n• Great teammate should be ready to go beyond to help immediate team and do not be averse to not shy away from asking for help if needed.\\n• Ability to translate ideas into solutions based on user and business needs\\n• Open Eagerness to learn new technologies and bring new ideas to the table\\n\\nEducation:\\n\\n• Bachelors Degree or equivalent.\\n• Masters would be a plus.\\nprovided by Dice\",\n",
       "  '• Hands-on experience in Azure Data Factory, Informatica and good understanding on data obfuscation or data masking techniques • Design, construct, install, test and maintain highly scalable data management systems • Build automated data delivery pipelines and services to integrate data • Build and deliver cloud-based deployment and monitoring capabilities consistent with DevOps models • Develop solutions in agile environment for the overall data domain • Deep experience with developing SQL • Must have Deep experience developing with MS SQL • Must be able to do ETL (SSIS, Azure Data Factory, Informatica) • Understand Data Security\\n\\nNearest Major Market: Manhattan\\nNearest Secondary Market: New York City\\nJob Segment:\\nDatabase, Engineer, SQL, Technology, Engineering',\n",
       "  'Solid experience as a Data Engineer for at least seven years.\\nStrong skill-sets in Python, pySpark,Scala,Hadoop Stack, Data Pipeline using ETL or other tool.\\nEquipped in handling high volume data and AWS (Redshift, S3,lambdass).\\nExcellent Communication skill',\n",
       "  \"Feedzai is the market leader in managing financial risk with AI. We're coding the future of commerce with today's most advanced risk management platform powered by big data and machine learning. Founded and developed by data scientists and aerospace engineers, Feedzai has one mission: to make banking and commerce safe. The world's largest banks, processors, and retailers use Feedzai's fraud prevention and anti-money laundering products to manage risk, while improving customer Experience.\\n\\nYou are going to be in charge of making our clients happy and successful. Everything you do matters: all your code, machine learning models, advisory, management, and other actions/roles will have a material impact on the way our clients run their business and how effectively we fight fraud and protect people from wrongdoing. You will be able to interact and meet many people, from widely different cultures around the world and understand the business like few others. You will be able to say you protect people on a daily basis. You will be challenged with new technology, new processes, and new mindsets and will be asked to contribute to ensure continuous improvement. Full, holistic view and impact is what you will get within Customer Success. Come and change the world with us.\\n\\nResponsibilities:\\nExecute full software development life cycle\\nDevelop flowcharts, layouts and documentation to identify requirements and solutions\\nWrite well-designed, testable code\\nIntegrate software components into a fully functional software system\\nTroubleshoot, debug and upgrade existing systems\\nDeploy and support systems in production\\nComply with best practices and industry standards\\nQualifications:\\nExperience in Java software development\\nA BS or MS in computer science, or a comparable field, or equivalent experience\\nExcellent English communication skills, both verbal and written\\nAvailability to travel up to 10%\\nExperience with Zookeeper, RabbitMQ, Cassandra, Ansible, Docker\\nExperience in the financial services, payments industry or e-commerce a plus!\\nFeedzai is an equal opportunity Employer\\n\\nFeedzai does not accept unsolicited resumes from recruiters or employment agencies\\n\\nPM19\",\n",
       "  \"1010data values:\\n\\n\\nIntegrity: Doing the right things for the right reasons\\n\\nAgility: Adapting and thriving in a dynamic environment\\n\\nTeamwork: Combining our strengths to do amazing things\\n\\nPassion: Channeling enthusiasm to drive excellence\\n\\nCreativity: Unleashing curiosity to defy the norm\\n\\nAbout the role:\\n\\n\\nAs a Software Data Engineer at 1010data, you will be responsible for designing, maintaining, and optimizing large-scale automated ELT processes. Working actively with data scientists and analysts specializing in enterprise data warehousing, you will leverage industry-standard data orchestration tools as well as in-house proprietary scheduling and automation tools to create efficient and reliable ELT jobs which support 1010data's product offerings and data warehousing needs for our customers. As we incorporate more cloud technologies into our processes, you will be at the forefront of exploring and defining best practices, and helping us transition our products to be more scalable.\\n\\nAs part of the onboarding process, you will learn about 1010data's proprietary technology stack. Our query engine, query language, database, and data storage layer were all developed and fine-tuned in-house over the lifetime of the company. ELT processes heavily rely on these components, whether they are written in Python and Airflow , K, or our proprietary data orchestration tools. You will be formally trained in the latter as a new 1010data employee. The concepts should be familiar to anyone with exposure to database techniques like normalization/indexing/partitioning, MapReduce, columnar database architecture and distributed systems.\\n\\nWhat you will take on:\\nTaking end-to-end ownership of data products and custom solutions for our clients\\nCoordinating with the systems, core, data science, and analytics teams to build and maintain data products and custom solutions for our clients\\nDesigning and writing automated scripts to preprocess terabytes of data from our partners/clients\\nDesigning and writing new enterprise-scale ELT/ETL workflows from scratch in Python using Airflow, Docker, Kubernetes, AWS, etc.\\nModifying/redesigning legacy ELT/ETL processes to leverage cutting-edge open source and proprietary technologies\\nEnsuring quality, reliability and uptime for critical automated processes\\nMigrating our products and processes into the cloud while drastically reducing our in-house data center footprint\\nWhat you already have:\\n\\n\\nRequired Skills:\\nAt least 1-2 years of professional experience programming in Python\\nExposure to ETL/ELT pipeline automation\\nExposure to basic database concepts\\nPreferred Skills:\\nGood understanding of Data Engineering, NoSQL databases and database design, distributed systems and/or information retrieval\\nKnowledge of Apache Airflow\\nFamiliarity with functional/vector programming\\nDBA experience\\nAbility to plan and collect requirements for projects, and interact with the analyst and data science teams\\nEducation:\\nSTEM Bachelor's required, graduate degree is a big plus\\nAbout 1010data:\\n\\n\\n1010data travels at the speed of thought to make Big Data discovery easy; we power sub-second responses to analyses run on billions of rows of data. 1010data is defining the way the world interacts with data. Come be a part of it. Come do powerful things with data.\\n\\nAn essential tool to more than 850 of the world's top retail, manufacturing, telecom, government and financial services enterprises including The New York Stock Exchange, Dollar General, P&G, and RiteAid; the 1010data platform is a highly differentiated product that is becoming the industry standard for Big Data Discovery and Data Sharing.\\n\\nWith more than 30 trillion rows of data in our private cloud, 1010data is designed to scale to the largest volumes of granular data, the most disparate and varied data sets, and the most complex advanced analytics. All while delivering lightning-quick system performance.\\n\\n1010data is an equal opportunity employer. We embrace humans of every background, appearance, race, religion, color, national origin, gender, gender identity, sexual orientation, age, marital status, veteran status, and disability status.\",\n",
       "  'Job Description\\nData Engineer\\n\\nData Engineer required for a globally recognized, online betting company based in Manhattan. They aim to create more efficient gaming betting products, that are machine learning based, which will ensure that the predictive engine is profit-maximizing. The team would be predominantly made up of Data Engineers, who are a key part of the modern technology created to drive their product pricing. The company has an overall headcount growth of 64 in the last 6 months.\\n\\nAs a Data Engineer, you will be required to work on a collaborative team of data scientists and other sports, technology and media executives. The Data Engineer will be expected to maintain and create the centralized data warehouse, data lake and ETL pipeline for the company’s raw data. This data will need to be gathered and organized for the Data Science team for efficient machine learning research. The Data Engineer will also be responsible for maintaining the graph and relational databases for the applications.\\n\\nKey responsibilities:\\nOptimizing workflows for Data Science research by writing software\\nProduction database development for applications\\nUsing a best practice approach to create and develop the ETL pipeline\\nLoading data from the data lakes to the data warehouse\\nSummary of skills:\\nAn experienced python developer is preferred\\nAbility to efficiently process large datasets\\nAn understanding of machine learning algorithms\\nA basic knowledge of software engineering principle\\nGraph database knowledge is preferred\\nThe manager is looking to interview a suitable Data Engineer as soon as Friday 19th, July. The positon should begin at the start of August.',\n",
       "  'We are an award-winning healthcare technology company deploying a suite of intuitive patient engagement solutions that empowers healthcare organizations to create memorable patient experiences by fostering connections along the healthcare journey to ensure the best possible outcomes for providers, staff, patients, and their loved ones.\\n\\nAs a Data Engineer at CipherHealth, you will take an active role in designing the future of the data engineering practice. A successful candidate will combine strong technical skills and a passion for creative problem solving.\\n\\nResponsibilities\\nDelivering and maintaining highly available computing platforms\\nCollect and transform data from a number of sources, storing it in highly optimized database systems\\nCreating data integration services\\nBuilding data products\\nMaintaining ongoing reliability, performance, and support of the infrastructure--providing solutions based on application needs and anticipated growth\\nData Stack\\nMongoDB, Postgres, Redshift\\nRuby, Mongo, and VueJS\\nPentaho Kettle\\nPeriscope Data for dashboarding\\nQualifications\\nStrong programming skills in Ruby/Python and SQL\\nGreat collaboration and team working skills\\nPractical experience in best practices for developing data pipelining frameworks\\nAbility to learn autonomously and quickly\\nExtremely organized and detail-oriented with effective multitasking and prioritization skills\\nExcellent written and verbal communication skills\\n3 - 5 years of experience working in software development, data engineering, or related STEM fields\\n3+ years of working experience with relational databases and data warehousing',\n",
       "  \"Id: 175877 Location: New York, NY Job Type: Direct Hire Job Description\\n\\nClient is seeking a Senior Engineer to join Data Insights team. This role is a hands-on engineering position responsible for the build and continued evolution of a cutting edge data platform. This next generation data platform will include a modern data processing and data persistence pipeline including technologies such as Hadoop/Spark (S3 + EMR), and NoSQL. In addition to the technical contributions, this role will be responsible for technical leadership and system architecture within the Data Insights Team.\\n\\nRESPONSIBILITIES\\n\\nReal time data integration with AWS services involving message queues and streams: SQS, SNS and NoSQL data stores such as DynamoDB\\nDesign, develop, deploy and manage a reliable and scalable data analysis pipelines, using technologies including Python, Spark, Redshift and SQL Server\\nIntegration with 3 rd party vendors via Restful Web APIs and Flat files\\nExperience designing systems that leverage serverless compute tools such as AWS Lambda\\nAct as a mentor and thought-leader within your team and the Equinox engineering group\\nPlan strategic initiatives and work with Directors and VPs to create roadmaps and define architecture\\nParticipate in cross-functional initiatives to develop new capabilities, including hands-on development responsibilities\\nAbility to integrate data from a variety of sources, assuring they adhere to data quality and accessibility standards\\nDocument processes and standard operating procedures\\nEvaluate and conduct POC's with new technologies\\n\\nQualifications\\n\\nBachelor's Degree Required: Computer Science or Engineering discipline preferred\\n5+ years technology experience working in an Engineering/Development/ Data Warehouse\\n5+ years developing with Python\\n3+ years working within a Data Warehouse environment\\n2+ years working with AWS public cloud (certification a plus)\\n2+ years working with MPP databases, distributed databases, and/or Hadoop\\nExperience managing or leading small to medium sized engineering terms in dynamic environments\\nUnderstanding of modern engineering design principles (distributed systems, stateless processes, etc)\\nUnderstanding of Data Warehouse principles, including Kimbal-style Dimensional Modeling\\nCreative, flexible, and quick to learn\\nWork successfully in a multi-project, deadline driven environment, within Agile framework\\nExpertise in SQL: 10 out of 10, SQL Ninja analytic capabilities, SQL ELT development and optimization on distributed systems.\\nExperienced with NoSQL datastores & serverless environments\\nExperience with Redshift a plus\\nExperience with SQL Server a plus - provided by Dice\",\n",
       "  'Job Description\\n\\nAt Butterfly Data Engineering we are fusing a diverse set of data streams such as manufacturing data, commercial data, real-time IoT data, mobile analytics data, logistics data to continuously improve our product design, guide feature development and optimize manufacturing and operations.\\n\\nAs part of our team, your core responsibilities will be:\\nRapidly implement and maintain elegant and robust data pipelines and create self-service data analytics infrastructure that immediately creates value for business owners\\nQualifications\\n\\nBaseline skills/experiences/attributes:\\nData modeling and hands on development and administration of relational and NoSQL databases (BigQuery, Elastic Search)\\nPrinciples and patterns of data processing architectures (batch, streaming, lambda, serverless, etc)\\nDev-Ops experience managing services on GCP (Ansible, Docker, Kubernetes)\\nHands on knowledge of ETL tools, frameworks and practices: Airflow, Python 3\\nData Visualization and dashboard systems (Tableau, Plotly, Seaborn, Matplotlib)\\nStrong programming experience in Python (Pandas, Numpy, Notebooks)\\nAdditional Information\\n\\nWe offer great perks:\\nFully covered medical insurance plan, and dental & vision coverage - as a health-tech company, we place great worth on our teams well-being\\nCompetitive salaried compensation - we value our employees and show it\\nEquity - we want every employee to be a stakeholder\\nPre-tax commuter benefits - we make your commute more reasonable\\nFree onsite meals + kitchen stocked with snacks.\\n401k plan - we facilitate your retirement goals\\nBeautiful office overlooking the Flatiron building in NYC\\nThe opportunity to build a revolutionary healthcare product and save millions of lives!\\nFor this role, we provide visa assistance for qualified candidates.\\n\\nButterfly network does not accept agency resumes.\\n\\nButterfly Network Inc. is an E-Verify Company and is an equal opportunity employer regardless of race, color, ancestry, religion, gender, national origin, sexual orientation, age, citizenship, marital status, disability or Veteran status. All your information will be kept confidential according to EEO guidelines.',\n",
       "  'Working as a data scientist, you will be part of the Data Solutions and Analytics team within Market Data at Tradeweb. With live and extensive historical data across the Institutional, Wholesale, and Retail fixed income markets globally, you will have the unique opportunity to produce models to advance the fixed income marketplace. Working in partnership with platform business heads, product managers, and the market data team, you will develop models that will improve executions and generate ideas for our clients. Your models will produce live data both within our trading platforms as well as generate independent commercial data products. You are comfortable working in a results-driven environment and want to be part of an innovative team that is forging new ground and redefining the world of investment and trading in fixed income securities.\\n\\nJob Responsibilities\\nCreate predictive models using current and emerging methodologies in data science. Candidates will possess a deep understanding of statistics, machine learning, causal predictive modeling, and ideally optimization\\nCollaborate across the organization to drive projects from beginning to end: frame business questions, collect and analyze data, research, prototype, build pipelines, and share insights.\\nWork with technology to ensure robust translation to production environments and create solutions that operate effectively live and at scale.\\nQualifications\\n7+ years of experience in applying statistical and machine learning techniques such as regression and classification models, cluster analysis, neural networks, ensembles, random forests.\\nAdvanced degree (MS or PhD) in a quantitative field such as Statistics, Computer Science, Mathematics, Physics, Engineering, Economics, or similar.\\nExpert programming skills for data analysis and machine learning. Experience using version control and general software development best practices for contributing to a collaborative code base.\\nStrong communication and collaboration skills. Ability to communicate technical modeling concepts and relevant aspects of modeling platforms to non-technical audiences.\\nWillingness to learn fixed income and improve across all technical skill areas.\\nAbility to work in a start-up environment that is fast paced and maintain a focus on rapid prototyping of capabilities.\\nDemonstrated leadership and self-direction.\\nEnjoys teaching and collaborating with others.\\nExperience using the python data science stack a plus, e.g. Pandas, Jupyter notebooks, scikit-learn, Tensorflow, Anaconda etc.\\nAbout Tradeweb:\\n\\nTradeweb is a leading, multi-asset class, electronic trading venue for fixed income, derivatives and ETFs. Tradeweb offers institutional, wholesale and retail market participants unparalleled liquidity, cutting-edge technology and a broad range of data solutions that deliver better price discovery, order execution and trade workflows. The company serves more than 2,000 clients including banks, mutual funds, hedge funds, pension funds, insurance companies, central banks, corporates, inter-dealer brokers, brokers, financial advisors and registered investment advisors in over 50 countries across the Americas, Europe and Asia. Tradeweb operates trading platforms in more than 25 products including government bonds, mortgages, corporate bonds, municipal bonds, interest rate swaps, credit defaults swaps rates, repurchase agreements and equity derivatives. In an average trading day, Tradeweb facilitates more than $525 billion in notional value traded.\\n\\nTradeweb Markets LLC (\"Tradeweb\") is proud to be an EEO Minorities/Females/Protected Veterans/Disabled/Affirmative Action Employer.\\n\\nhttps://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf',\n",
       "  \"WHO WE ARE\\nLooker is on a mission to bring better insights and data-driven decisions to every business. Everything we do is aimed at making sure our customers love every aspect of Looker, from our products and technologies to our ease of doing business and our support. We are looking for curiously brilliant individuals to join our team as we reinvent data analytics. Get data-driven and see yourself at Looker.\\n\\nWHAT WE'VE GOT GOING ON\\nLooker is searching for a Data Engineer with experience in implementing modern data architectures. The Data & Analytics team focuses on Internal Analytics and Data Engineering supporting our business. You will be the Technical Lead on our Data Engineering Pod responsible for bringing our infrastructure to the next level as we overhaul our data, pipeline, security architecture, and modeling layer. Looker’s amazing success is just getting started and you will be working for a company that truly values the power of data. Your first projects will include helping to scale our data infrastructure and optimizing our warehouse performance and footprint. You will be working with other engineers and analysts on the Data & Analytics team to support both our internal warehousing efforts and prototyping new best practices for our Professional Services and Department of Customer Love.\\nWHAT WE NEED YOU TO DO\\nPartner on the design for the next implementation of our secure, global data architecture\\nExecute engineering tasks with maturity in variety of languages including JVM-based, ruby, and other scripting languages\\nOwn large engineering projects\\nMentor staff by providing opportunities to execute on your engineering agenda\\nOwn optimization and monitoring projects for our Massively Parallel Processing (MPP) Database and Pipeline technologies\\nSet and implement data security standards\\nArchitect and drive implementation for self-serve data processing throughout our business\\nWork with a variety of AWS, GCP, and Azure technologies\\nWork with stream and queue-based solutions\\n\\nWHAT YOU BRING TO LOOKER\\n7+ years experience in implementing modern data architectures\\n7+ years experience with Data Warehouse Systems\\nExperience Implementing self service data processing for Data Analysts and Scientists\\nJava and Python experience in production\\nFamiliarity with Spark and/or MapReduce\\nDeep familiarity with one or more workflow tools such as Airflow, NiFi, Azkaban, etc.\\nA strong desire to show ownership of problems you identify and proven ability to empower others to get more done\\nFamiliarity with modern BI and exploration tools such as Notebooks\\nUnderstanding of hardware performance\\nFamiliarity with Linux systems\\nSome familiarity with streaming approaches preferred\\nCS Degree preferred\\nSome experience preferred with Node, Ruby, or Scala\\n\\n#LI-SK1\\n\\nA LITTLE MORE ABOUT LOOKER\\nLooker is a unified Platform for Data that delivers actionable business insights to every employee at the point of decision. Looker integrates data into the daily workflows of users to allow organizations to extract value from data at web scale. Over 1700 industry-leading and innovative companies such as Sony, Amazon, The Economist, IBM, Spotify, Etsy, Lyft and Kickstarter have trusted Looker to power their data-driven cultures. The company is headquartered in Santa Cruz, California, with offices in San Francisco, New York, Chicago, Boulder, London, Tokyo and Dublin, Ireland. Investors include CapitalG, Kleiner Perkins Caufield & Byers, Meritech Capital Partners, Redpoint Ventures and Goldman Sachs. For more information, connect with us on LinkedIn, Twitter, Facebook and YouTube or visit looker.com.\\n\\nLooker aspires to be a workplace that is not only free of discrimination, but one that fosters inclusion and belonging. We strongly believe that diversity of experience, perspective, and background lead to a better environment for our employees and a better product for our users. We encourage you to join us in changing the way businesses use data.\\n\\nFor information on how Looker uses your information, visit Looker's Privacy Policy.\",\n",
       "  \"Job Description - Data Engineer\\n\\nData engineers work on the pipelines that sit at the core of this architecture. We work to make these pipelines faster, more fault tolerant and to expand their scope.\\n\\nThe volume of data is large: we're working with 7 of the top 20 largest retailers in the world (+ many more not in the top 20), and are ingesting data from them both in a regular batch and in near-real time.\\n\\nWe've carefully selected the types of data to ingest to favor high signal data, so we care deeply about maintaining the correctness and completeness of the data being ingested as that directly affect the output of our models, and therefore the output of our product. Your work directly impacts both the predictions we're able to make, and the day to day performance our customers experience when using our product.\\n\\nGetting more specific, you will:\\nDesign and build complex data pipelines on the Spark platform, ingesting both batch and real time datasets\\nOptimize the performance of data pipelines, getting into the internal of Spark and potentially other technologies to make gains\\nSpec and build the infrastructure that the data pipelines run on\\nWork with our data science team to deploy predictive models at scale\\nBuild tools to continuously validate incoming data and proactively identify and communicate data anomalies before they manifest into problems.\\nWe're a small team, so you'll be working on (and be able to meaningfully contribute to) high impact projects from your first day.\\n\\nWhat's the day-to-day like?\\n\\nInspired by Basecamp, we work in ~8 week product cycles. First, we work together (engineering + product) to identify the projects we think will have the biggest impact on our company goals. Here's an example of a recent project we conceptualized and delivered over one of these cycles:\\n\\nBuild out entity resolution functionality\\n\\nSomeone may make a purchase both online and in a store, to the retailer this may look like two people. The same person could have used a different email address when signing up for the retailer's mailing list compared to when they bought something online. To get the best input to our models, we need to be able to identify that the purchases and the email list sign up came from the same person.\\n\\nA feature to link multiple customer identifiers to the same person and remove duplicate customers by rule came about in two phases. The first was working with a retailer that could provide us with mappings of their online and offline purchases. A few months later, a separate retailer needed a rules based approach to deduplicating their customers. The challenge here is being able to ingest mapping information on customer ids and apply rules to deduplicate customer records at the same time. If we deduplicate by rule first, we may lose ids that should map to each other, but if we have multiple entries for a customer, we may generate incorrect mappings.*\\n\\nIt speaks to the flexibility of our pipeline that we were able to combine these two processes into a single step in our pipeline. Once the entity resolution process was deployed at scale on our Spark cluster, retailers could take advantage of one or both of these features. It has made a significant impact in our ability to find high value customers - customers that typically make many purchases in multiple channels and has multiple touchpoints with the retailer.\\n\\nQualifications:\\n5 or more years of experience as a software engineer.\\nDegree in Computer Science or a deep competency achieved via other means.\\nFamiliarity with Spark, Ruby on Rails, AWS, and SQL-based databases\\nHigh standards for code quality and maintainability.\\nNice to Have's:\\nExperience with Scala, R, and/or Chef.\\nConsistent record of delivering significant features or building out platforms and services.\\nExperience working in e-commerce.\\nWhat it's like to work here:\\nOn Monday we eat and meet as a team to chat projects and progress.\\nWe're 70ish genuinely nice people. We work together and experiment with how to do things. It's an amazing environment to both explore your professional interests and shape your career trajectory.\\nWe move quickly. You build something and the next day it comes to life. You see and feel an immediate impact with the collective efforts of the team.\\nWe're building a company and a team we love. We're in it for the long run.\\nThe perks\\nWe're a flexible work environment\\nCompetitive salary and meaningful equity\\nHealth, dental and vision insurance (100% covered)\\nFree lunch every day, plus free water - hot and cold!\\nUnlimited vacation: take as much time as you need (we recommend at least 3 weeks)\\nMonthly unlimited MetroCard\\nprovided by Dice\",\n",
       "  \"At Urbint, our mission is to make communities more resilient. We do this by pairing external data with artificial intelligence to identify areas of high risk and prevent catastrophic loss for utilities and infrastructure operators across the country. We are a team of close-knit engineers, entrepreneurs, and data geeks who obsess over problem-solving, new technologies, and making a positive impact in our communities.\\n\\nAt Urbint, you will collaborate within a cross functional team of other software engineers, product designers, machine learning engineers and product managers to architect, create and maintain custom data pipelines, data lake(s) and a data warehouse to feed our customer facing applications and internal experimentation and modeling teams. Our data sources range from large, high security enterprise databases and storage services hosted by our customers to disparate open data from government organizations to proprietary third party aggregators and auditability and security are very important to their results. You will be working to solve direct customer needs and finding common abstractions to apply and guide the future of information at data driven organization.\\n\\nRequirements\\n8+ years of software development experience focused on web technologies including significant production work with Python\\n3+ years of experience designing, building and maintaining enterprise data pipelines and/or warehouses\\nHigh level knowledge of machine learning algorithms and how ML models are built and deployed\\nDemonstrable knowledge of big data databases such as columnar data stores (e.g. Cassandra or BigTable) or Hadoop as well as SQL (MySQL, MSSQL or PostgreSQL) and ability to select the right tool for the job\\nExperience with queued work management and message processing (e.g., Kafka, RabbitMQ)\\nExperience working closely with product and account support personnel to help prioritize the best solutions to the largest problems.\\nReliable organization and communication skills and follow through on verbal and written commitments.\\nPersistent approach to problem-solving and ability to see solutions through to completion even in the face of complexities or unknowns. A proactive mindset that drives you to pursue solutions rather than waiting for the answers to come to you.\\nAttention to detail in work and ability to identify ambiguities in specifications.\\nExceptional written and verbal communication skills, especially when communicating trade-offs between technical decisions to non-technical colleagues.\\nFlexibility to work and maintain focus in an evolving environment. Ability to let go of previous projects and move on to new ones or to dig deeper into existing projects and grow them depending on the business needs.\\nBenefits\\n\\nWhat We Offer:\\nMission Driven - Some companies use AI to serve better digital ads and trade stocks, we seek to make our communities more resilient.\\nTop Compensation - Competitive compensation package.\\nBest in Class Medical Coverage - 100% benefits and premiums paid.\\nPrime NoHo Location - Our office sits in the heart of NYC’s historic NoHo district and is just minutes away from the BDFM, 6, and RW subway lines.\\nHealth Perks - Gym reimbursement and Citibike membership.\\nStrong Culture - collaborative office focused on teamwork, humility, and hustle.\\nCatered lunch on Thursdays, plus a kitchen filled with snacks and drinks.\\nWe're an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\",\n",
       "  \"We're redefining what's possible in public education.\\n\\nWe set out to tackle this education crisis with a groundbreaking school design that delivers a rigorous, whole-child education to students from all backgrounds. Today, as the fastest growing, highest-performing charter school network in New York, our network of 45 K-12 schools outperforms every district in the state, proving irrefutably that all children are capable of excellence.\\n\\nReporting to the Director of Data Management & Analytics, the Data Architect will help to enhance the learning experience for thousands of children across the five boroughs of NYC by defining critical architecture and ETL design patterns used to drive academic performance and increase operational efficiency. Imagine working for a company that has the iterative, constant improvement practices of a startup applied at scale to curriculum and schooling operations. Our data team is at the heart of how this organization operates.\\n\\nIn this role you will:\\n\\n- Model and architect data of both relational, columnar, and noSQL databases;\\n- Use standard methodologies and design patterns to build highly scalable and secure data products;\\n- Contribute to performance metric definitions that drive our business;\\n- Reverse engineer data models and document standards, metadata, and entity and domain relationships;\\n- Define the strategy and principles for data quality management, data governance (security and access), and data archival and retention;\\n- Partner with internal and external constituents to learn about their processes and department specific KPIs to design solutions that solve real problems;\\n- Partner with Business Analysts, Data Engineers, and Data Analysts to build the full lifecycle of data products end-to-end.\\n\\nWe’d be excited to hear from you if:\\n\\n- You are experienced in using Open source technologies (Apache Spark, Hadoop, hbase, CouchDB, Hive, Pig, RabbitMQ, Kafka, etc.);\\n- You have experience designing and building a data warehouse, streaming architecture, and machine learning pipelines;\\n- You are proficient in common programming languages including Python and R;\\n- You are familiar with reporting packages and data visualization platforms including Sisense, Looker and Tableau;\\n- You are experienced in ETL/ELT and workflow scheduling with Matillion, Talend ESB, Airflow, AWS Data Pipelines, Lambda, SQS, etc.;\\n- Architecture and security certification preferred;\\n- You can articulate data findings with concise charts and graphs in visualization tools;\\n- You seek a fast paced, collaborative environment with capacity to work with multiple stakeholders to complete a variety of projects;\\n- You demonstrate patience when working with poor quality data and lack of existing documentation;\\n- You can mentor and educate teammates in best practices and new patterns;\\n- You have 5-7 years of experience on a data science team;\\n- You hold a strong interest in education reform and working for a mission driven organization.\\n\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We actively seek applications from people of all backgrounds to strengthen our community and the perspectives needed to flourish in a multicultural world. Success Academy offers a full benefits program and opportunities for professional growth.\\n\\nLearn more about our philosophy, benefits, and team at https://jobs.successacademies.org/working-here/.\\n\\n\\n</br>Apply\",\n",
       "  'Responsibilities\\nInteract with external / internal data providers to understand the nature of data.\\nGather information on general data delivery schedule for all sources.\\nBuild databases and schemas for data warehousing and analysis.\\nDevelop ETLs to move data into the warehouse and analysis layers.\\nDevelop logical checks in ETL process to check for duplicates, obvious data errors etc.\\nJudge whether an ETL needs to be a batch process or stream process and use appropriate libraries.\\nSchedule and maintain ETLs for different sources.\\nStructure / Re-Structure various schema objects and periodically check on query execution plans to maintain optimal performance.\\nWork with data scientists to ensure the data is formatted in a way that is optimal for their model.\\nCreate and maintain schemas to store MMM results.\\nServe as a go to person for ad hoc queries, schemas, views etc.\\nTechnical Skills\\nFamiliar with big data processing and concepts like MapReduce, spark RDDs etc.\\nKnowledge of cloud storage platforms on AWS, Azure etc.\\nJava, Python and Scala are required skill sets.\\nGood command over SQL, ETL Best practices and data warehousing concepts.\\nFamiliarity with ELT.\\nFamiliarity with various modes of file storage like Parquet, ORC, Avro etc.\\nPrior experience in working with Redshift/Snowflake a plus.',\n",
       "  'Role: Sr Data Engineer\\n\\nLocation: Union, NJ\\n\\nDuration: 6 Months\\n\\nJob Description:\\n\\nData Warehouse and Business Intelligence group is responsible for providing actionable insights into various aspects of the business to drive value and support improvements within the business. DW and BI group focus areas include Merchandising, Supply Chain, eCommerce, Customer & Digital Analytics, Finance and many others.\\n\\nRequirements:\\n5+ years of SQL experience with Teradata\\n3+ years of experience in either Java / Python.\\n3+ years of experience with Unix / Linux including shell scripting.\\nUnderstanding of Architectural principles of data integration and Data Warehousing.\\nSelf-starter with the ability to work independently or as part of a project team.\\nExperience working as a Data Engineer building data mart / Data Warehouse.\\nGood to Have:\\nExperience building data pipeline in cloud.\\nBe versed in Google Cloud Platform (BigQuery) or Microsoft Azure (Azure SQL Data Warehouse).\\nQualifications:\\nCandidate should possess 8+ years of overall IT experience.\\nCandidate should possess 5+ years of Data Warehousing experience.\\nCandidate should possess advanced SQL capabilities.\\nHands-on expertise in Java/Python in Data Processing\\nKnowledge of relational database techniques. Data Warehouse concepts and architecture.\\nExperience working in Unix environment and a scripting language - shell / python.\\nBachelor s Degree in Information Systems, Computer Science or related field.\\nApply online OR send resume to veeresh.murugan@qstride.com\\n\\n- provided by Dice',\n",
       "  \"Position Summary:\\n\\nAs a Data Engineer you will get to play a key role in the delivery of powerful data-driven products that support the 32BJ Health Fund's mission of providing high-quality and low-cost healthcare to its union members. The Data Engineer will be responsible for providing internal analysts with accurate datasets by implementing best practices in data collection, movement, storage, and transformation of large datasets. This individual will work with both current ETL/Data Warehousing and provide direction for future development of data storage, streaming and pipeline architectures.\\n\\nEssential Duties and Responsibilities:\\nWork with Health Fund analytics, Operations and IT to implement new data infrastructure and migrate existing data from disparate sources into a robust data warehouse\\nUsing SQL, and other data transformation tools as well as languages such as Python and R, develop automated processes for transforming incoming data from multiple vendors and loading it into a centralized data warehouse for use by Health Fund Operations and Analytics\\nDesign, build, and manage automated test frameworks and scripts that support a continuous integration/continuous delivery (CI/CD) approach\\nGenerate subsets of data, and provide APIs and variables needed for interfacing with internal tools as well as public-facing websites\\nWorking with IT and Operations, to evaluate business cases for use of scalable cloud solutions such as Azure and Dynamics 365\\nInterface with internal teams and vendors IT to work through data quality issues and champion HIPAA compliant best practices in data handling and transfer\\nCreate clear documentation (user guides, quick starts, schemas, and data dictionaries) of established data structures and use cases\\nProvide tutorials and working sessions to empower analysts in creating queries and accessing right level of data and establish oneself as an expert resource on internal and external data sources and management\\nQualifications:\\nBachelor s degree in Computer Science or a related discipline; Advanced degree preferred\\n5+ years of full-time experience or demonstrated accomplishments in relevant subject areas\\nPrior experience in working with healthcare claims is a must\\nStrong knowledge of SQL with hands-on industry experience is a must\\nKnowledge of methods for handling non-relational JSON and XML formatted data\\nFamiliarity with database performance optimization techniques\\nprovided by Dice\",\n",
       "  'Our Solution\\n\\nDemystData is an External Enterprise Data Platform, used to better discover, test, and access third party data. Leading banks and insurers leverage our platform to find and test new data to incorporate into decision processes across the customer lifecycle.\\n\\nThe Challenge\\n\\nThe Client Data Engineer assists customers with their end-to-end data needs, including helping clients explore and understand the growing ecosystem of Demyst data partners, use our platforms to access data for testing and evaluation, and design, develop, and deploy workflows for accessing external data at scale within client systems. The Client Data Engineer consultatively develops clear solutions to use cases with our proprietary software products. This role supports the internal and external processes that lead to customer acquisition, onboarding, satisfaction, growth, & retention with a focus on connecting customer needs to data capabilities. The candidate has a strong command of data, analytics, data compliance, data engineering, and has the project management skills necessary to pull from other experts across the Demyst team to deliver client value.\\n\\nResponsibilities\\nImplement and configure the DemystData platform to ingest data from a variety of sources and ensure that the data pipeline is consistent and reliable.\\nAdvise clients by recommending data sources fit for purpose, and supporting data cleaning and access\\nAdvise clients on target state architecture for data ingestion, management and decision workflows\\nWork onsite with customers to manage complex data access projects deployments (20-60% of time)\\nIdentify business requirements, define use cases, and understand data sources that support client data use cases\\nExecute cost benefit analyses\\nRequirements\\nBA/BS (or MA/MS) in a technical discipline (e.g., computer science, software engineering, data science, etc.)\\n2-4 years working within a data centric solutions provider, e.g. within analytical consulting or technology providers\\nExceptional technical project delivery track record\\nStrong ability to influence through consultation and strong communication skills\\nExperience with AI tools such as DataRobot, H20, Python, and AI frameworks like Spark, TensorFlow\\nObject-oriented software development skills (e.g., Java, C#, Python, Ruby, Scala)\\nStrong alignment with core organizational values\\nBenefits\\nWork with the largest consumer and business external data market in an emerging industry that is fueling AI globally\\nHave an impact in a scaling but small team offering real autonomy and responsibility for client outcomes\\nStretch yourself to help define and support something entirely new that will impact billions\\nWork within a strong, tight-knit team of subject matter experts\\nSmall enough where you matter, big enough to have the support to deliver what you promise\\nDistributed working team and culture, recognition of outcomes and merit, not presenteeism\\nGenerous benefits & competitive compensation\\nDemystData is committed to creating a diverse, rewarding career environment and is proud to be an equal opportunity employer. We strongly encourage individuals from all walks of life to apply',\n",
       "  'Overview\\n\\nSmartling is seeking a Data scientist to join our engineering team. We are a small, tight-knit, dynamic group of engineers building and maintaining various smart systems that our customers rely on to maximize their value in using our platform. In this role, you will be responsible for tasks like identifying metrics and points of data to track and pre-processing data via Python.\\n\\nYou\\'re a great match if you love to solve hard problems. You will work directly with engineering to solve our customers\\' problems with data models and work in the natural language space with billions of data points.\\n\\nOur Engineering team approach\\nAgile / Scrum.\\nTwo pizza teams.\\nData-driven decisions.\\nTasks at hand\\nIdentifying metrics and points of data to track.\\nPre-processing data via Python.\\nModel and feature selection.\\nModel training and validation via various Python libraries.\\nEvaluation of models once deployed.\\nTraining with the team to help with best practices and knowledge sharing.\\nParticipate in the engineering process: pull requests, sprints, scrums, etc.\\nMust haves\\nProficient in Python.\\nKnowledge of data science techniques (sampling, feature engineering, models, etc).\\nBasic understanding of linear algebra, calculus, and statistics.\\nMachine Learning knowledge: 1+years of experience\\nBasic understanding of Notebooks and Terminals.\\n2 years of coding and data analytics.\\nNice to have\\nExperience with Java.\\nExperience with Spark.\\nCloud technology experience: SageMaker, Glue, EMR, S3, etc.\\nYou are\\nCurious. The desire to learn new techniques, technologies, and frameworks.\\nCreative/Innovative. Demonstrate inventiveness and the ability to find information or solutions needed for assignments.\\nAutonomous/Self-started. Work well and achieves results with little or no supervision.\\nTeam Player. Willing to do PRs, give advice, and contribute to the team.\\nProblem Solver. Provide workable solutions or consults with secondary resources to devise solutions.\\nAdaptable. Maintain focus and positive attitude amidst change or under pressure.\\nWith a \"can do attitude\". Able to roll up your sleeves and drive projects to their completion.\\nWhat matters to Smartling?\\nTo help our clients grow their businesses and to help you grow as an individual both professionally and personally.\\nCompetitive salary and Employee Stock purchase plan.\\nAn opportunity to learn and advance your career.\\nAn energetic, value-driven, and fun culture.\\nTake a break when you need it – Flexible PTO.\\nCompany paid medical/dental/vision/life insurance.\\nCommuter Benefits.\\nConvenient office in the middle of Manhattan - just blocks from Penn Station and Grand Central.\\nSmartling, Inc. is an equal opportunity employer. No third party recruiters.',\n",
       "  \"Group Nine Media is the parent company of four mission-driven, category-leading brands covering food, drink and travel (Thrillist), news and entertainment (NowThis), animals and activism (The Dodo) and science, tech and exploration (Seeker). We're one of the world's largest digital-first media companies, boasting over 4 billion video views every month, meaning that the work you do here will be impactful and geared towards informing our readers on topics that matter.\\n\\nThis role will be on our Data Insights team. We are a small, agile team responsible for building and architecting the platform where all our big data is collected, and the products and reporting that surface that data. We are responsible for everything from predictive models that help inform and optimize our creative process, to dashboards and web tools that display data in intuitive ways.\\n\\nYou will also work closely with our Audience Development team, our Sales team, and our Studios group. You’ll get an understanding of how big data is used at Group Nine Media to make decisions, set goals, and drive sales.\\n\\nWe are a team of people from various social, economic, and ethnic backgrounds who value professional development and work/life balance. It's our belief that our team and our work are both made better by our diversity. If this sounds like you, keep reading!\\n\\nAs a person who will help establish Group Nine Media as an insights leader in the media industry, in a typical week you’ll:\\nHelp build Data Products at Group Nine, including everything from reporting dashboards to deploying predictive models to production and other custom workflow tools\\nWork alongside other engineering teams to develop services/APIs that give us access to the insights we learn through this work\\nResearch new and innovative ways to create data science products and services\\nWork with our data platform engineers to recommend what data we collect, how we store it and how it is accessed\\nImplement data validation processes to ensure data quality\\nTake hypothesis from the insights team, understand if they are possible or not in terms of the state of capabilities, and test them under a rigid scientific framework to prove its validity\\nDesign, track and report on experiments to validate your hypotheses\\nThe next great Data Engineer will have:\\nAt least 2 years of professional experience in the field of data engineering or data science.\\nA BS/MS/PhD preferably in Computer Science, Statistics or Applied Mathematics - but we welcome other STEM-related fields or equivalent experience\\nProficiency with Unix/Linux Environments, Github, SQL and Python\\nExperience using Python or R for Data Science, or at least have some familiarity\\nQualities of an ideal candidate:\\nYou value humility, patience, diversity and curiosity\\nYou enjoy using your strong analytical problem solving skills to improve the business\\nYou have excellent communication skills. This is a role that interacts with many members across the business, especially on the Product and Insights teams.\\nYou have a passion for evangelizing how data can impact the decisions we make\\nYou are strongly interested in developing internal tools to share data findings\\nYou are interested in Statistics, Machine Learning and Data Science.\\nYou’re an independent, self-starter who sees the freedom to shape a team and their future work as an exciting challenge\\nYou have a strong understanding of the digital media landscape, how monetization works in this industry, and the problems it faces with regards to data and privacy\\nYou know the value of a great hypothesis and you can design a well-thought out experiment to test that hypothesis\\nWe use Google Cloud Platform, so being familiar with these products is a plus\\nCover letter not required but it might help set you apart from other candidates. We promise that we read them!\\n\\nThe perks\\n\\nWe have amazing people. We can promise you’ll work alongside some of the smartest and most interesting people in the industry. Our people work hard but we always have fun doing it. From pride to wellness to diversity, employees lead clubs that matter. We also know how to host a top-notch happy hour. Bottom line: we care about each other and we take our values seriously.\\n\\nWe value development. We are a high-performance organization that challenges itself to continuously grow. We invest in employee development. You’ll need to be great to get hired here and we promise you’ll get even better once you’re here.\\n\\nWe care about you. We offer company-paid parental leave for everyone #inclusion. We also offer health, dental and vision insurance, along with a selection of voluntary benefits. Knowing that time out of the office is as important as time spent in the office, our vacation, sick, and federal holiday allowances ensure you have time to take care of yourself.\\n\\nWe invest in your future. We offer competitive compensation and a matching 401(k) plan. Depending on the role, options may also be granted.\\n\\nWe value diversity and inclusivity. Group Nine Media is an Equal Opportunity Employer and is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to\",\n",
       "  \"MongoDB Atlas Data Lake is a new service in the MongoDB Atlas family - it was recently announced at MongoDB World (MongoDB's largest annual user conference) and is available in public beta. It brings the technology that made MongoDB the most popular document database in the world and applies it to the great data lakes of the cloud. As companies have accumulated more and more data in cloud storage like Amazon S3, the need to process that data effectively has risen.\\n\\nWith MongoDB Atlas Data Lake, you use the MongoDB Query Language, which is built for rich, complex structures and works with data stored in JSON, BSON, CSV, TSV, Avro, and Parquet formats. Data is analyzed on demand with no infrastructure setup and no time-consuming transformations, pre-processing or metadata management. There's no schema to pre-define, allowing you to work with your data faster.\\n\\nOur team is small and nimble and based out of New York City. We're passionate about distributed system design and providing the most compelling platform for users to work with data. We primarily develop in the Go programming language.\\n\\nThe ideal candidate will\\nHave 1+ years of experience developing robust back-end software at scale, including experience with concurrency and distributed systems\\nHave substantial, demonstrable experience writing high-quality software\\nHave strong computer science fundamentals, including data structures, algorithms, and software design\\nHave demonstrable aptitude for continuous learning of new technologies\\nBe an effective communicator and problem solver\\nHave 1+ years of experience programming in a high-level server-side language like Go, Java, C++, or Python\\nHave some experience with MongoDB\\nPosition Expectations\\nIdentify, design, implement, test, and support new features for the Atlas Data Lake service\\nCollaborate with team members over best practices and core concepts\\nGive and receive consistent, high quality code review feedback\\nEstimate task complexity and report progress and risks to peers and managers\\nPeriodically work with support and field teams to diagnose and remediate customer issues\\nSuccess in this role means\\nIn one quarter on the team, you will have contributed to a feature that will ship in the next stable release of the Atlas Data Lake\\nIn two quarters, you will be actively involved in code and design reviews for new features\\nIn one year, you will be participating in the team project roadmap, leading design for specific features or projects, supporting other teams, and helping to interview and mentor new hires and interns\\n*MongoDB, Inc. provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws.*\",\n",
       "  'Job Description\\nWarby Parker is on the lookout for a motivated Senior Data Engineer to help every department across our organization monitor, track, and improve their work. Our team’s reach is wide, and our efforts to collect and share data impact nearly every employee. In this role, we’ll tailor your day-to-day responsibilities to your skillset. You may support our work by developing integrations between our rapidly growing tech systems and our data warehouse. Or maybe you’ll design sophisticated BI data models, advocate data governance best practices, and own the tools used by analysts across Warby Parker. Either way, our ideal candidate is equipped with analytical thinking, communication, and collaboration skills, as well as a fluency in SQL/relational modeling. The ability to write code with razor-sharp attention to detail will also set you up for success here.\\n\\nAt the end of the day, we’re out to prove that businesses can scale, be profitable, and do good in the world. As you contribute to the company’s success, you’re also supporting our Buy a Pair, Give a Pair program and Pupil’s Project. Interested in joining us? Read on!\\n\\nWhat you’ll do:\\nAsk thoughtful questions and listen carefully to get to know your business partners’ vocabulary, problems, goals, and constraints\\nWork with a Product Manager and other Engineers to scope out and prioritize potential solutions to complex data problems\\nDevelop data models that meet business partners’ needs\\nDream up ways to help our stakeholders solve problems they may not have even thought of yet\\nImplement tested, production-ready ETL code in a mix of database environments\\nProvide thorough and helpful documentation to ensure that our decisions can be shared and understood in the future\\nWho you are:\\nBacked by 5+ years of software engineering experience with data pipelines or other data-intensive applications\\nComfortable working with the tools of modern software engineering, like working at the command line, using version control, writing tests, performing code review, and more\\nExperience working in an iterative, Agile environment\\nAble to write SQL\\nExcited to help bring the rigor of modern software development practices to the business intelligence space\\nAn analytical thinker who can understand the needs of an analyst or business expert\\nAble to exercise judgment on all aspects of project execution, while also soliciting input from colleagues and stakeholders\\nA kind, empathetic listener and proactive, effective communicator (in writing and in person!)\\nA team player who’s able to collaborate with people of various skills and backgrounds\\nNot on the Office of Inspector General’s List of Excluded Individuals/Entities (LEIE)\\nExtra credit—definitely not required, but if you have these skills we can put them to good use!\\nThe ability to develop transformations in modern cloud databases such as BigQuery, Snowflake, or Redshift\\nExperience applying the techniques of Kimball-style dimensional data modeling in an Agile environment\\nExperience developing, debugging, and deploying infrastructure within AWS or a similar cloud environment\\nExperience with streaming event services, such as Kinesis or Kafka\\nThe ability to visualize data and teach others to, too\\nExperience with data governance, data dictionaries, and other processes or techniques to ensure that our work is maximally useful to the company\\nA background in teaching, mentoring, tutoring, or lecturing\\nAbout us:\\nWarby Parker was founded with a lofty objective: to offer designer eyewear at a revolutionary price while leading the way for socially conscious businesses. By circumventing traditional channels and designing our frames in-house, we’re able to offer top-quality glasses and sunglasses (plus an uncommonly delightful shopping experience) at a fraction of the traditional going price.\\nSince starting out in 2010, we’ve set up headquarters in New York City and Nashville, built our own optical lab, and opened retail locations all around the U.S. and Canada. As we grow, we’re committed to proving that businesses can scale and be profitable while doing good in the world. For every pair of glasses we sell, a pair is distributed to someone in need—to date, that’s over five million pairs.\\nOf course, all work and no play makes a dull workplace. Who likes that? At Warby Parker, you can look forward to company outings, volunteering and learning opportunities, and just great company. Teammates can also connect around common interests, backgrounds, and identities, no matter their home base, through our various employee resource groups. (We’re happy to say that the Human Rights Campaign has named us a Best Place to Work for LGBTQ+ employees!) That sense of community keeps us excited to walk through the door every day. Good work, good people.\\n\\nSome benefits and perks of working at Warby Parker:\\nHealth, vision, and dental insurance\\nFlexible “My Time” vacation policy\\nRetirement savings plan with a company match\\nParental leave (non-birthing parents included)\\nCell phone plan reimbursement\\nA health-and-wellness stipend\\nFree eyewear, plus discounts for friends and family\\nAnd more—just ask!',\n",
       "  \"The $21B Powersports market (motorcycles, ATVs, UTVs, jetskis, snowmobiles) is all about fast and fun, but the purchasing process is slow and frustrating. Octane enables consumers to live their passion by making powersports purchases instant, seamless, and widely available. Octane's credit product expands coverage to 50% of responsible consumers underserved by incumbent banks while our automated underwriting and digital buying experience cuts the time to close a transaction from hours to minutes. The market is choosing the Octane solution, as evidenced by ~25% of powersports merchants active on our platform (up 50% Y/Y) and an annualized origination run rate that is up over 3 X Y/Y. We are both the platform and the lender, which means we have both high growth and positive unit economics—rare for a fintech. We have raised >$60M in venture capital from leading investors such as IA Ventures, Valar Ventures, and Contour Venture Partners.\\n\\nOctane Lending is looking for a Senior Data Engineer to focus on facilitating our company's data needs. Data is the fuel that we use to drive decisions for the company. From informing credit policy to creating product enhancements to updating servicing protocols, availability and accessibility of data is critical. As a Senior Data Engineer, you will collaborate with a team to architect a data infrastructure that is stable and scalable to support data analytics, reporting, and visualization. Your input and contribution will have direct impact on our future data strategy and our technology roadmap.\\n\\nRESPONSIBILITIES\\nExpose a variety of data sources into a centralized data warehouse capable of supporting analytics, reporting, and visualization\\nBuild and manage ETL processing that is efficient, scalable, and performant\\nDesign and implement secure and auditable cloud based data infrastructure\\nTest and optimize data architecture performance\\nCollaborate with teammates and internal teams to improve products\\nReview code and fix bugs\\nMentor junior team members to follow engineering best practices\\nRequirements\\n5+ years experience working in software and data engineering\\nExtensive knowledge of ETL processing, data pipelines, and data warehouses\\nProficiency with Python\\nExperience using Spark and SQL\\nExperience using AWS or other cloud services provider\\nExperience working with data that has regulatory, privacy, auditing, and security requirements (ideally in the Consumer Lending Industry)\\nBenefits\\nA working environment filled with passionate, happy, smart people\\nHealth Care Plan (Medical, Dental & Vision)\\nRetirement Plan (401k)\\nLife Insurance (Basic, Voluntary & AD&D)\\nPre-Tax Commuter Benefits\\nPaid Time Off (Vacation, Sick & Public Holidays)\\nShort Term & Long Term Disability\\nOffice snacks, full fridge, and drinks\\nMonthly team happy hours and lunches\\nFun company outings like bowling, yoga, cycling, mixology, cooking classes, & more!\\nOctane Lending is an equal opportunity employer committed to providing equal employment opportunity without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or any other protected status with respect to recruitment, hiring, promotion and other terms and conditions of employment.\",\n",
       "  'Data Engineer\\nNew York, NY\\nDuration: until end of this year (w/ strong possibility until end of 2020)\\n\\nBack- end Data Engineer (DataStage, Spark/ Scala), Data Modeling and SQL. This will be in support of a new mission to consolidate data sets and analytics into client platform.\\n\\nWilliam Perry\\nEclaro International Inc.\\ndirect dial 1.703.310.6896\\ntoll free 1.866.352.6110\\nwperry@eclaroIT.com\\n\\n- provided by Dice',\n",
       "  \"The Role\\n\\nTripleLift is seeking an experienced data engineer to join our team full time. We are a fast growing startup in the advertising technology sector, trying to tackle some of the most challenging problems facing the industry. As a data engineer, you will be responsible for building and improving our data pipeline and reporting infrastructure. This role consists of a variety of responsibilities from building out our large scale data pipelines to building and managing a set of data driven API's with the corresponding front end components.\\n\\nCore Technologies\\n\\nWe employ a wide variety of technologies here at TripleLift to accomplish our goals. From our early days, we've always believed in using the right tools for the right job, and continue to explore new technology options as we grow. The data engineering team primarily uses the following technologies:\\nApache Spark & Scala for our core data processing framework\\nJava & VoltDB to process to power our sub second real-time data pipeline\\\\\\nPHP7 & Symfony to power our reporting API's\\nApache ZooKeeper & Apache Kafka to power are message bus\\nRedshift/Snowflake & Druid as our main data stores\\nAmazon Web Services, Docker, Mesos, & Marathon to power our services\\nCurrent Projects / Expected Responsibilities\\nBuild out TripleLift's real time data pipeline to support bringing all the data and metrics in our system up to date with latency measured in milliseconds\\nRebuild our spark pipeline to make it more extensible, maintainable, and scalable\\nTake our monolithic API system and split it out reporting into a microservice based system built from scratch to power all our reporting, scheduling, and alerting needs.\\nRequired Skills\\n\\nExperience in either of the following:\\nLarge scale distributed data processing & pipelining\\nBuilding out Asynchronous API's and services\\nExperience with RDBMS such as PGSQL\\nExperience working with queues or other messaging systems such as RabbitMQ or Apache Kafka\\nPreferred Skills\\nExperience with a object oriented language such as Java or PHP\\nExperience with a functional based language such as Scala\\nExperience with a distributed data processing framework such as Spark\\nExperience building out RESTful APIs\\nExperience with distributed data bases such as Druid, VoltDB, Snowflake, or Cassandra\\nBasic frontend knowledge and competencies\\nEducation Requirement\\n\\nA Bachelor's degree in computer science,or a related discipline is preferred, although candidates with relevant experience who hold other degrees will be considered.\\n\\nExperience Requirement\\n\\nAt least 2 years of working in a professional, collaborative environment.\\n\\nLocation\\n\\nNew York, NY\\n\\nEngineering at TripleLift\\n\\nThe formal engineering department is filled with dedicated, smart, and hardworking individuals who take a great deal of pride in their work. Some of the reasons we all enjoy working here:\\nYou get ownership and accountability for projects and have a voice in how the work happens\\nAn onus on collaboration and learning over pure short-term output\\nInnovation is baked into how we operate; we're accepting of new ideas and opened to change\\nDedicated time set aside each week to pair with others to learn or experiment\\nTeam social events such as cooking classes, D&D, and gaming nights\\nAbout TripleLift\\n\\nTripleLift makes native advertising simple, scalable, and effective. Leveraging pioneering computer vision technology, TripleLift seamlessly transforms visual content like images and video into native ads that match the unique look and feel of a publisher's website. TripleLift's native inventory is accessible through the industry's first real-time, native programmatic exchange, helping marketers reach millions of consumers across any device, at scale. Since 2012, TripleLift has delivered meaningful results for some of the world's biggest brands through what it calls the next evolution of display advertising.\\nBenefits and Company Perks\\n\\nAmazing company culture\\nCompetitive salary and performance-based bonuses\\nOngoing professional development\\nComprehensive Medical, Dental and Vision insurance\\nEquity options\\n401(k) program\\nSnacks on snacks on snacks\\nYoga, massages, and meals\\n\\nTripleLift Awards\\n\\nInc. Magazine's list of fastest-growing companies - 2017, 2018, 2019\\nCrain's Best Places to Work - 2015, 2016, 2017, 2018\\nForbes Next Billion Dollar Companies - 2018 Inc.\\nCrain's Fast 50, Fastest Growing Companies in New York - 2017, 2018\\nDeloitte Technology Fast 500 - 2017, 2018\\nCEO & Co Founder Eric Berry won the Ernst & Young Entrepreneur Of The Year 2019 New York Marketing and Advertising Award\\n\\nNote: The Fair Labor Standards Act (FLSA) is a federal labor law of general and nationwide application, including Overtime, Minimum Wages, Child Labor Protections, and the Equal Pay Act. This role is a FLSA non-exempt role.\",\n",
       "  \"Recently acquired by Microsoft, our client is a New York City-based technology company that works with the US's largest e-commerce retailers. Our client delivers intelligent vendor marketing solutions created for the next generation of e-commerce. Their solutions help retailers automate, implement and scale brand-funded marketing programs on e-commerce sites.\\n\\nData plays an integral role in our client's product, and software engineers on their data engineering team build the pipelines that power reporting and analytics for their e-commerce promotions platform. The infrastructure and applications that you'll build on the data engineering team will have broad and critical reach in powering real-time auction decisions, becoming multipliers on our revenues, and forecasting supply and demand for our customers.\\n\\nRequirements\\nShip high-quality, well-tested, secure, and maintainable code\\nDesign, develop, and maintain data pipelines and back-end services for real-time decisioning, reporting, optimization, data collection, and related functions\\nManage automated unit and integration test suites\\nWork collaboratively and communicate effectively with a small, motivated team of engineers and product managers\\nExperiment with and recommend new technologies that simplify or improve PromoteIQ's stack\\nParticipate in an on-call rotation and work occasional off-hours\\nQUALIFICATIONS:\\nBS/MS in Computer Science or a related technical field\\nSeeking candidates with 4+ years of experience in:\\nArchitecting, building, and maintaining end-to-end, high-throughput data systems and their supporting services\\nDesigning data systems that are secure, testable, and modular, particularly in Python, as well as their support infrastructure (shell scripts, job schedulers, message queues, etc.)\\nDesigning efficient data structures and database schemas\\nWorking with distributed systems architecture\\nIncorporating data processing and workflow management tools into pipeline design (AWS EMR, Airflow, Kafka, etc.)\\nUsing profiling tools, debugging logs, performance metrics, and other data sources to make code- and application-level improvements\\nDeveloping for continuous integration and automated deployments\\nUtilizing a variety of data stores, including data warehouses (ideally Redshift), RDBMSes (ideally MySQL), in-memory caches (ideally Aerospike and Redis), and searchable document DBs (ideally Elasticseach)\\nWrangling large-scale data sets\\nBenefits\\nComprehensive benefits including medical, 401K and more.\",\n",
       "  \"The Data Science team at Ordergroove is looking to hire a Data Engineer to join our growing lineup. As we expand our platform to include more data driven offerings, our data storage and access requirements are growing rapidly. This team is responsible for designing and building the components of this system and ensuring their integrity and performance.\\n\\nOur team impacts the entire Ordergroove platform which powers the retail experience of some of world's largest brands. On this platform the data science team are creating the foundational infrastructure for a new collection of data and AI driven products to enable frictionless commerce. If you're interested in a crucial role building scalable infrastructure for highly visible products, then this role might be for you. Check out our current tech stack here.\\n\\nWhat you will do:\\nDesign data platform components to backstop new products.\\nBuild resilient and scalable solutions for both batch and real-time services.\\nEvaluate and test the tradeoffs between competing technologies and architectures.\\nModel and structure data storage according to product requirements.\\nMonitor and optimize performance of our platform.\\nAutomate deployment of new infrastructure.\\nAbout You:\\nYou write production level Python on a daily basis (R additionally a plus)\\nYou have hands-on experience with Google Cloud Platform\\nYou are skilled at writing and optimizing SQL (core application DBs use MySQL Google BigQuery)\\nYou ideally have experience with Apache Airflow\\nYou have built/administered distributed systems at scale (e.g. Hadoop, Spark, Cassandra etc.)\\nYou understand at least one Python-based web framework (Django, Flask etc)\\nYou preferably have some DevOps experience (especially with Chef and Kubernetes)\\nYou have 2+ years experience in a related role\\nAbout Ordergroove:\\n\\nWe're a close-knit team of passionate engineers, marketers and innovators whose mission is to help our clients become an indispensable part of their customers' lives through Relationship Commerce. We've raised over $40M in venture capital and are planning to grow our team to 100 people this year!\\n\\nRecently named one of the 2019 Best Companies to Work For in New York, our collaborative culture celebrates independent thinking and curiosity. We demand the best of ourselves and each other and never miss an opportunity to celebrate our success. Company events include hackathons, wine tours, summer BBQs, trips to baseball games and game nights.\\n\\nWe compete hard for the industry's best talent by offering great employee benefits including one of the most flexible PTO policies you'll find, highly competitive compensation and meaningful stock option grants. Our New York HQ is minutes from all of the major subways and boasts a 6,000 square foot private outdoor terrace overlooking beautiful downtown Manhattan.\\n\\nWhen you join the Ordergroove team you'll be surrounded by smart, fun people working to transform the business of commerce! If you're ready for a high-impact role at a company creating groundbreaking solutions, we invite you to learn more and apply!\\n\\nAt Ordergroove we are committed to creating a welcoming and supportive environment for all people. We do not discriminate against anyone for any reason. We encourage people with different backgrounds and experiences to join our growing team.\",\n",
       "  'VillageCare – Redefining Wellness\\n\\n\\n112 Charles St., New York, NY\\n\\nVillageCareMax\\n\\nJob Title: Data Engineer\\n\\nRoles & Responsibilities:\\n\\nThe VillageCare Data Engineer will be responsible for helping to build ETL processes for Village Care’s Data Warehouse in an AWS cloud environment. The Data Engineer will build ETL pipelines, get analysis tools working properly, and stand up core data processing components in Village Care’s cloud-based data processing environment.\\nBuilding python-based ETL jobs\\nLight web application development of purpose-built internal tools\\nTechnical guidance in support of our project management team when defining the scope of data integration projects\\nContribute to designs of new components in modeling and data pipelines\\nRemain current on emerging open source data processing projects and tools\\nMust have experience with AWS services, Redshift data bases and HIPAA compliant architecture models.\\n\\nQualifications:\\nThree or more years of experience working with *nix-based, open source data processing tools\\nFluent in Python, SQL, Spark, Hadoop, AirFlow and/or similar technologies/toolsets\\nTwo or more years of experience developing production ETL applications\\nFour or more years of experience in software development\\nThree or more years of experience with SQL\\nCurious, informed and opinionated about data processing technologies\\nExperience with structured and unstructured data storage and modeling\\nDeep understanding of database and filesystem storage/access\\nExperience with various data engineering architecture patterns\\nInterest in Data Science and Data Analysis\\n\\nPreferred Education and Experience\\nBS or MS in Computer Science\\nExperience writing production Python\\nImplementation experience with Airflow, Python, Spark etc.\\nExperience with healthcare data formats (x12 EDI, HL7, etc)\\nExperience implementing stream processing pipelines (Spark, etc)\\nMapReduce/Hadoop ecosystem experience (Hive, HDFS/S3, Presto)\\nExperience with source control technology like Git\\nExperience with testing frameworks (unit and end-to-end)\\nFamiliar with the usage of Continuous Integration/Continuous Deployment frameworks in AWS like Jenkins, CircleCI, Code deploy and code commit\\nUnderstanding of Docker implementation in AWS\\nUnderstanding of AWS serverless services like Lambda/API gateway\\nGood to have: AWS elastic beanstalk, AWS cloud formation\\nIntegrity\\n\\nYou are a team member who serves as a positive example and reflection of why others trust the intentions of VillageCare by:\\nBeing honest and trustworthy\\nMeeting your commitments and obligations\\nAcknowledging your role in actions or events with unsatisfactory outcomes\\nCustomer Focus/Cultural Awareness\\n\\nYou are a team member who understands the importance of strong customer service internally and externally and you demonstrate this by identifying customer needs and expectations, and responding to them in a timely and effective manner. You are consistently customer focused by:\\nDemonstrating an awareness of the needs of individuals through recognizing multiple levels of connections\\nAnticipates and prevents delays or other things that can adversely affect the customer.\\nKeeping customers informed about the status of pending actions and inquires\\nFlexibility/Agility\\n\\nYou are a team member who adjusts quickly and effectively to changing conditions and demands. You understand that change is a necessary and an inevitable aspect of organizational life as well as an opportunity to learn new things. As such, you are flexible and agile by:\\nMaintaining a positive view of potentially stressful situations\\nAccepting and adapting to organizational or departmental changes\\nViewing change as opportunities for VillageCare to grow in a direction that better serves our clients and our employees\\nResult Oriented/Innovative Thinking\\n\\nYou are a team member who consistently looks for new and innovative approaches that will improve efficiency in your role. You champion new ideas and build upon existing processes by:\\nUsing data/fact-based information to make decisions relevant your role\\nUnderstands that obstacles will occur and refuses to use them as an excuse for not achieving results\\nBEVital\\n\\nYou are a team member that consistently supports VillageCare’s larger organizational culture by displaying a commitment to the three cultural drivers that make VillageCare and our employees vital to the healthcare space by:\\nExceeding expectations in both internal and external customer service areas\\nUsing data and key information to inform decisions pertinent to your role (where applicable)\\nUtilizing relationships, tools and positivity to enhance organizational performance through communication and collaborative team work\\nVillageCare is committed to superior outcomes in quality health care. Do you share a common commitment to* patient care, customer service and passion *for individuals’ well-being ?\\n\\nApply now!\\n\\nVillageCare:\\n\\nWith over 25,000 people served in 2017, VillageCare’s mission is to promote healing, better health and well-being to the fullest extent possible.\\n\\nVillageCare began in 1977 as a project by community volunteers to rescue and reorganize a for-profit nursing home slated for closure. It has become a much larger organization that provides post-acute care, community-based services and managed long-term care. As a result of this history, VillageCare has become a valued resource for the people we serve, their caregivers and other provider organizations with which we partner.\\n\\nVillageCare is committed to the tenets of diversity and workforce that are strengthened by the inclusion of and respect for our differences. We offer our employees a highly competitive compensation and benefits package, a 403(b) retirement plan, and much more.\\n\\nVillageCare is an equal opportunity employer. We promote recognition and respect for individual and cultural differences, and we work to make our employees feel valued and appreciated, whatever their race, gender, background, or sexual orientation.\\n\\n* EOE Minorities/Women/Disabled/Veterans*',\n",
       "  \"Data Engineer\\nIf you are a talented Data Engineer/Architect, please read on!\\n\\nBased in NYC, we are one of the fastest growing tech companies in the nation! We are disrupting one of the largest commodities markets that has completely run astray in the last couple decades. We have our eye on utilizing algorithms in order to overhaul this market and provide customers with the most efficient and positive user experience that they have encountered in far too long. We are leveraging the latest and greatest technologies to develop a platform that not only accomplishes this goal, but can take this multi-billion dollar market to new heights and we have already built a rock-solid foundation. Now it's time for some serious growth and we need someone like yourself to help us get to where we want to go!!\\n\\nThe right candidate will possess a solid mix of the below skills/experience and will be passionate about all things data! If the above sounds interesting and you share our passion for change, we would love to speak with you immediately!\\nTop Reasons to Work with Us\\n1. Awesome Team/Company Culture\\n2. Work with New Technologies\\n3. MASSIVE Growth Potential!\\nWhat You Need for this Position\\nRequired (A Solid Mix of the Below):\\n\\n- Data Engineering/Architecture\\n- NoSQL\\n- Hadoop\\n- Big Data\\n- Relational Databases (MySQL, Postgres, etc.)\\n- OOP/Scripting (C++, Java, Python, R, Spark, Hive, etc.)\\n- Microservices\\n- Containers\\n- Data Ingestion/Pipelines/Analytics\\nWhat's In It for You\\n- Daily lunch stipend\\n- Medical, dental, and vision plans\\n- Strong PTO\\n- 401k\\n- Fully stocked pantry and keg on site\\n- Monthly team happy hours\\n- Summer Fridays\\n- Work with a fun and fast moving team\\nSo if you are a talented Data Engineer/Architect, please apply today!\\n-\\nApplicants must be authorized to work in the U.S.\\n\\n\\nCyberCoders, Inc is proud to be an Equal Opportunity Employer\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\\n\\nYour Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\",\n",
       "  'Job Title\\nAzure Database/Big Data DevOps Engineer\\n\\n04-Oct-2019\\n\\nTD Description\\nTD Securities provides a wide range of capital market products and services to corporate, government and institutional clients. The firm works with clients around the world, focusing selectively and strategically on the key financial centers: Toronto, Montreal, Calgary, Vancouver, New York, London, Singapore and Tokyo. www.td.com\\n\\nAuto req ID\\n250781BR\\n\\nCountry\\nUnited States\\n\\nJob Requirements\\n• Strong programming skill with experience in API and Webhook development using Python, Ruby, PowerShell and Shell Scripting languages\\n\\n• Experience with automating\\n(provisioning, configuration management, deployment...) and integrating Azure Data PaaS solutions (Azure SQL, Azure Cosmos DB, Azure SQL Datawarehouse, Azure Data Lake, Azure Databricks, Azure HDInsight…).\\n\\n• Design and build modern Data Pipelines/Streams and Data Service APIs to assist with data migration (on-premise SQL to Azure SQL databases, on-premise Hadoop to Azure Data lake…)\\n\\n• Write and use Azure RM templates\\n\\n• Understand product features for DR/BCP options and how fits in overall application architecture\\n\\n• Create design/architecture patterns covering HA and DR/BCP\\n\\n• Understand security features and for Azure Data solutions (data protection, authentication, RBAC, audit logging…)\\n\\n• Ability to troubleshoot Application connectivity/access to Azure data products (NSG, basic routing)\\n\\n• Understand and develop concepts related to deploying databases via CI/CD pipeline\\n\\n• Ensure that all cloud solutions follow internally defined security and compliance controls\\n\\n• Implement the enterprise cloud capability and enhance the cloud orchestration platform for automated provisioning, management and scalability of hosts, containers, applications and cloud services\\n\\n• Proficiency in cloud automation using native Azure CLI\\n\\n• Ability to participate in fast-paced DevOps Engineering teams within Scrum agile processes\\n\\n• A critical thinker with strong research and analytics skills\\n\\n• Self-motivated with a positive attitude and an ability to work independently and or in a team\\n\\n• Able to work under tight timeline and deliver on complex problems\\n\\nHours\\n40\\n\\nJob Description\\nWe are looking for an Azure Database/BigData DevOps Engineer to work across multiple diverse businesses within TD to deliver enterprise data services/capabilities and solutions on Azure. The perfect candidate will have previous public cloud experience delivering enterprise data solutions within financial services including knowledge of the security and regulatory requirements. The role will initially focus on delivering Azure SQL Database followed by adoption of additional Azure data services.\\n\\nAs a BigData/DevOps Engineer, you will work in collaboration with cloud engineering, network, security and risk management to deliver secured Azure data solutions (SQL Database, Cosmos DB, HDInsight, Databricks, Data Factory…) that meet security policies and standards within TD. You will collaborate with developers in our Azure engineering team and lines of business to implement and continuously improve the framework and tools to support self-service automation of the platform. You will develop design patterns and custom code in Jenkins, Groovy, Python and PowerShell to automate database builds, deployment, and automate testing for the platform.\\nYou will have a strong passion for writing code to automate and integrate technologies and eliminate manual steps. You will work with our business developers and internal teams to maintain a backlog of features and integration enhancements to continue to automate the platform. You will develop code using our agile JIRA tools to manage a backlog of enhancements and bug-fixes, managing source code in Stash and binaries in Nexus. You will also develop automated integration tests to run on our Jenkins CI (continuous integration) platform for test automation to help support bug free releases.\\n\\nInclusiveness\\n\\nAt TD, we are committed to fostering an inclusive, accessible environment, where all employees and customers feel valued, respected and supported. We are dedicated to building a workforce that reflects the diversity of our customers and communities in which we live in and serve, and creating an environment where every employee has the opportunity to reach their potential.\\n\\nIf you are a candidate with a disability and need an accommodation to complete the application process, email the TD Bank US Workplace Accommodations Program at USWAPTDO@td.com . Include your full name, best way to reach you, and the accommodation needed to assist you with the application process.\\n\\nEOE/Minorities/Females/Veterans/Individuals with Disabilities/Sexual Orientation/Gender Identity.\\n\\nProvince/State\\nNew York\\n\\nCity\\nNew York City\\n\\nQualifications\\n• Expertise in Azure SQL and Cosmo DB database covering\\no Deployment, configuration options\\no Understand DR/BCP and HA configuration options and how fits in overall application architecture and RTO/RPOs.\\no Security features (data protection/encryption set-up, AD authentication, access controls/RBAC, audit logging…)\\no Monitoring and performance tuning\\no Methods for data migration\\n\\n• Experience migrating on-premise databases to Azure\\n\\n• Exposure to 3rd party database configuration/migration tools like Flyway\\n\\n• Experience deploying/configuring Azure Data services: Databricks, ADLS, Blob, Data Factory, Data Lake, SQL Data Warehouse\\n\\n• 3+ years of experience developing platform orchestration code in Python and Groovy\\n\\n• 3+ years of experience writing automation pipelines in Jenkins\\n\\n• Demonstrated knowledge of cloud provisioning and administration, cloud bursting, cloud interoperability, cloud disaster recovery and business continuity strategies, as well as performance measurement and monitoring in the cloud\\n\\n• Must be a self-starter, demonstrated ability to take independent action to achieve results\\n\\n• Excellent written and verbal communication skills with the ability to communicate clearly with all levels within the team\\n\\n• Highly developed critical thinking, analytical and problem-solving skills\\n\\n• Bachelor’s degree in Computer Science or related field\\n\\n• Advance knowledge of software development lifecycle and working in an Agile Scrum team\\n\\n• Demonstrated effective leadership and analytical skill\\n\\nWork Location\\n31 West 52nd Street\\n\\nBusiness Line\\nTD Securities\\n\\nJob Category - Primary\\nSecurities / Wholesale Banking\\n\\nJob Category(s)\\nSecurities / Wholesale Banking\\n\\n**Province/State (Primary)\\nNew York\\n\\nCity (Primary)\\nNew York City\\n\\nJob Family\\nTDB\\n\\nTime Type\\nFull Time\\n\\nEmployment Type\\nRegular',\n",
       "  'Our client, the leading streaming media and content provider, is looking for a Senior Data Engineer to join their team. This team is responsible for the build and support of vital technology solutions supporting the scheduling, acquisition, processing, and distribution of content. The team partners with various business units to enable the services required to deliver premium content to domestic and global platforms as well as third party partners by bringing together platform, software, data and business intelligence services.\\n\\nThe team is looking to add a Senior Data Engineer to be part of a team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation.\\n\\nThe individual will be involved in detailed technical design, development and implementation of applications using Amazon AWS platform. Working within an agile environment, the Senior Data Engineer will provide input into architectural design decisions, develop code to meet business needs and ensure the applications we build are meeting high standards of quality and supportability. The individual will have the opportunity to develop his/her technical knowledge and skills while evolving the overall technical maturity of the team. This position will be based in Seattle.\\nPRIMARY RESPONSIBILITIES\\nPartner with architects and business leaders to design and build robust services using streaming and batch data.\\nKey contributor in building identity services that will enable us to share profiles across the organization in support of marketing and analytics.\\nWork independently and part of teams that will ingest data from a variety of source types including viewership, behavioral, attribution, content metadata etc.\\nStructure and munge ingested data in support of various use cases including analytics, marketing execution and cross divisional data sharing.\\nREQUIREMENTS\\nBachelor’s degree in Computer Science, Computer Engineering, or equivalent\\n2-3 years of experience in the AWS Cloud environment.\\n3-5 years of experience using Python and SQL.\\n4-6 years of experience working with various database methodologies such relational, columnar, NoSQL.\\nExperience with developing and maintaining production data pipelines\\nNice to have: 1 year of experience working on Snowflake Cloud Datawarehouse on AWS.\\n#GDTech',\n",
       "  'JOOR Engineering:\\nWe\\'re growing our engineering team and seeking members who want to set the tone for what technology in the wholesale industry looks like. You\\'ll tackle complex problems and build the systems that support emerging, independent businesses all the way up to global brands and retailers. Our clients rely on our products to grow their business and we are committed to building a fast and scalable product that they love, all while using best practices and the latest technologies. Our engineers are given a greenfield of opportunity to advance their skills, work collaboratively and see their solutions make an immediate impact for brands and retailers. Read the JOOR Engineering Blog to learn more.\\n\\nWho We Are:\\nAs the world\\'s industry-standard wholesale platform for fashion, beauty and home, JOOR is making wholesale smoother and smarter for brands and retailers. The JOOR platform seamlessly creates a single, collaborative ecosystem for the entire wholesale process. Brands and retailers can easily manage their selling and buying, access the vital metrics that drive their business in real time, leverage trends as they see them develop, and work together to act on emerging opportunities. We believe how people and companies accomplish goals is just as important as reaching those goals. We\\'re committed to building a diverse, inclusive team of uniquely talented people to drive the digital transformation of the industry we love.\\n\\nJOOR is headquartered in New York City and has offices in London, Los Angeles, Madrid, Melbourne, Milan, Paris, Philadelphia and Tokyo.\\n\\nWhy JOOR:\\n\\nWe welcomed an exciting new CEO and management team\\nWe are building software that is transforming an industry\\nOur industry-leading technology won an American Business (Stevie) Award\\nWe are global and rapidly growing\\nWe are one of the most diverse and inclusive tech companies\\nWe have been recognized as a break-out technology start-up driving innovation in the fashion & retail industry.\\n\\nWhat We\\'re Looking For:\\n1+ years experience as an iOS engineer\\nExperience with Objective C and Swift in a production environment\\nExperience with modern app architectures like MVVM, Viper, etc.\\nExperience with Functional Reactive frameworks like ReactiveKit, RxSwift, etc.\\nExperience in Core Data is a plus\\nBelieve in code reviews and unit tests (Quick+Nimble is a plus) for all code\\nExcited to work in an Agile, TDD environment\\nWhat We Offer:\\nAccess to Market Weeks to see the product in action\\nCollaborate with our teams in NY, LA, London, Madrid, Melbourne, Milan, Paris, Philadelphia and Tokyo\\nNeed a break? Flexible \"My Time\" policy - We want you at your best!\\nMedical, Dental, Vision, Commuter and 401k plans\\nRegular social events, including happy hours, lunch & learns, company off-sites, Meetups and speaker series\\nTransparency into the state of our business via monthly all-hands meetings, showcasing the company\\'s performance in relation to revenue and growth\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.',\n",
       "  \"Zeta Global Analytics division provides email, database and interactive marketing services worldwide to clients. The company’s services include strategic campaign planning and optimization, test design, creative development (including rich media), campaign execution, web application development, research, and advanced analytics.\\n\\nThe Analytics team is currently seeking a Data Analyst. As a Data Analyst, you will use a variety of data sources and software tools such as SAS, SQL, Excel and internal tools to develop reports and insights based on specification provided by clients or client analytical leads.\\n\\nPrimary Job Responsibilities:\\nWork with digital marketing and internal account teams to understand clients’ requests, design proper metrics reports and dashboard.\\nHandle data gathering and analysis construction with a variety of data sources and tools.\\nDevelop reports and customized analysis, data organizing and cleansing, and analysis and presentation\\nCreate easy-to-understand data visualization, presentations and provide logical insights and actionable recommendations to clients.\\nWork with BI and technical team to prioritize and identify the opportunity to automate reports.\\nStreamline the reporting process; develop repeatable and scalable process and coding across multiple systems and platforms\\nWork with various businesses and technical teams to make sure the data presented are accurate and flawless.\\nWell document the standard reporting or analysis procedures and other necessary documents.\\nJob Requirements:\\nMinimum Bachelor’s degree in Computer Programming, Statistics, Marketing Analytics or related technical/quantitative field; Master degree a plus\\n1 year of professional experience in an analytical role (prior experience in Agency/consulting or Email industry a plus), digital marketing, database marketing and web analytics a plus\\nExperience working in rational database, SQL coding, SAS or R or Python programming and extensive Excel skills\\nExperience working in BI tools, such as Tableau, Microstrategy, Jasper, etc.\\nExperience in development, presentation and/or deployment of predictive model and segmentation is strongly preferred; knowledge of Machine Learning approaches a plus\\nExperience working with 3rd party data providers such as Acxiom, IRI a plus\\nAble to take broad specifications or requests from client and ask the right “questions” to help fine tune the request to help deliver reporting needs. Be able to conduct data investigation when issue arises both independently and/or working with cross functional technical resources, if required, to solve problem\\nGood verbal and written communication skills; Attention to details, be able to identify data gaps/abnormality in output\\nAble to plan, organize, & work on multiple projects at once\\n#LI-GM1\\n\\nCompany Summary\\n\\nZeta is a data-driven marketing technology innovator whose SaaS-based marketing cloud helps 500+ Fortune 1000 and Middle Market brands acquire, retain and grow customer relationships through actionable data, advanced analytics and machine learning.\\n\\nFounded by David A. Steinberg and John Sculley (former CEO of Apple and Pepsi-Cola) in 2007, the company's highly-rated ZetaHub technology platform has been recognized in Gartner's Magic Quadrant for Digital Marketing Hubs (February 2017) and in its Magic Quadrant for Multichannel Campaign Management (April 2017), competing with offerings from Oracle, IBM, Salesforce and Adobe.\\n\\nOperating on four continents with 1,300+ employees, the company is headquartered in New York City, with Centers of Excellence in Silicon Valley, Boston, London, and Hyderabad, India.\\n\\nZeta Global is an Equal Opportunity/Affirmative Action employer and does not discriminate on the basis of race, gender, ancestry, color, religion, sex, age, marital status, sexual orientation, gender identity, national origin, medical condition, disability, veteran’s status, or any other basis protected by law.\\n\\nRecent News\\n\\nZeta Global Recognized by The Relevancy Group as Industry Leader for the 4th Consecutive Year\\n\\nZeta Global is Recognized as a Visionary by Gartner for the First Time\",\n",
       "  \"The customer journey starts with a question. And consumers expect answers. Yext puts businesses in control of their facts online with brand-verified answers in search. By serving accurate, consistent, brand-verified answers to consumer questions, Yext delivers authoritative information straight from the source — the business itself — no matter where or how customers are searching. Taco Bell, Marriott, Jaguar Land Rover, and businesses around the globe use the Yext platform to capture consumer intent and drive digital discovery, engagement, and revenue — all from a single source of truth. Yext's mission is to provide perfect answers everywhere.\\n\\nThe Yext Engineering Team drives the technology behind our revolutionary product offering! We're looking for Software Engineers to help us continue building out our product and services. Our team works in an agile environment running two-week sprints that culminate with demos of features in progress. You will work alongside engineers from the top universities and tech companies in the world, hands-on with the code from day one.\\n\\nResponsibilities\\nParticipate in full life-cycle software development\\nDesign, implement, and deploy highly scalable and reliable systems\\nBuild storage systems, libraries, and frameworks.\\nContribute ideas for new features and identify areas for improvement proactively\\nCollaborate effectively across teams, including outside of engineering\\nWrite clean, tested, and well-documented code\\nMinimum Requirements\\nBA/BS in Computer Science, a related field, or a similar college level education\\n4+ year of industry experience\\nStrong foundation in data structures, algorithms and software design\\nFluency with Java, C++, Python, or similar (we primarily code in Java)\\nOpenness to new technologies and creative solutions\\nComfortable working within a fast-paced high growth startup environment\\nCompensation, Benefits & Perks\\n\\nYext offers the following exceptional benefits: competitive compensation, 401k, unlimited snacks, daily meal allowance, flexible hours/paid time off, and excellent health/dental/vision insurance. We treat our employees well and offer tremendous growth opportunities. Challenging work pushes our people to be creative in a casual environment that is caring, fun, and collaborative. We believe that when you have smart, happy people working together you can produce something special.\\n\\nAbout\\n\\nYext has been named a Best Place to Work by Fortune and Great Place to Work®, as well as a Best Workplace for Women. Yext is headquartered in New York City with offices in Amsterdam, Berlin, Chicago, Dallas, Geneva, London, Miami, Milan, Paris, San Francisco, Shanghai, Tokyo and the Washington, D.C. area.\\n\\nYext is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ethnicity, religion, creed, national origin, ancestry, genetics, sex, pregnancy or childbirth, sexual orientation, gender (including gender identity or nonbinary or nonconformity and/or status as a trans individual), age, physical or mental disability, citizenship, marital, parental and/or familial status, past, current or prospective service in the uniformed services, or any characteristic protected under applicable law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know.\\n\\n#LI-JP1\",\n",
       "  \"Relocation to London required.\\n\\nImagine writing the code at the core of your company's success\\n\\nG-Research is a leading quantitative research and technology company. By using the latest scientific techniques, we produce world-beating predictive research and build advanced technology to analyse the world's data.\\n\\nSoftware Engineering is core to our business. By designing and implementing real-time systems, our engineers are solving some of the world's most complex financial problems.\\n\\nThe Role\\n\\nSecurity of in-house software development is central to the business' goals. Most of our systems are developed in C#, but we also utilise Java, modern web technologies, big data stacks & associated languages. Reporting into the Security Development Team Manager, the candidate will be working alongside a mix of software developers and security specialists.\\n\\nThe key responsibilities of the role are\\n\\nDeveloping & maintaining security application back & front ends, such as:\\nInformation flow visualisation\\nAsset classification + management software\\nCryptographic key management\\nSecurity risk metric + analysis\\nDeveloping security libraries and controls:\\nShared security components for use by the engineering division\\nBespoke security tools & controls for use by the security division\\nIntegrating third party security technology (e.g. authentication)\\nContributing to architecture and team outreach regarding secure development lifecycle\\nMaturing software engineering & quality practices within Information Security\\nWorking closely with other developer teams to ensure security best practice\\nTeam development challenges include solutions for code integrity, cloud enablement of high-assurance software, build chain security, dependency management, endpoint authentication & attestation, data tagging & flow control, and sandboxing. We are required to meet both high usability and high availability standards in our security technology.\\n\\nWho should apply?\\n\\nYou will be a competent and intelligent software engineer with an established skillset. In order to develop security software with good usability and availability characteristics, experience in full stack and front-end development is welcome as well as security-focused development. An interest in computer security is required; prior experience is not necessary but is advantageous. The candidate should be willing to cross-train to develop their security expertise and this role offers exceptional opportunities for skills and career development.\\n\\nWe are considering candidates with three or more years of experience up to a senior level. Any subset of the following skills is desired:\\nProven ability to develop high quality software in a statically typed object-oriented language. Refactoring skills welcome.\\nExperienced using modern development practices in a team setting\\nAgile, Continuous Delivery, TDD, BDD.\\nOpen source development experience.\\nExperience with cloud computing software stacks\\nApache Hadoop stack including Metron, Nifi, Spark\\nContainerisation & scalable deployment of software.\\nUI/Frontend development skills including an appreciation of the user experience\\nData visualisation & visualisation libraries (e.g. D3)\\nAngular, HTML/CSS, graphic design.\\nRecent work portfolios welcome.\\nAppreciation of good software architecture and knowledge of platform internals\\ng. compilers, language VMs, operating systems, assembly code.\\nGood communication skills the successful candidate will act as a conduit between the information security team and the other software teams within the business.\\nStrong academics good A-level (or equivalent) results combined with a 2.1 or better from a top university in computer science/software engineering.\\nCandidates from all commercial backgrounds are encouraged to apply.\\nWhy should you apply?\\nHighly competitive compensation plus annual discretionary bonus\\nInformal dress code and excellent work/life balance\\nComprehensive healthcare and life assurance\\n25 days holiday\\n9% contributory pension scheme\\nCycle-to-work scheme\\nSubsidised gym membership\",\n",
       "  'Overview\\n\\nData analysts (DA) are responsible for understanding and modeling data for business use. Business savvy and able to translate data into something to provide better business decisions, Data Analysts help to shape data products for our users. They contribute to product design to enhance data products and build tools.\\nDataset Analysis and Design\\nLead discovery, source system analysis, and data profiling for data products\\nCollaborate with engineering in understanding and designing requirements for dataset on-boarding\\nCommunicate with suppliers and internal teams to resolve ambiguities and ensure completeness of products\\nMaintain the highest possible quality of data products while achieving efficient throughput\\nCreate processes and architect tools to aid in the data on-boarding process\\nData Product Research\\nResearch and develop models to detect data quality issues or identify data patterns that can translate into business rules\\nEngage with partners, clients, and industry leaders to understand how data is used in their firms and transfer that knowledge internally\\nResearch and design data standards with partners and industry leaders\\nEngage with suppliers and the internal supplier management team to understand industry practices\\nAbout You\\nYou have a keen understanding of financial data including how it is used in a wide variety of roles throughout the financial industry\\nYou have an appreciation for the nuances of data and knowledge of common errors\\nYou love working on high-performing teams, collaborating with team members, and improving our ability to deliver delightful experiences to our clients\\nYou are excited by the opportunity to solve challenging data related problems and find the technology behind it to be fascinating\\nYou are an expert at dealing with ambiguity\\nYou are extremely attentive to detail\\nYou are a self starter and can excel with minimal direction\\nYou are a superb communicator and can clearly communicate complex concepts to others\\nQualifications\\n4+years of experience in a research function, ideally working with financial data\\nSpecific experience in a research role working with structured and unstructured data, APIs, and web scraping.\\nExperience modeling and validating financial data for a variety of uses within the financial industry\\nExperience with Python\\nHistory of outstanding organizational skills and ability to build scalable process\\nUnderstanding of statistics (e.g. hypothesis testing, regressions)\\nAbout Crux:\\nCrux is a data delivery and operations company that takes on the critical, yet commoditized, tasks of ingesting data and getting it ready for analysis by financial institutions. We are experiencing rapid growth from financial firms turning to Crux to process and onboard the data they need.\\n\\nCrux is transforming how the financial services industry works with data. For years, data discovery, operations, and delivery have been expensive, time consuming, and frustrating for financial firms. We are solving this challenge by creating a new technology solution: the go-to platform for data delivery. Crux is in high demand and we have a clear path to success.\\n\\nFounded in 2017 by a seasoned team of data professionals, Crux has grown to a team of 70 employees in San Francisco and New York. It is backed by leading industry players, including Citi, Goldman Sachs, and Two Sigma.\\n\\nAt Crux, diversity is valued and and treatment of employees and applicants are based on merit, talent and qualification. We encourage people from underrepresented groups to apply. We believe the key to success is bringing together unique perspectives and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. For qualified applicants with criminal histories, consideration will be consistent with the requirements of the San Francisco Fair Chance Ordinance. All your information will be kept confidential according to EEO guidelines.',\n",
       "  '</h4>\\n\\nDo you crave a collaborative organization where your contributions will make a strong impact?\\n\\nDo you want to develop products in the latest cloud-based technologies building ecosystems rather than creating client-facing slides?\\n\\nAre you ready to roll up your sleeves and embrace a work culture that’s insanely passionate and committed to bringing the latest advanced analytics to life?\\n\\nThe Data Science & AI group at PA Consulting is your dream community. As part of the fastest growing innovation practice within PA Consulting, you will work with the latest advanced analytics, machine learning, and big data technologies to generate actionable insights from data and develop innovative data products. We focus on Life Science, Healthcare, Energy & Utilities and CPG sectors and work with various data sets, from social media to public health data. Our domain focus is broad and covers everything from computer vision and NLP, recommender engines, classification and clustering algorithms, linear programming and optimization.\\n\\nJob Requirements\\n\\n\\n• 2-5 years professional experience as a data scientist, software engineer or statistical modeler\\n• Master’s degree from top tier university in Computer Science, Statistics, Economics, Physics, Engineering, Mathematics, etc.\\n• Expertise in machine learning algorithms and methods\\n• Strong understanding and application of statistical methods\\n• Experience writing production level code in one of the following: Python, Java, C++, C\\nPreferred:\\n• Experience working with database systems (e.g. SQL, NoSQL, MongoDB, Postgres, ect.)\\n• Experience working with big data distributed programming languages, and ecosystems (e.g. S3, EC2, Hadoop/MapReduce, Pig, Hive, Spark, etc)\\n• Experience building scalable data pipelines and with data engineering/ feature engineering.\\n• Webscraping leveraging Beautifulsoup, Selenium, Scrapy, etc\\n• Experience with front end (UI), HTML5, JavaScript, CSS, R Shiny, Tableau\\n• Experience leveraging ML techniques to build recommender systems, NLP engines, computer vision algorithms, etc..\\n\\nAbout PA Consulting Group\\n\\n\\nBringing Ingenuity to Life: We’re an innovation and transformation consultancy that believes in the power of ingenuity to build a positive-human future in a technology-driven world. Our diverse teams of experts combine innovative thinking with breakthrough-technologies to progress further, faster.\\n\\nWith a global network of FTSE 100 and Fortune 500 clients, we’ll offer you unrivalled opportunities for growth and the freedom to excel. Combining strategies, technologies and innovation, we turn complexity to opportunity and deliver enduring results, enabling you to build a lasting career.\\n\\nIsn’t it time you joined us?\\n\\nWe are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. VEVRAA Federal Contractor',\n",
       "  'AnyVision Research is a world leading team of scientists and engineers that creates AI for the Real World. With deployments and clients spanning the globe, we are recognized as leaders in the fields of Face recognition, Object recognition, Object detection, and Visual Target Tracking.\\nAnyVision provides a diverse and flexible work environment so that we can recruit and retain the best talent. Working for AnyVision you will:\\nBe a part of a diverse, multinational and inclusive team\\nReceive challenging work and assignments\\nReceive world class training and experience on cutting edge systems and complex projects\\nBe supported by a flexible and supportive work environment\\nAs we grow in scope and ambition into 2020, we are looking for highly motivated C++ Computer Vision engineers to join our Research Engineering team.\\nYou are:\\nA talented and motivated engineer with a strong interest in development and building software systems. You are also passionate about the impact that AI is having in the world and you want to be part of it. You have completed a degree in a scientific or engineering discipline. You can adapt to new environments and solve new problems quickly. You are adept at producing innovative solutions based on real-world product requirements. You have strong communication and teamwork skills.\\nYou will:\\nAs a member of the Research Engineering team you will work with the Research team to develop software that ensures robust and reliable execution of our cutting-edge AI portfolio. This includes algorithm implementation, hardware optimization and proof of concept prototyping. Working at AnyVision offers the opportunity to help deliver world leading AI technology. We offer unique opportunities and challenges to learn new technologies and be at the very bleeding edge of the computer vision and machine/deep learning industry. Within the research engineering team, projects include:\\nDeveloping and optimizing machine/deep learning and computer vision algorithms\\nProducing reference implementations of state-of-the-art artificial intelligence algorithms that have been developed by our machine/deep learning researchers\\nIntegrating deep learning components into robust pipelines\\nCreating tools and tool sets (custom kernels, APIs, test systems/suites, etc.) for state-of-the-art hardware\\nResearch and development into emerging heterogeneous languages (Halide, TVM, OpenCL)\\nPrototyping new hardware specific deep learning inference engines and pipelines (e.g. Qualcomm SNPE, ARM NN)\\n\\n\\nRequirements:\\nRequired Skills/Experience:\\nA bachelors degree or higher in a relevant engineering or mathematical field\\nStrong knowledge and experience of C++11 onwards\\nExperience developing robust, maintainable, efficient and testable code for secure and reliable systems\\nMinimisation of memory footprints, data movements, allocations and de-allocations\\nDevelopment of fast, scalable and correct concurrent programs\\nCross-platform (Linux, Android and/or Windows) development\\nProfiling and optimisation of performance and memory system usage\\nExperience with software development practices e.g. debug tools, agile, design patterns\\nExperience with source control and collaborative development tools\\nExcellent communication and teamwork skills\\nMinimum 1 year industry experience\\nDesirable Skills/Experience:\\nA PhD in a relevant engineering or mathematical field\\nComputer vision experience\\nMachine learning frameworks (TensorFlow, PyTorch, Caffe, Scikit-learn)\\nOpenCL/CUDA/C++AMP/other GPGPU frameworks\\nExperience developing for hardware devices such as DSPs, FPGAs, embedded Linux\\nClang/LLVM compiler projects\\nKnowledge/experience of domain specific languages (e.g. Halide/TVM)\\nOpenVX\\nBenefits:\\nHighly competitive salary and regular salary reviews\\nTechnical training\\nContributory pension scheme\\nPrivate healthcare\\nGym membership\\nFlexible working\\nRegular company social events and activities',\n",
       "  \"About US\\n\\nERGO is a marketing technology company that drives the relevance of what’s inside an email with Smart Content. The best way to understand what we do is to imagine that you’re reading your favorite magazine, and with every page you turn, the magazine learns to anticipate your needs and interests and adapts with relevant content.\\n\\nOur platform leverages big data to drive our Smart Content modules in digital channels. We use our technology platform and professional-services offering to help grow Fortune 1000 brands.\\n\\nPlease apply with your resume and a thoughtful cover letter telling us why you're the best fit for the job.\\n\\nAbout the Role\\n\\nThe Data Engineer is responsible for developing, maintaining, testing and evaluating data solutions systems in order to load, transform and query large data sets from a variety of sources.\\nDesign, develop and optimize database queries\\nWork with our Data Scientists to create data-driven insights and reports for senior management\\nAssist Data Scientists in developing processes to migrate data from various formats and data sources (Oracle, MySQL, Sybase, flat files, etc.) to database architecture\\nDesign and implement tools to analyze very large data set of raw data\\nProcess unstructured data into a form suitable for analysis\\nPerform ad hoc data updates and other follow-on data services\\nAutomate recurring analytics reports for clients\\nWhat the Role Requires\\nBachelor’s degree or higher in Computer Science, Information Systems or related field\\nHighly proficient in SQL (MySQL, Postgres)\\nExpertise with relational databases (implementation, queries, modeling)\\nWorking knowledge of distributed data stores (Cassandra, Redshift, Hadoop, HBase)\\nProficient in scripting language of choice (Python, R, PHP, Ruby)\\nExpertise with optimizing query performance\\nFamiliarity with NoSQL technologies (Mongo DB, DynamoDB)\\nDeep understanding of data structures and schema design\\nDetail-oriented, proactive problem solving skills\\n\\n\\nPerks\\nChoice of Blink Gym or Citibike membership\\nBagel Mondays\\nWorking From Home Fridays\\nTotal of 23 PTO package that includes things like: company shut down between Christmas and New Years, three 4-day weekends on the summer holidays, and a 4-day Thanksgiving break. This also includes 10 PTO days to use at your convenience.\\nOffice bar cart, daily produce, healthy and unhealthy snacks, Nespresso\\nPool table\\nHealth, vision & dental benefits\\n401k plan with matching\\nPet-friendly office\\nReferral bonus program\\nA comprehensive career development program and training\\nFun company outings and events\\nVery cool industrial-style West Village studio space\\nThe best perk? Our people. If you’re ready to join us, apply now!\",\n",
       "  \"Software Guidance & Assistance, Inc., (SGA), is searching for a Data Engineer for a CONTRACT assignment with one of our premier Financial clients in New York, NY.\\n\\nResponsibilities :\\nWork closely with Data Owners, Business and Big Data Project team to work out data sourcing requirements\\nData Discovery and understanding in Source Systems\\nCapture meta data and map data elements to business data requirements\\nData profiling\\nPrepare technical connectivity (to Source Systems) specifications for development team\\nSpecification and implementation of Data Quality Checks and Requirements\\nServe as an Data SME during UAT testing and discussions with IT stakeholders\\nRequired Skills:\\nTechnology background in Databases\\nStrong Motivation to communicate with Source System Owners\\nAbility to write sophisticated SQL queries\\nStrong data analytical skills, experience with data analytical tools\\nStrong communication and presentation skills\\nExperience in DWH strongly suggested\\nUnderstanding of Agile requirements flow\\nExperience in Agile tools (Confluence, Jira etc)\\nControl-M, rundeck\\nPreferred Skills:\\n-Background in Big Data a plus\\nSGA is a Certified Women's Business Enterprise (WBE) celebrating over thirty years of service to our national client base for both permanent placement and consulting opportunities. For consulting positions, we offer a variety of benefit options including but not limited to health & dental insurance, paid vacation, timely payment via direct deposit. SGA accepts transfers of H1 sponsorship for most contracting roles. We are unable to sponsor for Right-to-Hire, Fulltime, or Government roles. All parties authorized to work in the US are encouraged to apply for all roles. Only those authorized to work for government entities will be considered for government roles. Please inquire about our referral program if you would like to submit a candidate for any of our open or future job opportunities. SGA is an EEO employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, national origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status. To view all of our available job postings and/or to learn more about SGA please visit us online at www.sgainc.com. - provided by Dice\",\n",
       "  'Work within a Big data & cloud consulting delivery technical team to drive client’s enterprise technology transformation on cloud, Big Data solutions platform architecture & engineering, application migration to cloud, cloud business solutions, ML & AI platforms in cloud.\\nQualifications for Cloud Data Engineer\\nAdvanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.\\nExperience building and optimizing ‘big data’ data pipelines, architectures and data sets in cloud environments.\\nExperience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.\\nStrong analytic skills related to working with unstructured datasets.\\nBuild processes supporting data transformation, data structures, metadata, dependency and workload management.\\nSuccessful history of manipulating, processing & extracting value from disconnected datasets.\\nWorking knowledge of message queuing, stream processing, & highly scalable ‘big data’ lakes.\\nStrong project management and organizational skills.\\nExperience supporting and working with cross-functional teams in a dynamic environment.\\nWe are looking for a candidate with 4+ years of experience in a Data Engineer role, who has attained a Graduate degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. They should also have experience using the following software/tools:\\nBig data tools: Hadoop, Spark, Kafka, etc.\\nOne or more with Data Warehouse tools – Snowflake, Redshift, Bigquery etc.\\nSQL & NoSQL databases– RDS, DynamoDB, Mongo etc.\\nData pipeline, Data Integration and workflow management tools, such as Alteryx, Snaplogic etc.\\nOne or more public cloud service providers AWS/Azure/ GCP cloud services\\nStream-processing systems: Storm, Spark-Streaming, etc.\\nObject-oriented/object scripting languages: Python, Java, C++, Scala, etc.\\nInfra as Code: AWS Cloud-formation, Terraform, Azure resource manager, etc.\\nExperience in designing and engineering cloud infra, security & connectivity controls for deploying and managing data systems in cloud.\\nPreferred AWS, Azure or GCP certified architect\\nAbout USEReady:\\nUSEReady Inc. was founded in 2011 and our mission is to help organizations succeed by fast tracking their business performance. We relentlessly strive towards value-driven innovation using advanced BI, data management, and infrastructure management services.\\nWith offices in New York, New Jersey and Bangalore, our global delivery service is enriched with winning strategies, technology expertise and customer fanaticism. We have a proven track record of success in various domains such as capital markets, insurance, healthcare, pharma, retail and media.\\nRanked #113 in Inc. 5000\\nRed Herring Top 100 North America Winner\\nTableau Gold Partner for 5 consecutive years\\nTableau Services Partner of the Year 2015\\nTableau Services & Training and Alliance partner nominee of the year 2016\\nAlteryx, Collibra, Snowflake, AWS & Microsoft Partner\\nKey partners for Informatica\\n2017 Tableau Software Partner Award Winners\\nMarketing Innovation Award – Tableau 2018',\n",
       "  \"Roivant is a global biopharma company that aims to improve health by rapidly delivering innovative medicines and technologies to patients. We do this by building Vants – nimble, entrepreneurial biotech and healthcare technology companies with a unique approach to sourcing talent, aligning incentives, and deploying technology. At Roivant, we will give you the resources and freedom to tackle some of the most important and challenging problems in healthcare. If you find that exciting, we want to work with you.\\n\\nWe are seeking a thoughtful, hands-on Data Engineer to join Roivant's Data Architecture team. Data Architecture is a broad team that develops and operates the data platform used by developers throughout the Roivant family. The team implements all data ingestion, storage, and analytics tooling and provides other ad hoc database needs.\\n\\nKey Responsibilities:\\nYou will join a small team partnering with product owners and developers at Roivant and Vants to provide end-to-end data solutions for technology tools and products.\\nYou will follow best coding practices to build and optimize tools for data ingestion and storage, including components of Roivant's Data Lake/Warehouse platform.\\nYou will automate and maintain data processing pipelines, implement modern ETL infrastructure, and continuously improve the efficiency of our platform.\\nYou will serve as a subject matter expert on big data analytics projects that provide insights for business and technical stakeholders.\\nRequirements:\\nBA/BS degree with strong academic performance, preferably in a quantitative field\\n4+ years experience with Python, database development, Git, Linux and AWS (S3, EC2, SNS, Lambda, SQS)\\nExperience with Spark, terraform, Docker, big data and/or healthcare data preferred\\nKnowledge of Scrum and desire to work in an incredibly fast-moving, agile environment\\nTeam player with strong communication skills and the ability to work with minimal supervision\\nQuick and scrappy learner who adapts well to a fast-moving environment and gets things done; experience in high-growth or startup environments a plus\",\n",
       "  'Job Title: AWS Data Engineer\\n\\nDepartment: Cloud Data\\n\\nJob Type: Long term\\n\\nLocation: New York, NY\\n\\nJob Overview:\\n\\nAs an AWS Data Engineer you will participate in all aspects of the software development lifecycle which includes estimating, technical design, implementation, documentation, testing, deployment and support of application developed for our clients. As a member working in a team environment you will work with solution architects and developers on interpretation/translation of wireframes and creative designs into functional requirements, and subsequently into technical design.\\n\\nResponsibilities:\\nCreate data pipelines for a state-of-the-art analytics application in the financial industry\\nCode data processing jobs in AWS Glue, Python, and Spark.\\nWork with stakeholders to identify and document requirements.\\nConfigure and schedule data pipelines\\nTranslate business requirements to technical specifications and coded data pipelines.\\nTroubleshoot data pipelines\\nSKILLS AND EXPERICENCE REQUIRED:\\nPassionate coders with 3-5 years of application development experience.\\nProficiency with AWS Glue, Spark with Python is a must.\\nExpert knowledge of developing in AWS environments\\nKnowledge of data formats and ETL and ELT processes in a Hadoop environment including Hive, Parquet, MapReduce, YARN, HBase and other NoSQL databases.\\nExperience in dealing with structured, semi-structured and unstructured data in batch and real-time environments.\\nExperience with working in AWS environments including EC2, S3, Lambda, RDS, etc. Familiarity with DevOps and CI/CD as well as Agile tools and processes including Git, Jenkins, Jira and Confluence.\\nClient facing or consulting experience highly preferred.\\nSkilled problem solvers with the desire and proven ability to create innovative solutions.\\nFlexible and adaptable attitude disciplined to manage multiple responsibilities and adjust to varied environments.\\nFuture technology leaders- dynamic individuals energized by fast paced personal and professional growth.\\nPhenomenal communicators who can explain and present concepts to technical and non-technical audiences alike, including high level decision makers.\\nSolid foundation in Computer Science, with strong competencies in data structures, algorithms and software design.\\nKnowledge and experience in developing software using agile methodologies.\\nProficient in authoring, editing and presenting technical documents.\\nAbility to communicate effectively via multiple channels (verbal, written, etc.) with technical and non-technical staff.\\nEducational Qualifications:\\nRequired - Bachelor s degree in Computer Science, Information Technology, Computer Engineering or closely related or equivalent\\nPreferred - Master s degree in Management Information Systems (MIS), Computer Science, Big Data or Analytics or equivalent\\nTravel:\\nOpen to travel based up on the nature of the engagement\\nEqual Employment Opportunity\\n\\nReliable Software employment does not discriminate on the basis of race, religion, gender, sexual orientation, age or any other basis as covered by federal, state, or local law.\\n\\nEmployment decisions are based solely on qualifications, merit and business needs.\\n\\nShameer K\\n\\nIT Recruiter\\n\\n(O) 248-814-2363\\n\\n2260 Haggerty Road, Suite#285, Northville, MI 48167.\\n\\n- provided by Dice',\n",
       "  \"Relocation to London is required.\\n\\nG-Research is a leading quantitative research and technology company. Using the latest scientific techniques and advanced analysis methods, we find patterns in large, noisy data to produce world-beating predictive research.\\n\\nThe Role\\n\\n- 10 week summer programme (July to September 2020)\\n09:00-17:30 working hours\\nBased in Central London\\n\\nOur business is to predict the future of financial markets, applying scientific techniques to find patterns in large, noisy, and rapidly changing datasets. Our mission as a team is to help discover, enrich and analyse data sources that will drive tomorrow's research initiatives.\\n\\nIn this role, you will apply your knowledge to support the exploration, enrichment and even creation of new datasets for research. You will use your knowledge of data blending, statistical analysis, and machine learning methods to help scale and automate the way we analyse, validate and visualise diverse data sources; and you will help build tooling to enable data science initiatives across the company.\\n\\nThis is a team at the forefront of the company's data strategy, responsible for finding solutions to some of the many important data challenges within the firm, meaning you will have a unique opportunity to make an impact from the beginning. The successful candidate will be comfortable working within a multi-disciplinary team, collaborating with industry leading data scientists and financial experts, developing solutions that adapt to a rapidly changing data landscape.\\n\\nThe role will offer exposure to cutting-edge technologies in a high growth industry, with opportunities to learn about multi-asset class systematic investing and big data development in an innovative and forward-thinking firm.\\n\\nWho are we looking for?\\n\\nThe ideal candidate will, at minimum, have experience in most of the following areas:\\nA strong undergraduate or Masters student in a numerical subject (Machine Learning, NLP, Mathematics, Data Science, Computer Science, Statistics, Physics, Engineering, etc.)\\nAdvanced knowledge of Python and PySpark, in particular packages such as Pandas, Numpy, SciPy, Matplotlib (or equivalent)\\nPractical understanding of SQL and NoSQL databases\\nExperience selecting, developing and refining machine learning models\\nKnowledge and/or interest in Natural Language Processing\\nDemonstrable proficiency in statistical data analysis and data visualisation\\nConfidence to build relationships with both internal teams and external vendors, and ability to communicate effectively with both technical and non-technical audiences\\nPrevious financial experience is not required, although an interest in financial markets and securities is desirable\\nSelf-starting positive attitude, ability to thrive in an autonomous environment and willingness to learn\\nWe also like to see active Git Hub/Kaggle profiles (but these aren't a prerequisite)\\nPrevious experience in finance is not required, although an interest in finance and the motivation to rapidly learn more is a prerequisite for working here.\\n\\nWhy should you apply?\\nHighly competitive compensation plus accommodation allowance\\nActive G-Research community with weekly intern activities\\nCompany-wide summer party and weekend away\\nInformal dress code and excellent work/life balance\\nCentral London office close to 5 stations and 6 tube lines\\n*** Want to meet our quant team?***\\n\\nWe are running interactive presentations with members of our quant team in the coming weeks. To register your interest in attending please email your name and your preferred location to quant.events@gresearch.co.uk:\\n\\nUSA\\n\\nNew York, Friday 20th September\\n\\nBoston, Monday 23rd September\\n\\nUK\\n\\nCambridge, Thursday 26th September\\n\\nOxford - TBC\",\n",
       "  'Two Sigma is a different kind of investment manager. Since 2001, we have used data science and technology to derive insights to forecast the future and discover value in markets worldwide. Our team of scientists, technologists and academics looks beyond traditional finance to understand the bigger picture and to develop creative solutions to some of the world’s most difficult economic problems. Our work spans markets and industries, from insurance and securities to private investments and new ventures.\\n\\nThe Strategic Data Science team’s mission is to unlock new high-potential revenue streams by harnessing Two Sigma’s data, technology and modeling capability into a scalable and portable prediction engine and monetizing this engine into new investment products or non-investment business lines.\\n\\nIn particular, the Strategic Data Science team is working on building a systematic and data-driven private investment focused modeling environment, with application to investment products across the Two Sigma businesses. The team’s ambition is to leverage Two Sigma’s accumulated know-how, proprietary data-science platform and scientific investment approach in highly fragmented, illiquid and unstructured markets. By leveraging these tools, we can develop a meaningfully differentiating investment process in an industry dominated by institutions relying on local knowledge, personal networks and incomplete information.\\n\\nThe Strategic Data Science team is seeking a data scientist to join our growing team and contribute to building the initial data and modeling platform used to guide our team’s investment process and asset selection. With statistics, economics, and computation at the heart of all our work, we will be most successful when data science is used to empower our team to make data-driven, evidence-based investment decisions that scale quickly. Our mission is to give our team a significant competitive edge in this alpha-phase of the initiative, while also building a platform that scales and provides an enduring advantage: the world’s first data-driven private asset prediction engine.\\n\\nYou will take on the following responsibilities:\\nIndependently generate and articulate hypotheses on what may affect private markets, asset valuations, and private deal processes\\nLeverage existing data sets, as well as identify and trial new data sets, in order to interrogate yours — and the team’s — hypotheses\\nPresent your work internally to the working teams, other modeling teams, members of Two Sigma’s leadership team and various Two Sigma businesses\\nSpend time with private market partners to better understand how it works, how software is used, and what data is critical to industry practitioners daily workflows\\nDevelop a deep understanding of private markets\\nYou should possess the following qualifications:\\nStrong independent development of predictive models in common open-source statistical programming languages\\nExperience working with high-dimensional and sparse datasets, as well as recognition of when and how to combine such datasets to enhance their value\\nFamiliarity with time-series and spatial analysis methods and software, particularly mapping tools\\nDesire to work in a dynamic and evolving research and development environment, where you may be working on several parallel research tracks and will be expected to switch context frequently\\nStrong communication skills, with an emphasis on the ability to understand your audience and flexibility to communicate complex topics to non-experts\\nTwo Sigma employees enjoy the following benefits:\\nCore Benefits: Fully paid medical and dental insurance premiums for employees and dependents, competitive 401k match, employer-paid life & disability insurance\\nPerks: Onsite gyms with laundry service, wellness activities, casual dress, snacks, game rooms\\nLearning: Tuition reimbursement, conference and training sponsorship\\nTime Off: Generous vacation and unlimited sick days, competitive paid caregiver leaves\\nWe are proud to be an equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics',\n",
       "  \"Become a Part of the NIKE, Inc. Team\\n\\nNIKE, Inc. does more than outfit the world’s best athletes. It is a place to explore potential, obliterate boundaries and push out the edges of what can be. The company looks for people who can grow, think, dream and create. Its culture thrives by embracing diversity and rewarding imagination. The brand seeks achievers, leaders and visionaries. At NIKE, Inc. it’s about each person bringing skills and passion to a challenging and constantly evolving game.\\n\\nNIKE, Inc.'s storytellers, Marketing and Communication sets the brand tone. A creative force of specialists tell Nike’s stories of innovation and sport through advertising, brand strategy, digital engagement and product presentation. Using channels ranging from retail stores to social media, Marketing & Communication teams connect the science and art of Nike innovations to the hearts and minds of athletes around the world.\\nJoining us in\\nNike’s Digital Innovation Studio in New York City, the Associate Data\\nScientist, SNKRS App will embed themselves within Nike Digital’s SNKRS digital\\nproduct management team serving as a data and analytics center of excellence.\\nWith our strategic focus on the evolution of the digital launch business and\\nstrengthening the consumer’s connection within the Nike digital ecosystem, this\\nposition plays a key role in driving sneaker culture forward for both Nike and\\nthe Jordan Brand.\\n\\nWe are looking\\nfor scientists with tech industry experience to help build our core data products\\nand machine learning and statistical modeling processes. We want someone who\\nloves modeling and writing code, someone who likes to dive in, solve problems,\\nand create premier experiences for Nike consumers.\\nKey responsibilities:\\nUse contemporary data\\nscience and analytics methods and tools to derive actionable business insights\\nMaster big data\\nfundamentals: gather, clean, and validate large amounts of data on Nike’s cloud\\nplatform\\nCreate foundational\\npipelines and repeatable production queries in our data warehouse to analyze\\nlarge sets of internal data on billions of consumer interactions\\nDevelop processes and\\ncontinuous improvement of core projects through automation and process\\nenhancement\\nCreate reports and\\nvisualizations of core app features and marketing campaigns to help communicate\\ntechnical results and concepts to a non-technical audience\\nProvide ongoing insights\\nand analysis on consumer engagement, experience, and usage of the SNKRS\\nplatform\\nContribute to larger\\nconsumer statistical modeling and machine learning projects\\nManage multiple concurrent projects in\\nfast-paced environment\\nBA/BS degree required; a\\nfocus in statistics, econometrics, applied math, or a quantitative behavioral\\nsciences field strongly preferred\\n2+ years’ experience\\ndoing quantitative analysis on digital products or digital marketing\\nexperiences is a plus\\nRequired Skills\\nAbility to work in\\nmultiple data environments and perform complex joins, data cleaning, and\\nfeature creation using SQL or SQL based languages (Hive) is required\\nExperience with\\nstatistical software, e.g., R or Python\\nExperience working with\\nreal-world, structured and unstructured data\\nExperience with data\\nvisualization, e.g., Tableau\\nExperience analyzing and\\nmanipulating data sets to understand patterns and provide insights\\nExcellent communications\\nskills, in particular, the ability to communicate technical content to general\\naudience\\nCore mathematical\\nability to understand and apply state-of-the-art machine learning algorithms\\nand/or statistical modeling\\nPreferred Skills\\nExperience collecting /\\ncreating data when the perfect data don’t exist\\nConsumer experience with\\nNike digital apps (NTC, NRC, SNKRS, Nike App)\\nKnowledge\\nof Nike’s physical merchandise and company history\\nPassion\\nfor Nike sneakers and sneaker culture\\n\\nNIKE, Inc. is a growth company that looks for team members to grow with it. Nike offers a generous total rewards package, casual work environment, a diverse and inclusive culture, and an electric atmosphere for professional development. No matter the location, or the role, every Nike employee shares one galvanizing mission: To bring inspiration and innovation to every athlete* in the world.\\n\\nNIKE, Inc. is committed to employing a diverse workforce. Qualified applicants will receive consideration without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, gender expression, veteran status, or disability.\",\n",
       "  'Customer Operations Data Scientists formulate the analytical frameworks and provide the technical rigor to measure the efficacy of, and shape the future for, Squarespace’s award-winning Customer Operations team. In close partnership with management, you’ll identify the fundamental questions we need to ask and have ownership over the data-driven means of addressing them.\\n\\nDepending on your area of focus, challenges you may tackle in a typical day include:\\nAnalyzing how and why customers request support and partnering with stakeholders on initiatives related to ticket deflection\\nTesting and measuring the effectiveness of new support offerings\\nBuilding dashboards to allow management to understand operational efficiency and performance in real time\\nBridging customer interactions with business outcomes and product usage\\nIdentifying and analyzing performance and efficiency opportunities with Customer Operations leadership\\nUnderstanding the drivers of ticket volume and forecasting demand for customer service\\nOptimizing ticket queueing\\nSegmenting customers to predict the most effective communication or remediation method\\nWe are seeking a Data Scientist who demonstrates sharp intellectual curiosity, data fluency, and a collaborative work ethic. This role reports to the Director of Analytics in New York.\\n\\nRESPONSIBILITIES\\nCollaborate with business partners and manage stakeholders\\nEmpower data-driven decision-making across the org by defining analytical frameworks to guide key business decisions; develop and maintain dashboards to provide visibility into relevant metrics\\nSurface insights from terabytes of proprietary user, product, and marketing data; communicate compelling recommendations to key stakeholders\\nFormulate innovative hypotheses; design and implement rigorous methodologies for testing them\\nUtilize cutting-edge statistical/machine learning techniques to build models furthering the org’s understanding of and ability to predict consumer behavior\\nContinuously get better - seek out opportunities to further develop your analytical, engineering, statistical, etc. toolkit\\nQUALIFICATIONS\\n2+ years of relevant analytical experience working with data or MS in a relevant technical field\\nStrong competency with SQL and a scripting language (Python preferred)\\nAbility to communicate effectively with a wide-range of audiences\\nComfort working on open-ended problems with limited guidance\\nStrong data intuition and ability to apply statistical concepts to quick moving initiatives\\nAbility to devise data-driven solutions to business problems\\nABOUT SQUARESPACE\\n\\nSquarespace makes beautiful products to help people with creative ideas succeed. By blending elegant design and sophisticated engineering, we empower millions of people — from individuals and local artists to entrepreneurs shaping the world’s most iconic businesses — to share their stories with the world. Squarespace’s team of more than 950 is headquartered in downtown New York City, with offices in Dublin and Portland. For more information, visit www.squarespace.com/about.\\n\\nPERKS\\nHealth insurance with 100% premium covered\\nFlexible vacation & paid time off\\nEquity plan\\n401(k) plan with employer match\\nFree lunch and snacks\\nDog-friendly workplace\\nAnd more\\nToday, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.',\n",
       "  'Instagram is a global community of more than 1 billion, which means jobs here offer countless ways to make an impact in a fast growing organization. Instagram was built to connect people to the people and interests they love. Our app has played a critical part in forming meaningful communities where people can connect with each other and share what matters most to them.The mission of the Instagram Machine Learning (IGML) team is to power smarter products for Instagram through the use of machine learning. As a Data Scientist in IGML, you will work cross-functionally with teams across Instagram to power and improve all of their machine learning models, by focusing on ranking, retrieval, and even resource efficiency. Youll also be working and with some of the world leading experts in Machine learning!\\n\\nResponsibilities:\\n\\nDesign and implement data-driven systems to increase the value of modeling, labeling, ranking and efficiency for all product teams in Instagram that leverage machine learning.\\nDevelop a framework to standardize feature adoption and feature deprecation across all engineering teams in Instagram.\\nCreate forecasts and models for supply and demand to support our quarterly and annual capacity planning needs.\\nUnderstand how feature values change over time, either due to user behavior or data quality issues, and how that impacts all IGML models.\\nMininum Qualifications:\\n\\nMS or PhD Degree in Machine Learning.\\n5+ years of experience working in an analytics organization.\\nExperience working independently and as a member of a cross-functional team.\\nExperience in querying and manipulating raw datasets for analysis.\\nSQL experience.\\nExperience with visualizations, dashboards, and reports.\\nExperience with advanced data modeling, machine learning algorithms, and data science techniques, including the majority of the following topics: decision trees, cross validation, model evaluation, forecasting, classification, clustering, linear regression and GLM, and other similar topics.\\nCommunications experience and experience explaining technical concepts and analysis implications to varied audiences and be able to translate business objectives into actionable analyses.\\nPreferred Qualifications:\\n\\nExperience with Hive, ETL, R, and Python preferred.',\n",
       "  \"Introduction\\nAt IBM we have an amazing opportunity to transform the world with cognitive technology. By using the vast amounts of information available today to identify new patterns and make new discoveries, we are helping cities become smarter, hospitals transform patient care, financial institutions minimize risk, and pharmaceuticals find cures for rare diseases.\\n\\nData scientists work with enterprise leaders and key decision makers to solve business problems by preparing, analyzing, and understanding data to deliver insight, predict emerging trends, and provide recommendations to optimize results. Data scientists use a variety of data (structured, unstructured, IoT streaming), analytics, AI tools, and programming languages often using a cloud infrastructure to handle the volume and veracity of data streams.\\n\\nArmed with data, modeling expertise, and analytic results, the data scientist communicates conclusions and recommendations to stakeholders in an organization's leadership structure. Business acumen is an important skill for data scientists to effectively communicate their findings to business leaders, data scientists need strong consulting, communication, visualization, and storytelling skills.\\n\\nYour Role and Responsibilities\\nSTART DATES FOR THIS POSITION ARE IN 2020\\n\\nData Scientists are in demand across IBM's growth areas. If hired, you will be matched to a team based on business demand, location and fit. Join the forward-thinking teams at IBM solving some of the worlds most complex problems there is no better place to grow your career!\\n\\nWhat Youll Do as an Entry-Level Data Scientist:\\nYou will implement and validate predictive models as well as create and maintain statistical models with a focus on big data.\\nYou will be exposed to and incorporate a variety of statistical and machine learning techniques such as logistic regression, experimental design, generalized linear models, mixed modeling, CHAID/decision trees, neural networks and ensemble models.\\nYoull communicate with internal and external clients to understand business needs and provide analytical solutions.\\nYou will use leading edge tools such as COGNOS, Watson Studio and Watson Machine Learning.\\nYoull work in an Agile, collaborative environment, partnering with other scientists, engineers, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\\nWho You Are:\\nYou are great at solving problems, debugging, troubleshooting, and designing & implementing solutions to complex technical issues.\\nYou thrive on teamwork and have excellent verbal and written communication skills.\\nYou have strong technical and analytical abilities, a knack for driving impact and growth, and some experience with programming/scripting in a language such as Java or Python.\\nYou have a basic understanding of statistical programming in a language such as R, SAS, or Python.\\nYou have an interest in, understanding of, or experience with Design Thinking and Agile Development Methodologies\\nRequired Professional and Technical Expertise\\nBasic understanding of statistical programming in a language such as R, SAS, or Python.\\nExperience with programming/scripting in a language such as Java or Python.\\nKnowledge of statistical concepts such as regression, time series, mixed model, Bayesian, clustering, etc., to analyze data and provide insights.\\nPreferred Professional and Technical Expertise\\nAdvanced knowledge of statistical concepts such as regression, time series, mixed model, Bayesian methods, clustering, etc., to analyze data and provide insights.\\nAbout Business Unit\\nNo matter where you work in IBM, you are making an impact. As an Early Professional with IBM, you will be taking on a key role with one of our industry-leading business units to work on the technology that is solving our most challenging problems and changing the way the world thinks.\\n\\nYour Life @ IBM\\nWhat matters to you when youre looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBMs greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nWe consider qualified applicants with criminal histories, consistent with applicable law.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.\",\n",
       "  \"At the NBA, we're passionate about growing and celebrating the game of basketball. Through the intensity of the game and the amazing athletic skill of our players, we deliver excitement to hundreds of millions of fans around the world.As a global sports and media business, the NBA is so much more. While Basketball Operations runs the league's on-court activities, other departments manage relationships with television and digital media partners, develop marketing partnerships with some of the world's most recognizable companies, oversee the licensing of NBA merchandise, and handle a wide range of responsibilities that drive the NBA's success.\\n\\nPosition Summary:\\n\\nThe Data Scientist will support the Basketball Integrity Analytics Lead in compiling and analyzing data relating to game play, team and referee trends in conjunction with sports betting data. The position will contribute to efforts to keep all Integrity stakeholders abreast of key analyses undertaken using a variety of reporting and visualization tools.\\n\\nMajor Responsibility:\\n\\n• Create and execute models for flagging anomalous performance, trends etc. that may be consistent with but not necessarily indicative of manipulation\\n• Design and undertake secondary checks to understand if the flagged event/trend under investigation is of genuine concern.\\n• Prepare reports for senior leadership in BS&A as well as related groups within the NBA such as Legal/Security and undertake follow-up analysis as directed\\n• Conduct innovative and original analysis to better understand player and referee performance and incorporate player tracking data and other proprietary data in these models\\n• Use statistical techniques to better understand salient betting trends and market movements, especially on NBA games across different markets\\n• Work with technical partners to automate data flows and reduce reliance on manual data development\\n\\nRequired Skills/Knowledge:\\n\\n• 3+ years of experience in data analytics, ideally related to the sports or gaming industries\\n• Self-starter with exceptional problem-solving skills and ability to communicate with a non-technical audience and deal effectively with senior management\\n• Experience with a database management language (e.g. SQL), scripting languages (e.g. R or Python) and data visualization tool (e.g. Shiny or Tableau)\\n• Applied knowledge of statistical regression methods and other supervised learning techniques and unsupervised learning techniques (e.g. dimensionality reduction and clustering)\\n• Confident individual, able to work in a fast-paced environment and manage short term deliverables along with long-term projects. Must exercise good judgement and be able to maintain confidentiality with sensitive projects\\n\\nEducation:\\n\\nBachelor's Degree Required; Major in Mathematics, Statistics, Computer Science or related discipline or Master's Degree are preferred\\n\\nWe Consider Applicants For All Positions On The Basis Of Merit, Qualifications And Business Needs, And Without Regard To Race, Color, National Origin, Religion, Sex, Gender Identity, Age, Disability, Alienage Or Citizenship Status, Ancestry, Marital Status, Creed, Genetic Predisposition Or Carrier Status, Sexual Orientation, Veteran Status, Familial Status, Status As A Victim Of Domestic Violence Or Any Other Status Or Characteristic Protected By Applicable Federal, State, Or Local Law.\\n\\nNearest Major Market: Manhattan\\nNearest Secondary Market: New York City\\nJob Segment: Database, Scientific, Computer Science, SQL, Technology, Engineering\",\n",
       "  \"Disney Streaming Services (DSS) is creating the future of digital video entertainment products for one of the largest media brands in the world. Were focused on delivering extraordinary experiences and distributing the incredible library of Disney content through all forms of interactive media. Our digital technology leadership and capabilities are married with a passion for crafting dynamic consumer-facing applications that integrate live and on-demand multimedia. We create great digital media products for millions of fans around the globe, and we're looking for strong performers who can help us improve what we do every day. This is an opportunity to impact experiences that will be used by millions of users worldwide.\\n\\nThe Data Scientist, Experimentation is a critical position within DSS and in the Data organization to apply advanced statistical methods to design and analyze A/B experimentations for product and algorithm/model changes. We make product and business decisions based on what we learned from A/B testing. Therefore, it is essential to ask the right questions and measure the right things to answer them.\\n\\nResponsibilities :\\nDevelop sampling and allocation methodology for experimentations.\\nDesign and analyze A/B experimentations for product and algorithm/model changes.\\nCreate actionable metrics and design effective experiments.\\nExplain experimentation results to both non-technical and technical stakeholders.\\nCollaborate with engineering teams to improve the experimentation pipeline and platform.\\nProcess, cleanse, and verify the integrity of data used for analysis.\\nBasic Qualifications :\\nAt least 2 years of relevant industry experience.\\nDemonstrated ability to design, analyze and present A/B experimentation results and recommendations to business and technical clients.\\nStrong statistics knowledge and experience to strengthen the validity of A/B tests, including statistical distributions and testing/sampling methodologies.\\nExpertise in programming languages for data processing and analysis (e.g., Python and R) and big data access tools.\\nExtensive background in statistical methods and data mining\\nExcellent communication and presentation skills.\\nGraduate degree in Statistics, Mathematics, Economics, Computer Science preferred, or equivalent professional experience.\",\n",
       "  \"Company Description:\\n\\nQuartet is a healthcare technology company striving to improve the lives of people with mental health conditions and was ranked #29 of Modern Healthcare's 2019 Best Places to Work in Healthcare. We connect people to a personalized care team to get them the right care at the right time. Our collaborative technology platform and range of services brings together physicians, mental health providers, and insurance companies to effectively improve patient outcomes and drive down healthcare costs.\\n\\nBacked by $153MM in venture funding from top investors like Oak HC/FT, GV (formerly Google Ventures), F-Prime Capital Partners, Polaris Partners and Centene Corporation, Quartet is headquartered in NYC and is currently operating in several markets across the United States Pennsylvania, Washington, Northern California, New Jersey, North Carolina, Louisiana, and Illinois.\\n\\nRole Description:\\n\\nWe are looking for talented data scientists with a passion for addressing business needs through data modeling and analysis to join our team. As an individual contributor on this team, you will work side-by-side with senior members of the team and internal partners from across the organization. This position requires strong problem-solving, analytical and communication skills; experience with multitasking and prioritizing in a fast-paced environment; and the tenacity to execute against deliverables in a timely, high-quality fashion.\\n\\nAs a data scientist at Quartet, you will work on a range of projects -- developing statistical analyses to study impact of Quartet interventions; predicting mental health needs among populations; building machine learning models to suggest timely and appropriate behavioral health care interventions for patients. You'll develop a deep understanding of Quartet interventions and the predictive models and algorithms that enable effective integration of mental health into primary care to steer patients towards better health. You will learn and apply statistical methodologies to measure outcomes and impact of interventions.\\n\\nYou'll work with datasets that include millions of detailed medical, pharmacy, lab claims, EHR, and application data. You will help with development and validation of new algorithms that enhance our system in terms of scalability, reliability and accuracy.\\n\\nThe ideal candidate will be an entrepreneurial, motivated data scientist who is well-versed in data analysis and algorithm implementation and eager to learn new things and make an impact on the industry. Health data experience is a plus, but it's not necessary.\\n\\nResponsibilities:\\nWork with an interdisciplinary technical team to develop statistical models in Quartet's platform.\\nApply statistics, data mining, machine learning techniques to develop studies to evaluate patient outcomes and recommendations to meet patients' and doctors' needs.\\nDesign and develop effective models, features, and algorithms involving multiple datasets - user activity, medical claims, pharmacy claims, lab test claims etc.\\nDerive insights from descriptive analysis that drive a data-informed process for experimenting with new products to improve patient outcomes.\\nLearn and contribute to team efforts to make our code scalable, our work output data-rich and actionable, and our research reproducible.\\nQualifications:\\n2-3 years experience as a data scientist.\\nFormal training in statistics and computer science.\\nKnowledge of mathematical fundamentals: probability theory, linear algebra and statistics.\\nStrong data transformation and extraction skills with SQL databases.\\nStrong statistical programming skills in both Python and R.\\nComfort/self-sufficiency with Amazon Web Services infrastructure.\\nComfort/self-sufficiency with Linux servers.\\nComfort/self-sufficiency with git for version control.\\nAbility to execute, starting from problem definition, to a working implementation.\\nAbility to clearly communicate across disciplines and work collaboratively.\\n\\nEmployee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, mental healthcare coverage of 15 free therapy sessions + unlimited copay reimbursements, medical, dental + vision coverage, generous parental leave, commuter benefits, 401K, stock option grants, gym benefits.\\n\\nWant to know what Quartet life is like? Click here to meet our team.\\n\\nQuartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet.\\n\\nPlease note: Quartet interview requests and job offers only originate from quartethealth.com email addresses (e.g. jsmith@quartethealth.com). Quartet will also never ask for bank information (e.g. account and routing number), social security numbers, passwords, or other sensitive information to be delivered via email. If you receive a scam email or wish to report a security issue involving Quartet, please notify us at: security@quartethealth.com.\\n\\nHave someone to refer? Email talent@quartethealth.com to submit their details to us.\",\n",
       "  'You will be the entrepreneurial leader that builds and owns analytics-based assets that bring the value of journey analytics to clients and their customers, at scale.\\n\\nYou will lead a team of specialists, data scientists and data engineers under the mentorship of solution leadership, through all stages of an engagement: defining the project, crafting the business solution, gathering and structuring data, building machine learning models, communicating business impact to senior client leaders, and being the change agent within an organization to make our recommendations a reality.\\n\\nYou will help business leaders answer critical questions related to customer journey mapping, customer experience transformations, digital adoption, multichannel acquisition, and other industry-specific challenges. Through diagnostics, predicting customer behaviour, tracking and prescribing interventions to journeys, training up client analytics teams and joint value capture programs, you will help clients achieve tangible improvements across every customer, patient and product journey.\\n\\nYour time will be spent 60% on client delivery, 20% on asset development and 20% on client development.',\n",
       "  \"SUMMARY\\n\\nZola, the fastest growing wedding company in the country, is seeking a Data Analytics Manager / Data Scientist that will help define and build Zola's internal and customer-facing data products. As the Data Analytics Manager / Data Scientist, you will be part of a hard-working, cross-functional team that is responsible for creating Zola's data advantage. This is a unique opportunity to join one of the hottest start-ups that is reinventing the wedding planning and registry experience to make the happiest moment in our couple's lives even happier! From engagement to wedding and decorating your first home, Zola is there, combining compassionate customer service with modern tools and technology.\\n\\nESSENTIAL FUNCTIONS\\nPartner with Business Units, Leadership, Product and Engineering to consistently iterate and improve upon Zola's ability to generate personalized recommendations (home style profiles, wedding style profiles, etc.)\\nMaintain our existing multi-touch attribution and LTV models. Work with key business stakeholders to improve upon these models as necessary\\nBuild a model capable of identifying the next best action for Zola couples based on their stage in the wedding planning journey\\nQuantify the expected business impact of your model using historical data. Seek feedback from relevant business partners on how to improve the model based on their knowledge of our products\\nWork with the Engineering team to deploy this model to production\\nDesign experiments to test the efficacy of your model. Iterate and improve upon the model as necessary\\nSKILLS & QUALIFICATION\\nDegree in CS, applied mathematics, or a related discipline\\n4+ years of experience in an analytical role in a fast-paced environment\\nRetail or consumer experience is a plus\\nAbility to work cross-functionally, apply a structured approach to problem-solving in the presence of ambiguity, and communicate relevant insights to senior leadership\\nStrong knowledge of SQL and prior statistical modeling experience with at least one of the following languages: Python or R\\nKnowledge of data warehouse design and the ability to audit existing data models\\nPrior experience applying supervised and unsupervised learning techniques to business problems in a B2C context. Ability to quantify and articulate the expected business impact of your statistical modeling work is a must\\nABOUT ZOLA\\n\\nWe're Zola, the wedding company that will do anything for love. We're reinventing the wedding planning and registry experience to make the happiest moment in our couples' lives even happier. From engagement to wedding and decorating your first home, Zola is there, combining compassionate customer service with modern tools and technology all in the service of love.\\n\\nBased in NYC and launched in October 2013, we have quickly become the fastest-growing wedding company in the United States. In 2017 we launched Zola Weddings, a free suite of wedding planning tools including wedding websites, checklist, guest list, and registry, all in one place. We've raised over $140 million in financing, and we're proud to call Thrive Capital, Canvas Ventures, Lightspeed Venture Partners, Comcast Ventures, and Goldman Sachs, among others, our investors and partners. Our team brings deep experience from working on beautiful, award-winning e-commerce sites and mobile apps in past roles at Gilt Groupe, Amazon, Bloomingdale's, Gap, J.Crew, Toys R Us, Chloe + Isabel, Yahoo, and Host Committee, among many others.\",\n",
       "  \"What We're About\\n\\nTelaria (NYSE: TLRA), (formerly Tremor Video), is the leading independent data-driven software platform built to monetize and manage premium video inventory with the greatest speed, control, and transparency, wherever and however audiences are watching.\\n\\nWe are looking to leverage our vast amounts of advertising data to make informed decisions around business optimizations and efficiencies. We are a small and efficient team building out a solution in an exciting space with lots of green field ahead of it. Believing we're just scratching the surface of the power of our data, we're actively searching for passionate and analytical data scientists to help us extract actionable insights in order to improve our product offerings.\\n\\nWhy You'll Be Excited\\nHaving a large stake and impact on the product and business direction and bottom-line\\nCollaborating with innovative and goal-focused engineering and business teams\\nApplying statistics, modeling and ML to improve the efficiency of systems relating to bid traffic shaping and infrastructure costs\\nDiving into a wide range of advertising business topics, such as forecasting, revenue yield, auction dynamics, and exchange optimization\\nInfluencing and steering the improvement and development of our data platforms, data pipelines, and data science processes\\nPerforming deep dive analyses to understand and optimize the key levers of our growth\\nWhy We'll Be Excited About You\\nYou have strong verbal and written communication skills that help you express your work in meaningful ways to cross functional teams\\nYou are passionate about digging into data sets\\nYou are able to write efficient and well-structured SQL queries on large data sets\\nYou have a solid background and understanding of statistical analysis, experimental design, and machine learning aspects (regression, classification, clustering, supervised/unsupervised, etc.)\\nYou have experience with data extraction, exploration, and analysis using programming languages (Java, Python, R, or similar) and data technologies (Spark, etc.)\\nYou have a degree in Mathematics, Statistics, Computer Science, or another applicable quantitative field\\nYou have familiarity with concepts relating to feature extraction and selection\\nBonus: You have previous advertising domain knowledge/expertise\\nWhy We (and You'll) Love It Here\\nWe are a technology and data-driven business\\nWe embrace analytical thinking, kind, and results driven people\\nWe have a plethora of challenging and interesting problems to solve\\nWe help and support each other in creating a productive work/life balance\\nCompensation & Benefits:\\n\\nAt Telaria we place an emphasis and importance on ensuring our total rewards are competitive, aligned with industry and to help you create a productive work/life balance. Benefits are highly subsidized and include medical, company paid dental, vision, employer contributed Health Savings Account, 401k matching, corporate gym discounts, pre-tax health and commuter savings, life insurance, 5 and 10-year Sabbatical programs, Discretionary Time Off (a.k.a. open vacation policy!), Paid Parental Leave, an Employee Referral Program, Employee Stock Purchase Plan (ESPP), and much more! All this is within a collaborative work environment you can personalize and topped with engaging programs like Micro-Mentorship, and Team Sports.\\n\\nTelaria values diversity and is proud to be an equal opportunity employer. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\",\n",
       "  'Did your things make you go “wow” today? At SmartThings, we are dedicated to making every home a Smart Home - and as the IoT platform of Samsung, with millions of users and connected devices, we are well on our way. We strive to create an easy-to-use, secure, and above all intelligent IoT solution for the home that delights our customers. Our fun, intelligent, and creative teams need your help to make our things a little more connected, and a lot smarter.\\n\\nThis is a key role in SmartThings’ New York-based audio-video engineering team. The AV team is building highly scalable software to support several ambitious new IoT product features. We are looking for a talented data scientist with an interest in the fields of computer vision, security, and home automation.\\n\\nThe ideal candidate should have experience using a variety of data mining/analysis methods, using a variety of data tools, building and implementing models to run on a variety of different hardware, using/creating algorithms and creating/running simulations. You should be comfortable working with a wide range of stakeholders and functional teams. You will work in an ecosystem of mobile apps, smart embedded devices, cloud-based backend services, and cutting-edge consumer electronics, contributing to the evolution of the Internet of Things.\\nResponsibilities\\nBuild and apply statistical and deep learning models\\nPerform analysis and derive insight from data that align with strategic product objectives\\nHelp drive the organization’s data science and ML strategy as related to its home security product road map\\nBuild rapport and manage relationships with other stakeholders including: engineering, product, and business development\\nProgramming in C and/or Python\\nRequirements\\nBS in ECE, EE, CS or related discipline or equivalent practical experience\\n4+ years experience with designing, developing, testing and productionizing machine learning algorithms\\nStrong communication skills, both verbal and written\\nAbility to work collaboratively\\nMotivation to learn and expand knowledge of AI and machine learning\\n\\nPreferred Qualifications\\nExperience in Software Development in one or more programming languages: C/C++, Python, Scala or other functional language\\nStrong proficiency in at least one of the following: SQL, AWS, scikit-learn, and any deep learning framework\\nKnowledge of internet technologies, protocols and cloud infrastructure\\nTravel: May include 10-15%\\n\\nTake your career to the next level at SmartThings ....... APPLY TODAY!',\n",
       "  \"This is a full-time role based out of our office in FiDi in downtown Manhattan.\\n\\nWe're looking for a Data Scientist to join our team, working cross-functionally across marketing, product, and engineering. Our ambitious product roadmap and increased focus on marketing mean that there's tons of exciting work to do in this space.\\n\\nResponsibilities\\nCollaborate with marketing, product, design and engineering to scope out current and future needs\\nWork as our lead data strategist, identifying and integrating new datasets\\nWork with the marketing team to identify and execute analytical experiments to produce meaningful insights\\nUse insights from data analysis to identify new opportunities and power our systems and solutions\\nMine, interpret, and clean our data and collect large datasets and variables\\nAnalyze data for trends and patterns, and interpret data with a clear objective in mind\\nImplement analytical models into production by collaborating with engineers\\nCommunicate findings and solutions to stakeholders and implement improvements as needed to operational systems\\nExperience\\n4-6 years experience\\nBachelor's degree in statistics, applied mathematics, or a related discipline\\nExpert SQL knowledge\\nProficiency with data mining, mathematics, and statistical analysis\\nAdvanced pattern recognition and predictive modeling experience\\nExperience with data visualization software (Tableau, Looker, etc), Excel and programming languages (i.e., R, Python)\\nExperience and/or comfort working in a dynamic, startup environment, with several ongoing projects\\nPreferred Qualifications\\nMaster's degree in data science, stats, applied math, or related discipline\\nExperience with Google Cloud Platform services, such as Big Query, Dataflow, Data Studio, Cloud SQL\\nWhy BlockFi?\\n\\nBlockFi has experienced incredible growth since our launch in August 2017. From raising over $75MM in debt and equity capital to helping thousands of clients (and growing!) do more with their crypto assets, we have established a dominant position as the debt and credit crypto market leader in the US. As we expand our product suite and geographic footprint, our addressable market will grow exponentially.\\n\\nBlockFi's leadership team has decades of experience in the traditional financial services and banking world, and we take a conservative approach to regulation that will position us well for sustainable long-term growth and expansion.\\nCompetitive salary\\nUnlimited vacation / sick days\\nEmployer paid health coverage (vision, dental, 401k)\\nWork alongside an enthusiastic, collegial, and driven team in a highly meritocratic environment\\nWeekly lunch, office snacks, and coffee\",\n",
       "  'Amazon is investing heavily in building a world class advertising business and we are responsible for defining and delivering a collection of advertising products that drive discovery and sales. Our products are strategically important to our Retail and Marketplace businesses driving long term growth. We deliver billions of ad impressions and millions of clicks daily and are breaking fresh ground to create world-class products.\\n\\nThe Advertising Fraud Management team owns prevention, detection and mitigation of fraudulent behavior within the Amazon Advertising ecosystem. Our mission is to mitigate impact of abusive activity without adding friction for our Advertising customers. The team owns building out predictive risk scoring algorithms based on real time signals to help maintain a seamless experience for our paying customers. These algorithms help control features that are available to eligible advertisers and complement the machine learning initiatives to detect and prevent suspicious activity on our stack. The Fraud Management team owns interfacing with various components of the Advertising stack to ensure the right controls are implemented across registration, campaign management, ad serving and billing functions.\\n\\nIn a typical day, you will work closely with talented software engineers, product managers and business groups. Your work will include cutting edge technologies that enable implementation of sophisticated models on big data. As a successful data scientist in our Advertising Fraud Management team, you are an analytical problem solver who enjoys diving into data, is excited about investigations and algorithms, can multi-task, and can credibly interface between technical teams and business stakeholders. Your analytical abilities, business understanding, and technical savvy will be used to identify specific and actionable opportunities to solve existing business problems in fraud prevention, detection and mitigation, through collaboration with engineering and business teams. Your expertise in synthesizing and communicating insights and recommendations to audiences of varying levels of technical sophistication will enable you to answer specific business questions and innovate for the future.\\n\\nMajor responsibilities include:\\n· Translating Advertising Fraud Management business questions and concerns into specific analytical questions that can be answered with available data using statistical and machine learning methods; working with engineers to produce the required data when it is not available\\n· Providing feedback to our engineering teams on the applicability of technical solutions from the business perspective\\n· Presenting critical data in a format that is immediately useful to answer questions about the inputs and outputs of Advertising Fraud Management systems and improving their performance\\n· Communicating verbally and in writing to business customers with various levels of technical knowledge, educating them about our systems, as well as sharing insights and recommendations\\n· Improving upon existing Advertising Fraud Prevention, Detection and Mitigation statistical or machine learning methodologies by developing new data sources, testing model enhancements, running computational experiments, and fine-tuning model parameters for new methodologies.\\n· Supporting decision making by providing requirements to develop analytic capabilities, platforms, pipelines and metrics then using them to analyze trends and find root causes of predictive inaccuracies in terms of false positives and false negatives.\\n· Utilizing code (Python, R, Scala, SQL etc.) for analyzing data and building statistical and machine learning models and algorithms.\\n\\n\\nBasic Qualifications\\n\\n· MS with 2+ years of industry experience or Bachelors with 5+ years of experience in Quantitative field (CS, ML, Mathematics, Statistics, Physics)\\n· 3+ years of experience with data querying languages (e.g. SQL), scripting languages (e.g. Python), or statistical/mathematical software (e.g. R, SAS, Matlab)\\n· Experience in creating data driven visualizations to describe an end-to-end system\\n· Highly skilled in data and math/statistical methods (i.e. modeling, algorithms)\\n· Evidence of using of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc. in data analysis projects.\\n\\nPreferred Qualifications\\n\\n· Depth and breadth in quantitative knowledge. Excellent quantitative modeling, statistical analysis skills and problem-solving skills. Sophisticated user of statistical tools.\\n· Experience processing, filtering, and presenting large quantities (Millions to Billions of rows) of data\\n· Combination of deep technical skills and business savvy enough to interface with all levels and disciplines within our customers organization\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment\\n· Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and business audiences\\n· Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment\\n· Experience in advertising is a plus\\nAmazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.',\n",
       "  \"Prognos is a NYC-based healthcare startup whose mission is to improve health by driving the best actions learned from the world's data. In order to achieve this goal we have curated the world's largest clinical lab dataset--covering almost 200M patients in the US--and are currently deploying cutting-edge technology for predicting disease at the earliest possible time.\\n\\nThe Mission of the Data Science team at Prognos is to develop, deploy and maintain AI/ML pipelines within all of Prognos' products, addressing business-relevant problems in close collaboration with Engineering, Clinical and Product teams.\\n\\nWe are looking for a hands on (IC) Data Scientist to join the team, and help us move this mission-critical task forward. This position will focus on building a platform for flexibility and efficiently building predictive models of disease incidence and progression.\\n\\nCandidates must have a degree in Statistics/Biostatistics, Computer Science, or a related quantitative field; Masters or higher is preferred. At least 1 year of hands on machine learning (ML) experience required.\\n\\nRequired Skills and Experience\\nStrong statistical analysis background with a deep understanding of a variety of ML algorithms and statistical modeling techniques\\nAbility to code a proof of concept (in a programming language of their choice)Willingness to stay abreast of and evaluate recent advances in NLP and sequence modeling in general\\nExperience with relevant software tools, such as Python, Gensim, scikit-learn, statsmodels, and/or others. You'll be free to use whatever tools you feel are appropriate\\nComfortable extracting data using SQL, Apache Spark or other distributed compute engine\\nAccustomed to working with GIT and shared codebases\\nExtensive data cleaning and textual data manipulation experience\\nStrong communication skills. Experience presenting experimental to people who are not Data Scientists\\nAbility to collaborate with the team and translate existing research into practical solutions and products\\nAbility to build and maintain relationships with various collaborators across the company, and take ownership of data science projects\\nPreferred Skills and Experience:\\nExperience with complex, high dimension, sparse time series data\\nExperience with healthcare data and/or insurance data a plus\\nPrognos Values & Culture\\nBe Collaborative: Always do what is best for the client. Check your ego at the door. We deliver premium value at a premium price. Practice blameless problem solving. Create win/win solutions.\\nBe Courageous: Do the right thing, always. Look at the facts and don't assume.\\nBe Curious: Think big and start small. Be relentless about improvement. Be predictive. Be curious and always ask why\\nBe Enthusiastic: Celebrate success. Be enthusiastic and positive. Let's have fun. Have purpose and believe in the greater good.\\nBe Driven: Act with a sense of urgency. Take ownership and honor commitments. Either find a way or make one. Deliver results.\\nBe a Superstar: Make quality personal. Deliver remarkable client service. Be better than your previous self. Go above and beyond.\\nOur culture guide: http://bit.ly/2ISMzjl\\nAbout Prognos\\n\\nPrognos is a healthcare AI company focused on predicting disease to drive decisions earlier in healthcare in collaboration with payers, Life Sciences and diagnostics companies. The Prognos Registry is the largest source of clinical diagnostics information in 40+ disease areas, with over 25B medical records for 200M patients. Prognos has 1000 extensive proprietary and learning clinical algorithms to enable earlier patient identification for enhanced treatment decision-making, risk management and quality improvement. The company is supported by a $42M investment from Safeguard Scientifics, Inc. (NYSE: SFE), Merck Global Health Innovation Fund (GHIF), Cigna (CI), GIS Strategic Ventures, Hikma Ventures, Hermed Capital, and Maywic Select Investments. For more information, visit www.prognos.ai.\\n\\nOur Mission\\n\\nTo improve health by driving the best actions learned from the world's data\\n\\nOur Vision\\n\\nTo prevail over disease and empower people everywhere to live life to the fullest\\n\\nSelected Perks\\nFree FreshDirect food, snacks, drinks, etc. delivered to the NYC office weekly\\nFlexible work arrangements and unlimited PTO\\nMonthly Happy Hours with Leadership\\nSome of our benefits include: Health Insurance, Life Insurance, Short Term and Long Term Disability, Dental, Vision, 401k, HSA, FSA, Dependent care flexible spending, commuter benefits, free access to One Medical Group, Gym discounts, flexible work hours and locations, access to the WeWork network, a Health Advocate, Employee Stock Option Plan, and more\",\n",
       "  \"Cadre is building a technology-driven investment platform for commercial real estate, opening up access to a 16 trillion dollar asset class favored by private equity firms, hedge funds, and institutional investors.\\n\\nWe're looking for a Data Scientist to join our Engineering team and partner with our acquisitions and asset management organization in order to support automation of deal flow and inform investment strategy. The successful candidate will develop and implement machine learning algorithms from commercial real estate datasets, understand our CRE investment business and develop practical research applications to drive it forward, and mentor junior members of the team.\\n\\n\\nWHAT YOU'LL DO\\nBuild and deploy machine learning models to accelerate our acquisition and asset management processes\\nPartner with Acquisitions to facilitate a quantitative approach to drive top-down investment strategies\\nResearch traditional and non-traditional commercial real estate data sources to integrate into various models\\nDevelop and drive the data science roadmap\\n\\nWHAT YOU HAVE\\nA bachelor's degree in computer science, applied mathematics, or other relevant quantitative disciplines\\nGraduate degree in a similar technical discipline is a plus but not required\\nPrior financial industry experience or work with spatial / temporal datasets is a plus but not required\\n3+ years of hands-on experience developing and deploying machine learning models, statistical modeling and advanced analytics\\nStrong written, verbal, and interpersonal communication skills.\\nSelf-starter, thriving in a startup environment\\n\\nBENEFITS & PERKS\\n100% of healthcare premiums covered\\n401(K) with company match\\nSignificant equity\\nUnlimited PTO\\nAmazing office space in the heart of SoHo with fully stocked kitchen\\nMuch More!\",\n",
       "  'Who we want:\\nAre you committed to using your research talents to provide global organizations with critical advice for their growing needs using advanced analytics?\\nAre you a leader who applies your instinct and expertise to discover breakthroughs that are key to clients’ growth?\\nAre you a driven professional who can manage multiple projects, set a standard of excellence and follow through on commitments for exceptional results?\\nDo you instinctively connect with others, understand individuals’ needs and share your passion for analytics to contribute to shared goals?\\nDo you excel at building predictive models using various data sources and techniques to inform practical business decisions?\\nWhat you will do:\\nGallup data scientists help clients effectively use data to make better decisions. You will apply your knowledge of various statistical and machine-learning techniques to a wide variety of challenging projects — from custom builds for client needs to helping automate solutions to the challenging problems clients face every day. You will partner with client teams to increase Gallup’s impact on a large scale by helping explain and predict large-scale social behavior (e.g., consumer spending, lifestyle trends, political stability, election outcomes, and employee performance and retention) using data from Gallup, the web and third parties (e.g., governments, IGOs and NGOs). We have data that no one else has. This gives you an unparalleled opportunity to use your creativity to explore new avenues of social research as part of a legacy founded by George Gallup in 1936 that set the gold standard in survey research methodology.\\n\\nWhat you need:\\nA master’s degree or Ph.D. from a statistics, engineering, mathematics, computer science, computational social science, physics or operations research program is preferred.\\nMinimum of eight years of work experience (advanced degrees count toward years of experience).\\nExpert-level production coding in Python is required.\\nMastery in conducting analysis in Python and/or R is required; additional analytic software experience is a plus.\\nMinimum of two years of experience building production-level machine learning and predictive analytics systems along with data pipelines.\\nA deep understanding of the mathematical fundamentals of machine learning and statistics, with an emphasis on non-parametric, non-linear methods (e.g., random forests, support vector machines, neural networks) and natural language processing.\\nMinimum of one year of experience working within distributed systems and managing workflows in a cloud infrastructure.\\nCandidates must be authorized to work in the United States.\\nGallup is an equal opportunity/affirmative action employer that celebrates, supports and promotes diversity and inclusion. We will consider all qualified applicants without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation or gender identity, or any other legally protected basis, in accordance with applicable law.',\n",
       "  \"Squarespace’s Data Scientists create the analytical frameworks and provide the technical rigor to drive pivotal business decisions. As an Associate Data Scientist, you’ll be supporting the team in analyzing key business problems, building data pipelines and feature sets for analysis and modeling work, and providing rigorous reporting to enable and evangelize data-driven decision-making throughout the organization. Furthermore, you’ll be expected to continuously develop the technical, statistical, and analytical skills that formulate the basis of a data scientist’s toolkit.\\n\\nDepending on your area of focus, challenges you may tackle in a typical day include:\\nMonitoring Squarespace’s conversion flow, tracking changes in trial or subscription behavior\\nWorking with Product Managers to design, implement, and analyze the results of A/B tests to optimize the product for a superior user experience\\nEnsuring our key data pipelines are operating correctly, and completing data quality checks and investigations to verify the accuracy of our reporting\\nSupporting our client teams in accessing data for legal, contractual, or analytical reasons\\nReconciling data from different web tracking/analytics systems, and developing a deep understanding of what drives the difference\\nSupporting efforts to accurately define information about customers, such as their location or their site type\\nBuilding dashboards to allow management to understand performance in real time\\nWe are seeking multiple Associate Data Scientists who demonstrate sharp intellectual curiosity, data fluency, and a collaborative work ethic. These roles report to the Analytics team lead in New York.\\n\\nRESPONSIBILITIES\\nEmpower data-driven decision-making across the org by maintaining analytical tools to guide key business decisions; develop and maintain dashboards to provide visibility into relevant metrics\\nHelp surface insights from terabytes of proprietary user, product, and marketing data; communicate compelling recommendations to key stakeholders\\nContinuously get better - seek out opportunities to further develop your analytical, engineering, statistical, etc. toolkit\\nQUALIFICATIONS\\nCurrently enrolled student with a graduation date in 2019 or Spring 2020\\nPursuing a Bachelor's or Master’s in a relevant technical field (e.g. CS, Math, Economics, etc.)\\nFamiliarity with both SQL and a scripting language (Python preferred); Strong competency with at least one of them\\nAbility to communicate effectively with wide-range of audiences\\nComfort working on open-ended problems with limited guidance\\nStrong data intuition and ability to think probabilistically\\nAbility to devise data-driven solutions to business problems\\nResearch experience is a plus\\nAbout Squarespace\\n\\nSquarespace makes beautiful products to help people with creative ideas succeed. By blending elegant design and sophisticated engineering, we empower millions of people — from individuals and local artists to entrepreneurs shaping the world’s most iconic businesses — to share their stories with the world. Squarespace’s team of more than 1,000 is headquartered in downtown New York City, with offices in Dublin and Portland. For more information, visit www.squarespace.com/about.\\n\\nPerks\\nHealth insurance with 100% premium covered\\nFlexible vacation & paid time off\\nUp to 18 weeks of parental leave\\nEquity plan\\n401(k) plan with employer match\\nFree lunch and snacks\\nSquarespace sends engineers to speak at and attend the most relevant and impactful conferences throughout the year\\nDog-friendly workplace\\nGender Affirmation Surgery\\nEducation reimbursement\\nToday, more than a million people around the globe use Squarespace to share different perspectives and experiences with the world. Not only do we embrace and celebrate the diversity of our customer base, but we also strive for the same in our employees. At Squarespace, we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender, gender identity or expression, or veteran status. We are proud to be an equal opportunity workplace.\",\n",
       "  'Our client is looking for an extraordinary Data Analyst or Data Scientist to join the band and help their Data Infrastructure organization provide better tools for Insights Producers and understand the impact of their tools and products. The Data Infrastructure organization is responsible for helping the client use data to make better and more well information decisions.\\n\\nRequired Skills & Experience\\nDegree in Computer Science/Engineering, Mathematics, Statistics, Economics, or another quantitative field; advanced degree is desirable.\\n2+ years (5+ years for senior role) of relevant experience analyzing complex data with SQL, Python, and/or R.\\nModeling and statistical knowledge is a strong preference. Feature engineering for machine learning models is a real plus.\\nExperience with visualizations and dashboarding (e.g., Tableau or similar BI software) is a strong preference.\\nExperience creating and scheduling datasets is highly valued. Knowledge of Google BigQuery and Java/Scala is a plus.\\nWhat You Will Be Doing\\nSupport leadership with research on key business initiatives and challenges.\\nPerform analyses on large sets of data to extract actionable insights that will drive decisions, with a particular focus on editorial curation and the clients’ key music brands.\\nDefine success measures and build tools that predict and track performance of strategic projects.\\nCommunicate data-driven insights and recommendations to non-technical audiences, through clear visualizations and presentations.\\nYou are a natural communicator, who focuses just as much on the delivery and the “so what” of your insights, as you do on the technical craft of extracting them.\\nYou are comfortable building and maintaining relationships with senior management, as well as team members around the world.\\nYou are capable of tackling very loosely defined problems, and comfortable leading and owning a research agenda for long term analytics projects.\\nYou are passionate about music and popular culture in general. Previous experience in music, media, entertainment, or tech industries is a plus.\\nThe Offer\\nCompetitive Salary: Up to $70K/year, DOE\\nYou will receive the following benefits:\\nMedical Insurance & Health Savings Account (HSA)\\n401(k)\\nPaid Sick Time Leave\\nPre-tax Commuter Benefit\\nApplicants must be currently authorized to work in the United States on a full-time basis now and in the future.\\n\\nWorkbridge Associates, part of the Motion Recruitment network, provides IT Staffing Solutions (Contract, Contract-to-Hire, and Direct Hire) in major North American markets. Our unique expertise in today’s highest demand tech skill sets, paired with our deep networks and knowledge of our local technology markets, results in an exemplary track record with candidates and clients.',\n",
       "  \"Mine rich user data sets and find insights that will drive the decisions that power our platform.\\n\\nThe Mission\\n\\nviagogo is on a mission to revolutionize the way people get tickets to live events, making it possible for anyone, anywhere in the world to see their favorite artists, teams or shows live. We've helped millions of people from almost every country attend live events, and we're just getting started. We believe creative individuals with innovative ideas are the engine for our rapid growth, so we're looking for the world's smartest data scientists to join our team, where you'll be given the freedom to make meaningful and measurable improvements impacting millions of people.\\n\\nYour Role\\n\\nData scientists sit at the core of our business, constantly working to identify new product features and improvements that enhance the experience for our users and drive protable revenue streams for the company. You'll be part of a team using data to automate manual processes & human decisions, the work carried out by our Data Scientists is exciting and challenging, but crucial to scaling viagogo across all geographies, languages and currencies. You'll mine rich data sets and nd insights that will drive the decisions that power our platform. You'll be given autonomy to make product decisions as soon as you're capable, with senior members of the product team on hand to guide and support you. Every piece of work you complete will tie directly to a business goal so you can measure the impact you have on viagogo and quickly you'll come to understand of the technical underpinnings of how ideas turn into reality at an ecommerce tech company operating at a global scale.\\n\\nTypes of projects include:\\nDesigning, running and analyzing feature experiments (A/B tests) on millions of users of our website and apps\\nOptimizing the coverage and efciency of ad campaigns through search engines (PPC) and social media platforms\\nIdentifying opportunities to acquire new inventory and optimize the quality/pricing of inventory in all geographies\\n\\nYour Skills\\n\\nYou get things done. Given a problem, you won't rest until you've found the answer.\\nYou have a degree in a quantitative eld like science, math, economics, engineering or similar or industry experience working as a data analyst\\nYou have excellent critical-thinking and problem-solving skills\\nYou are thorough, detail-oriented and organized, with the ability to manage multiple parallel projects\\nYou have entrepreneurial drive to initiate and drive projects and achieve stretch goals with minimal guidance\\n\\nYou want to learn. Becoming an effective data analyst requires learning the tools of the trade. We'll teach you how to use tools like Excel, SQL and R to not only analyze data to nd the insights you're looking for, but to visualize and present it most effectively for stakeholders to make decisions.\\n\\nYou want to make an impact. You don't just want to produce pretty tables and graphs; you want to understand what the results mean and how to utilize your ndings to make the best product/business decision possible.\\n\\nYou are a team player. You work not just to advance your own skills or ideas, but also to help the team and the business succeed. This means being open to feedback from others and giving it in return. It also means recognizing that the best ideas might not always be your own.\",\n",
       "  \"ABOUT US\\n\\nSimon Data was founded in 2015 by a team of successful serial entrepreneurs. We're a data-first marketing platform startup, and we approach our work seriously; we tackle problems in a scrappy and disruptive fashion, yet we build for scale to support our clients at big data volume.\\n\\nWe are the first and only enterprise customer data platform with a fully-integrated marketing cloud. Moving beyond the limitations of both categories, Simon's platform empowers businesses to leverage enterprise-scale big data and machine learning to power customer communications in any channel. Simon's unique approach allows brands to develop incredible personalization capabilities without needing to build and maintain massive bespoke data infrastructure.\\n\\nOur culture is rooted in organizational transparency, empowering individuals, and an attitude of getting things done. If you want to be a valuable contributor on a team that cultivates these core values we would love to hear from you.\\n\\nTHE ROLE\\n\\nAs Simon's Associate Data Scientist you will scale our predictive features' ability to analyze data efficiently and meaningfully. In this role you will be responsible for developing general schemata for product and marketing data, and will use them to align and report on the quality and integrity of complex datasets. You will produce clear, statistically rigorous analyses of client data for consumption by an audience of marketing professionals and domain experts. You will get to explore datasets describing both customer behavior and messaging interactions to drive advanced segmentation and campaign efficacy analyses.\\n\\nThis role is crucial to the business as we expand our capabilities, and bring more ML-powered tools to our customers. You are the right person for this role if you have a high proficiency in communicating data insights to non technical stakeholders, are creative in your approach to finding solutions for current data sets and able to iterate as needed, and have a strong desire to learn new skills and technical approaches as you advance your career in data science.\\n\\nWHAT YOU'LL DO\\nWrite SQL queries in a number of dialects to extract and schematize diverse data sets\\nAnalyze and transform data in order to derive meaningful insights for customers and internal stakeholders\\nCreatively generate clear visualizations of large and complex data sets\\nExtract features from data that use calculated insights to improve performance of statistical models that drive Simon's products\\nQUALIFICATIONS\\nM.S. Data Science/Statistics or related field, or equivalent experience\\n2+ years working in digital marketing, business analysis, or related field\\n1+ years experience writing SQL queries\\nPractical experience visualizing complex datasets (R/ggplot, python/matplotlib, SAS/Tableau, Excel, etc.)\\nMeaningful experience writing Python or other production coding language (Java, C#, etc) is a plus\\nVisa sponsorship for this role is currently not available\\n\\nDiversity\\n\\nWe're proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status.\",\n",
       "  \"Description: </br>Looking for opportunities to use cutting edge technologies analyzing petabytes of data in a world class Hadoop cluster, generating insights to guide consumers in their journey to wellness and help them achieve their health ambitions, whether its running the Inca Trail Marathon or playing tackle football with their grandkids? Aetnas Member Analytics team is focused on delivering strategically-impactful programs and tools to help members across all life stages feel the joy of achieving their best health, in their own way.\\n\\nThis position will work within a cross-functional team, delivering predictive analytics solutions to drive customer engagement and next-best-action, and improve customer experience across critical journeys, e.g., onboarding, breast cancer, maternity. The candidate will be responsible for extracting and manipulating data from multiple, large data sources and using the data to deliver predictive model and solutions to drive member behavior change and experience across channels (mobile app, web, email, social, call center, Apple)\\n\\nPosition is open to NYC, Wellesley MA and Hartford CT.\\n\\n62819\\n\\n</br></br> Fundamental Components: </br>\\nDevelops and/or uses advanced algorithms and statistical predictive models and determines analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes\\nUses strong programming skills to explore, analyze and interpret large volumes of data in various forms, and solve complex business problems\\nSupports deployment of insights across multiple channels, i.e., web, mobile app, email, social, call center, Apple watch\\nDemonstrates strong ability to communicate technical concepts and implications to peers, managers, and business partners\\n</br></br> Background Experience: </br>\\n2-5 years of progressively complex related experience\\nDemonstrates proficiency in most areas of mathematical analysis methods, machine learning, statistical analyses, and predictive modeling and in-depth specialization in some areas.\\nExpertise in using R or Python to manipulate large data sets and develop statistical models\\nExpertise in data management in an Hadoop environment, including use of Hive\\nExcellent problem solving skills, critical thinking and conceptual thinking abilities\\nStrong ability to communicate technical concepts and implications to business partners\\nSome knowledge of health care industry preferred\\n</br></br> Additional Job Information: </br>Aetna is about more than just doing a job. This is our opportunity to re-shape healthcare for America and across the globe. We are developing solutions to improve the quality and affordability of healthcare. What we do will benefit generations to come.\\n\\nWe care about each other, our customers and our communities. We are inspired to make a difference, and we are committed to integrity and excellence.\\n\\nTogether we will empower people to live healthier lives.\\n\\nAetna is an equal opportunity & affirmative action employer. All qualified applicants will receive consideration for employment regardless of personal characteristics or status. We take affirmative action to recruit, select and develop women, people of color, veterans and individuals with disabilities.\\n\\nWe are a company built on excellence. We have a culture that values growth, achievement and diversity and a workplace where your voice can be heard.</br></br> Potential Telework Position: </br>No</br></br> Percent of Travel Required: </br>0 - 10%</br></br> EEO Statement: </br>Aetna is an Equal Opportunity, Affirmative Action Employer</br></br> Benefit Eligibility: </br>Benefit eligibility may vary by position. Click here to review the benefits associated with this position.</br></br> Candidate Privacy Information: </br>Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.</br></br>\",\n",
       "  \"Relocation to London is required.\\n\\nG-Research is a leading quantitative research and technology company. Using the latest scientific techniques and advanced analysis methods, we find patterns in large, noisy data to produce world-beating predictive research.\\n\\nThe Role\\n\\n- 10 week summer programme (July to September 2020)\\n09:00-17:30 working hours\\nBased in Central London\\n\\nJoining G-Research's Summer Internship Programme, you will be given a meaningful and challenging research project that demands the application of innovative yet pragmatic mathematical and computational analysis.\\n\\nUsing rigorous scientific methodology, robust statistical analysis and pattern recognition you will extract meaningful predictive signals from financial time-series and use these to predict future dynamics.\\n\\nYour project will give you the opportunity to use a wide range of techniques in areas such as machine learning, natural language processing, deep-learning and optimisation in a practical and challenging context. Additional work may involve the implementation of back-testing frameworks to ensure signal robustness or the creation of a pipeline which constructs and simulates the performance of a portfolio derived from various input signals.\\n\\nFor the duration of the internship, you will collaborate with one of our Quantitative Researchers who will act as a mentor as you work on your independent project. You will receive structured feedback and reviews to help you to improve and develop, culminating in a final presentation of your research ideas to senior management. Upon successful completion of the programme, many interns are offered the opportunity to join us full-time once they have completed their studies.\\n\\nTaking part in G-Research's Summer Internship Programme will give you an in-depth insight into our academic approach to the world of quantitative finance and allow you to explore the thriving city of London, while you get to know your fellow interns and colleagues through a full itinerary of fun social events.\\n\\nWho are we looking for?\\n\\nThe ideal candidate will, at minimum, have experience in the following areas:\\nStrong background in mathematics\\nIntermediate level of programming in at least one OO language (Python/C# desirable)\\nAn interest in applying data science, machine learning & optimisation techniques to real-world problems\\nYou should be in the final or penultimate year of a Masters or PhD in a highly technical or quantitative subject such as mathematics, physics, statistics, engineering or computer science\\nWe also like to see active Git Hub/Kaggle profiles (but these aren't a prerequisite)\\nPrevious experience in finance is not required, although an interest in finance and the motivation to rapidly learn more is a prerequisite for working here.\\n\\nWhy should you apply?\\nHighly competitive compensation plus accommodation allowance\\nActive G-Research community with weekly intern activities\\nCompany-wide summer party and weekend away\\nInformal dress code and excellent work/life balance\\nCentral London office close to 5 stations and 6 tube lines\\n*** Want to meet our quant team?***\\n\\nWe are running interactive presentations with members of our quant team in the coming weeks. To register your interest in attending please email your name and your preferred location to quant.events@gresearch.co.uk:\\n\\nUSA\\n\\nNew York, Friday 20th September\\n\\nBoston, Monday 23rd September\\n\\nUK\\n\\nCambridge, Thursday 26th September\\n\\nOxford - TBC\",\n",
       "  \"Data Scientist\\n\\nNew York, NY\\n\\nHOMER has a newly created opportunity for an exceptional Data Scientist to join our energetic, growing team in New York! You will have a profound impact on a company that touches the lives of children, parents, and teachers across the globe with one of the most celebrated reading and learn-to-read platforms in the industry. We are looking to create the best digital learning experience available. Your work will help build a beautiful, personalized experience to meet every child's specific needs as they learn to read, love to read, and read to learn. Success in this position means you have helped the children using our products find the lessons, stories, and other content that best fit their age and interests.\\n\\nAs a Data Scientist, you will report into the Director of Data & Analytics, and will be responsible for developing deep expertise in our data, analyzing it thoughtfully and creatively, and distilling complex findings into compelling narratives. You will work with various business units and stakeholders to answer key strategic questions and identify actionable takeaways. As you grow, you'll take on new responsibilities, from developing advanced data analytics visualization tools, to designing and improving our algorithms for product personalization features and marketing optimization applications. If you are passionate about leveraging data to support business decisions, we want to hear from you!\\n\\nHOMER is a venture-backed, high-growth business that offers competitive compensation including equity and full benefits in a creative, flexible environment that invests in professional development.\\n\\nYour responsibilities will include:\\nWorking with various business teams at Homer to understand their analytics questions, and determine the appropriate data sources and analysis approaches needed to answer them\\nExecuting analytics projects, applying appropriate statistical and machine learning techniques to produce key insights and actionable takeaways, and presenting findings and recommendations to key stakeholders\\nAnalyze business metrics and KPIs to understand trends, identify issues, and opportunities for optimization\\nHelp identify and assess metrics and KPIs that could be tracked in order to measure impact and business outcomes\\nAnalyze A/B test results and perform cohort studies to track long-term impact on user behavior\\nBe actively involved in the development of our analytics tools for A/B testing, segmentation, funnel analysis, and other analytics needs\\nTackling machine learning projects across various applications and business use cases\\nMust Haves:\\nExcellent problem solving, analytical thinking and communication skills\\nProficiency in Python, SQL, and statistical tools such as R\\nExperience with data visualization tools (e.g. Looker, Tableau, D3/Javascript)\\nAbility to query and extract data from various data sources, including relational databases (e.g. MySQL), non-relational databases (e.g. Couchbase or MongoDB), and column store (e.g. Vertica/Redshift)\\nStrong statistics and mathematical fundamentals\\nBachelor's degree in a quantitative field such as Computer Science, Mathematics, Statistics, or Economics\\n2-5 years of relevant work experience\\nNice-to-Haves:\\nKnowledge and experience in machine learning is a plus\\nExperience designing highly usable and compelling dashboards/visualizations is a plus\\nWe like people who:\\nAre passionate about data, love getting their hands dirty with data and shaping it into information\\nAre meticulous, thorough, and display excellent attention to detail\\nAre intellectually curious and comfortable working with new or unconventional technologies\\nAre self starters, super proactive and can act as the CEO of their position\\nHave an innate desire to deliver results\\nEnjoy working hard and working smart\\nAre passionate about our industry (education) and our consumers (children, parents and educators)\\nWhat you'll get:\\nHOMER offers competitive compensation including equity and full benefits\\nSmart, passionate, and engaged co-workers\\nExcellent top-tier Medical/Dental/Vision benefits\\nThe chance to have a big impact, quickly\\nThe rare opportunity to make a dent in the universe. We're bringing a love of reading and learning to children globally\\nHOMER is an equal opportunity employer and enthusiastically encourages people from a wide variety of backgrounds and experiences to apply. HOMER does not discriminate on the basis of race, color, religion, sex (including pregnancy), gender, national origin, citizenship, age, mental or physical disability, veteran status, marital status, sexual orientation or any other basis prohibited by law.\",\n",
       "  'Qualia Investments is a proprietary trading desk that leverages technology to enter volatile, underdeveloped markets. We believe that the influence of consumer buying patterns has created an ‘Experience Economy’ in certain retail markets, where assets are traded on a latitude of secondary exchanges. Trading assets reliant on outdated, retail-oriented infrastructure creates significant friction to market participants. However, it welcomes disruption through product innovation and market insight.\\n\\nThe software engineering team works closely with the rest of the firm, building tools, exploring trading ideas, and designing as well as maintaining the firm’s software systems. We are a liquidity provider and a market accelerator, rapidly deploying products and insights to create new stakeholder opportunities. Our team works tirelessly to develop strategies that help us understand the black box of consumer behavior and the role experience plays in price-value theory.\\n\\nOur team is composed of traders and hackers working together to solve complex, abstract problems. Our office culture is unique and we embrace individuality and diversity of thought, encouraging our employees to take risks and expand expertise within a comfortable environment. We encourage those to apply who are passionate about self-learning and the non-traditional workplace.\\nYou will:\\nPolish our in-house trading dashboard into enterprise products that fit a variety of clients’ needs\\nDistill loose ideas into valuable data products\\nIdentify and incorporate new data sources into our platform\\nBuild and maintain ETL pipelines in Apache Airflow\\nYou will need a background in:\\nPython (2+ years software development experience)\\nData modeling\\nExposure to a task scheduler (Airflow, Luigi, etc)\\nAbility to architect solutions in the cloud (preferably on AWS)\\nDocker\\nCI/CD\\nNice to Haves:\\nExperience with AWS lambda, or other FaaS platforms\\nFamiliarity with React\\nOur hiring is non-discriminatory to race, age, gender, background or experience. We embrace diversity of thought and intentionally hire from a wide array of backgrounds.\\nBenefits & Culture\\n100% coverage of health, vision and dental insurance\\n401K employer match (3%)\\nStock Options\\nFrequent (optional) team events\\nUnlimited PTO\\nCommunal space with full scale-kitchen, lounge and quiet corners',\n",
       "  \"What is Slice?\\n\\nSlice is the leading technology and marketing platform made exclusively for local pizzerias, making it super easy to order delicious, authentic local pizza anywhere, anytime. We serve the $45 billion U.S. pizzeria market in two ways: by providing a pizza-centric mobile and web ordering experience for consumers, and by empowering local restaurants with the technology, tools, and marketing to grow their business, while helping them compete with Big Pizza. Can you imagine what a small mom and pop pizza shop could achieve with the resources of Domino’s?\\n\\nWhat you'll do:\\nLeverage your quantitative expertise to understand how Slice can apply supply and demand levers to unlock marketplace opportunities; techniques will include experiment design, predictive modeling, observational analysis, pattern recognition and any others as needed\\nPartner with Product and Marketing teams to set clear goals, measure progress against them and influence their respective roadmaps\\nEnsure consistent tracking, analysis, and reporting of core product and marketing programs and channel KPIs in weekly dashboards\\nBuild and maintain executive dashboards in Looker\\nImprove the accessibility and organization of data\\nCollaborate with Data Engineering to understand all data flows, build new flows as needed and create appropriate data dictionaries for business users\\nSimplify complex raw data into user friendly explores, looks, and dashboards for audiences across the organization\\nPerform complex ad-hoc data and analysis requests\\nWhat we're looking for:\\nBSc in a quantitative discipline (MSc preferred)\\n3+ years of specialized experience in Data Science or Advanced Analytics solving complex analytical problems in a technology environment\\nProficiency in Python and SQL\\nFluency in at least one statistical software (R preferred), Excel and at least one visualization tool (Looker, Mode, Tableau, etc.)\\nStrong data modeling and transformation skills\\nExcellent verbal and written communication skills\\nHistory of Slice:\\n\\nSlice was born in 2010 and has quietly bootstrapped its way to building a network of more than 11,000 pizzerias nationwide. Ilir Sela, our founder and CEO, started the company as a passion project to help his friends and family in the pizza business, but he quickly saw a massive opportunity to champion these small businesses by bringing their craft to the masses. Ilir has since built an amazing team of operators, marketers, technologists, and investors — all dedicated to making it easier for people to enjoy their favorite local pizza while helping local shops succeed.\\n\\nOur Pizza Philosophy\\n\\nSlice is on a journey to be the most valuable pizza brand on the planet. We connect makers and eaters to enrich their lives through the power of specialization and pizza expertise. Backed by generations of knowledge and cutting-edge technology, we give makers the platform and voice to take charge of their industry, expand their coverage, and fill the world with authentic cuisine. When passionate makers turn their craft into their livelihood, we all live happier, fuller lives. People stop accepting the homogenous, mass-produced pies of big chains and robotic trucks because the quality, variety, and authenticity of Slice is both hyper-convenient and known worldwide. We believe pizza isn’t just food. It’s a slice of community, a slice of culture, a slice of life.\\n\\nWe’re growing our family every day — so, if you’ve got a passion for local, authentic pizza and the drive to help share it with the world, we’d love to have you on the team! Check out a few awards we’ve recently won for our workplace and culture: Inc., Crain's, BuiltinNYC\\n\\nSlice is an Equal Opportunity Employer and is committed to building an inclusive environment for people of all backgrounds and everyone is encouraged to apply. We do not discriminate on the basis of race, color, gender, sexual orientation, gender identity or expression, religion, disability, national origin, protected veteran status, age, or any other status protected by applicable national, federal, state, or local law.\",\n",
       "  \"At Skillshare, we’re building a platform to connect curious, lifelong learners everywhere. As a Data Scientist focused on Product Analytics, you will be a key member of the cross-functional team that tracks and communicates business metrics, applying your curiosity and data intuition to ensure that we understand the health of our business and keep a pulse on any changes in membership and engagement. You will deep-dive into how our students and teachers are using our platform and your data-driven insights will directly influence the development and success of our product. In this highly collaborative role, you will be an integral part of our small and growing data science team and you will work closely with product leaders, engineers, marketers, supply and operations teams as well as the executive team.\\nWhat you'll do:\\nTrack our business health: you will do everything from identifying, vetting and developing key metrics, to designing dashboards and reporting to our core business operations team and executives on a regular basis\\nApply the tools of exploratory data analysis and statistical inference and modeling to understand user behavior trends and improve our product (e.g., A/B experiment design & analysis, customer retention & engagement modeling)\\nCollaborate with data & engineering teams to ensure that we are instrumenting the right data and that our data model design supports our analytics needs\\nConsult with teams throughout Skillshare on how to frame and scope analyses, leverage our data and tooling most effectively, and apply analytics best practices\\nCommunicate results effectively at all levels of the organization and ensure all stakeholders understand implications and recommendations from your analysis\\nWhat you'll need to be successful:\\nSignificant professional experience working with data\\nExperience with SQL and relational databases\\nExcellent communication and ability to collaborate with cross-functional teammates with a wide range of analytical backgrounds\\nStrong data intuition-- good nose for identifying and interpreting trends and anomalies in our data, and ability to prioritize and execute an efficient data deep dive\\nExperience conducting exploratory data analysis, statistical inference and modelling (including A/B experiment evaluation)\\nCuriosity combined with business thought leadership and strategic thinking: your analyses are oriented around driving impact and value\\nKnowledge of self-service BI tools such as Looker, Chartio, Tableau\\nHands-on experience with statistical software (Python or R)\\nEnthusiasm for conducting reproducible analysis (you believe in code review, version control and documentation even if you haven’t used Github yet!)\\nStrong attention to detail and great passion for data integrity\\nWhy you want this job:\\nExposure: This will be a high-profile role at Skillshare, as you’ll be working directly with the head of data, CEO and rest of the executive team to dig into some of our critical business questions.\\nImpact: Leveraging and understanding our data is at the heart of our growth - your analysis will be the foundation for many of our most strategic decisions.\\nData challenges: With millions of registered users and tens of millions of minutes watched every month, you will have the opportunity to dig in and answer some complicated business questions.\\nOur mission: We’re doing work that matters – connecting lifelong learners around the world and empowering them to pursue their creativity.\\nOur team: We have a passionate, smart team that is a lot of fun to work with.\\nFlexibility: We believe that doing your best work means living a full life. That means different things for everyone, so we optimize for trust, invest to support remote teams, have an unlimited vacation policy (with a required minimum!), and encourage work-life balance.\\nAbout Skillshare:\\n\\nSkillshare is an online learning community whose mission is to connect curious, lifelong learners everywhere – and, in so doing, build a more creative, more generous, and more prosperous world. Today, our community has grown to millions of members who come to Skillshare to learn creative and entrepreneurial skills, network with peers, and even teach a class themselves. We are backed by Union Square Ventures, Spark Capital, Amasia, Spero Ventures, and Burda Principal Investments.\\n\\nSkillshare is committed to building a diverse team that reflects a variety of backgrounds, perspectives, and skills. We’re proud to be recognized as a top place to work by Crain’s, and one of the five best places to work for women by Bpeace. We work to ensure a consistent interview process, fair compensation, and inclusive work environment for all.\",\n",
       "  \"Data Scientist - with any one or more expertise: DataRobot, AutoML, Spark Beyond and/or H2O\\n\\nThe Role Simply stated, this executive leadership position will be responsible for and resourced to create a vision of how data science can continue to transform our organization. Our businesses develop and market data used every day for critical functions in healthcare, transportation, and finance. At our core we leverage data and expertise to answer critical questions that come up as people do their jobs, from selecting the right drug for a patient to estimating a repair for a car. We have an extraordinary diversity of data, across industries and applications therein. Plus, we're growing both organically and by acquisition (which means even more data sets!).\\n\\nAs successful as we are, our aspiration is to do even better. Data it at the heart of what we do but we think we can improve. We've seen the amazing feats achieved by modern AI technology and believe we are at the beginning of a period of unprecedented innovation. We're starting an ambitious project to assist our businesses with building their products and developing new ones. This work will require handling transactional data, manufacturing information, and human-curated content. We'll leverage ML, deep learning, semantic modeling, probabilistic programming, and likely pioneer some new techniques. If the idea of this much diversity, of industry, of technique, of content, excites you then we want to speak to you!\\n\\nThis is a hands-on role. Although you will be leading a small team, we fully expect your hands will be on keyboard often. Right now, the project is in its initial phase: the first few members of the team have been recruited, the lab space has been built out, the individual projects along our journey need to be selected and defined. We have resources, executive sponsorship at the highest levels of the company, and a multi-year commitment. What we need is an entrepreneurial leader who is passionate about modern analytical capabilities/techniques and is at home working to define, build, and (of course) execute this program.\\n\\nResponsibilities\\n\\nCombine top analytical skills with knowledge of our businesses to build and produce models to impact our business in positive ways.\\n\\nWork with other data scientists with a broad range of analytical expertise and subject matter experts to deliver data products and provide business insights through quantitative analysis (Predictive Modeling, Optimization, Visualization, etc.)\\n\\nAssist in the testing and implementation of the models and analysis created\\n\\nExtract data from various applications and systems, in particular large relational databases, manipulate, explore data and build models using quantitative, statistical and visualization tools.\\n\\nMentor and manage Data Scientists and Engineers\\n\\nPresent findings, both formally and informally, to audiences at all levels of the organization\\n\\nIn collaboration with the product management, content development, and engineering teams, identify opportunities to leverage data science techniques in order to create new or improve existing products\\n\\nEvangelize the use and potential of data science within the organization and in particular the executive, product management and engineering teams\\n\\n10+ years of experience in relevant areas of computer science, including applied machine learning, deep learning, NLP or related disciplines\\n\\nStrong statistical background and experience implementing systems in production\\n\\nStrong foundation in coding skills relevant to data science, e.g., Pig, Hive, SQL, Python, etc.\\n\\nExtensive experience solving analytical problems using quantitative approaches\\n\\nExperience developing production-quality data products using the results of quantitative research\\n\\nTrack record of successfully managing a diverse team of highly skilled individuals.\\n\\nMust be able to communicate effectively with (non-technical) senior executives internally and externally. Presentation skills are essential.\\n\\nFamiliarity with modern data pipelines and ETL practices\\n\\n.\",\n",
       "  \"Crossix is seeking intellectually curious, resourceful, and collaborative Data Scientists to join our Advanced Analytics team. This is an excellent opportunity to help us build the technology and data science products that power our business and be at the forefront of innovation in the healthcare technology space.\\n\\nThe team is guided by its core values as it works to solve the most challenging problems in healthcare data and analytics:\\nSingular Focus\\nSpeed\\nHumility\\nOwnership\\nChallenge\\nWhat You'll Do\\nApply machine learning, data mining, and statistical analysis techniques to large health and consumer data sets to build new products and methodologies\\nCollaborate closely with a team of data scientists, product managers, and executives to discover and deliver product offerings from prototype to massive scale\\nRapidly build prototype product solutions, communicate findings, and iterate\\nExplore and find meaning in high volumes of data to evaluate data quality and extract actionable insights that will help drive business decisions; execute data querying, data cleansing, and experiment design\\nDraw from prior experience and technical expertise to identify product improvements and inform testing plans; break overall objectives down into underlying problems that can be prioritized and solved\\nMaster core parts of the Crossix technology platform. Technologies include Spark, SQL, Python, R, AWS, and proprietary data mining software\\nWork with engineering and development teams to improve and implement features in Crossix's platform\\nWhat You've Done\\nGraduate level degree in quantitative discipline with at least 2 years of work experience; 6+ years of relevant post-collegiate work experience without graduate degree\\nAdvanced knowledge and professional experience in statistical modeling, machine learning and data mining\\nStrong problem-solving skills with an emphasis on product development\\nStrong hands-on coding skills in statistical modeling programming languages such as R and Python\\nAdvanced SQL skills; expertise in best practices and tools for interacting with large data sets\\nExperience with AWS for data-warehousing and processing is a plus\\nExcellent written and verbal communication skills\\nWho You Are\\nHave a desire and preference for working in a fast-paced, entrepreneurial environment\\nEnjoy having clear ownership of a goal even if the path to get there is not entirely clear\\nHave a curiosity to figure out new problems\\nAre humble and truly think about the success of the group before your own contribution\\nAre comfortable challenging existing norms, thinking and teammates, always doing so respectfully\\nAbout the Team – Crossix is the market leader in delivering hard-to-come-by insights that enable healthcare marketers to plan, measure, and optimize their marketing campaigns with confidence. Using our own proprietary technology and network of health and non-health data, our analyses pinpoint the tactics, programs, and channels that improve performance and boost sales, enabling better healthcare communications. And we do it all while protecting consumer privacy.\\n\\nLeadership – With decades of combined experience and an unrivaled track record of healthcare innovation, our leadership team sets the standard for us. Their knowledge and expertise continually challenge us and the industry – through their work, their speaking engagements at conferences and their thought leadership published in the top industry publications.\\n\\nCulture – We know that our employees set us apart. Along with competitive salaries and benefits, we invest in creating compelling opportunities for professional development and career growth. We also believe that diversity is essential to building an environment where everyone can feel they belong. We're continuously building an inclusive company where everyone feels welcome and heard. Come join our rapidly growing team!\\n\\nWe are an equal opportunity employer and welcome all qualified applicants regardless of race, color, religion, sex, gender identity, sexual orientation, marital status, ancestry, national origin, age, disability, genetic information, or veteran status.\",\n",
       "  'This role is part of the Core team within the Samsung NEXT Product organization. The Core team is building today a set of large scale products that are directly linked with Samsung world wide and have the potential of capitalizing on Samsung’s massive consumer reach across mobile phones and smart TVs.\\n\\nAbout NEXT Product\\n\\n\\nThe NEXT Product organization is a hyper-growth startup within Samsung NEXT. We’re a globally distributed product development team in search of builders and creators to help conceive, grow and scale new products and categories.\\n\\nSuccessful candidates, at all levels within the organization, will: approach all things team-first, take ownership and “be the change you seek”, have strong written and verbal communication skills, have high EQ, enjoy fast-paced, outcome-driven environments and be inspired to learn and explore daily both inside and outside of your field of expertise.\\n\\nThe Role\\n\\n\\nAs a Machine Learning Engineer, you will build Machine Learning systems fusing and analysing multi-sensory signals to solve real-world challenges in real-time. We are looking for a candidate that has industry experience with Computer Vision (Machine Learning) and has worked with large data sets and in real-time. This position is located in our New York office.\\n\\nRESPONSIBILITIES\\nWork closely with our Engineering and Data teams in NYC, San Francisco and Korea to build real-time solutions for Smart Spaces by applying Computer Vision methods to this complex domain.\\nBuild models for Human Activity Detection and Recognition, Object Detection and tracking and Scene Segmentation in real-time\\nDesign, develop, test, deploy, maintain and improve ML models/infrastructure and software that uses these models.\\nManage individual project priorities, deadlines and deliverables.\\nMINIMUM QUALIFICATIONS\\nMS degree in Computer Science or related quantitative field\\n5+ years of experience in one or more of the following areas: machine learning (CV)\\nExperience with machine learning frameworks\\nExperience working in an Agile environment and within a distributed team\\nPREFERRED QUALIFICATIONS\\nPh.D. degree in Computer Science or related quantitative field\\nResearch experience in Deep Learning.\\nAbout Samsung NEXT\\n\\n\\nWe partner with and build software alongside innovators to develop ideas into products, grow products into businesses and scale businesses globally.\\n\\nFounded in 2012, Samsung NEXT has four key functions in the global software ecosystem:\\nProduct - Building new software and services businesses, at scale.\\nVenture - Investing in early-stage startups to help entrepreneurs build and scale their businesses.\\nPartnerships - Helping startups successfully partner with the variety of businesses within Samsung.\\nM&A - Acquiring startups to connect and scale with our businesses.\\nSamsung is an EEO (Equal Employment Opportunity)/Veterans/Disabled/LGBTQ employer. We welcome and encourage diversity as we strive to create an inclusive workplace.',\n",
       "  \"About Us\\n\\n\\nNS1's mission is to manage the world's application traffic. We are the market leader in DNS and traffic management software and services, and our customers include the biggest properties and largest enterprises on the internet, such as Salesforce, LinkedIn, Squarespace, Pandora, Imgur, Yelp, Dropbox, and many more. Our modern DNS technologies enable optimized application delivery, couple via our APIs into the tooling and processes of today's DevOps organizations, and deliver reliability and performance at global scale. We operate a worldwide, highly tuned Managed DNS network, and also deliver our technologies to customers as single-tenant software deployments. We solve incredibly challenging problems on behalf of our customers, in the most mission critical parts of their stack.\\n\\nThe Role\\n\\n\\nNS1 is bringing data science to modern application traffic steering and traffic management. Our ideal candidates will be comfortable with (and excited about) taking on greenfield projects on a small but growing team. You should be capable of articulating the core problem statement, generating your own hypotheses, testing ideas independently and communicating your findings to a team of stakeholders including engineers. We value simplicity and efficiency, always starting with a first approximation before choosing more complex methods. The data sets are internet-scale (true big data). As such, ideal candidates are familiar with distributed computing frameworks and the unique challenges that arise with developing and deploying analytics at scale (streaming analytics, Bayesian inference, machine learning, etc).\\n\\nYour Skills\\n\\n\\nData scientists come from all backgrounds. Even if you do not meet all the requirements, send us a note telling us why you are interested. In general, you would be a strong candidate if you are familiar with more than a few of the following:\\nPython (required), Go, Scala, C++ (nice-to-have)\\nStatistical inference, modeling, or machine learning techniques\\nResearch and experiment design\\nJupyter Notebooks, Numpy, Pandas, Sklearn, or other relevant data science toolkits\\nData pipelining, ETL, data cleaning\\nGithub, Jira, Docker\\nDatabases and indexes including Redis, Postgres, InfluxDB, Elasticsearch\\nAWS: Kinesis, DynamoDB, Lambda, SQS\\nSpark, Kaftka, Sagemaker, Hadoop or other big data tools\\nNice to have:\\n\\n\\nPhD or other research intensive background\\nDomain expertise in networking, application architecture or traffic engineering\\nExperience working in an engineering-heavy culture (think Agile, Jira, DevOps, Unit Testing, Jenkins, etc)\\nExpertise in building big data platforms from the ground up\\nOur technology stack at NS1, and the many systems you'll have an opportunity to work with here:\\nOur globally distributed platform is comprised of many subsystems including:\\nCustom built DNS software that's deployed on physical hardware and an anycasted network that spans nearly 30 facilities globally\\nREST API, and Portal\\nDeployment automation, CI/CD, unit/integration testing\\nMonitoring, metrics collection and alerting\\nTraffic load balancing, filtering, and DDoS mitigation tools\\nMessaging, persistent DB and caching systems\\nOther technologies and integrations include:\\nLinux, Ansible, Docker & other container platforms\\nBGP, BPF/IPTables, SDN, packet analysis\\nMongoDB, Redis, RabbitMQ, SQL\\nPython (Twisted), Bash, C, C++14, React, Redux, D3\\nHadoop/HDFS/OpenTSDB, Grafana, Bosun\\nIntegrations with third party SaaS, APIs, and libraries, various Open Source projects including REST API clients and integrations\\nWorking @ NS1\\n\\n\\nWe're a fast-growing, well-funded startup based in the heart of New York City's Financial District with offices and team members around the world. Working at NS1, you'll come to understand our team is unique, both in and out of the workplace. We have PhDs, musicians, artists, and athletes working side by side, dedicated to delivering first class products. We're hardworking, but we're also a compassionate group. We understand that outside of NS1 is a world that places demands on our time. Our leadership team is dedicated to open and honest communication and we continuously strive to foster a culture of transparency, flexibility, and creativity.\\n\\nWe offer:\\ncompetitive compensation (salary and stock options)\\nmedical, dental, and vision\\ncommuter benefits\\n401k\\nflexible hours and time off\\nchoice of workstation\\nNS1 is an equal opportunity employer.\",\n",
       "  'About Us VIDEO\\n\\nFounded in 2013, Pivotal Software, Inc. combines our leading cloud-native platform, tools, and methodology to empower the world\\'s largest organizations to adapt to change and build great software. Our technology unleashes developer productivity, while fulfilling our mission to transform how the world builds software.\\n\\nYou\\n\\nAs a data scientist on Pivotal\\'s Data Science team, you\\'ll be working on a wide variety of data problems for a diverse range of clients. You will often be asked to learn new technologies and domains on the fly. You should be comfortable working under deadlines and making tough decisions. Consequently, you will frequently have to balance achieving an immediate goal with scalability and productionalizability.\\n\\nThe role offers room for personal and professional growth, and you won\\'t be working in isolation. Data Science at Pivotal is an encouraging and supportive team, where ideas and challenges are addressed collaboratively. We\\'re looking for the kind of person who will try and solve a problem on their own first, but isn\\'t afraid to ask for help or say \"I don\\'t know.\"\\n\\nUs\\n\\nThe Data Science team at Pivotal is primarily a consulting practice; we are tool agnostic, working with our customers to solve real world problems. Our customers, like us, are cross-disciplinary. We service engagements with use cases running from customer churn to optimization to detecting fraud and misconduct. We are not just doers, we are also educators and enablers.\\n\\nYour Day\\n\\nWhile there is no such thing as a \"typical day\", these are activities we frequently find ourselves doing:\\nWorking with clients to uncover and frame new opportunities for data science. Clients often come to us without a clear understanding of what we can do, so this is our chance to open their eyes to new possibilities for their businesses.\\nExploring client datasets, looking for actionable insights we can present.\\nEngineering features, training models, tuning hyperparameters and evaluating the results. We emphasize rigor, because data science done right at this stage leads to models that shine in production.\\nTaking the models we build into production. This is an exciting stage for anyone who likes collaborating with engineering teams and seeing their model become real when users interact with it.\\nHelping our clients develop their internal data science practices, from hiring and recruiting to data capturing, so that they can be successful when we hand off the project.\\nRequired Skills / Experiences\\nClear and empathetic communicator. You\\'ll be the one sharing your insights with clients and stakeholders at check-ins, documenting your work, and even explaining your model to client data teams as part of a handoff. As such, communication and empathy are essential parts of your toolkit.\\nAdvanced knowledge of statistical modeling and/or machine learning methods. These are the tools we need to go from analysis to prediction.\\nStrong programming skills. Left to our own devices most of us work in Python, but learning the client\\'s tech stack is an important part of the job.\\nStrong exploratory data analysis skills. Every engagement starts with an investigation of the data, and thorough EDA saves us a lot of headaches in the long run.\\nSome travel is expected, depending on location and skillset. We mostly work out of the Pivotal office closest to the client, but sometimes we have to be on site for an extended period of time.\\nAt least a bachelor\\'s degree in an analytical or technical field. This could be applied mathematics, statistics, computer science, operations research, economics, etc. Higher education welcome and encouraged.\\n\\nThis role will support US government clients that require US citizenship. Given this, US citizenship is required for you to apply.\\nDesired Skills / Experiences\\n2+ years of work in a data-centric field (data science or data engineering).\\nExperience with relational databases.\\nExposure and experience working in a Linux environment.\\nYou have a specialization in an area like NLP, optimization, or image processing.\\nHands-on experience working in a distributed computing environment or proven theoretical understanding of parallelism.\\nPivotal is an Equal Employment Opportunity employer that will consider all qualified applicants, regardless of race, color, religion, gender, sexual orientation, marital status, gender identity or expression, national origin, genetics, age, disability status, protected veteran status, or any other characteristic protected by applicable law.',\n",
       "  \"TED seeks an enthusiastic content-driven data analyst to shape our understanding of our digital audience and help develop strategy to drive engagement. We're seeking someone who has a proven track record leveraging data in a meaningful way to inform strategy, support team and org-wide goals, and drive the continued growth across platforms.\\n\\nIn this role, you'll pull and analyze large data sets, communicate actionable insights and takeaways to cross-org stakeholders via data visualization and dashboards, advise on recommended content/audience strategy, and help support ongoing KPI and goal maintenance.\\n\\nThe role sits within TED's newly formed Audience Development team and works collaboratively with analytics, editorial, product and partnerships teams.\\n\\nQUALIFICATIONS / SKILLS\\n2- 3 years of hands-on experience in related field\\nProven ability to leverage data to formulate a story and provide recommendations for action\\nProficiency in data analytics tools, including Google Analytics, Adobe Suite, Parsel.ly, Chartbeat or Sprout/Simply Measured\\nData visualization skills; experience with Tableau, Looker or a similar data visualization platform\\nAdvanced Microsoft Office skills (including Excel Report Builder, charting and analysis of large data sets, etc.)\\nAttention to detail + ability to detect and resolve data/analytics quality issues\\nFlexible and able to manage multiple assignments in a dynamic environment\\nMedia or journalism background preferred; experience with social media analytics and understanding of changing media landscape\\nDeep understanding of referral channels, such as search engines, social media, direct and emails.\\nRESPONSIBILITIES\\nManage, organize and report on data in a clear, concise and actionable format\\nBring data and insights to life through data visualization, storytelling and actionable recommendations\\nProduce daily, weekly, monthly and quarterly analytics reports detailing engagement across platforms, as well as ad hoc deep dives around larger goals + needs\\nAssist on reporting + tracking efforts around TED Recommends product\\nWork with cross-org data analysts to pair referral and content data with wider O&O reporting and goals\\nWork collaboratively with cross-functional teams to support social, editorial and product efforts\\nIdentify opportunities to optimize the tagging and data pipeline\\nWORKPLACE CONDITIONS / PHYSICAL, MENTAL AND VISUAL DEMANDS\\nGeneral: Office environment\\nPhysical: Repetitive movement of wrists, hands, and/or fingers\\nMental/Visual: Concentrated mental and/or visual attention; work involves performing tasks to very close accuracy and quality specifications\\nMachines, Tools, Equipment, Electronic Devices, Computer Software: Ability to use common office equipment such as telephone, computer and copier\\nComputer programs to include MS Office, Mac programs, Google Docs, functional related databases\\nTravel: Upon request\",\n",
       "  \"WHO WE ARE\\n\\n\\nThe Farmer's Dog was created to radically improve the $90 billion global pet food industry starting with a subscription service that sends freshly-made food directly to customers' doors. Long term, our aim is to simplify every part of pet care, bettering the lives of the animals who make our lives better. We recently raised our Series B and are backed by early investors of Warby Parker, Dollar Shave Club, Sweetgreen, and Glossier.\\n\\nJoin The Farmer's Dog team as we continue to figure out ways of bringing peace of mind to customers, health to their companions, and much-needed change to the way people feed and care for their pets.\\n\\n#mustlovedogs\\n\\nWHAT'S THE ROLE?\\n\\n\\nThe Farmer's Dog is looking for a Data Scientist specializing in natural language processing to join the Data Strategy & Insights team. You will be tasked with leveraging diverse data sets and your math & NLP skills to deepen and broaden the insights provided through our data as they relate to operations, finance, marketing, customer experience, and all aspects of our business.\\n\\nAs a Data Scientist, you will be a strong collaborator with the many amazing people at The Farmer's Dog.\\n\\nExample projects you could work on:\\nProvide insights to our CX / Customer Service teams on which issues most deeply affect our dogs and humans\\nHelp to predict issues before they arise so our team can resolve them seamlessly for our customers\\nWork with our massive data sets of customer feedback to help identify best areas of improvement\\nEXPECTATIONS:\\n\\n\\nYou should feel extremely comfortable with all aspects of\\nCNN\\nRNN\\nWord Embedding\\nFeature Engineering\\nSequence Modeling\\nForecast Modeling\\nData cleaning and wrangling\\nStrong familiarity with Tensorflow and/or Keras\\nTHE IDEAL CANDIDATE:\\n\\n\\nHave a look at our company and the other roles we are hiring. If this is your dream job, we encourage you to apply, regardless if the description below is you or not. Maybe we got it wrong? If so, show us.\\n\\nIn our mind, our ideal candidate might have a background similar to the following:\\nAn advanced degree in a quantitative or social sciences field\\nWork experience in a high tech company\\nWork experience in a retail or e-commerce business\\nA meaningful personal project\\nAbility to work in quick iterations, communicating strongly and openly between iterations.\\nNatural curiosity -- asking the next question and seeking to understand at the nuanced level to spark a new idea\\nPlays nicely with others (humans and dogs)\\nPrefer dogs to other species.\\nPERKS & BENEFITS\\nBrand new dog-friendly office in Greenwich Village (complete with free-roaming friendly dogs)\\nComprehensive Healthcare, Dental, and Vision\\nFlexible PTO and WFH policy\\nDiscounted fresh food for your pup\\nFresh breakfast, snacks and coffee for the humans\\nStrict daily belly rub quota\\nPS Adding in Deep Learning keyword here. If you found this job programmatically, let us know and we will fast track you ;)\",\n",
       "  'Description\\nPosition at Rockstar New York\\n\\nAt Rockstar Games, we create the games we would want to play ourselves.\\n\\nA career at Rockstar is about being part of a team working on some of the most creatively rewarding, large-scale projects to be found in any entertainment medium. You would be welcomed to a friendly, inclusive environment where you can learn, and collaborate with some of the most talented people in the industry.\\n\\nRockstar New York is seeking an experienced Data Scientist to join our Analytics practice and help advance our business intelligence capabilities. Successful candidates will work with analytics and product leadership to assure that the most relevant real-time and historical data is identified, tracked, analyzed, and made actionable across all of our games. This is a full-time permanent position based out of Rockstar’s unique game development studio in the heart of downtown Manhattan.\\n\\nWHAT WE DO\\nThe Rockstar Analytics team provide insights and actionable results to a wide variety of stakeholders across the organization in support of their decision making.\\nWe work together with a number of departments to design and implement data and pipelines.\\nWe collaborate as a global team to develop cutting-edge data pipelines, data products, data models, reports, analyses and machine learning applications.\\nRESPONSIBILITIES\\nPartner with analysts and live producers to identify strategic business questions, key metrics, and actionable insights.\\nProvide consumer-ready analysis to Analytics team leadership, live producers, product managers and partner groups.\\nAssure Rockstar’s ongoing competitive advantage through best-in-class business and game data analysis.\\nInitiate and carry out analytic experiments aligned with long-term, strategic initiatives.\\nCombine great data analytics skills with strong business acumen to provide insights that will drive continued success.\\nDesign, develop and deliver machine learning enabled solutions to address critical business or game questions.\\nDesign and build validation tests to assess the efficiency of the model (or algorithm) in place and provide strategic insights to stakeholders.\\nConduct proactive in-depth analysis and predictive modeling to uncover hidden opportunities.\\nDevelop mechanisms to objectively measure the performance of initiatives and propose recommendations for improvement.\\nDevelop frameworks, models, tools, and processes to ensure data influences decisions at all levels.\\nWork within a team of data analysts and engineers.\\nREQUIRED\\n3+ years in a data science or similar role in the marketing, finance, forensics or technology fields required.\\nExtensive knowledge of machine learning techniques such as k-NN, Naive Bayes, SVM, Decision Forests, Data Mining, Clustering, and Classification.\\nProficiency in statistics such as distributions, predictive modeling, data validation, statistical testing, regression.\\n2+ years of experience in machine learning/ statistical languages and systems such as Python, Matlab, R, SAS.\\nBachelor’s degree in Computer Science or related field, with a strong quantitative background.\\nAbility to develop and maintain good relations and communicate with people at all hierarchical levels.\\nStrong problem-solving skills.\\nAbility to reconcile technical and business perspectives.\\nAutonomy and entrepreneurship.\\nStrong team spirit.\\nPassion for Rockstar Games and our titles.\\nDESIRED\\n2+ years using SQL (or a SQL-like language) required, other programming experience highly preferred.\\nExperience with Vertica and Hadoop, an asset.\\nGraduate degree (MBA, MSc or Master’s, PHD), an asset.\\nGame industry experience strongly desired.\\nHOW TO APPLY\\n\\nPlease apply with a CV and cover-letter demonstrating how you meet the skills above. If we would like to move forward with your application, a Rockstar recruiter will reach out to you to explain next steps and guide you through the process.\\n\\nRockstar is proud to be an equal opportunity employer, and we are committed to hiring, promoting, and compensating employees based on their qualifications and demonstrated ability to perform job responsibilities.\\n\\nIf you’ve got the right skills for the job, we want to hear from you. We encourage applications from all suitable candidates regardless of age, disability, gender identity, sexual orientation, religion, belief, or race.',\n",
       "  \"Data Science at Bowery means building production-facing data products which automate the agricultural and operational functions in our network of indoor vertical farms. The Data & AI function at Bowery is central to the vision of the company as well as its day to day operation. You’ll work with our team of data scientists and analysts as well as agricultural scientists and agronomists who bring the domain expertise to our data-driven solutions.\\n\\nWhat you’ll do at Bowery\\nYou’ll create innovative quantitative solutions and advance the state of indoor farming alongside mechanical engineers, agricultural scientists, and the farm operations team.\\nYou’ll analyze historical data related to crop growth and environmental characteristics to forecast yields for future harvests and uncover insights about ways conditions can be optimized.\\nYou’ll use the key drivers of Bowery's business and agricultural science to design experimental frameworks to ensure data integrity and empower us to more easily analyze and act upon data from new trials.\\nYou’ll optimize work and production schedules to achieve the highest utilization of time and space in the farm.\\nYou’ll help shape the way our data science team does work - researching and making key decisions about what we build, how we build it, and which tools are best for solving our problems.\\nYou’ll work alongside software engineers to implement data processing and visualization systems that make data readily available and simplify how insights are communicated.\\n\\nWho you are\\nBA/BS in field which shaped your thinking around quantitative problems. Advanced degree and/or 4+ years industry experience preferred.\\nYou have a strong mathematical background in probability, statistics, and optimization algorithms.\\nYou have insight into building solutions to our fundamental agricultural challenges, either from training in a related field (e.g. biostatistics, genetics, biology) or through a deep passion in the area.\\nYou are fluent in a scripting language such as Python or R for analysis and modeling.\\nYou are proficient with query languages (SQL/Postgres) and maybe even dream in SQL\\nYou have a deep understanding of and have applied various machine learning techniques for solving real-world problems.\\nYou must be able to translate business and agricultural questions into data science problems, solve those problems, and interpret the output back into clear outcomes.\\nYou are excited about how advancements in machine learning and IoT can work in harmony to improve the state of the planet.\\nYou are confident taking ownership of projects from start to finish and enjoy the process of turning nebulous ideas into reality.\\nYou believe that teams succeed and fail together and take responsibility for ensuring the success and safety of your teammates.\\n\\n\\nAbout Bowery\\nBowery is growing food for a better future by revolutionizing agriculture. Our modern farming company combines the benefits of the best local farms with advances made possible by technology to grow produce you can feel good about eating. BoweryOS, our proprietary software system, uses vision systems, automation technology, and machine learning to monitor plants and all the variables that drive their growth 24/7. Because we control the entire process from seed to store, Bowery farms use zero pesticides, 95% less water, and are 100+ times more productive on the same footprint of land than traditional agriculture. Bowery produce is currently available at select Whole Foods and Foragers stores in the Tristate area, and featured on the menus of Tom Colicchio’s New York restaurants Craft and Temple Court. Based in New York City, the company has raised over $120 million from leading investors including GV, General Catalyst, GGV Capital, First Round Capital, Temasek and Almanac.\\n\\nWe are an equal opportunity employer. We welcome people of different backgrounds, experiences, abilities and perspectives. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status.\",\n",
       "  'Responsibilities\\nCombine top analytical skills with knowledge of our businesses to build and produce models to\\nimpact our business in positive ways.\\nWork with other data scientists with a broad range of analytical expertise and subject matter\\nexperts to deliver data products and provide business insights through quantitative analysis\\n\\n(Predictive Modeling, Optimization, Visualization, etc.)\\nAssist in the testing and implementation of the models and analysis created\\nExtract data from various applications and systems, in particular large relational databases,\\nmanipulate, explore data and build models using quantitative, statistical and visualization tools.\\nMentor and manage Data Scientists and Engineers\\nPresent findings, both formally and informally, to audiences at all levels of the organization\\nIn collaboration with the product management, content development, and engineering teams,\\nidentify opportunities to leverage data science techniques in order to create new or improve existing\\n\\nproducts\\nEvangelize the use and potential of data science within the organization and in particular the\\nexecutive, product management and engineering teams\\n10+ years of experience in relevant areas of computer science, including applied machine learning,\\ndeep learning, NLP or related disciplines\\nStrong statistical background and experience implementing systems in production\\nStrong foundation in coding skills relevant to data science, e.g., Pig, Hive, SQL, Python, etc.\\nExtensive experience solving analytical problems using quantitative approaches\\nExperience developing production-quality data products using the results of quantitative research\\nTrack record of successfully managing a diverse team of highly skilled individuals.\\nMust be able to communicate effectively with (non-technical) senior executives internally and\\nexternally. Presentation skills are essential.\\nFamiliarity with modern data pipelines and ETL practices\\nprovided by Dice',\n",
       "  \"Introduction\\nClient Technical Specialists (CTP) are the technical experts and advisors to clients, IBM sales teams and/or IBM Business Partners. As a CTP you understand the client's business requirements, technical requirements and/or competitive landscape. You apply your business insights, build and maintain client relationships, incorporate hardware, software and services into client-valued solutions and ensure client readiness for the implementation of technical solutions. This is an opportunity to shape the future for both IBM and its clients. Start your journey now!\\n\\nYour Role and Responsibilities\\nYour Role & Responsibilities:\\nAs part of an entrepreneurial team, you will be the Subject Matter Expert on Data and AI Statistical models and how they apply to business problems. Led by a solution architect, you will advise on and help implement models in Machine Learning, Optimization, Neural Networks, and Artificial Intelligence such as Natural Language, Transfer Learning, Deep Learning and other quantitative approaches. You will also leverage deep skills and best practices to provide expertise and leadership to help design IBM Data Science and AI solutions that will help our clients drive technology benefits and business outcomes across industries. You will work with cutting edge technologies such as Watson, as well as Open Source approaches such as Python and Jupyter notebooks as well as with a passionate team of people who are driving the innovation and digital transformation to cross-industry enterprise clients with the adoption of IBM Data Science and AI. An ideal candidate will be familiar with Design Thinking, Statistics, building Supervised and Unsupervised machine learning models, and data cleansing techniques using various utilities and programming techniques.\\n\\nKey Responsibilities:\\nRun and statistically evaluate statistical models such as Machine Learning, Optimization, Neural Networks and Artificial Intelligence\\nPartner with Scrum Masters, Product Owners, Solution architects and peer data scientist and development data engineers to create solutions to meet business and technical opportunities\\nUnderstand and communicate technical advantages and tradeoffs between Data Models\\nExplore and develop new technical skills and industry practices while absorbing professional knowledge quickly and using demonstrated interpersonal skills to be an effective ambassador for IBM Data Science and AI\\nUse exceptional communication skills and with input from product management, development, and architecture thought leaders, work to deliver high quality end-to-end Solutions at Scale in response to the identified business requirements from our clients; ensure the results are statistically valid\\ncldpakat\\n\\nRequired Technical and Professional Expertise\\nTechnical degree in Computer Science or another field relevant to Data Science\\n1+ years of experience working with Machine Learning, Optimization, Neural Networks and/or Artificial Intelligence\\n1+ years of experience with a Data Science programming language such as Python or R\\nDeep understanding of Statistics\\nFluent in English\\nAbility to Travel 75% and conduct Client Facing/Technical Solutions\\nPreferred Technical and Professional Expertise\\nExperience with Jupyter Notebooks\\nDeep understanding of Statistical Machine Learning Models with Python or R\\nSupporting Relevant business domain knowledge such as Finance or Health Care\\nAbout Business Unit\\nDigitization is accelerating the ongoing evolution of business, and clouds - public, private, and hybrid - enable companies to extend their existing infrastructure and integrate across systems. IBM Cloud provides the security, control, and visibility that our clients have come to expect. We are working to provide the right tools and environment to combine all of our clients data, no matter where it resides, to respond to changing market dynamics.\\n\\nYour Life @ IBM\\nWhat matters to you when youre looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBMs greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.\",\n",
       "  \"Company Description\\n\\nRunning a small business is hard. Getting insurance for your small business is even harder. It takes forever, the process is antiquated, and one wrong decision can be disastrous.\\n\\nAt Attune, we’re changing small business insurance in a big way.\\n\\nAt the forefront of the insuretech industry, we are rebuilding small business’s access to insurance from scratch, making it instant, easy, and safe. How? Instead of requiring that a business answers hundreds of questions (seriously), our sophisticated platform aggregates the necessary data from different sources, and then uses incredibly advanced analytics to create tailored products that can be delivered in mere minutes, not days or weeks.\\n\\nWe are at the beginning of our journey and are looking for innovators who are curious and excited to drive change.\\n\\nBacked by Two Sigma, AIG, and Hamilton Insurance Group, we have the horsepower and partners to make a big impact. Unlike other start-ups, Attune has the funding, expertise, and support to modernize and move quickly. If you are energized by making the complex, simple; the time consuming, easy; and the antiquated, tech-enabled, we want you on our team.\\n\\nDisrupting an industry through data, technology and speed isn't easy – but the challenges we’ll tackle together are some of the most rewarding you’ll experience in your career.\\n\\nJob Description\\n\\nYou'll join a growing analytics team that drives Attune’s business model forward by solving some of the most complex analytical problems in the industry. You will partner with the actuarial, underwriting, claims, product, revenue and customer service functions within Attune to frequently test new hypotheses, implement learnings and create edge.\\n\\nResponsibilities include:\\nWorking on initiatives like: improving insurance risk predictions, analyzing drivers of claims, understanding user engagement patterns, finding the best potential customers to contact, reducing load on our customer service team, and identifying abnormal broker behavior\\nCreating new predictive models in insurance pricing, pricing elasticity, reserving, claims analytics and etc.\\nExploring new datasets to see how they can add value to the business\\nHelping to incorporate analyses and models into daily workflows for other departments\\nContributing to our ETLs, internal web applications, and BI dashboards\\nDoing quick ad-hoc analyses and supporting engineers to troubleshoot issues\\nQualifications\\nExperience working in an statistics, analytics or data focused role\\nExperience with predictive modeling in general\\nFamiliarity with advanced statistical analysis methods (e. g., bayesian statistical modeling, hierarchical modeling and etc) in addition to machine learning regression/classification/clustering techniques\\nWorking knowledge of bayesian software language (STAN, JAGS, or OpenBUGS) a plus\\nWe are flexible about what tools you've worked with, but comfort in either Python or R is a must, as is basic SQL.\\nWe don't require experience with deep learning or with tools for working with massive data sets\\nProperty Casualty insurance experience and knowledge is a plus, but not a must\\nTraits for success:\\nYou enjoy working with programmers and other data scientists to constantly improve the integrity and automation of your work. You are frequently sharpening your software skills.\\nYou are well organized and can handle many unrelated requests without losing track of them.\\nYou are quick with numbers and back-of-the-envelope calculations.\\nYou have strong written and verbal communication skills.\\nYou are a hands-on problem solver who is comfortable with ambiguity and loves a fast-paced environment.\\nYou have strong interpersonal skills and are capable of building relationships to drive success.\\nAdditional Information\\n\\nWhat we offer you:\\nAn opportunity to change the small business landscape.\\nA great working environment that lives continuous improvement and encourages sharing ideas and taking risks to find better ways of doing things.\\nA culture that promotes great relationships both inside the office and outside through activities in our community and company sponsored intramural clubs and events.\\nEquity compensation.\\nMedical, dental, and vision from day one and 401(k) matching.\\nFully stocked kitchens with free snacks & drinks.\\nAttune Insurance Services, LLC is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected Veteran status.\",\n",
       "  'Job Number: R0070875\\n\\nData Scientist\\n\\nThe Challenge:\\n\\nAre you excited at the prospect of unlocking the secrets held by a data set? Are you fascinated by the possibilities presented by IoT, machine-learning, and artificial intelligence advances? In an increasingly connected world, massive amounts of structured and unstructured data open up new opportunities. As a data scientist, you can turn these complex data sets into useful information to solve global challenges.\\n\\nWe have an opportunity for you to use your leadership and analytical skills to improve Booz Allen’s commercial practice. You’ll work closely with clients to understand their questions and needs, then dig into their data-rich environment to find the pieces of their information puzzle. You’ll mentor teammates, conduct data analysis and modeling, design data-driven solutions and approaches for clients, and use the right combination of tools and frameworks to turn that set of disparate data points into objective answers to help our commercial security clients make informed decisions. You’ll provide your customer with a deep understanding of their data, what it all means, and how they can use it. Join us as we use data science for good in commercial Cybersecurity.\\n\\nEmpower change with us.\\n\\nYou Have:\\n\\n-3+ years of experience with data science, mathematics, statistics, CS, economics, or data-driven problem-solving\\n\\n-Experience with conducting analytics, data transformation, or visualization using tools, including Python, Java, JavaScript, or R\\n\\n-Experience in collaborating with multi-disciplinary teams\\n\\n-Experience with working in an Agile development environment, including providing analytics or CS support\\n\\n-Ability to operate independently and manage staff\\n\\n-Ability to travel extensively up to 75% of the time\\n\\n-BA or BS degree\\n\\nNice If You Have:\\n\\n-Experience with Cybersecurity or fraud domains preferred\\n\\n-Experience in a consulting or client-facing environment\\n\\n-Possession of excellent oral and written communication skills, including using presentation expertise to convey complex ideas to client and internal staff\\n\\n-Possession of excellent problem-solving skills, including in a collaborative environment\\n\\n-MA or MS degree\\n\\nBuild Your Career:\\n\\nAt Booz Allen, we know the power of analytics and we’re dedicated to helping you grow as a data analysis professional. When you join Booz Allen, you can expect:\\naccess to online and onsite training in data analysis and presentation\\nmethodologies, and tools like Hortonworks, Docker, Tableau, and Splunk\\na chance to change the world with the Data Science Bowl—the world’s premier data science for social good competition\\nparticipation in partnerships with data science leaders, like our partnership with NVIDIA to deliver Deep Learning Institute (DLI) training to the federal government\\nYou’ll have access to a wealth of training resources through our Analytics University, an online learning portal specifically geared towards data science and analytics skills, where you can access more than 5000 functional and technical courses, certifications, and books. Build your technical skills through hands-on training on the latest tools and state-of-the-art tech from our in-house experts. Pursuing certifications? Take advantage of our tuition assistance, onsite boot camps, certification training, academic programs, vendor relationships, and a network of professionals who can give you helpful tips. We’ll help you develop the career you want, as you chart your own course for success.\\n\\nWe’re an EOE that empowers our people—no matter their race, color, religion, sex, gender identity, sexual orientation, national origin, disability, veteran status, or other protected characteristic—to fearlessly drive change.\\n\\n</br>',\n",
       "  'We are looking for a Data Scientist that will help us discover the information hidden in vast amounts of data, and help us make smarter decisions to deliver even better products. Your primary focus will be in applying data mining techniques, doing statistical analysis, and building high quality prediction systems integrated with our products.\\n\\nRequirements\\n\\nResponsibilities\\nEnhancing data collection procedures to include information that is relevant for\\nbuilding analytic systems\\nProcessing, cleansing, and verifying the integrity of data used for analysis\\nDoing ad-hoc analysis and presenting results in a clear manner\\nSelecting features, building and optimizing classifiers using ML techniques\\nProviding custom analytics for individual clients as needed.\\nSkills and Qualifications\\nExcellent understanding of machine learning techniques and algorithms, such as Linear and logistic regressions, Naïve Bayes, SVM, Random Forests, etc.\\nExperience with data science toolkits, such as Numpy & Scikit-Learn.\\nExperience with data visualization tools.\\nProficiency in using query languages such as SQL & Hive\\nExperience with NoSQL databases, such as Cassandra and Elastic search\\nGood applied statistics skills, such as distributions and statistical testing,\\nGood scripting and programming skills in languages such as Python or Java\\nHighly motivated with a positive attitude and desire for continuous learning\\nData-oriented personality\\nGreat communication skills',\n",
       "  \"Join the A.I. company tackling some of the most consequential problems facing the world today. Gro Intelligence is a fast-growing, mission-driven data and analytics company dedicated to understanding the interconnected global markets for all agricultural products. From the food we eat to the clothing we wear and the gasoline in millions of cars across the globe, agriculture touches our daily existence like few other industries. And it's being completely upended by climate change, trade wars, finite global resources, fickle public policies, a growing population and ever-changing human tastes (e.g. organic produce, plant-based proteins, etc). Come help us understand the complex interplay of these various forces and their ripple effect on global markets.\\n\\nWith offices in both New York and Nairobi, and the financial backing of prominent investors including TPG Growth and Data Collective, Gro is a diverse team of more than 70 technologists, scientists, and business professionals. Our team is growing rapidly and we're looking for highly motivated, intellectually curious individuals with a shared commitment to building software that impacts the world on the most basic level.\\n\\nGro has been featured prominently alongside the company's founder and CEO in several recent outlets including Recode Decode with Kara Swisher, Fast Company's Most Creative People, Fortune's Most Powerful Women in Business, and Vanity Fair's Inside the Hive with Nick Bilton.\\n\\nWhat You'll Do:\\nDevelop original computational models to predict agricultural and environmental performance and conditions using geospatial, weather, crop, trade, and pricing data.\\nDevelop systems and techniques for very large scale computations\\nWork with scientists, analysts and engineers to integrate local and sector specific factors and work closely with specialists in the field to further internal product development and external collaborations\\nDo original research on machine learning models and algorithms, working with closely with research scientists\\nUnderstand and contribute to the advancement of the state of the art in academic and industry research\\nWhat You Have:\\n3+ years related experience\\nGraduate degree in Engineering, Computer Science, Math, Statistics, Physics or a related field\\nExperience with big data computation (parallel processing, distributed storage)\\nStrong computer science fundamental (data structures and algorithms)\\nStrong understanding of statistics, mathematics\\nExperience with natural language processing\\nAbility to work in Python, Java, C++\\nFamiliarity with statistical modeling tools (R, MatLab)\\nAbility to work collaboratively with team members across functional roles and strong communication and leadership skills.\\nPassion for solving high impact problems with data driven tools.\\nWho You Are:\\nSmart with excellent problem solving skills\\nCurious, love for turning ideas into reality\\nProductive, self motivated\\nCreative, collaborative, diligent\\nStrong attention to detail with the ability to understand the big picture\\nLearner, interested in new skills and adapting to new technologies\\nAbility to work with geographically distributed and culturally diverse team\\nGro Intelligence is proud to be an equal opportunity employer and will consider all qualified applicants regardless of color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, actual or presumed belonging to an ethnic group, or any other legally protected status. If you have a disability or special need that requires accommodation, please let us know.\\n\\n]]>\",\n",
       "  'A Bioinformatics Analyst/Data Scientist position is available in the NIH-funded research laboratory studying human lung biology, aging and disease in the Department of Medicine, Weill Cornell Medical College, to implement advanced bioinformatics approaches to analyze big data, including gene expression data generated using single-cell and bulk RNA-sequencing.\\n\\nQualifications and requirements:\\nBachelor’s or Master’s degree in Computer Science, Computational Biology, Bioinformatics or related field is required. PhD degree and post-doctoral experience a plus.\\nProficiency in Python, R and Linux is required.\\nExperience integrating and analyzing high-dimensional data is required.\\nKnowledge of molecular biology, cell biology and genomics is preferred.\\nExperience analyzing genomic and/or transcriptomic data, including single-cell and bulk RNA-sequencing data is preferred.\\nExcellent analytical and problem-solving skills are required.\\nExcellent written and verbal communication skills are required.\\nSuccessful candidates are expected to be highly creative and have a strong motivation to contribute to discoveries relevant to human lung disease (COPD, asthma, IPF, lung cancer). Successful candidates will work closely with postdoctoral researchers, scientists and students in a research laboratory settings and have the opportunity to actively participate in basic science and translational research projects focused on stem cell mechanisms of human lung biology and disease. Current methodologies in the lab include single-cell and bulk RNA-sequencing of human lung samples obtained from individuals of different age, with and without lung disease, identification and mapping of disease pathways to specific tissue domains using spatial gene expression and advanced imaging methodologies (including imaging CyTOF), generation of patient-derived stem cell pathway libraries and 3D organoids.\\n\\nTo apply, please send your CV and the contact for three references to: Renat Shaykhiev, MD, PhD, Department of Medicine, Weill Cornell Medical College, New York, NY.\\n\\nJob Type: Full-time',\n",
       "  \"Vroom.com is a venture-backed, fast-growing start-up focused on revolutionizing the car buying experience. Our approach is unique in that we recondition pre-owned vehicles to a high standard, sell online, and deliver anywhere across the US. We have experienced tremendous growth in our first 5 years of operation and have become a disruptive force in the automotive industry. Vroom is an exciting, accelerating workplace, and there's no better time to join the team than right now.\\n\\nVroom.com is on a mission to transform the used car market into a modern, online and data-driven industry! We are constantly exploring new ways to improve our car pricing algorithms, to optimize our inventory and merchandising models, and to leverage data to better understand our customers. To help out, we are looking for experienced Data Scientists to join our Data Science and Analytics Team. The ideal candidates will be looking to apply their passion for data and technology to drive business-critical decisions. This is a role with high visibility into the business and with strong potential for growth within the company.\\nResponsibilities\\nOwn end-to-end data workflows and develop deep domain expertise on the underlying actors and behaviors manifested through data\\nImplement and validate predictive models and create and maintain statistical models to optimize inventory and with a focus on the used car market pricing, vehicle attributes and demand indicators\\nIdentify standard and novel data sources to assess consumer-related demand levels and other market indicator\\nAcquire, organize and leverage insights derived from that data\\nCollaborate with data engineers to rapidly deploy newly developed models\\nDevelop and maintain ETL pipelines to optimize the integration and the use of all available data sources\\nPropose solutions and strategies to business challenges\\nQualifications\\nStrong and demonstrated technical skills and a desire to continue to expend on those skills\\nA quantitative/technical/analytics education (Masters/PhD in a quantitative discipline preferred)\\nContemporary data engineering / data science technical skills (e.g. SQL, R, Python) and a desire to try out the cutting edge\\nExperience in supply chain planning, demand modeling, inventory optimization, and/or AdTech technology and industry preferred\\nProficiency in applying statistical/machine learning methods in the real world\\nBenefits\\n\\nThis full-time role offers competitive compensation; health, dental, and vision insurance through United Healthcare; a 401k plan; fully company-paid short term disability, long term disability, and life insurance; access to a healthcare concierge service with virtual visits; and 15 annualized days of paid vacation.\\n\\nBut our biggest benefit is being part of a low-ego, high performing team that's transforming the used car market into a modern, online and data-driven industry. We are looking for people who want to be a part of a contemporary startup culture. What gets us out of bed is working with talented people on a mission that matters.\\n\\nTo Apply\\n\\nIf you think you might be who we’re looking for, apply below with your resume and a cover letter telling us why you think you’d be a great addition to the team.\",\n",
       "  'JobDescription :\\nThe Senior Data Scientist will be a part of the S&P Global Market Intelligence (SPGMI) Data Science team.\\n\\nThe Role:\\n\\n• Discover insights and identify opportunities through the use of statistics, algorithms, data mining and visualization techniques\\n• Build and evaluate models, make predictions, gather results, and communicate findings to stakeholders\\n• Use advanced business knowledge and advanced machine learning techniques to acquire, combine & transform multiple datasets to solve a business use case\\n• Collaborate with engineering and product teams to create and build strategic models that drive product improvements while maintaining cost efficiency\\n• Help in building and maintaining re-usable machine learning and model validation procedures for the rest of the team to use\\n\\nExperience and qualifications:\\n\\n• Bachelor’s Degree in Mathematics, Statistics, Computer Science, Engineering, Operations Research or related fields preferred (Master’s degree an advantage)\\n\\n• 2+ years practical experience with statistical analysis and creating complex models, preferably in the financial services sector\\n\\n• Excellent analytical and problem solving skills\\n\\n• Advanced experience in at least one data analysis/data transformation package (R, Python, Alteryx)\\n\\n• Exposure to one or more data discovery, data visualization tools\\n\\n• 2+ years of hypothesis testing, web analytics and python scripting\\n\\n• Experience with Machine Learning\\n\\n• Ability to remain focused and to think logically in a fast-paced environment\\n\\nGrade 10 (For Internal Purpose)\\n\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.\\n\\nJob ID :\\n250069\\n\\nPosted On :\\n10-17-2019\\n\\nLocation :\\nNew York, NY US',\n",
       "  \"Two Sigma is a different kind of investment manager. Since 2001, we have used data science and technology to derive insights that forecast the future and discover value in markets worldwide. Our team of scientists, technologists and academics looks beyond traditional finance to understand the bigger picture and develop creative solutions to some of the world’s most challenging economic problems. Our work spans across markets and industries, from insurance and securities to private investments and new ventures.\\n\\nTwo Sigma is looking for Data Scientists from a variety of backgrounds to help propel and enhance its data-driven investment initiatives. As a Two Sigma Data Scientist, you will explore a breadth of challenges: identifying timely and unique data sets, diving deep into a diverse set of data domains, visualizing and exploring underlying data drivers, and developing data set features and forecasts.\\n\\nYou will take on the following responsibilities:\\n\\nApply statistical analysis & modeling techniques with finance intuition to datasets large and small, advance existing initiatives and open opportunities to pursue new and previously unexplored research topics across a wide variety of industries and domains\\nOperate and extend the data science platform to deliver production-grade data curation and analysis services\\nOwn end-to-end data workflows and develop deep domain expertise on the underlying actors and behaviors manifested through data\\nVisualize and explore data sets to enable the ideation and generation of new, predictive feature\\n\\nYou should possess the following qualifications:\\n\\n3+ years of experience in data analysis or similar role\\nAdvanced degree in Computer Science is preferred\\nExperience applying statistical methods (distribution analysis, classification, clustering, etc.)\\nStrong coding skills with data-frames are a prerequisite, example platforms include Pandas, R, Matlab, and Apache Spark Demonstrated experience highlighting innovation, creativity, and intuition, e.g the ability to laterally identify other sources of useful information and think 'outside the box'\\nPrior Experience in finance is not required\\n\\nYou will enjoy the following benefits:\\n\\nCore Benefits: Fully paid medical and dental insurance premiums for employees and dependents, competitive 401k match, employer-paid life & disability insurance\\nPerks: Onsite gyms with laundry service, wellness activities, casual dress, snacks, game rooms\\nLearning: Tuition reimbursement, conference and training sponsorship\\nTime Off: Generous vacation and unlimited sick days, competitive paid caregiver leaves\\n\\nWe are proud to be an equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics.\",\n",
       "  \"YOUR ROLE\\n\\nCompIQ is building a future where predicting your career path and potential earnings is easy and fun. To do this, we use artificial intelligence and machine learning to help companies and industry professionals make informed decisions in their career. To solve such a large problem, we’ve put together an incredible team and are selectively adding to it.\\n\\nYou will play a mission-critical role in helping advance the intelligence machine that powers CompIQ's products.\\n\\nYou will have the opportunity to constantly learn new things, take on responsibilities and help propel a successful startup to even further success. Companies and consumers alike will rely heavily on what you build! If challenges excite you, and you are ready for a large one, let us know.\\n\\nSKILLS AND EXPERIENCE\\n\\n4+ years of experience or 2+ years of experience if you hold a graduate degree in a relevant field (statistics, mathematics, (astro)physics, computer science, etc) of hands on data science using Python including:\\n\\n• Data wrangling: acquire, clean, analyze and present data\\n\\n• Text mining and Natural Language Processing including regular expressions, packages TM, NLTK, etc.\\n\\n• Machine Learning: clustering, regression/classification\\n\\n• Statistics\\n\\nDESIRED SKILLS/EXPERIENCES\\n\\nExperience with databases, APIs, web scraping, financial modelling, economics, software engineering standards (git, collaboration and project management software, software development lifecycle understanding, general computer science principles), science background\\n\\nLocated in or willing to relocate to New York City\",\n",
       "  \"ABOUT US\\n\\nSimon Data was founded in 2015 by a team of successful serial entrepreneurs. We're a data-first marketing platform startup, and we approach our work seriously; we tackle problems in a scrappy and disruptive fashion, yet we build for scale to support our clients at big data volume.\\n\\nWe are the first and only enterprise customer data platform with a fully-integrated marketing cloud. Moving beyond the limitations of both categories, Simon's platform empowers businesses to leverage enterprise-scale big data and machine learning to power customer communications in any channel. Simon's unique approach allows brands to develop incredible personalization capabilities without needing to build and maintain massive bespoke data infrastructure.\\n\\nOur culture is rooted in organizational transparency, empowering individuals, and an attitude of getting things done. If you want to be a valuable contributor on a team that cultivates these core values we would love to hear from you.\\n\\nTHE ROLE\\n\\nAs a Data Scientist at Simon, you will be working as part of a collaborative/user focused team and be responsible for designing and building smart systems that drive revenueour statistical models are at the core of our product, and will only become more so as we continue to develop and add features. We take an approach to ML that is data-first, and requires principled modeling decisions: we don't believe in theory-crafting models before we have collected the data that will power them, as well as built out the business process that will continue to generate that data. In the model building process, we prioritize interpretable models whose training and performance yield insights about the underlying process, along with optimizing the selected objective.\\n\\nOur technologies of choice are Python in the backend and React/Redux in the frontend, and our tech stack includes Django, MySQL, Redshift, S3, DynamoDB, and Elasticsearch storage, asynchronous tasks over RabbitMQ, and distributed data processing over Elastic MapReduce and Spark.\\n\\nWHAT YOU'LL DO\\nBuild ML products that leverage Simon's extraordinary data access to drive real business value\\nBuild high-quality statistical models by executing the entire model-building process, including data cleaning, feature extraction, model selection, and predictive validation\\nContribute to the tooling and interfaces used to support the data science process at Simon\\nRepresent Simon DS in conversations with stakeholders at our client companies\\nAdvance Simon as a thought leader in data science, by writing blog posts and papers, and presenting at industry conferences\\nGuide internal product and technology strategy by representing data science perspectives and requirements in conversations with your peers\\nQUALIFICATIONS\\nPh.D. in Statistics/Machine Learning, or equivalent\\nExcellent communication of statistical concepts to expert & non-expert audiences\\nBroad and up-to-date knowledge of machine learning models (and their performance characteristics) for classification and regression tasks\\nSpecific experience designing and building machine-learning models\\nFluency in at least one statistical coding environment (numpy/pandas, R, etc.)\\nComfort coding in at least one non-statistical language (e.g. Python or Java, not R or Matlab)\\nFluency in SQL\\nProduction-level software engineering experience is a plus\\nExpertise in causal inference, experiment design, reinforcement learning, and related fields is a plus\\nDiversity\\n\\nWe're proud to be an equal opportunity employer open to all qualified applicants regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or expression, Veteran status, or any other legally protected status.\",\n",
       "  \"THIS IS BLUE APRON\\n\\nAt Blue Apron, our mission is to make incredible home cooking accessible to everyone—from the novice cook just getting started to the experienced chef. Each week we send hundreds of thousands of customers all the pre-measured and perfectly proportioned ingredients they need to prepare delicious and healthy meals at home. We introduce our members to new ingredients, flavors, and cooking techniques with seasonally inspired recipes that are always delicious, fun and easy to prepare. We make fresher, healthier food available at better prices by rethinking the grocery supply chain from the table all the way back to the farm.\\n\\nWHO'S IN THE KITCHEN\\n\\nBlue Apron's Analytics and Business Insights team develops critical metrics, actionable insights and insightful forecasts on our rapidly expanding customer base and the broader meal kit marketplace. This team also powers business decisions around complex supply chain and logistical challenges.\\n\\nWHAT'S ON THE MENU\\n\\nThe Data Scientist will drive efforts to iteratively improve Blue Apron's long and near term forecasting methodologies and company-wide testing methodologies. This will include efforts to size risk and opportunity, diligently test new testing infrastructure, and determine the success of strategies in marketing, digital and physical products changes.\\n\\nCandidates will be motivated self-starters who are comfortable navigating large datasets, building and/or interpreting predictive models, partnering with technical teams, and understanding customer demand signals. They will be intimately familiar with advanced statistical testing methodology and interpretation. They will be comfortable presenting findings through both verbal and written formats. Candidates will feel confident in their abilities to influence decision making at the highest levels, understand requirements in working with cross functional teams, and be confident in presenting to individuals of all levels throughout the company.\\n\\nKey responsibilities include:\\nWork closely with Demand Forecasting & Operations, Engagement Analytics, Acquisition Analytics, and Data Engineering teams to enable continuous implementation of sophisticated predictive statistical models on big data by developing new data sources, testing model enhancements, running computational experiments, and fine-tuning model parameters\\nFormalizing assumptions about how demand forecasts are expected to behave, creating definitions of outliers, developing methods to systematically identify outliers\\nFinding root causes of forecast inaccuracy for Near-term and Long-term Forecasting\\nOwns the design, development, and publication of automated A/B testing reports, including the accuracy of the underlying data\\nCounsel business owners for hypothesis and success metrics definition as well as result interpretation and business impacts\\nHave a deep understanding of statistics and testing methodologies\\nNECESSARY INGREDIENTS\\nBachelor's or Master's degree in a quantitative field such as Statistics, Applied Mathematics, Physics, Engineering, Computer Science, or Economics\\n2-5 years of experience in time series forecasting and in applying descriptive statistics, building predictive models and visualization to solve real-world problems\\n2+ years of experience with data querying languages (e.g. SQL) and statistical/mathematical software (e.g. R, Python)\\nProven ability to convey rigorous technical concepts and considerations to non-experts\\nStrong statistics knowledge and experience to strengthen the validity of A/B tests, including statistical distributions and testing/sampling methodologies.\\nIn depth knowledge of web and mobile analytics (Google Analytics or Amplitude) a plus\\nNatural curiosity and desire to learn\\nBlue Apron provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Blue Apron complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\",\n",
       "  \"A BIT ABOUT OUR DATA SCIENCE & ANALYTICS TEAM\\n\\nKinship's Data Science & Analytics team is core to our strategy. We're using digital, data and customer insights to transform our business by finding answers to problems that we've often never asked ourselves before. Our vast data assets are being combined to build a 360° view of Pets and Pet Owners to not only power Kinship's businesses but also influence the next great ideas in the Petcare space. This role will be key in helping us understand the power of our data, and how this translates into value for our company, Pets, and Pet Owners. Frequently the projects will be ambiguous, but that's part of the fun; you will determine the best way to leverage our data to tell the right story for any given audience.\\n\\nHOW YOU'LL CREATE A BETTER WORLD FOR PETS\\nApply data science approaches to understand and predict pet and pet owner behaviors\\nUse machine learning techniques, visualizations, & statistical analysis to gain insight into various data sets – some readily available, and some you create and curate yourself\\nCollaborate with internal and external teams to ensure we focus on pet-centric product and service recommendations\\nSupport new pet technology businesses and partners by generating actionable insights from our data assets\\nDevelop compelling stories that provide insight into the drivers of business performance and Pet/Pet Owner behavior\\nWHO WE NEED TO CHART THE FUTURE OF PET CARE\\n\\nWe strive to hire people who are:\\n\\nOptimistic. Those who's boundlessly energy and enthusiasm for what's next shines through in everything they do. We seek to work with people who are intrinsically happy, and who will drive our vision and purpose while managing the complexities of our businesses.\\n\\nPurposefully Inquisitive. Those who are courageous and use their deep business insights to cultivate innovation. We want the trailblazers in tech. Those who are entrepreneurs at heart, ask the tough questions, adapt quickly to new situations, and analyze data in new ways to push our big ideas forward.\\n\\nOpen to All. Those who are inclusive leaders, committed to learning, and leveraging our differences as strengths. We hire people who are naturally collaborative and thrive in a flat and flexible organization. Those who are thoughtful communicators, and seek to foster meaningful relationships across our community of diverse partners.\\n\\nAnd for this role, we hope you have the following skills we require to round out our team:\\nExceptional written & verbal communication, coupled with critical thinking skills.\\nTruly inspired by, and want to live, our purpose of creating a better world for pets.\\n1+ years' experience in a data science role handling varied and complex data\\nProficiency in machine learning modeling and statistical thinking (random forest, decision trees, supervised and unsupervised modeling, etc.)\\nHands-on experience with Python is required; Familiarity with PySpark is also desirable\\nComfortable with ambiguity, with a passion for collaboration to achieve objectives\\nA Bachelor's degree in quantitative field (economics, statistics, business, computer science) or equivalent experience\\nIf you also had these experiences, you'd knock it out of the dog park:\\nPassion for growing and strengthening a business using data driven approaches\\nFamiliarity with cloud-based computing services e.g. AWS, Databricks, etc.\\nEnjoys explaining how models and systems work to both non-technical and technical stakeholders.\",\n",
       "  \"FactSet’s product suite of smart analytics and unique data empower the world’s leading financial service professionals to make more informed decisions every day. At our heart is an inclusive community unified by the spirit of going above and beyond. Our philosophy is to embrace diversity, and that our best ideas can come from anyone, anywhere, at any time. We continuously look ahead to advance the future and technology of our industry, by rolling up our sleeves to solve tough problems together, and by learning from our successes, as well as our failures.\\n\\nBeing a software engineer at FactSet is to shape the future of investments technology. Our engineers use cutting edge technologies including machine learning, natural language processing, predictive analysis, and cloud computing to solve some of the investment community’s greatest challenges – relying every step of the way on some of our most creative minds to create sleek and intuitive UIs that make our products among the industry’s easiest to use.\\n\\nWe’re looking for hard-working and out-of-the-box thinkers from all software engineering disciplines to bring new perspective and fresh ideas to our team. Engineers are aligned with specific teams where they design and implement applications for integration within the FactSet product suite and deployment to investment professionals worldwide.\\n\\nOur engineers find the right balance between FactSet's flexible environment where everyone can contribute individually, yet at the same time cultivate a community where they can depend on each other for help, learning, and development.\\n\\nResponsibilities:\\nAnalyze data sets and protoype as many experiments as necessary to converge to the optimal practical solution\\nWork with New York-based Alpha Pro machine learning team in order to expand the predictive modeling services, and work with state-of-the art stack of data modeling paradigms\\nGet responsibility right from the first day and the unique chance to enrich the environment of key metrics Portware’s predictive services rely on\\nExtract, transform, and load data\\nWrite reusable research code and prototypes for predictive models\\nTake part in all aspects of the software life cycle, including specification, analysis, design, development, unit testing, product deployment and support\\nSearch, read, understand, and communicate relevant academic papers related to your projects\\nParticipate in brainstorming sessions for new ideas\\nRequired skills:\\nPursuing a MSc/PhD in a quantitative field, such as Applied Mathematics, Computer Science, Engineering, Physics, Operations Research, Econometrics, Stochastic Finance.\\nExcellent analytical skills\\nHands-on design and implementation of machine learning solutions\\nInterest in the financial markets as well as previous experience in financial services\\nSounds knowledge and application of advanced statistical methods and time series analysis\\nProficiency with R, Python, and Matlab\\nStrong computer science fundamentals (data structures & algorithms) and solid object-oriented design skills\\nExcellent communication skills\",\n",
       "  'Title: Scientist Data Analytics\\nJob ID: BM765165812\\nLocation: New York, NY\\n\\nWe are seeking a Scientist Data Analytics who will play a vital role in assessing and developing analytic methods for viral vector and cell-based therapeutics and advancing their transformative cell and gene based therapies for life-threatening disorders.\\n\\nThe primary responsibility of this position is to establish and conduct exploratory and routine analytic testing to define critical characteristics of cell and gene-based therapeutics. The successful candidate will focus on developing reproducible and robust analytic techniques with the associated proper documentation to facilitate testing and control of high quality viral vector and cell-based therapies.\\n\\nResponsibilities:\\nSupport cross-functional testing for viral vector and cell products\\nQualifying new and established test methods that assess the quality attributes of the product to support process development and quality control activities\\nIdentify new and novel methods to characterize and test gene therapy products\\nProvide analytical expertise both internally and externally including supporting comparability/characterization programs\\nTechnical writing to support literature or regulatory submissions, medical and other program development aspects of internal and partnered assets\\nQualifications:\\nBachelor or Masters degree in Biological Sciences, Bioengineering, or an associated discipline with more than 3-years relevant experience in analytics development within a biopharmaceutical or pharmaceutical environment\\nStrong background in cell biology and vectorology (retrovirus/lentivirus and AAV) with experience in development and implementation of analytic methods.\\nTrack record of skilled analytic development for a variety of biologics and multiple targets\\nDemonstrated skills in designing, executing, and interpreting experiments\\nExperience in managing third-party contractors, CMOs and CTOs, including the ability to motivate and inspire action\\nFor more information about TEEMA and to consider other career opportunities, please visit our website at www.teemagroup.com',\n",
       "  'Job Description:\\n\\nData Scientist\\n\\nResponsibilities\\n\\n- Manipulate and analyze large amounts of data to discover trends/patterns and draw meaningful insights\\n\\n- Use predictive modeling and statistical techniques to enable smarter business processes\\n\\n- Devise and utilize algorithms and models to mine big data stores, perform data and error analysis to improve models, and clean and validate data for uniformity and accuracy\\n\\n- Communicate analytic solutions to stakeholders and implement improvements as needed to operational systems\\n\\n- Create and maintain optimal data pipeline architectures\\n\\nRequirements\\n\\n- Excellent understanding of machine learning techniques and algorithms and their real-world advantages/drawbacks\\n\\n- Knowledge in applied statistics, such as distributions, statistical testing, regression, etc.\\n\\n- Excellent scripting and programming skills such as R and Python. Familiarity with Scala, Java or C++ is an asset\\n\\n- Experience using big data frameworks (e.g. Apache Hadoop, Apache Spark, etc.)\\n\\n- Proficiency in using query languages such as SQL, Hive, Impala, etc.\\n\\n- Experience building and optimizing \"big data\" data pipelines, architectures and data sets\\n\\n- Experience with data visualization tools is a plus\\n\\nShift:\\n\\n1st shift (United States of America)\\n\\nHours Per Week:\\n\\n40',\n",
       "  'MORE ABOUT THIS JOB\\n\\n\\nConsumer and Investment Management (CIMD)\\n\\nThe Consumer and Investment Management Division includes Goldman Sachs Asset Management (GSAM), Private Wealth Management (PWM) and our Consumer business (Marcus by Goldman Sachs). We provide asset management, wealth management and banking expertise to consumers and institutions around the world. CIMD partners with various teams across the firm to help individuals and institutions navigate changing markets and take control of their financial lives.\\n\\nConsumer\\n\\nConsumer, externally known as Marcus by Goldman Sachs, is comprised of the firm’s digitally-led consumer businesses, which include our deposits and lending businesses, as well as our personal financial management app, Clarity Money. Consumer combines the strength and heritage of a 150-year-old financial institution with the agility and entrepreneurial spirit of a tech start-up. Through the use of machine learning and intuitive design, we provide customers with powerful tools that are grounded in value, transparency and simplicity to help them make smarter decisions about their money.\\n\\nRESPONSIBILITIES AND QUALIFICATIONS\\n\\n\\nJob Summary & Responsibilities\\n\\nAs part of the decision and data science function for Marcus, you will be at the forefront of a data-driven initiative to optimize decision making. This role will draw upon your knowledge of programming and mathematics. In this role you will:\\nRapidly prototype early-stage solutions and design / evaluate predictive models and advanced algorithms to drive business decisions throughout the customer lifecycle (prospecting, acquisition, underwriting, fraud, collections, enhancing customer experience)\\nUnderstand the systems and the business processes that populate those systems with data\\nCarry out data processing including statistical analysis, variable selection, and dimensionality reduction, custom attribute engineering, as well as the evaluation of new data sources\\nLeverage methods from diverse disciplines such as machine learning, deep learning, artificial intelligence, statistical modelling, information theory, information retrieval and other areas to gain customer insights, draw conclusions and work with business partners to put those insights into action\\nParticipate in data architecture decisions and partner with technology teams to implement models/algorithms in production\\nHelp document your assumptions and methodologies, as well as carry out validation and testing to facilitate peer reviews and independent model validation\\nThink strategically on a higher level, proposing new business metrics or suggesting alternatives, creating highly interpretive models that imply new context and new semantics for data\\nBasic Qualifications\\nBS/MS or PhD in a quantitative field - Applied Mathematics, Physics, Engineering, Computer Science\\nQuantitative background including an understanding of probability and statistics\\nStrong programming background in compiled or scripting languages (C/C++, Python, Java, etc.)\\nAbility to explain complex models and analysis to diverse audience\\nPreferred Qualifications\\nExperience in data science, advanced statistics\\nFamiliarity with statistical computing languages or packages (R, Matlab, numpy/scikit-learn, Tensorflow, Keras, Pytorch)\\nFamiliarity with advanced ML models - neural networks (feed forward, CNNs, RNNs, LSTM), Hidden Markov Models, random forests, SVMs, multivariate analysis, clustering, dimensionality reduction or participation in Kaggle data science competitions\\nExperience with distributed computing (Hadoop, Spark)\\nExperience in a start-up business or a new business line within a larger organization\\nABOUT GOLDMAN SACHS\\nThe Goldman Sachs Group, Inc. is a leading global investment banking, securities and investment management firm that provides a wide range of financial services to a substantial and diversified client base that includes corporations, financial institutions, governments and individuals. Founded in 1869, the firm is headquartered in New York and maintains offices in all major financial centers around the world. © The Goldman Sachs Group, Inc., 2019. All rights reserved Goldman Sachs is an equal employment/affirmative action employer Female/Minority/Disability/Vet.',\n",
       "  'About Cubist\\nCubist Systematic Strategies is one of the world’s premier investment firms. The firm deploys systematic, computer-driven trading strategies across multiple liquid asset classes, including equities, futures and foreign exchange. The core of our effort is rigorous research into a wide range of market anomalies, fueled by our unparalleled access to a wide range of publicly available data sources.\\nJob Description\\nWe are passionate about data. We collaborate to build elegant, effective, scalable and highly reliable solutions to empower predictive modelling in finance.\\nCubist’s data services group is looking for a Data Scientist to join our dedicated team. Our group is responsible for the timely delivery of comprehensive and error-free data to some of the most demanding and successful systematic Portfolio Managers in the world.\\nThis exceptional individual will be a member of a small team of Data Scientists who play a vital role in ensuring the smooth day-to-day implementation of a large research infrastructure, and the live production trading of billions of dollars of capital across global capital markets, including equities, futures, options and other financial instruments.\\nJob Responsibilities\\nIdentification of new data sets\\nEngaging with vendors to understand characteristics of datasets\\nBuilding processes and technology tools to ingest, tag and clean datasets\\nAnalysis of datasets to generate descriptive statistics and propose potential applications of data\\nResearch of potential “alpha signals” for presentation to Portfolio Managers\\nMonitoring and enhancing the automated data collection and cleansing infrastructure\\nResearch on new technologies for improved data management and efficient retrieval\\nDesirable Candidates\\nPh.D. in computer science, mathematics, physics, statistics or another disciplines involving rigorous quantitative analysis techniques\\nAt least 1 year of experience as a Data Scientist, quantitative researcher or in a similar role\\nExperience working with large data sets, including classification, regression, distribution analysis, and predictive modeling\\nExperience applying statistical tests to large data sets\\nProgramming skills in SQL, TSQL, SQL Server or PL-SQL\\nProgramming skills in Python and at least one of C#, C++, or Java\\nFinancial industry experience preferred but not required\\nExperience dealing with intraday, tick and order book data a plus\\nStrong problem solving skills\\nIntellectual curiosity and a love of learning\\nAttention to detail and a love of process\\nStrong oral and written communication skills',\n",
       "  \"As a Data Scientist, you'll utilize advanced quantitative & statistical analysis techniques to drive business model innovation for Via, and work closely with our senior management to help drive decisions.\\n\\nResponsibilities and Duties\\nAdeptly interpret and utilize mass quantities of data to generate innovative hypotheses & insights, and present these insights to the different stakeholders\\nUse sophisticated statistical methods to solve problems, leveraging up-to-date academic research and tools\\nQuantitatively test hypotheses about customer and driver behavior using large sets of proprietary data; leverage results to increase conversion and retention at every touch point\\nDesign and implement novel experiments to better understand current operation as well as expansion to new markets\\nQualifications\\nObsessed with data; analytical and rigorous, with a thorough understanding of statistics and machine learning\\nExtraordinary communicator with demonstrated writing and editing skills.\\nPassionate about elegant visualization; you understand the importance of graphic techniques in communicating a quantitative idea effectively\\nDeep understanding of business concepts within strategy, operations, and marketing\\nPhD in statistics, machine learning, physics, math, systems biology, or highly quantitative fields in social sciences, including 2+ years of graduate-level research experience (or the equivalent).\\nHave experience with predictive modeling and statistical analysis techniques in a business environment\\nMastery in some or all of the following: SQL, Python, R, and Tableau\\nAt Via, we're on the cutting edge of mobility. We're building revolutionary technology that's changing the way people get around. It's on-demand transit on a mass scale, a smarter transportation that's friendly to our planet. From on-demand autonomous shuttles in Australia to dynamically routed bus fleets in Singapore, our sophisticated operating system is powering transportation in the world's biggest cities and is sought after by prominent transportation players globally. We've provided more than 50 million shared rides already, and we're growing at an astonishing rate. We have offices in more than 15 countries and deployments in more than 50 markets, with a goal of hundreds of deployments within the next two years. If you're someone who relishes wearing multiple hats, never backs down from a challenge, and loves getting things done, we'd love to hear from you!\\n\\nVia offers above market compensation packages and benefits, including equity, health insurance, and relocation assistance.\\n\\nVia is an equal opportunity employer.\",\n",
       "  'Do you have the analytic skills to find insight in a pile of healthcare data? Do you thrive in a fast-paced environment? Milliman’s NYC Health Practice is seeking professionals from a variety of analytic and clinical backgrounds to join our vibrant staff. We offer challenging projects, world-class resources, expert colleagues, a collaborative environment, and diverse client projects. Work, learn, and grow with us.\\n\\nMilliman’s NYC Health Practice consults to clients on disease processes, treatments, costs, healthcare reform, risk, and payer systems. Our clients span all sectors of the health industry. Our services include analyzing financial arrangements, innovative contracts, disease states, therapeutic value, benefit design, healthcare reform, and health systems. We have brought unparalleled actuarial, analytical, and operational expertise to these clients.\\n\\nMilliman is the world’s premier actuarial consulting firm with over 3,000 employees and $800 million in revenue. We attribute our 60+ years of continuous growth to our environment where independent and talented individuals succeed as outstanding professionals. We have a collegial, rigorous, supportive and non-bureaucratic environment.\\n\\nWe seek energetic and imaginative professionals to join our client-oriented team at our Penn Station, New York City location. Successful candidates will have the potential to become project/client leaders and become recognized as expert senior consultants.\\n\\nPrimary roles\\nHands on advanced data analytics and project management\\nApply innovative techniques to large claims datasets to analyze a wide array of healthcare-related topics\\nInvestigate discrepancies between medical concepts and their manifestation in claims datasets\\nContribute to hypothesis development for quasi-experimental designs\\nAssist junior colleagues with the development of their programming and analytic skills\\nRole in client management that will increase over time\\nQualifications:\\n\\nRequired:\\nAt least five years of experience analyzing very large medical insurance claims with SAS and/or SQL\\nUnderstanding of medical coding systems (CPT, ICD-9, DRG, etc.)\\nNew York City location; this job requires working with colleagues in our office\\nLimited travel—working at client site is rare\\nKnack for prioritizing and completing multiple projects and tasks\\nStrong oral and written communication skills\\nStrong problem solving and analytical skills\\nAbility to work both independently and as part of a team\\nExperience or knowledge of the healthcare industry and payer systems\\nPreferred:\\nExperience in insurance, finance, or outcomes research\\nExperience scrubbing large, non-standardized claims datasets\\nExperience with MarketScan and/or Medicare (or similar) databases\\nPharmD, RN, MD, MPH, MS, PhD, FSA, ASA or similar background\\nRequirements\\nCapable of carrying out a given task with all details necessary to get the task done well\\nPharmD, RN, MD, MPH, MS, PhD, FSA, ASA or similar background\\nExperience using advanced excel functionality (VBA, macros), SAS, and/or SQL in a professional capacity\\nUnderstanding of medical coding systems (CPT, ICD-9, DRG, etc.) \\\\nExperience or knowledge of the healthcare industry and payer systems \\\\n\\nExperience with MarketScan and/or Medicare (or similar) databases',\n",
       "  \"DIRECT CLIENT REQUIREMENT\\n\\nJob Title: Data Scientist\\n\\nDuration: 6 Months\\n\\nLocation: New York, NY\\n\\nJob description:\\n\\nOur healthcare client has an immediate need for a Data Scientist for a contract role to be based in NYC office.\\n\\nThe ideal candidate will build data science solutions at scale, and to provide thought leadership & give presentations as a Subject Matter Expert in advanced statistical techniques and mathematical analyses.\\nDevelop new capabilities to deliver business solutions applying data science algorithms\\nDetermine how to leverage data science, machine learning, and other analytical techniques to improve product features and capabilities.\\n5+ years related experience with healthcare data analytics, statistical analysis, predictive modeling, machine learning techniques, and visualization tools.\\nExcellent problem solving skills, critical thinking and conceptual thinking abilities Advanced, in-depth specialization in mathematical analysis methods, predictive modeling, statistical analyses, and big data technologies such as Python, R, Hadoop or Hive.\\nRequired Qualifications of the Senior Data Scientist\\nEducation: The Senior Data Scientist has to have a bachelor's degree in Statistics, Machine Learning, Mathematics, Computer Science, Economics, or any other related quantitative field. An equivalent of the same in working experience is also acceptable for the position.\\nExperience: A candidate for this position must have had at least 5 years of working experience working with in data science capacity within a fast-paced and complex business setting, preferably working as a Senior Data Scientist. The candidate will also have experience working with natural language processing as well as experience working with machine learning libraries, for example, xgboost, OpenCV, sklearn, among others.\\nThe candidate must also have had substantial experience working on strategy or full-life cycle data science as well as experience working with data mining tools such as SAS, Python, SPSS, and R. A suitable candidate for this position will also have had experience in data mining and predictive modeling inclusive of linear and non-Linear regression, logistic regression, and time series analysis models.\\nThe candidate will also have had experience working with deep learning algorithms and large datasets as well as experience working with unstructured data and experience cleaning and manipulating data.\\nCommunication Skills: Communication skills are an absolute necessity for the Senior Data Analyst both in written and verbal form. Communication skills are imperative for the Senior Data Analyst in his managerial position in regard to the conveyance of information and instructions down the line to junior data science personnel, which will determine their performance and the effectiveness with which they execute their duties.\\nThe Senior Data Scientist will also be required to have exceptional communication skills in his collaborative role and in the delivery of clear, engaging, and understandable reports. The Senior Data Scientist must be capable of tailoring complex messages into simplified and business applicable material for key stakeholders and senior data science management.\\nMS Office Software: A suitable candidate will have exceptionally good skills in the use of Ms Word, Ms Excel, PowerPoint, and Outlook, all necessary for the creation of both visually and verbally engaging reports and presentations, for senior data science management and key stakeholders.\\nThe candidate must also have demonstrated exceptionally good skills in SQL server reporting services, analysis services, domo, integration services, NoSQL, Tableau, Salesforce, or other data visualization tools.\\nTechnological Savvy/Analytical Skills: A candidate for this position will have exceptionally good computer skills and be all-around technologically adept. He will also have a strong foundation in data structures, algorithms, statistics, and machine learning as well as be highly proficient in Java, C++, or other equivalent languages.\\nA suitable candidate for this position must additionally have strong SQL skills, Teradata, Greenplum, and be highly skilled in working with platforms, such as Hadoop.\\nInterpersonal Skills: The Senior Data Scientist must have exceptional interpersonal skills that make him better suited for the position. He must be a result-oriented individual, be a self-motivated and proactive needing minimal supervision, be a strategic and creative thinker, have an ability to handle multiple simultaneous projects, be highly organized able to prioritize and meet tight deadlines, have a keen eye for detail, have an ability to work comfortably in a group/collaborative setting, take accountability for business and team performance, have an ability to stay calm and composed in times of uncertainty and stress.\\nPeople Skills: A suitable candidate will also be a likeable, relatable, and approachable individual who is able to form strong bonds with other people. This must be couple with confidence, which will together inspire trust in his senior and stakeholders who will trust in his insights and judgments and inspire the same in junior data scientist who will readily follow in his directives.\\n\\nAbout us:\\n\\nSince 2002, APN Consulting has been inspiring success in IT through meaningful connections between employers and candidates. We are trusted by a vast array of companies, from small enterprises to some of the world s most trusted brands to present the best talent for their contract, contract to hire and full-time positions. To learn more, please visit us online at www.apnconsultinginc.com\\n\\n- provided by Dice\",\n",
       "  \"Revlon has developed a long-standing reputation as a color authority and beauty trendsetter in the world of color cosmetics and hair care. Since its breakthrough launch of the first opaque nail enamel in 1932, Revlon has provided consumers with high quality product innovation, performance and sophisticated glamour. In 2016, Revlon acquired the iconic Elizabeth Arden Company and its portfolio of brands, including its leading designer, heritage and celebrity fragrances. Today, Revlon's diversified portfolio of brands is sold in approximately 150 countries around the world in most retail distribution channels, including prestige, salon, mass, and direct to consumer. Revlon is among the leading global beauty companies, with some of the world’s most iconic and desired brands and product offerings in color cosmetics, skin care, hair care, hair color and fragrances under brands such as Revlon, Revlon Professional, Elizabeth Arden, American Crew, Almay, Cutex, Mitchum, Elizabeth Taylor, Christina Aguilera, Britney Spears, Juicy Couture, Curve, Shawn Mendes and John Varvatos.\\n\\nResponsibilities:\\nWe are looking for a highly motivated and experienced data scientist to drive data-driven strategy and product decisions. You will be responsible for extracting actionable insights from the usage of our web and e-commerce apps as well from sales, marketing and manufacturing datasets\\nModel lead and customer behavior to direct marketing and sales strategy, e.g. customer segmentation, predict Net Promoter Score, identification of potential churn risk and upsell opportunities.\\nAnalyze e-commerce data to identify opportunities to improve conversion. Run A/B testing to test hypotheses and optimize e-commerce platform\\nSegment users, design and surface key metrics (e.g user retention), analyze feature usage, identify associations between mobile and web usage\\nIdentify opportunities to improve customer support. Alert on emergent reliability issues based on support interactions, cluster patterns of failure e.g. using NLP, work with manufacturing team to predict issues based on self-test data\\nBuild dashboards and presentations to surface insights to leadership\\nExperience:\\n3 + years of data science experience\\nKnowledge of statistics including hypothesis testing\\nKnowledge of supervised and unsupervised ML techniques for data science: clustering, association-rule mining, prediction, anomaly detection\\nAble to create compelling data visualizations, using Power BI or similar tooling\\nExperience with at least one of the following data analysis software: Python, R\\nExperience with Python data science ecosystem: pandas, numpy, scipy, Jupyter Notebooks/Labs\\nAdvanced SQL user\",\n",
       "  'Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.\\n\\nFounded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 6,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.\\n\\nJob Title:\\n\\nAdvanced Analytics Consultant\\n\\nResponsibilities:\\n\\n· Deliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data space.\\n\\n· Working closely with project teams to build predictive models for a wide variety of applications, including sales forecasting, customer analytics, pricing analytics, text mining, and optimization & simulation.\\n\\n· Be a part of client engagements and interact face to face with clients\\n\\n· Gather models’ requirements, design experiments and analyses, query and manipulate data, build and deploy predictive models, and interpret the outcomes for consumption by client leadership.\\n\\nQualifications:\\n\\n· 3+ years of consulting or industry experience in the data science field\\n\\n· Deep hands-on experience with at least one commercial analytic tool (e.g. SAS, SPSS, Alteryx, or Rapidminer) AND one open source analytics language (e.g. R, Python, or Weka)\\n\\n· Hands-on experience in coding/programming\\n\\n· Experience working with databases, data modeling, ETL, and Hadoop technologies\\n\\n· Building a wide variety of analytical models, including (but not limited to) decision trees, random forest, linear & logistic regression, market basket, neural networks, naïve Bayes, and support vector machines\\n\\n· Extracting, cleansing and visualizing data\\n\\n· Excellent problem solving skills & attention to detail\\n\\n· Ability to translate complex models into visuals and actionable business insights\\n\\n· Excellent communications and presentation skills; proficient in PowerPoint\\n\\n· Highly self-motivated to deliver both independently and with strong team collaboration.\\n\\n· Ability to creatively take on new challenges and work outside comfort zone.\\n\\n· Master’s degree in a Statistic/Quantitative discipline preferred, but not required\\n\\nSlalom is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.',\n",
       "  'Blink Health is a well-funded healthcare technology company on a mission to make prescription drugs more accessible and affordable for everyone. We\\'re scaling up in a highly complex vertical to change the way Americans access the prescription drugs they need.\\n\\nOur proprietary platform and supply chain allows us to offer everyone whether they have insurance or not amazingly inexpensive prices on over 15,000 medications. With the addition of telemedicine and home delivery for prescriptions, Blink is providing a life-changing experience for people all over the country and fixing how opaque, unfair and overpriced healthcare has become. We are a highly collaborative team of builders and operators who invent new ways of working in an industry that historically has resisted innovation. Join us!\\n\\nSuccess:\\nYou are a data scientist who, above all, is excited to build products that give our customers the best possible experience purchasing prescription medications. You have professional experience working on a team turning ideas into reliable, maintainable code.\\nThe Data Team is a small team building the Big Data infrastructure at Blink.\\nThe team is responsible for architecting and building infrastructure, frameworks and tooling to enable data driven decisions; developing reports, dashboards, and metrics to provide accurate and timely information; and supporting various product and business groups with recommendation and in-depth analyses.\\nOur data platforms are built using tools available on AWS including Redshift, Data Pipeline, Spark, Looker.\\nYou will collaborate closely with engineers, analysts, and business stakeholders across the organization to provide accurate, timely data and efficient, impactful insights.\\nYou will also be working with external vendors and services. You will work with our senior analyst taking ownership of our self-service analytics tool, Looker.\\nYou will be the liaison between marketing, engineering, all business teams and the data team. By developing and maintaining Blink\\'s source of truth business logic data layer, you will have a direct, visible, and profound effect upon a data-driven organization that is revolutionizing the way people pay for prescription medicine.\\nHow to achieve success/acumen:\\n\\nAll Blinkers are expected to operate with our value of \"Good Giving\" in mind. Our culture is infused with the dedication and enthusiasm of employees who continuously strive to make a difference. Here\\'s how you will do that in this role.\\n\\nGood Execution - Do your best work\\nDevelop and maintain data products and analytics infrastructure\\nOptimize queries for performance\\nWork with internal customers to develop optimized models and views for their needs\\nGuide business analysts on high-quality efficient SQL\\nDevelop business analysis solution in our self-service analytics tool\\nGuide the rest of the organization on best practices\\nWork on in-depth analyses and discover data value\\nGood Owner - Be the CEO of your role\\nHelp the team ship new products and features successfully with the right skills, mentorship and organizational resources to deliver amazing experiences for our patients\\nCoach, mentor and develop engineers to reach individual goals while fostering cross-functional collaboration and product success\\nTake responsibility for the long term success of product projects and people\\nGood Learning - Learn something new every day\\nDemonstrate curiosity and an interest in learning new techniques and improving upon best practices to stay up-to-date with current and emerging trends\\nLead by example putting new ideas into action, failing fast and learning from each experience\\nGood Feedback - Consider the perspective of others\\nListen actively and respond effectively through a variety of channels\\nGive and receive candid and constructive feedback\\nPromote trust and encourage teamwork allow the product team to do their best work\\nDesired experience:\\n2+ years experience having worked in a Data Science role.\\nYou have experience working with an analytical platform.\\nYou are an expert in SQL.\\nYou are very interested in learning about MPP databases. If you worked with one, that is a big plus.\\nYou are very interested in becoming a great data modeler. If you already are, that is great.\\nYou are comfortable in a fast-paced, agile environment.\\nCan communicate effectively with other engineers, designers, product managers, and anyone else at Blink\\nCollaborative and Communicative - you love being part of a diverse team, building consensus, and establishing credibility. You are a Class-A communicator: curious, thoughtful, clear, and (mostly) correct.\\nBA/BS degree from top institution; MBA or CompSci degree preferred but not required.\\nWhy Join Us:\\n\\n\\nAt Blink, we put humans first. We want everyone at Blink to be able to do the best work of their lives. We are a relentlessly learning, constantly curious and aggressively collaborative cross-functional team dedicated to inventing new ways to improve the lives of our customers.\\n\\nLearn more:\\nBlink Website\\nBlink Pharmacy App for Android\\nBlink Pharmacy App for iOS\\n\\nWe are an equal opportunity employer and value diversity of all kinds. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.',\n",
       "  \"Hello, World! Codecademy has helped over 45 million people from around the world upgrade their careers with engaging, accessible, and flexible education on programming and data skills. We provide over 200 hands-on interactive lessons ranging from Python to R to Javascript and everything in between. Our learners have gone on to start companies, new jobs, and new lives thanks to what they've learned with Codecademy, and we're thrilled to be working to take that impact to the next level.\\n\\nCodecademy was started in 2011 by two college students in a dorm room at Columbia that were frustrated by the huge gap between education and employment. A few years later, we are a rapidly growing, diverse team of 75+ in SoHo, NYC. We've raised over $40m in venture capital funding from top investors including Union Square Ventures, Kleiner Perkins, Naspers, Y Combinator, and more.\\n\\nIf you want to help build a business that impacts tens of millions of people each year and helps them lead better lives, join us!\\n\\nWHAT YOU'LL DO\\n\\n\\nAs a Data Scientist focused on Analytics and Experimentation, you will work on an impactful team to analyze our millions of learners. We capture terabytes of data on how users engage with our platform. As Codecademy continues our rapid growth, we want to build a data-informed culture that uses hypothesis testing, experimentation, and exploratory analysis to guide our decision-making process.\\n\\nYou will join a small but growing team of Data Scientists. Our work is in high-demand from all corners of Codecademy. We work on a variety of problems and have a real impact on the business and product. If you have a proven background in data and you are excited about making code education accessible, we want to hear from you!\\nApply exploratory data analysis and causal inference to answer complex questions about our users.\\nCollaborate across teams to help scope out analyses, through a combination of experimentation (A/B testing) and quantitative user research.\\nDesign experiments and evaluate results to test and iterate on new product ideas.\\nPerform deep dives into our data to build understanding around our business.\\nWork with our data science and engineering teams to maintain data integrity.\\nMentor and consult with a cross-functional team of data scientists, engineers, and product managers.\\nWHAT YOU'LL NEED\\n3+ years of industry experience in a data science, analytics, or research role. You have strong data intuition and knowledge of using data science best practices to drive impact.\\nExpert SQL - we use Redshift. Able to write clean and efficient queries on massive datasets.\\nApplied experience with statistical programming languages - R or Python preferred.\\nUnderstanding of statistical methods and when to use them (hypothesis testing, experiment design, sampling).\\nStrong written and verbal communicator. Comfortable working with loosely defined research problems.\\nWHAT WILL MAKE YOU STAND OUT\\nBackground in experimentation and measurement. We're looking to streamline our experimentation reporting framework and would love your help.\\nA workflow involving reproducible methods and version control - Github, Docker.\\nExperience automating dashboards with business intelligence tools - Looker, Tableau.\\nPassionate about teaching the world to code. Empathy for our learners, such as a background in education or past experience using our site.\\nInteresting questions you might work on\\nWhat are some different types of patterns in user behavior?\\nWhat do we do when we are unable to conduct an experiment?\\nCan we predict whether a learner will renew their subscription?\\nHow do we improve the relevance of our course recommendations?\\nHow can we scale our existing processes? (experiment reporting framework, forecasting)\\nAt Codecademy, we are committed to teaching people the skills they need to upgrade their careers. Codecademy aims to educate a richly diverse demographic of learners with our product and in order to accomplish this, we believe our team should reflect that rich diversity. Our company celebrates diversity in all of its forms-- race, gender, color, national origin, marital status, sexuality, religion, veteran status, age, ability, disability status-- and works to create an inclusive workplace where people of all backgrounds and beliefs are empowered to better their futures.\",\n",
       "  'ABOUT HOPPER\\n\\nAt Hopper, we’re on a mission to make booking travel faster, easier, and more transparent. We are leveraging the power that comes from combining massive amounts of data and machine learning to build the world’s fastest-growing travel app -- one that enables our customers to save money and travel more. With over $235M CAD in funding from leading investors in both Canada and the US, Hopper is primed to continue its path toward becoming the go-to way to book travel as the world continues its shift to mobile.\\n\\nRecognized as the fastest-growing travel app by Forbes and one of the world’s most innovative companies by Fast Company two years in a row, Hopper has been downloaded over 40 million times and has helped travelers plan over 100 million trips and counting. The app has received high praise in the form of mobile accolades such as the Webby Award for Best Travel App of 2019, the Google Play Award for Standout Startup of 2016 and Apple’s App Store Best of 2015.\\n\\nTake off with us!\\n\\nTHE ROLE\\n\\nAt Hopper, every dataset tells a story. Do you have what it takes to decipher the clues? bit.ly/2q6U8dq\\n\\nWe’re looking for a data-savvy individual to join our team as a Data Scientist to support data-centric product development to create consumer-focused research content based on our real-time feed of billions of flight search results, along with an archive of several trillion data points.\\n\\nYou may be a great fit for our team if you are excited about exploring huge (and sometimes messy) data sets and finding effective ways to simplify and communicate the results to a non-technical audience.\\nIN THIS ROLE, YOU WILL:\\nFrame and conduct complex exploratory analyses needed to deepen our understanding of our users. Partner with product, business and strategy teams to contribute to product improvements and initiatives\\nUse machine learning and big data tools on tremendously large and complex data to deepen our data-driven advice and personalized modeling\\nConduct research into various aspects of our business and employ statistical and modeling techniques when appropriate to make recommendations to non-technical stakeholders\\nA QUALIFIED CANDIDATE HAS:\\nA degree in Math, Statistics, Computer Science, Engineering or other quantitative disciplines\\nExtremely strong analytical and problem-solving skills\\nProven ability to communicate complex technical work to a non-technical audience\\nA strong passion for and extensive experience in conducting empirical research and answering hard questions with data\\nExperience with relational databases and SQL, especially Hive\\nExperience working with extremely large data sets\\nExperience in Pandas, R, SAS or other tools appropriate for large scale data preparation and analysis\\nExperience with data mining, machine learning, statistical modeling tools and underlying algorithms\\nProficiency with Unix/Linux environments\\nBENEFITS\\n\\n• Well-funded and proven startup with large ambitions, competitive salary and stock options\\n• Dynamic and entrepreneurial team where pushing limits is everyday business\\n• 100% employer paid medical, dental, vision, disability and life insurance plans\\n• Access to a 401k (Boston) or Retirement Savings Plan (Montreal)\\n• Easy commute with a paid-for public transportation or parking pass\\n• IATA Travel Agent Card for discounts in the travel industry\\n• Fully stocked kitchen with: coffee/tea, beer, bagels and snacks (both healthy and not-healthy)\\n• In Cambridge, work in a historic factory building near Kendall Square; in Montreal, work in an artist’s loft in the Mile End\\n• Team lunches, offsite activities and much more!',\n",
       "  \"About the Company:\\nIndustrious is the largest premium flexible workspace provider in the U.S. with over 80 locations in more than 45 U.S. cities. Its Workplace Experience platform, which pairs thoughtfully-designed spaces with hospitality-driven services and amenities, has reshaped the concept of coworking into a scalable solution for companies of all sizes and stages. Since its founding in 2013, Industrious has helped thousands of companies scale their businesses while maintaining the highest NPS scores in the industry. For more information, visit www.industriousoffice.com.\\nAs we shape the future of workplace experience, we're looking for motivated, thoughtful, and collaborative people who are excited to join a high-growth, warm, and welcoming team. For more information, please visit www.industriousoffice.com/careers.\\n\\nAbout the Role:\\nThe Business Intelligence Analyst at Industrious is a critical role responsible for driving analytics across our network of operations. You will focus on conceptualizing, executing, and systematizing how we uncover data-driven insights that grow our business and improve our members' experience.\\n\\nThis is an analytics role that will help ask and answer big strategic questions: Where should we put the next Industrious location? How should we price our spaces? What is the connection between our approach to hospitality and our net promoter score? Sometimes you'll be working with our BI team to blaze the trail, requiring the ability to figure out the approach, execute the first analysis, and share what it means for the business. Other times you'll build on an existing approach, working with other members of our team to level up our processes and build tools that will scale with us as we grow.\\n\\nThis is an amazing role for a curious, analytical mind who is excited to join a rapidly growing team, touch all elements of the business, and help set the foundation for the way we approach analytics and insights going forward.\\n\\nThe role is based in New York.\\n\\nYou are a great fit for this role if:\\nYou love rolling up your sleeves and digging through data to figure out why things are the way they are\\nYou have a strong analytical intuition and the curiosity to learn whatever it takes to get from your hypothesis to a solid answer\\nData engineering? Data analysis? Data science? You want to work with all three pieces of the analytics process\\nYou have a knack for asking the right questions to help people get the information and outcomes they need\\nResponsibilities:\\nMaintain core metrics for our unit operations, real estate, design and construction, marketing, and hospitality teams.\\nConduct ad hoc analyses on a wide variety of data sources using Excel and SQL.\\nBuild dashboards and reports in Tableau to share metrics and insights with all teams at Industrious.\\nHelp build and improve tools and processes that produce ongoing insights into our business.\\nMaintain our data pipelines (Stitch, Fivetran), and work with the engineering team to build new ones as needed.\\nRequirements:\\nStrong analytical and communication skills\\n2+ years of experience working with non-technical users to understand business problems and map abstract problems to data that can be used to solve them\\n2+ years of hands-on experience with SQL and proficiency with Excel\\nPrior experience with visualization tools such as Tableau (preferred) or Looker\\nWork or academic experience with cleaning, transforming, and analyzing data\\nPreferred qualifications:\\nStrong SQL experience\\nKnowledge of Python and/or R\\n\\nPerks:\\n\\nIn addition to our incredible team, there are lots of other fun reasons to work with us.\\nDaily breakfast, snacks, coffee, tea, and drinks at all Industrious locations\\nHealth care, including vision and dental\\nLearning & development grant\\n401K plan\\nSmart casual dress code\\nGenerous vacation time\\nPaid parental leave\\nTeam outings and annual company-wide offsite\\nIndustrious in the News:\\nHow Industrious is changing the coworking Industry\\nIndustrious CEO Sees Company as Less Risky Than WeWork\\nProfitable By 2020: Industrious Raises $80M From Brookfield, Equinox\\nIndustrious is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. Industrious believes that diversity is critical to our success in delivering great workplace experiences and is committed to creating an inclusive, mutually respectful environment which celebrates diversity of our people. We seek to hire on the basis of merit, competence, performance, and business needs.\",\n",
       "  \"WE ARE A TRANSFORMATIONAL PARTNER\\n\\n\\nWe marry design and engineering language in ways that produce impactful and memorable experience journeys. We partner all the way to continuously improve our clients' digital maturity. Our Studio network brings the optimal combination of skill, scale, and cost for each stage of the product development lifecycle. And to do this we need great transformational people that want to impact the projects and organizations that they work with.\\n\\nWe are looking for an exceptional Data Scientist to work with our cross-functional team, and join our world-class community of talented experts. Core to this need are expertise in:\\nMid to Senior Level Data Scientist with 5+ years of experience in Data Science\\nExperience in Banking domain is a plus\\nExcellent communication skills\\nPerson should be able to lead the entire data science project from\\nCoding and executing the project (from model building to model testing to model deployment)\\nPresenting regular updates to clients (including updates from exploration)\\nExcellent in R or Python\\nUnderstanding of Big Data and cloud platforms like (Cloudera Hadoop or Microsoft Azure) is a plus\\nIdeally you will also have:\\nUndergraduate Degree in Computer Science, Physics or Mathematics (Graduate Degree always is a plus)\\n5-10 years experience in Data Science role\\nAn Agile mindset with experience working in Agile environment\\nA spirit of collaboration and transparent communication\\nA natural curiosity for new scripting languages , frameworks and technologies\\nHigh personal code/development standards (peer testing, unit testing, documentation, etc)\\nWe are a thriving Community of top technology talent that is globally connected. We Engage, Make, Run and Evolve the technology that makes many brands that you know and love. So let's take this journey together. No matter where you are on your digital career roadmap, we can help you grow and have fun doing it.\\n\\nPack and move relocation available. Softvision LLC is an Equal Opportunity Employer. No 3rd Party Agency Candidates.\\n\\n]]>\\nApply Now!\",\n",
       "  'Hiring in Los Angeles, CA | New York, NY | San Francisco, CA\\n\\nWho We Are\\n\\nEDO is a data science software firm that develops analytics tools to make data accessible and actionable for the media and entertainment industry. Currently focused on film and TV, we work with many major movie studios and TV networks to help them forecast, market and distribute their content more effectively. Building from this strong base, we are growing into adjacent verticals.\\n\\nWe are a team of world-class engineers and data scientists backed by top leaders in entertainment and technology. Our co-founders and executive leadership have an established track record with other successful ventures.\\n\\nWhat You Will Do\\n\\nGenerate data-driven insights to help guide client decision-making\\nDiscover novel applications of data within entertainment\\nWork on a small team and take ownership of high impact projects\\nWork in a fast paced environment with continuous deployment and rapid product iteration\\nMeet and work directly with media and entertainment clients to understand their needs and translate into product solutions\\nUnderstand how the futures of media, entertainment and advertising are being defined by the innovative applications of technology and data\\nLearn about how a startup is built from an early stage\\n\\nWhat We Are Looking For\\n\\nSelf-driven individuals who take ownership of their work\\nAbility to build products quickly and efficiently\\nStrong understanding of data science practices and principles\\nInterpersonal and communication skills to work on a small team\\nWillingness to learn about our clients and their unique problems\\nExperience with R or Python and comfort with databases\\n\\nBenefits\\n\\nEarly-stage equity and competitive salary\\nMedical, dental, and vision insurance\\nMeals and snacks during work\\nMovie tickets, fitness discounts, commuter subsidies, and Apple hardware',\n",
       "  \"Strength Through Diversity\\n\\nGround breaking science. Advancing\\nmedicine. Healing made personal.\\n\\nQuality Operations- Corporate\\n\\nData\\nScientist- Req # 2508918\\n\\nThe\\nmission of Quality Operations is to\\nimprove health system performance and outcomes by providing leadership and care\\nteams with quantifiable data and insights needed for operations, clinical\\nperformance measurement, quality improvement, analytics and outcomes research.\\nWe provide solutions to integrate, cleanse, govern, translate and dispense\\ninformation across Mount Sinais clinical institutes and clinical care service\\nlines.\\n\\nOur team\\nis seeking a Data Scientist to design and implement innovative data\\nvisualization and statistical analyses to drive quality and process improvement\\nefforts.\\n\\nRoles\\n& Responsibilities:\\nProviding\\ninnovative visual answers to clearly and concisely meet end user needs while\\nconsidering all complexities that are inherent in health care data.\\nTranslating\\nrequirements for data-driven solutions with minimal guidance.\\nLeading\\nanalytics and/or statistics project from start to finish.\\nDocumenting\\nworkflow.\\nWork\\nwith clinical /internal team leads to define measures for assessment and\\nanalysis in support of operations management and quality and process\\nimprovement.\\nDetermine\\nand apply the most appropriate statistical analysis techniques in an agile\\nfashion.\\nCreate\\nmeaningful, appropriate data visualizations across various clinical and\\nbusiness domains.\\nUse\\ncommon collaboration tools (i.e. JIRA, git) along with scripting languages (R, SAS,\\nSQL) to create accurate and reproducible analyses.\\nRequirements:\\n\\nMaster's\\n(preferred) in Statistics, Mathematics, Economics or Public Health.\\nThree\\n(+) years of experience in statistical programming language (R, SAS).\\nPossesses\\nadvanced applied statistics skills including an understanding of probability\\nand its application to the analysis of data including statistical process\\ncontrol, process monitoring, and diagnostics.\\nExperience\\nin risk model development, hierarchical modeling and predictive analytics.\\nExcellent\\nat communicating difficult statistical concepts (oral and written).\\nSelf-motivated\\nand organized in pushing projects to completion.\\nAble\\nto learn quickly on the job, with minimal guidance.\\nExperience\\nworking in agile environment.\\n\\nPreferred Experience:\\n\\nExperience\\nwith performance improvement methodologies (Lean, Six Sigma) and data-driven\\nimprovement efforts.\\nExperience\\nin designing measures, surveys and studies helpful.\\nGeneral\\nunderstanding of healthcare departmental/clinical operations, workflow,\\npolicies, and procedures helpful.\\nStrength Through Diversity\\n\\nThe Mount Sinai Health System\\nbelieves that diversity is a driver for excellence. We share a common devotion\\nto delivering exceptional patient care. Yet were as diverse as the city we\\ncall home- culturally, ethically, in outlook and lifestyle. When you join us,\\nyou become a part of Mount Sinais unrivaled record of achievement, education and\\nadvancement as we revolutionize medicine together.\\n\\nWe work hard to acquire and retain\\nthe best people, and to create a welcoming, nurturing work environment where\\nyou can develop professionally. We share the belief that all employees,\\nregardless of job title or expertise, can make an impact on quality patient\\ncare.\\n\\nExplore more about this opportunity\\nand how you can help us write a new chapter in our story!\\nWho We Are\\n\\nOver 38,000 employees strong, the\\nmission of the Mount Sinai Health System is to provide compassionate patient\\ncare with seamless coordination and to advance medicine through unrivaled\\neducation, research, and outreach in the many diverse communities we serve.\\n\\nFormed in September 2013, The Mount\\nSinai Health System combines the excellence of the Icahn School of Medicine at\\nMount Sinai with seven premier hospital campuses, including Mount Sinai Beth\\nIsrael, Mount Sinai Beth Israel Brooklyn, The Mount Sinai Hospital, Mount Sinai\\nQueens, Mount Sinai West (formerly Mount Sinai Roosevelt), Mount Sinai St.\\nLukes, and New York Eye and Ear Infirmary of Mount Sinai.\\n\\nThe Mount Sinai Health System is an\\nequal opportunity employer. We promote recognition and respect for individual\\nand cultural differences, and we work to make our employees feel valued and\\nappreciated, whatever their race, gender, background, or sexual\\norientation.\\n\\nEOE\\nMinorities/Women/Disabled/Veterans\",\n",
       "  'Tracking Code\\n\\nSN1000701\\n\\nJob Description\\n\\nStarz is seeking a Data Scientist to support data-driven decision-making. The Data Scientist will have responsibility for analysis of consumer, content and product data, including the development of “data products” such as predictive models and segmentations. The ideal candidate is comfortable working with both large (e.g., event-level data warehouse tables) and small (e.g., marketing, research) data sets. The Data Scientist will have experience with a variety of data wrangling tools, modeling paradigms, optimization algorithms and simulation methodologies. Understanding how to use data science to support practical and actionable business insights is a must.\\n\\nThe position will report into the VP, Business Intelligence & Data Science and is based in New York, NY.\\n\\nRESPONSIBILITIES\\n\\nLead efforts to develop advanced predictive models to reduce marketing costs, increase customer LTV, understand content value and optimize content investment decisions\\nAssess new data sources and data import processes for efficiency and accuracy\\nEvaluate accuracy and precision of predictive models and optimization algorithms provided by other analysts or third-parties\\nDevelop A/B testing and experimental design (e.g., fractional factorial) paradigms for optimizing marketing effectiveness, content engagement and product experience\\nDevelop processes for monitoring model performance and data accuracy\\nEstablish departmental standards for data quality and KPI monitoring including sampling procedures and control charting (e.g., CUSUM or EWMA charts)\\nRequired Skills\\n\\nExceptional knowledge of machine learning algorithms and tools (e.g., clustering, random forest, neural nets, boosting)\\nFamiliarity with star schema databases and SQL\\nSolid foundation in statistics including probability density functions, regression and Null Hypothesis Significance Testing\\nProficient in (at least some of) Python, Snowflake, AWS/S3, SageMaker, R, Tableau, Alteryx and Microsoft Office\\nExperience working with commonly-used third-party data such as Google Analytics, Facebook Insights, Qualtrics, etc.\\nPrevious experience with media/entertainment and/or subscription businesses preferred but not required\\nMinimum of 5 years’ professional experience\\nBA/BS and Master’s degree in Data Science, Computer Science, Mathematics, Statistics or similar from a top-tier university\\nSTARZ (www.starz.com), a Lionsgate company (NYSE: LGF.A, LGF.B), is a leading global media and entertainment company that provides premium subscription video programming on domestic U.S. pay television networks and produces and distributes content for worldwide audiences, including its investment in the STARZ PLAY Arabia OTT service.\\n\\nSTARZ is the ultimate destination for obsessable TV, movies and more. Characters who pull you in and stories that stay with you. From bold Original Series to the best movies, whatever you love, STARZ ignites your passions.\\n\\nSTARZ offers a competitive compensation package and an attractive benefits program to all eligible employees including a variety of healthcare plans, dental and vision insurance, 401k, life/disability insurance. Eligible employees will enjoy paid time off in the form of vacation and company holidays.\\n\\nSTARZ is an Equal Opportunity Employer. This means that all applicants will receive consideration for employment regardless of gender, age, race, national origin, disability, color, religion, sexual orientation, gender identity and/or expression, veteran status, or any other characteristic protected by federal, state or local law. In addition, STARZ will provide reasonable accommodations for qualified individuals with disabilities.\\n\\nJob Location\\n\\nNew York, New York, United States\\n\\nPosition Type\\n\\nFull-Time/Regular',\n",
       "  'Date Posted:\\n\\n2019-05-08-07:00\\n\\nCountry:\\n\\nUnited States of America\\n\\nLocation:\\n\\nUTNB1: Corp - Brooklyn NY 55 Water St, Brooklyn, NY, 11201 USA\\n\\nUnited Technologies Corporation (UTC) has a deep history of innovation that brings together big thinkers, problem solvers and a culture for pushing the boundaries of whats possible. Our global reach and rich history uniquely positions us to succeed in the new digital economy. Our investment in digital innovations will make travel better, people safer and urbanizing cities more comfortable and connected.\\n\\nUTC is committed to leading in the digital era and will unleash the size and scale of its businesses on the digital world of big data and the Internet of Things. We are looking for the very best thought and technology leaders who will help grow our Digital Accelerator in Atlanta, GA. Come join us in this journey.\\n\\nAbout United Technologies NYSE UTX:\\n\\nWith revenues of approximately $57 billion, United Technologies Corporation (UTC) is a Fortune 50 company that provides high technology products and services for the aerospace and commercial building industries. Our aerospace businesses include Pratt & Whitney and UTC Aerospace Systems. Pratt & Whitney is a world leader in the design, manufacture and service of aircraft engines. UTC Aerospace Systems is one of the worlds largest suppliers of technologically advanced, aerospace and defense products.\\n\\nOur commercial building businesses include Otis Elevator and Climate, Controls & Security. Otis is the worlds largest manufacturer and maintainer of people-moving products, including elevators, escalators and moving walkways. UTC Climate, Controls & Security is a leading provider of heating, air conditioning and refrigeration systems, building controls and automation, and fire and security systems. These companies are leading to safer, smarter, sustainable and high-performance buildings.\\n\\nRanked among the worlds greenest companies, we do business in virtually every country of the world and have over 200,000 employees globally.\\n\\nPosition:\\n\\nWe are seeking highly motivated data scientists to join the Predikto team in Atlanta, GA. We are looking for people who have a passion for analyzing and uncovering digital insights from large, complex streams of data derived from the worlds most advanced aerospace, aviation and building management systems.\\n\\nAs a member of the team, you will work on the research, development, and implementation of complex machine learning systems to predict rare events and faults in aerospace and building system components.\\n\\nThis position will provide the unique opportunity to operate in a start-up-like environment within a Fortune 50 company. Our digital focus is geared towards releasing the insights inherent UTCs best-in-class products and services. Together we aim to achieve new levels of productivity by changing the way we work and identifying new sources of growth for our customers.\\n\\nJob Responsibilities include:\\nDevelop models and algorithms that will improve how we design, manufacture, monitor, and maintain industrial assets such as jet engines, aviation systems, elevators, HVAC, and other commercial systems\\nConduct exploratory data analysis\\nParticipate in early stage R&D work to further advance the ML platform\\nCollaborate with software development to implement and deploy newly developed technologies and algorithms\\nWork with business units and lead data scientists to implement real-world critical use cases on the platform\\nBuild scripts to automate, clean, transform, cross-reference and merge large sources of data utilizing Python\\nResults-oriented with a strong sense of ownership in delivering for our customer and businesses\\nQualification:\\n5+ years related experience\\nDeep knowledge of machine learning, statistics, optimization or related field\\nExperience with R, Python, Perl, Matlab\\nExperience with industrial/commercial applications of data science including prognostic and health management, supply chain optimization, and human capital management is a significant plus\\nExcellent written and verbal communication skills along with the ability to well work in cross functional teams\\nWork experience in programming, data science, machine learning\\nMotivated self-starter with a strong enthusiasm to learn\\nResults-oriented with strong communication and customer focus\\nAbility to deal well with ambiguous and undefined problems\\nEducation:\\nMasters or PhD in engineering, computer science, statistics, or operations research or related technical discipline\\nUnited Technologies Corporation is An Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or veteran status, age or any other federally protected class.\\n\\nPrivacy Policy and Terms:\\n\\nClick on this link to read the Policy and Terms',\n",
       "  \"Data Scientist\\n\\n16-Jul-2019\\n\\nRole/Responsibilities\\n\\nThe Data Scientist is a key member of the Analytic Tools & Solutions (ATS) group in MIS. ATS is responsible for developing the quantitative models and analytical tools used in the rating process and across the rating agency, as well as certain MIS technology innovation activities, including advanced capabilities in machine learning and artificial intelligence. The Data Scientist will be part of a team of individuals responsible for applying the latest techniques in Machine Learning and Distributed Computing to drive business value particularly related to Blockchain and digital assets. A successful data scientist will not only be technically competent, but will be able to work collaboratively with business stakeholders to increase analytical efficiency, drive new business insights, and develop new products.\\n\\nThe duties of the Data Scientist include:\\nDevelopment and Deployment of Machine Learning / Statistical Learning models for developing analytics and tools largely related to Blockchain and Digital / Crypto assets\\nFull cycle data management from collection and cleaning to processing\\nIdentify potential data sets (internal and external) which could be used to enhance analytics\\nStay up to date with the latest in statistical and machine learning methods\\nApplying sound software and architectural development practices in development and deployment of models as software products.\\nUsing cloud and distributed computing platforms for model development and deployment\\nCommunication of results to business stakeholders and decision makers\\nJob Req ID\\n16337BR\\n\\nLine of Business\\nCSS (CSS)\\n\\nSecurities Trading Policy (STP)\\nCandidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody’s Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.\\n\\nEntity\\nMoody's Investors Service (MIS)\\n\\nPosting Title\\nData Scientist\\n\\nEEO Policy\\nMoody’s is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody’s also provides reasonable accommodation to qualified individuals with disabilities in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com.. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.\\n\\nFor San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance. For New York City positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the New York City Fair Chance Act. For all other applicants, qualified applicants with criminal histories will be considered for employment consistent with the requirements of applicable law.\\n\\nClick here to view our full EEO policy statement. Click here for more information on your EEO rights under the law.\\n\\nCity\\nNew York\\n\\nQualifications\\nMaster’s Degree in Computer Science, Statistics, Applied Math, specialized Machine Learning program, or related field\\n3+ years’ practical experience in Machine Learning or statistics and/or distributed computing\\nWorking knowledge of Machine Learning techniques include Neural Networks, Tree-based Models, Linear models\\nIndustry knowledge of Blockchain and Digital / Crypto assets is highly preferred\\nExperience with one or more of the following programming languages is highly preferred: Python, R, and SQL\\nRegular/Temporary\\nRegular\\n\\nWorking at Moody's\\nMoody's is an essential component of the global capital markets, providing credit ratings, research, tools and analysis that contribute to transparent and integrated financial markets. Moody's Corporation (NYSE: MCO) is the parent company of Moody's Investors Service, which provides credit ratings and research covering debt instruments and securities, and Moody's Analytics, which offers leading-edge software, advisory services and research for credit and economic analysis and financial risk management. The Corporation, which reported revenue of $4.4 billion in 2018, employs approximately 13,100 people worldwide and maintains a presence in 42 countries. Further information is available at www.moodys.com.\\n\\nLOB/Department\\n\\nAnalytic Tools & Solutions Group\\n\\nExperience Level\\nExperienced Hire\\n\\nJob Category\\nEngineering & Technology\\n\\nJob Sub Category\\nSoftware Engineering\\n\\nMIS Culture of Excellence\\nAt MIS, our workplace culture supports our mission to be the Agency of Choice for our customers, employees and future employees. Our Culture focuses on Excellence and embodies behavioral qualities such as Integrity, Responsiveness, Collaboration, Respect and Passion. Our people are our core asset and we look beyond outcomes to ensure that behaviors and interactions matter.\\n\\nEntity\\nMoody’s Investors Service is among the world’s most respected and widely utilized sources for credit ratings and research. Our opinions and analysis on a broad range of credit obligors and obligations are valued around the world for their insight and rigor.\",\n",
       "  'Company Description\\n\\nButterfly Network is reinventing medical imaging and championing a new era of healthcare by creating the first ever pocket-sized, whole-body ultrasound device - the Butterfly iQ. This breakthrough technology has reduced the cost of the traditional ultrasound system by miniaturizing it onto a single semiconductor silicon chip. Our mission is to democratize healthcare by making medical imaging accessible to everyone around the world.\\n\\nSince inception, Butterfly has raised over $375 million. The iQ is FDA-cleared and is being sold in hospitals and clinics around the globe.\\n\\nJoining Butterfly Network is the opportunity to redesign the future of healthcare through the power of technology. Embark on a journey with us to maximize global impact, motivated by the idea that our products will change the lives of millions along with the people you love.\\n\\nJob Description\\n\\nWe are looking for a highly motivated and experienced data scientist to drive data-driven strategy and product decisions. You will be responsible for extracting actionable insights from the usage of our mobile, web and e-commerce apps as well from sales, marketing and manufacturing datasets. The ideal candidate will have experience understanding levers to improve engagement with SaaS products and with boosting conversion for e-commerce flows. The role is anticipated to grow quickly with the company’s success allowing the candidate to build a world-class team.\\n\\nAs part of our team, your core responsibilities will be:\\nModel lead and customer behavior to direct marketing and sales strategy, e.g. customer segmentation, predict Net Promoter Score, identification of potential churn risk and upsell opportunities.\\nAnalyze e-commerce data to identify opportunities to improve conversion. Run A/B testing to test hypotheses and optimize e-commerce platform\\nProvide data to inform cloud and mobile app product roadmap. Segment users, design and surface key metrics (e.g user retention), analyze feature usage, identify associations between mobile and web usage,\\nIdentify opportunities to improve customer support. Alert on emergent reliability issues based on support interactions, cluster patterns of failure e.g. using NLP, work with manufacturing team to predict issues based on self-test data\\nBuild dashboards and presentations to surface insights to leadership\\nMake data accessible to the company as a whole with self-service tools and training\\nQualifications\\n\\nBaseline skills/experiences/attributes:\\nBachelor’s or higher degree in a quantitative field\\n3 + years of data science experience\\nKnowledge of statistics including hypothesis testing\\nKnowledge of supervised and unsupervised ML techniques for data science: clustering, association-rule mining, prediction, anomaly detection\\nDeep experience with Python data science ecosystem: pandas, numpy, scipy, Jupyter Notebooks/Labs\\nAdvanced SQL user\\nAble to create compelling data visualizations, using Tableau or similar tooling\\nAble to manage projects with technical and non-technical stakeholders in a fast-paced environment\\nKnowledge of data model for source operational systems beneficial (Netsuite, Salesforce, Zendesk)\\nIdeally, you also have these skills/experiences/attributes (but it’s ok if you don’t!):\\nKnowledge of Git, Airflow and Google Cloud Platform tools\\nAdditional Information\\n\\nWe offer great perks:\\nFully covered medical insurance plan, and dental & vision coverage - as a health-tech company, we place great worth on our teams’ well-being\\nCompetitive salaried compensation - we value our employees and show it\\nEquity - we want every employee to be a stakeholder\\nPre-tax commuter benefits - we make your commute more reasonable\\nFree onsite meals + kitchen stocked with snacks.\\n401k plan - we facilitate your retirement goals\\nBeautiful office overlooking the Flatiron building in NYC\\nThe opportunity to build a revolutionary healthcare product and save millions of lives!\\nFor this role, we provide visa assistance for qualified candidates.\\n\\nButterfly network does not accept agency resumes.\\n\\nButterfly Network Inc. is an E-Verify Company and is an equal opportunity employer regardless of race, color, ancestry, religion, gender, national origin, sexual orientation, age, citizenship, marital status, disability or Veteran status. All your information will be kept confidential according to EEO guidelines.',\n",
       "  '11 West 19th Street (22008), United States of America, New York, New York\\n\\nAt Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nData Scientist, Digital Machine Learning\\n\\nAt Capital One, data is at the center of everything we do. When we launched as a startup we disrupted the credit card industry by individually personalizing every credit card offer using statistical modeling and the relational database, cutting edge technology in 1988! Fast-forward a few years, and this little innovation and our passion for data has skyrocketed us to a Fortune 200 company and a leader in the world of data-driven decision-making.\\n\\nTeam Description\\n\\nDigital ML is the data science and machine learning team inside Capital Ones Digital Products organization. We deliver real-time, personalized, intelligent customer experiences in Capital Ones suite of award-winning digital products, including our website, mobile app, emails, chatbot, and beyond. We partner closely with our product and engineering teams to build the data and modeling platforms crucial to the deep understanding of customers that enables our applications to delight them by adapting to their needs.\\n\\nAs part of Digital ML, you will:\\nExplore billions of clickstream events to discover the patterns in customer behavior, and use those patterns to model key customer outcomes\\nBuild and leverage an enterprise-wide taxonomy of customer data to optimize digital marketing initiatives\\nDevelop the real-time models that use vast amounts of customer data to anticipate customers needs and deliver the right message at the right time\\nDevelop the models that ensure our most important customer data is accurate, fighting fraud and other bad behavior, while enabling seamless digital experiences across all our products\\nRole Description\\n\\nIn Digital ML, you will work at all phases of the data science life cycle, including:\\nBuild machine learning models through all phases of development, from design through training, evaluation and validation, and partner with engineering teams to improve operationalizationin scalable and resilient production systems that serve 50+ million customers.\\nPartner closely with a variety of business and product teams across Capital One to conduct the experiments that guide improvements to customer experiences and business outcomes in domains like marketing, servicing and fraud prevention.\\nWrite software (Python, Scala, e.g.) to collect, explore, visualize and analyze numerical and textual data (billions of customer transactions, clicks, payments, etc.) using tools like Spark, Elasticsearch, and AWS.\\nThe Ideal candidate will be:\\nCurious and creative. You thrive on bringing definition to big, undefined problems. You love asking questions, and you love pushing hard to find the answers. Youre not afraid to share a new idea. You communicate clearly and effectively to share your findings with non-technical audiences.\\nTechnical: You have hands-on experience developing data science solutions from concept to production using open source tools and modern cloud computing platforms. You are not afraid of petabytes of data.\\nStatistically-minded. You have built models, validated them and back-tested them. You know how to interpret a confusion matrix or a ROC curve. You have experience with clustering, classification, sentiment analysis, time series analysis and deep learning.\\nCustomer and product oriented. You share our passion for changing banking for good.\\nBasic Qualifications:\\nBachelors Degree and 2 years of experience in data analytics, or Masters Degree and 1 year of experience in data analytics, or PhD\\nAt least 1 year of experience in open source programming languages for data analysis\\nAt least 1 year of experience with machine learning\\nAt least 1 year of experience with relational databases\\nPreferred Qualifications:\\nMasters Degree in STEM field (Science, Technology, Engineering, or Mathematics) and 1 year of experience in data analytics, or PhD in STEM field (Science, Technology, Engineering, and Mathematics)\\nAt least 1 year experience working with AWS\\nAt least 3 years experience in Python, Scala, or R\\nAt least 3 years experience with machine learning\\nAt least 3 years experience with SQL\\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.',\n",
       "  \"Description\\n\\nDesigns, develops and programs methods, processes, and systems to consolidate and analyze unstructured, diverse “big data” sources to generate actionable insights and solutions for client services and product enhancement.\\n\\nInteracts with product and service teams to identify questions and issues for data analysis and experiments. Develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources. Identifies meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers.\\n\\nWork involves some problem solving with assistance and guidance in understanding and applying company policies and procedures. Attention to detail critical. Ability to collect, organize, and display data in spreadsheet format. Follow-through skills necessary to get information from internal and third parties and have data errors/omissions corrected. Strong written and verbal communication skills. 0 - 2 years relevant work experience.\\n\\nOracle is an Affirmative Action-Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability, protected veterans status, age, or any other characteristic protected by law.\\n\\nQualifications\\n\\nAbout Moat/Oracle Data Cloud\\n\\nWe help brands and media companies push the envelope on privacy-friendly models of advertising, help them make sound product decisions that involve running fewer ads, help make their ads load faster, fight botnets, and strive to de-fund fake news. We help the world's top publishers and marketers make smarter decisions through real-time data. Our analytics and intelligence software is used by The New York Times, Condé Nast, Kellogg's, P&G, Spotify, and hundreds of other major platforms and advertisers, and we have unique measurement integrations into Facebook, Instagram, Twitter, Snapchat, and YouTube.\\n\\nAbout the Opportunity\\nResearch, invent, and implement new ad telemetry methods that are robust to reverse engineering\\nDesign experiments, analyze data, and draw conclusions to support business decisions and guide your own work\\nBuild predictive models for fraud detection, bot detection, and many other problems\\nWrite robust, distributed applications to perform streaming classification on billions of datapoints per day\\nIdentify and investigate suspicious websites and impression data to predict, infer, and outmaneuver the activity of people committing and disguising online ad fraud\\nAbout You\\nBachelor's, Masters or PhD in related field\\nExpertise in one or more of the following areas: data science and analysis, distributed data processing, bot detection, browser fingerprinting, security\\nYou have sound knowledge of statistics fundamentals\\nYou understand experimental design, and can build systems that support the measurement and interpretation of their own behavior\\nYou write clean, well-structured, production-quality code in Python and/or vanilla JavaScript\\nYou have strong Bash and Linux systems skills\\nYou are curious, and willing to accomplish things that nobody can tell you exactly how to do\\n]]>\",\n",
       "  'Bomoda is looking for a Data Scientist to join our team in New York!\\n\\nWe Are Seeking:\\n\\nAs a data scientist at Bomoda, you would be directly involved with the product team including computer scientists and data analysts, eliciting information from multiple data platforms and using that data to identify relevant and actionable trends and/or concepts. The product goal is to understand consumer behavior to better detect market conditions as well as the general perception of brands\\n\\nWhat You Will Be Doing...\\nHelp define and scope work required to build and maintain ML systems and models\\nEnsure the quality and robustness of such models\\nOversee any and all data science platform changes and feature development\\nWork with other technology and business stakeholders to identify areas where data science features are required\\nWork with other technology stakeholders to identify and implement best practices for data management\\nWork with integration teams to scope out new data sets for training as well as oversight of integration development within the data science team.\\nResearch approaches in data science that address problems outlined by the data science lead\\nWork with other technology stakeholders to implement best practices for data management\\n\\n\\nWhat We Are Looking For...\\n2-5 years data science experience in Python, Apache Spark, Tensorflow, Scikit-learn, pandas.\\nNatural language processing techniques\\nMachine-learning techniques\\nKnowledge of Big Data Architectures and AWS environment (Hadoop/Hive, Bigquery, S3, EC2, EMR, etc.)\\nStrong programming experience\\nExperience with SQL and/or NoSQL\\nMasters or PHD is a plus. Multilingual fluency / literacy is a plus\\nBomoda is proud to be an Equal Opportunity/Affirmative Action employer. Weber Shandwick recruits qualified applicants without regard to race, color, religion, gender, age, ethnic or national origin, protected veteran status, physical or mental disability, sexual orientation, gender identity, marital status or citizenship status.',\n",
       "  \"At Feather, our mission is to transform people's relationship with material goods to create a healthier and happier planet. You'll join a fast-paced, innovative team whose passion lies in delighting customers and helping people live light.\\n\\nFeather is seeking a Data hire to work closely with our leadership team and investors to help drive the strategic direction of the company.\\n\\nYou're a great fit if you're happy to get your hands dirty but also understand strategy and have the desire to build a team as we scale.\\n\\nWhat you'll be doing:\\nUse analytical thinking, mathematics, and data manipulation to build models that can be used across various functions within Feather's operations, engineering, and finance teams to make better and faster decisions\\nBuild scalable practices to create a solid foundation for Feather's backend data structure\\nClearly define analytical business problems and create innovative solutions for them by facilitating the application of cutting edge techniques\\nCreate a foundation for a data team and build the team over time by developing KPIs for the function and job descriptions for additional data hires.\\nTransform large sets of raw data into easily consumable reports and models to be read by technical and non-technical users\\nCreate dashboards and reports to empower teams (within Looker or excel).\\nWhat you need to be successful:\\n3+ years experience in a data-focused role\\nExperience in SQL or other programming languages (like Javascript, Python, SAS)\\nStrong written and verbal communication to easily articulate findings to teams with varying degrees of experience with data analysis\\nAbility to work as an individual contributor to tackle basic data needs, and context switch to use higher level thinking in team meetings and strategy planning.\\nExperience applying Machine Learning techniques and using platforms such as TensorFlow, Azure, R, Pandas, SparkR, etc.\\nExperience with Looker and the LOOKML, as well as creating models.\\nExperience and background in Finance is a big plus.\\nSome experience with AWS - Redshift services.\\nSome Data Engineering experience or knowledge of ETL process preferred.\\nWho you are:\\nBased in NYC (or willing to move)\\nExcited about joining a small team and growing with Feather\\nPassionate about Feather's mission\\nEligible to work in the United States\\nWhat you'll get when joining Feather:\\nCompetitive compensation\\nGenerous health, vision, dental, and ancillary benefits\\nFlexible vacation and paid time off\\nA 50% discount on your own Feather subscription\\nThe latest MacBook and a high-resolution external monitor\\nA stylish desk and an ergonomic chair (it's what we do, after all!)\",\n",
       "  \"Experience: Entry-level (PhD Program) or Experienced (Postdoc, Faculty, Scientific Lab, Finance Industry)\\n\\nEducation: PhD in Math, Science, or Engineering disciplines\\nThe PDT team - a quantitative investment manager - is hiring new or recent PhD graduates and experienced researchers (postdoctoral fellows, faculty, scientific lab, finance industry) to create and improve proprietary trading models and strategies while working closely with a deep bench of senior researchers.\\n\\nWe have a highly successful record of hiring, challenging, and retaining talented researchers from diverse academic backgrounds. Individuals interested in conducting innovative research with real-world impact seem to enjoy and excel at the types of problems we like to solve.\\n\\nPDT Researchers work in small, nimble teams where merit and contribution, not seniority, drive the discussion. We strive to foster an intellectually challenging environment that encourages collaboration and innovative ideas.\\n\\nIn our research-driven approach to the financial markets, our Chief Scientist oversees the group-wide research agenda, ensuring team members work on the most critical and interesting problems, with a focus on research rigor and standards.\\n\\nWhy join us? PDT Partners has a stellar twenty-five-year track record and a reputation for excellence. Our goal is to be the best quantitative investment manager in the world—measured by the quality of our products, not their size. PDT's very high employee-retention rate speaks for itself. Our people are intellectually extraordinary and our community is close-knit, down-to-earth, and diverse.\\n\\nResponsibilities:\\nWork closely with senior researchers on a variety of trading strategies and research projects, with the opportunity to conduct independent research and originate research topics over time\\nContribute to the long-term success of our research-driven algorithmic trading business\\n\\nRequirements:\\n\\n\\nPhD in math, statistics, physics, computer science, electrical engineering, operations research or other highly-quantitative and analytical discipline\\nSolid mathematical and analytical ability; exceptional problem-solving and modeling ability\\nStrong research intuition\\nStrong programming skills (Python, R, Matlab, C++)\\nExcellent communication and collaborative white board skills\\nMeticulous and detail-oriented, and innately driven to understand issues deeply\\nExperience with/interested in working with large data sets\\nSelf-motivated and highly-productive, with a strong sense of ownership and urgency\\nAble to work collaboratively and productively with others\\nEnjoy solving complex, difficult, real-world problems\\nEntrepreneurial and creative\\nFinance knowledge is not required or expected\",\n",
       "  'Help us create better connected\\nfutures\\nMake your mark. See the\\ndifference. Be recognized.\\nWork with aligned engaged team\\nmembers.\\n\\n\\nAbout\\nus\\n\\nAt Transit Wireless we\\ncreate even better connected futures.\\nOurs is a story of being big enough to deliver and maintain large-scale\\noperations but being nimble enough to make things happen.\\nOur future-makers thrive on rewriting yesterdays rules. So we put Wi-Fi, fiber\\nand wireless connectivity on subways and we create new services for our\\ncustomers and communities every day.\\nWe build technology and teams that people want to be part of and we have the\\ncourage to not only imagine, but to do what really matters.\\n\\nAbout the role\\nDevelop statistical models to\\ndrive key core products for an innovative company in a data rich\\nenvironment.\\nCombine real world intelligence\\nwith sensor data to drive actionable insights for our clients.\\nDesign and develop efficient\\nand scalable data pipelines between enterprise systems and analytics\\nplatform\\nWork closely with team and\\nparticipate in development and deployment of machine learning models and\\nfeature engineering pipelines\\nProvide technical expertise in\\nthe area of design and implementation of Ratings Integrated Data Facility\\nwith modern AWS cloud technologies such as S3, Redshift, EMR, Hive, Presto\\nand Spark\\nBuild and maintain a data\\nenvironment for speed, accuracy, consistency and ‘up’ time\\nSupport analytics by building a\\nworld-class data lake environment that empowers analysts to determine\\ninsights into revenue and power products across the organization\\nEnsure data governance\\nprinciples adopted, date quality checks and data lineage implemented in\\neach hop of the data\\nBe in tune with emerging trends\\nBig data and cloud technologies and participate in evaluation of new\\ntechnologies\\nEnsure compliance through the\\nadoption of enterprise standards and promotion of best practice / guiding\\nprinciples aligned with organization standards\\nAbout you\\nDemonstrated ability to distill data-driven\\ninsights through informative visualizations & executive level\\ncommunications\\nExperience with geospatial datasets &\\ntool-chains\\nExperience designing a measurement strategy to\\nevaluate the performance of complex systems against challenging requirements\\nDeep understanding of statistics and/or applied\\nmath as basis for choosing techniques & experiments\\nAbility to develop D&A related Microservices\\nin mainstream cloud infrastructure\\nLead the data science and data engineering team in a\\ncross-functional work environment\\nLead/mentor the team to design, develop and maintain D&A\\nsolutions\\nBS or MS degree in Computer\\nScience or Information Technology\\n8 years of experience as data\\nengineer at an innovative organization\\n4 years of hands-on experience\\nin implementing data lake systems using AWS cloud technologies such\\nas S3, Redshift, EMR, Hive, Presto and Spark\\nExpert managing AWS services\\n(EC2, S3, Route 53, ELB, VPC, cloudwatch, Lambda) in a multi account\\nproduction environment\\nExperience With Machine\\nLearning Frameworks, such as TensorFlow, PyTorch, H2O, scikit-learn,\\nTheano, Caffe or Spark MLib is an added advantage\\nExposure to\\nR, SparkR, SparklyR or Other R packages is a plus\\nExperience in constructing fast\\ndata staging layers to feed machine learning algorithms\\nExperience in building data\\nAPIs to consume analytic model output\\nFamiliarity with machine\\nlearning model training and deployment process is a plus\\nExperience with development\\nframeworks as well as data and integration technologies such as Python,\\nScala or Informatica\\nExpert knowledge of Agile\\napproaches to software development and able to put key Agile principles\\ninto practice to deliver solutions incrementally.\\nMonitors industry trends and\\ndirections; develops and presents substantive technical recommendations to\\nsenior management\\nExcellent analytical thinking,\\ninterpersonal, oral and written communication skills with strong ability\\nto influence both IT and business partners\\nAbility to prioritize and\\nmanage work to critical project timelines in a fast-paced\\nenvironment\\nWhy work with us\\n\\nWe are a courageous culture and we do what really matters, putting people,\\ncustomers and communities at the heart of every decision.\\nWe believe there is strength in diversity and opportunity through inclusion.\\nAnd we provide a learning environment, meaningful recognition for your\\ncontribution and competitive compensation.\\nHelp us imagine and create what’s next. Join us.\\n\\n</br>',\n",
       "  'Data Scientists are tasked with mining our industry-leading internal data to develop new analytics capabilities for our businesses.\\n\\nThe role requires a rare combination of sophisticated analytical expertise; business acumen; strategic mindset; client relationship skills, project management; and a passion for generating business impact.\\n\\nPrimary responsibilities for this position include leading, designing, building, testing and presenting forward-thinking, high-impact Data Science projects focused on strategic business initiatives.\\n\\nPrudential Financial employs 48,000 across the US, Asia, Europe and Latin America providing insurance, investment management and other financial products that are vital to the wellbeing of our customers and their families in over 30 countries. Prudential is reinvigorating Newark’s downtown area with our new Prudential Tower and Hain building development. Newark is Prudential’s Global Headquarters with over 5,000 employees across four downtown locations. We’re located a short walk or shuttle ride from Newark’s Penn and Broad Street Stations: a direct train ride or short drive (with free parking) from most of Northern and Central New Jersey and New York City.\\n\\nResponsibilities:\\nDevelop and maintain consultative relationships with key business stakeholders\\nIdentify, source, transform and join public, proprietary and internal data sources\\nModel large structured and unstructured data sources (e.g. financial transactional, time-series, text, speech/audio and image)\\nImplement advanced statistical methods for prediction and optimization including a wide variety of machine learning technologies (logit, regression, decision trees/forests, boosted models, clustering, etc.) for purposes including explorative analysis, survival analysis, segmentation, prediction and recommendation systems\\nPerform analysis and implement solutions that maximize business impact\\nPrepare and present written and verbal reports to key stakeholders\\nSome domestic travel may be required\\nExecute all aspects of an advanced analytical project under guidance\\nAdvanced degree (Masters or Ph.D.) in Mathematics, Statistics, Engineering, Econometrics, Physics, Computer Science, Actuarial, Data Science, or comparable quantitative disciplines\\nMaster’s degree graduates should additionally have at least one year of industry experience with responsibility for developing advanced quantitative, analytical, statistical solutions\\nHands-on experience applying a wide variety of statistical machine learning techniques to real world problems spanning analysis, predictive modeling and optimization on structured and unstructured data\\nExperience using tools such as Python, R, or equivalent for statistical modeling of large data sets',\n",
       "  'Company Overview\\n\\nH2O.ai is the open source leader in AI with a mission to democratize AI for everyone. H2O.ai is transforming the use of AI with software with its category-creating visionary open source machine learning platform, H2O. More than 18,000 companies use open-source H2O in mission-critical use cases for Finance, Insurance, Healthcare, Retail, Telco, Sales and Marketing. H2O Driverless AI uses \"AI to do AI\" in order to provide an easier, faster and cost-effective means of implementing data science. H2O.ai partners with leading technology companies such as NVIDIA, IBM, AWS, Intel, Microsoft Azure and Google Cloud Platform and is proud of its growing customer base which includes Capital One, Progressive Insurance, Comcast, Walgreens and MarketAxes. For more information and to learn more about how H2O.ai is driving an AI Transformation, visit www.h2o.ai.\\n\\nJob Summery:\\nCan you learn demonstrations (demos) built with R and/or Python? If you think of a cool demo and it doesn\\'t exist, will you raise your hand to get it built?\\nCan you code proficiently in at least one language used by data scientists and/or data engineers, and does it excite you to learn more?\\nAre you skilled at predictive modeling?\\nDo you view communication skills just as important as technical ones? Can you listen to the needs of your peers and customers and adapt where need be? Can you write a technical proposal and explain it in simple terms?\\nDo you have a competitive drive to be the best you can be?\\nCan you finish what you start? Can you own assignments given to you?\\nIf the answer is \"yes\" to these questions, you potentially could be an excellent fit to join the team of customer engineering makers at H2O.ai. We deliver world-class solution experiences for our customers and drive revenue for our organization. Some of the technical projects you will work on include: training advanced machine learning models at scale in distributed environments, influencing next generation data science tools and data products, and pioneering ideas and products in new areas, such as machine learning interpretability, automatic machine learning, model management, deployment pipelines, and GPU computing.\\n\\nQualifications and Skills:\\n\\nA great candidate for Customer Data Scientist/Sales Engineer should:\\nKnow Python, R, Java, Scala, Spark.\\nHave experience with Big Data including Hadoop, Spark, Kafka.\\nHave a working knowledge of ML algorithms for Regression and Classification problems.\\nUnderstanding of Supervised, Unsupervised, Deep learning techniques\\nKnowledge of XGBoost, Linear regression, GBM, GLM, LightGBM, Random Forest and other common ML algorithms\\nExperience using TensorFlow, Keras, Scikit libraries for performing ML .\\nHave strong interpersonal, communication and presentation skills.\\n2+ years\\' experience with performing customer facing activities as part of a pre-sales team or professional services team.\\nPre-sales experience is nice to have but not required.\\nComfortable with traveling up to 50%.\\nWe are interested in candidates coming from AI/ ML startups and have several years of work experience after completing their MSc or PhD degrees.\\n\\nH2O.ai Perks!\\nFlexible work hours and time off.\\nH2O.ai is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.',\n",
       "  'Company Description\\n\\nWe believe the economy is better when everyone has access. When everyone has room to grow. No one should be left out because the cost is too great or the technology too complex. We started with a little white credit card reader but haven’t stopped there. We’re empowering the independent electrician to send invoices, setting up the favorite food truck with a delivery option, helping the ice cream shop pay its employees, and giving the burgeoning coffee chain capital for a second, third, and fourth location. We’re here to help sellers of all sizes start, run, and grow their business—and helping them grow their business is good business for everyone.\\n\\nJob Description\\n\\nAs a Senior Data Scientist at Square working on Risk, you will lead projects that derive value from our unique, rich, and rapidly growing data. Specifically, you will lead the development of fraud detection algorithms and systems that protect Square and its customers from fraud and financial loss.\\n\\nYou will:\\nCreatively leverage both new and existing data to increase the effectiveness and efficiency of our risk infrastructure\\nWork with engineers to design machine learning solutions that operate effectively at scale\\nPartner with operatives to quickly respond to rapidly evolving threats\\nApply good software development practices and actively contribute to production code\\nHelp build the next generation of data products at Square\\nQualifications\\n\\nYou have:\\n2-4 years of relevant industry experience\\nA graduate degree in statistics, applied mathematics, computer science, physical sciences, or a similar technical field\\nExperience developing and deploying machine learning / deep learning solutions\\nThe versatility to communicate clearly with both technical and non-technical audiences\\nA willingness to solve problems using whichever tool is most appropriate for the situation\\nTechnologies we use and teach:\\nPython (numpy, pandas, sklearn, xgboost, TensorFlow)\\nMySQL, Hive\\nJava\\nGoogle Cloud Platform\\nTableau, Looker\\nAdditional Information\\n\\nAll your information will be kept confidential according to EEO guidelines.\\n\\nAt Square, our purpose is to empower – within and outside of our walls. In order to build the best tools for the businesses and customers we support all over the world, we have to start at home with a workforce as diverse and empowered as our sellers. To this end, we take great care to evaluate all employees and job applicants equally, based on merit, competence, and qualifications. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. We encourage candidates from all backgrounds to apply and always consider qualified applicants with arrest and conviction records, in accordance with the San Francisco Fair Chance Ordinance. Applicants in need of special assistance or accommodation during the interview process or in accessing our website may contact us by sending an email to assistance(at)squareup.com. We will treat your request as confidentially as possible. In your email, please include your name and preferred method of contact, and we will respond as soon as possible.',\n",
       "  \"Here’s the gist:\\n\\nVettery is fundamentally changing the way people hire and get hired. Leveraging machine learning models that track real-time data, monitor trends and predict hiring behavior, we’re able to help companies grow their teams with accuracy, speed, and compatibility. We’re currently working with over 20,000 companies of all sizes, ranging from Fortune 500 giants to startups based out of co-working spaces.\\n\\nThe focus of the data science team is digitizing the recruiting process - from cutting edge recommendations algorithms, to analytics for performance predictions, to NLP projects for candidate sourcing and profile creation. Vettery data scientists work on decision support, product optimization, and operations scaling projects across these areas. We’re growing quickly and this role has the potential for huge impact on our trajectory. You’ll be reporting directly to our VP/Head of Data Science and working closely with the product and engineering teams. The ideal candidate is a self-starter, a critical thinker and a good communicator who excels in a fast-paced environment. Join our team and help us build the game-changing data science products that will transform recruiting!\\n\\nWho you are:\\nPh.D. in Statistics, Operations Research, Mathematics, Computer Science, or other quantitative field.\\n1+ years of industry experience delivering and scaling successful and innovative machine learning products (e.g. recommendation engines, experimentation systems).\\nTrack record of success working both independently and with key stakeholders to identify and solve data science problems.\\nStrong skills in statistical languages (e.g. Python/R) and querying languages (e.g. SQL).\\nExperience with NLP a plus! (but not required)\\nWith the above said, we always encourage people of all backgrounds and experiences to apply. We understand that job listings don't always allow your unique work history to shine so we invite you to show us what you know!\\nWhat you’ll do:\\nWork with a highly motivated, fun and productive team.\\nMove quickly and deliver amazing data science products, developing creative solutions to our biggest data science and engineering challenges.\\nBuild machine learning infrastructure and models (e.g. recommendation engines, statistical models, NLP engines) that drive activity on our platform, scale our business, and enhance the user experience.\\nCollaborate with engineering and product leaders, as well as the co-founders, to frame and tackle a problem, both mathematically and within the business context.\\nCommunicate rationale and findings from analyses to facilitate operational decisions.\\nDesign and track experiments for data science products as well as analyses throughout the organization.\\nIn short, own all phases of the data science product lifecycle (exploratory data analysis, model development, model productionizing, rollout, and evaluation)\\nBenefits:\\nCompetitive salary\\nOpen vacation & sick time\\nMedical, vision, and dental insurance\\nApple laptop computer\\nFrequent team outings, lunches, and team building events\\nA beautiful office located in a sunny corner in the Flatiron district, recently named one of Fortune's 60 Best Companies to Work for in New York City\\nLots of free food: stocked kitchen + beverages\\nVettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.\",\n",
       "  \"About Point72\\nPoint72 Asset Management is a global firm led by Steven Cohen that invests in multiple asset classes and strategies worldwide. Resting on more than a quarter-century of investing experience, we seek to be the industry’s premier asset manager through delivering superior risk-adjusted returns, adhering to the highest ethical standards, and offering the greatest opportunities to the industry’s brightest talent. We’re inventing the future of finance by revolutionizing how we develop our people and how we use data to shape our thinking. For more information, visit www.Point72.com/working-here.\\nA Career with Point72's Market Intelligence Group\\nMarket Intelligence is shaping Point72 for the future by combining the most innovative data sources, analysis, and investment tools with the Firm’s traditional strengths at deep fundamental analysis of how companies and industries operate. Market Intelligence finds, tests, analyzes, and models alternative data; conducts deep fundamental research; and helps our investment teams generate alpha-producing ideas using our data and research. We produce investment insights by using machine learning techniques, fundamental company analysis, macro and sell-side research, and quantitative methodologies.\\nRole\\nAs a Data Scientist, you will support an idea generator testing investment theses using alternative data and building models to execute on the results of that research.\\nResponsibilities\\nTest research hypotheses and assumptions of researchers\\nPull data from disparate sources\\nDesign and validate models that transform data into actionable insights\\nIdentify and deploy statistical, machine learning, and deep learning methods that strike the right balance between predictive power and robustness\\nWrite efficient, modular, and dependable code, packages, libraries, and scripts\\nIterate quickly to test the additive impact of new data and research findings on alpha generation\\nDocument all work extensively\\nStay abreast of new research\\nDesirable Candidates\\nPh.D. (preferred) or M.S. in a technical field with an applied or experimental component\\n3+ years of experience in a relevant field researching real-world data problems (though not necessarily in finance)\\nExtensive experience developing algorithms and production-grade code\\nStrong programming skills in Python and SQL/NoSQL\\nExperience with cloud infrastructures\\nStrong written and verbal communication skills and a proven ability to collaborate with others\",\n",
       "  \"The Role\\n\\nWe're looking for a highly entrepreneurial Data Scientist to incubate our Analytics function focused on maximizing platform liquidity by formulating marketplace practices that strengthen the mutuality of interest between buyers and sellers on the company's exchange.\\n\\nThe perfect candidate will have a background in a quantitative or technical field, will have experience working with big data sets, and will have experience in data-driven decision making. You are focused on results, a self-starter, and have demonstrated success in using analytics to drive the understanding, growth, and success of a marketplace-based business. This position is based full time in our New York, NY office and will initially report into our Co-founder & President.\\n\\nAbout TripleLift\\n\\nTripleLift is the technology company that invents, powers and scales ads that earn consumer attention.\\n\\nHaving set the standard for respectful advertising, the company leverages its dynamic templating and computer vision technologies to flawlessly deliver and scale in-feed native ads, branded content experiences and programmatic OTT brand integrations that match the look and feel of the content that is being consumed. TripleLift is reshaping the digital advertising landscape with consumer centric advertising that drives results for advertisers and unlocks new revenue streams for digital media publishers, app developers and television networks.\\n\\nHeadquartered in New York City, TripleLift has offices across North America, Europe and Asia Pacific. For more information about TripleLift, please visit triplelift.com\\n\\nResponsibilities\\nApply expertise in quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how and why our customers buy and sell media on our platform\\nPartner with commercial and technical teams to solve problems and identify trends and opportunities\\nInform, influence, support, and execute key platform decisions and launches\\nAdditional responsibilities include but are not limited to:\\nUnderstanding ecosystems, customer behaviors, and trends\\nDefining, evaluating, and monitoring key platform metrics\\nUnderstanding root causes of changes in metrics\\nIdentifying new levers to help move key metrics\\nDesigning and evaluating experiments\\nBuilding models of customer behaviors for analysis\\nBuilding key data sets to empower operational and exploratory analysis\\nCommunicating state of business, experiment results, etc. to platform teams\\nInfluencing platform teams through presentation of data-based recommendations\\nSpreading best practices to platform teams\\nQualifications\\n5+ years experience doing quantitative analysis\\nBA/BS in Computer Science, Math, Physics, Engineering, Statistics or other technical field\\nProficiency in SQL and scripting languages (e.g, PHP, Python, Perl, etc)\\nAbility to communicate the results of analyses with platform and leadership teams to influence the strategy of the platform\\nUnderstanding of statistics (e.g, hypothesis testing, regressions)\\nExperience manipulating data sets through statistical software (e.g, R, SAS) or other methods\\nExperience automating analyses and authoring pipelines using analytics frameworks such as Databricks\\nBenefits and Company Perks\\nAmazing company culture\\nCompetitive salary and performance-based bonuses\\nOngoing professional development\\nComprehensive Medical, Dental and Vision insurance\\nEquity options\\n401(k) program\\nSnacks on snacks on snacks\\nYoga, massages, and meals\\nTripleLift Awards\\nInc. Magazine's list of fastest-growing companies - 2017, 2018, 2019\\nCrain's Best Places to Work - 2015, 2016, 2017, 2018\\nForbes Next Billion Dollar Companies - 2018 Inc.\\nCrain's Fast 50, Fastest Growing Companies in New York - 2017, 2018\\nDeloitte Technology Fast 500 - 2017, 2018\\nCEO & Co Founder Eric Berry won the Ernst & Young Entrepreneur Of The Year 2019 New York Marketing and Advertising Award\\nNote: The Fair Labor Standards Act (FLSA) is a federal labor law of general and nationwide application, including Overtime, Minimum Wages, Child Labor Protections, and the Equal Pay Act. This role is a FLSA exempt role.\",\n",
       "  'Data Scientist\\nLocations: New York, NY – Greensboro, NC - Chicago, IL – Raleigh, Durham, Chapel-Hill, NC\\nIn today’s world, customers expect companies to know who they are and what they want. Customers want to have products, services or experiences that best suit their needs delivered to them seamlessly across physical and digital channels.\\n\\nCustomers are human first: driven by dynamic wants, needs, and desires. The ability for brands to make personal, meaningful connections on a human level has never been greater and Hux by Deloitte Digital delivers on those experiences in a way that allows companies to own the customer journey end to end.\\n\\nHux is the Human Experience Platform by Deloitte Digital. We help companies connect key data sources to understand what matters most to people; connect to advanced technologies like AI and machine learning to sense and respond to those needs at scale; and connect their systems to unlock insights, create collaboration and drive acquisition, engagement and loyalty. Most importantly, we empower companies to connect with customers in personal, meaningful ways that respect them as people, not just customers.\\n\\nHux by Deloitte Digital gives companies the ability to build and leverage the connections – between people, systems, data and technologies – so they can deliver personalized, contextual experiences to customers at scale.\\n\\nWork you’ll do\\nAs a Hux Data Scientist, you will be tasked with solving data problems end to end. This means:\\nBeing able to get an understanding of the multi-channel marketing optimization problem space\\nPerforming exploratory data analysis to understand relationships, opportunities to influence outcomes and how to attribute cross channel outcomes\\nSpecifying a research plan\\nRunning experiments and extracting the necessary data\\nBeing able to interpret the models being generated\\nDeveloping a proof of concept to verify your ideas\\nAssisting the Data Engineering team in setting your proof of concept live in our production environment\\nClosing the loop to make sure the proposed solution is performing as it should once it is released\\nThe team\\n\\nOur Advertising, Marketing & Commerce team focuses on delivering marketing and growth objectives aligned with our clients’ brand values for measurable business growth. We do this by creating content, communications, and experiences that engage and inspire their customers to act. We implement and operate the technology platforms that enable personalized content, commerce and marketing user-centric experiences. In doing so, we transform our clients’ marketing and engagement operations into modern, data-driven, creatively focused organizations. Our team brings deep experience in creative and digital marketing capabilities, many from our Digital Studios.\\n\\nWe serve our clients through the following types of work:\\n\\n• Cross-channel customer engagement strategy, design and development\\n\\n• (web, mobile, social, physical)\\n\\n• eCommerce strategy, implementation and operations\\n\\n• Marketing Content and digital asset management solutions\\n\\n• Marketing Technology and Advertising Technology solutions\\n\\n• Marketing analytics implementation and operations\\n\\n• Advertising campaign ideation, development and execution\\n\\n• Acquisition and engagement campaign ideation, development and execution\\n\\n• Agile based, design-thinking, user-centric, empirical projects that accelerate results\\n\\nQualifications\\nMaster’s degree required, PhD in a quantitative field (engineering, mathematics, physics, machine learning, statistics or computer science) preferred.\\n3+ years of industry experience outside of academia.\\n3+ years of experience manipulating big data using opensource frameworks. This means being familiar with tools such as Spark, Kafka, Hadoop, Impala and Luigi. In addition, because we handle very large volumes of data, past work with cloud-based environment is required.\\nKnowledge of current modeling tools such as XGBoost or Vowpal Wabbit (or equivalent)\\nAbility to write high-quality Python code and familiarity with unit testing, source control and code review\\nExperience working in Agile teams and using Scrums to organize work\\nDemonstrated communication capabilities. Ability to explain your insights to people beyond the data science team. This means knowing how to best craft your message and selecting the right visualizations\\nGood problem decomposition skills and autonomy when faced with solving data problems.\\nHelpful, but not required:\\nA proven ability deploying automated data processing pipelines.\\nA deep interest in Data Science and AI. A good measure of this would be competition participation (Kaggle or other) as well as participation in open courses\\nIf you have code in the open domain (for example Github) or have written about AI/DS please share this with us\\nSome experience in Ad Tech or Marketing technology is a plus.\\n\\nHow you’ll grow\\n\\nAt Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe there’s always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.\\n\\nBenefits\\n\\nAt Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.\\n\\nDeloitte’s culture\\n\\nOur positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.\\n\\nCorporate citizenship\\n\\nDeloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloitte’s impact on the world.\\n\\nRecruiter tips\\n\\nWe want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area you’re applying to. Check out recruiting tips from Deloitte professionals.]\\n\\nHUX020\\nAs used in this posting, “Deloitte” means Deloitte Consulting LLP, a subsidiary of Deloitte LLP. Please see www.deloitte.com/us/about for a detailed description of the legal structure of Deloitte LLP and its subsidiaries. Certain services may not be available to attest clients under the rules and regulations of public accounting.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law.\\n\\nRequisition code: E20NYCCACSCG221-CM',\n",
       "  \"Education\\nBachelor's Degree\\nSkills\\nSQL\\nR\\nHive\\nAnalytics\\nBig Data\\nTitle: Analyst, Data Science\\n\\nLocation: New York City\\n\\nSummary:\\nJ.Crew's Data Science team is responsible for leveraging vast amounts of data to better understand and predict customer behavior that in turn helps improve customer experience and inform business decisions. In this capacity, our team works across the organization to represent the customer's point of view on matters involving marketing, merchandising, planning, capital investments, budgeting, and other financial decisions. The Analyst position is primarily responsible for managing, accessing, and analyzing large amounts of data for routine reporting and ad hoc projects.\\n\\nThe ideal candidate is:\\nAnalytical and curious, especially about how numbers can define company strategy\\nExtremely detail oriented\\nAn excellent communicator with little fear of asking questions and seeking the answers\\nComfortable with managing tasks efficiently through to completion\\nA high-energy, results oriented individual\\nEager to develop both analytical and technical skills required to effectively analyze large data sets\\n\\nResponsibilities include:\\nPull large sets of customer data from J.Crew's customer database; audit the quality of this data against other sources of data\\nMaintain regular reports, like customer dashboards\\nField requests from all functional business areas\\nSupport team projects, like customer lifetime value calculations and developing forecasts\\nLiaison with our marketing database provider to ensure data is being processed correctly, data feeds contain all necessary information, and manage any changes or updates to the current database to support the growing business\\n\\nQualifications:\\n1-2 years of experience showcasing strong analytical skills; experience with customer analytics or web analytics preferred\\nBS/BA degree or equivalent combination of education and experience sufficient to successfully perform essential functions of the job\\nAbility to pull and analyze large data sets.\\nProficiency with SQL, R, Python, Big Data Tools like Hive and SPARK.\\n\\nWe are committed to affirmatively providing equal opportunity to all associates and qualified applicants without regard to race, color, ancestry, national origin, religion, sex, marital status, age, sexual orientation, gender identity or expression, legally protected physical or mental disability or any other basis protected under applicable law.\",\n",
       "  'Etsy is looking for a Data Scientist focusing on Recommendation and Ranking to join our Data Science - Machine Learning team in Brooklyn.\\n\\nEtsy is looking for a Data Scientist to join our Data Science team in Brooklyn, New York. We are committed to advancing E-commerce related fields by building technologies that help Etsy buyers and sellers discover and celebrate handmade goods from all over the world. We are seeking individuals passionate in areas such as search ranking, recommender systems, information retrieval, and computational advertising.\\n\\nOur data scientists have the opportunity to make core algorithmic advances and apply their ideas in the dynamic world of E-commerce in strengthening Etsy’s global marketplace. Data scientists can publish their innovations at top tier conferences such as KDD, NIPS, ICML, ICLR, CVPR, ICCV, WWW, WSDM, SIGIR and etc. This position would be based in Brooklyn, New York.\\nAbout the Role\\nWe are working on:\\nSearch Ranking\\nRecommender System\\nComputational Advertising Ranking\\nInformation Retrieval\\nLearning to RankDeep\\nLearning to MatchQuery\\nUnderstandingIntent Understanding\\nAbout You\\nYou share our values (below) and are looking for a company that has a solid mission.\\nYou have strong analytical and quantitative skills. You are familiar with techniques in Recommender Systems, Information Retrieval, Computational Advertising, Search Ranking, Learning To Rank, Deep Learning to Match or related fields.\\nYou have a Ph.D. degree or Master degree in Computer Science, Machine Learning, or related fields.\\nYou have strong technical and programming skills. You are familiar with relevant technologies and languages (e.g. Python, Java, Scala, C++ and etc.). You have experience in or desire to learn Hadoop/Spark related Big Data technologies.\\nYou have demonstrated the capability to review and write technical papers.\\nYou can contribute to research that can be applied to Etsy products.\\nYou have the ability to quickly prototype ideas and solve complex problems by adapting creative approaches.\\nYou are a strong collaborator and communicator and you make the engineers around you grow and learn.\\n\\n\\nWhat’s Next\\n\\nInterested in working with us? Send us a cover letter and your CV or resume explaining why you’d be great for the job. We value your unique talents and point of view, so feel free to tell us what you are all about. And if you write, draw, craft, or contribute to something you’re proud of, we’d love to hear about it.\\nAt Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status.',\n",
       "  \"The Data Scientist will transform, analyze, package and present data to university administrative leadership to inform policy and decision making. The Data Scientist, in collaboration with stakeholders, develops innovative strategies for data derived insights.\\nRequired Education:\\nMaster's Degree or equivalent\\n\\nPreferred Education:\\nMaster's Degree Quantitative degree (Engineering, Mathematics, Statistics, Computer Science or computation-intensive Sciences and Humanities).\\n\\nRequired Experience:\\n5+ years experience translating business problems into data problems with different stakeholders or equivalent combination of education and experience. Relevant training/experience in data science and/or statistical methods.\\n\\nPreferred Experience:\\n2+ years Experience in quantitative and qualitative research, statistical analysis, reporting, and/or data analysis of large, longitudinal data sets. Familiarity with the operational and business needs of a large higher education institution.\\n\\nRequired Skills, Knowledge and Abilities:\\n• Demonstrated experience querying and processing large data sets. • Demonstrated experience analyzing and reporting on large data sets. • Demonstrated ability to tell stories with data using data visualization software or other reporting/BI tools (such as Tableau, PowerBI, ArcGIS, etc.). • Demonstrated experience in programming/scripting languages and statistical software (such as R, Python, or other programming language). • Demonstrated strong verbal and interpersonal communication skills. • Demonstrated proficiency in written communication skills. • Demonstrated ability to communicate advanced analytical concepts and complex quantitative analysis in a concise, clear and actionable manner. • Demonstrated ability to respond to changing priorities and ensure timely, accurate deliverables. • Demonstrated ability to work with diverse groups/populations.\\n\\nPreferred Skills, Knowledge and Abilities:\\n• Demonstrated experience in manipulating and analyzing complex, high-volume, high-dimensionality data from varying sources. • Demonstrated experience with intermediate statistical methods. • Demonstrated familiarity with diverse data analytics tools. • Demonstrated project management experience. • Demonstrated knowledge of database & data management concepts.\\n\\nNYU aims to be among the greenest urban campuses in the country and carbon neutral by 2040. Learn more at nyu.edu/sustainability.\\n\\nEOE/AA/Minorities/Females/Vet/Disabled/Sexual Orientation/Gender Identity\",\n",
       "  'Audible, an Amazon company, is the leading provider of premium digital spoken audio information and entertainment on the internet, offering customers a new way to enhance and enrich their lives every day.\\n\\nAudible Data Scientists are members of an interdisciplinary research team with an integral role in the design and integration of models to automate decision making throughout the business in every country. We empower the cutting-edge machine learning and deep learning techniques in the many areas of the business, including but not limited to Customer: segmentation, acquisition, retention, engagement; Product: customer experience optimization, simulation, testing and evaluation, Content: recommender system, natural language processing in text and voice, and International business etc. We translate business goals into agile, insightful analytics and seek to create value for both stakeholders and customers and deliver findings in a clear and actionable way.\\n\\nWe are currently looking for a Data Scientist to support the ongoing development of Product vertical. The candidate will be expected to work closely with our team members on the design, development, deployment of ML/NLP/DS models which built upon the cutting-edge technologies. Additionally, we are seeking candidates with strong rigor in data sciences, engineering, creativity, curiosity, and great judgment.\\n\\nKEY RESPONSIBILITIES\\n· Identifying necessary, relevant, and novel data sources and acquiring data, which often means building the necessary SQL / ETL queries, import processes through various company specific interfaces for accessing Red Shift storage systems, but also by building relationships with stakeholders and counterparts all over the world in order to form trusting, functional relationships that provide for a sustainable flow and sharing of information.\\n· Exploring data will occupy the largest portion of attention, and should be second nature in order to deeply understand the phenomenon being modeled, and the validity and reliability of the inputs, including but not limited to inspecting univariate distributions, exploring bivariate relationships, constructing appropriate transformations, and tracking down the source and meaning of anomalies when and where they arise.\\n· Model building should draw from any approach that enhances accuracy and understanding including statistical modeling, mathematical modeling, network modeling, social network modeling, natural language processing, machine learning, algorithms, genetic algorithms, and neural networks.\\n· Validating models against alternative approaches, expected and observed outcome, and numerous directly and indirectly relevant business defined key performance indicators.\\n· Reviewing models of peers for the purpose of reducing and managing risk to the business, and maximizing improvement of business practice and customer experience.\\n· Implementing models from the initial evaluation of the computational demands, accuracy, and reliability of the relevant ETL processes, and the integrity of the data sources in production, to the computational demands, accuracy, and reliability of the simpler scoring processes, to the computational demands, accuracy, and reliability of model training in higher and higher frequencies in the production environment.\\n· Operationalization will include identifying the stakeholders in the life of a model score throughout the company and around the world, forming trusting bonds, understanding the phenomenon, optimizing the model output for integration to the practice, assuring buy in from practitioners as well as leaders, training for proper utilization and communication, and assessing proper utilization.\\n· Model management will include developing sustainable, consumable, accurate, and impactful reporting on model inputs, model outputs, observed outputs, business impact, and key performance indicators.\\n· Data scientists must be able to discuss their research in any level of detail with their peers, and with appropriate calibration to stakeholders in small and large group settings. They are expected to acquire support and partnership from personalities through the company, including engineering and research teams at Amazon globally. Successful data scientists are expected to influence and mentor each other, influence and mentor their peers globally, and influence and mentor their partners in the business in every country.',\n",
       "  'Hi,\\n\\nGreetings of the Day!!!\\n\\nLooking for Data Scientist for below requirement.\\n\\nJob Title: Data Scientist with SQL, Python, R, Tableau.\\n\\nLocation: Manhattan, NY\\n\\nDuration: 12 Months Plus\\n\\nInterview Mode: F2F Mandatory.\\n\\nDescription:\\n\\nExperience requirements\\n5+ years as a data scientist leading efforts to identify relevant questions, collect data from a multitude of different data sources, organize the information, translate results into solutions, and communicate findings in a way that positively affects decisions\\n5+ years working with SQL, Python, R, Tableau and other data science programing languages and tools\\nStrong quantitative and problem-solving skills\\nExperience and passion for data wrangling, data cleaning, and ETL\\nExperience working with administrative data sets\\nExperience with statistical modelling and machine leaning analysis\\nExperience with Bayesian analysis\\nProficiency in GIS concepts and software (ArcGIS, Google Maps, QGIS, Carto)\\nAttention to detail for documenting work processes and writing clear instructions for technical tasks\\nAbility to distill complex material into actionable recommendations\\nExcellent written and oral communication skills\\nJOB Duties\\n\\nÂ Develop SQL and Python queries to analyze the completeness and quality of key data elements in StreetSmart, including demographics, caseload history, mental illness diagnoses, and substance abuse details\\n\\nÂ Develop and monitor a data cleaning prioritization plan, working with a data analyst dedicated to data cleaning\\n\\nÂ Manipulate and analyze administrative data in order to predict outcomes and make data-driven recommendations\\n\\nÂ Apply statistical and data mining techniques to conduct performance audits, trend analysis, and predictive analytics using StreetSmart data\\n\\nÂ Collaborate with team members to develop novel strategies for technical analysis\\n\\nÂ Evaluate ethical implications of design choices for predictive analytics models and automated decision support systems\\n\\nÂ Create and present compelling reports to stakeholders based upon project findings and methods\\n\\nÂ\\n\\nIf interested, kindly do share profiles to madhavi(at)impetususa(dot)com',\n",
       "  \"Our Teams mission statement:\\nWe are on a mission to harness the collective intelligence of humanity to solve the world's most impactful problems. We created an AI-powered problem-solving engine capable of coming up with creative ideas, hypotheses and solutions for a broad range of problem domains and industries, thus enabling social and business impact worldwide.\\nWe are our products global ambassadors. Working across countless industries and domains, equipped with our high expertise and unique set of skills, we enable our clients to drive the most business value out of our engine and maximize the revolutionary discoveries it provides.\\nWhat will you do?\\nSolve various problems and make a true impact across unlimited domains and industries (Financial Risk, Health, Ecommerce, Marketing, Oil & Gas Exploration and more.).\\nDeliver business value by translating complex data into meaningful insights.\\nBe the face of the company and the product directly interfacing with our partners around the globe.\\nPartner with our customers Data Science teams, helping them optimize their use of SparkBeyonds Discovery Platform.\\nWork directly with our in-house team comprised of the brightest minds in technology, research and mathematics as well as senior interfaces from leading companies across the globe (fortune 500 companies and many others).\\nUse your analytical skills and deep product understanding to consult our partners and provide them with advice and input of the highest quality.\\n\\nWhat will you need?\\n4+ years of relevant experience in Data Science\\nMS/PhD in Math, Statistics, Computer Science, Physics, Bioinformatics or another quantitative field\\nProven experience in translating business challenges into data pipelines & model framework\\nCustomer facing experience and excellent presentation and communication skill\\nExperience with Machine Learning techniques (classification, regressions, feature selection etc.)\\nProgramming skills that allow you to be self-sufficient in handling data (Python, SQL, Scala, Java)\\nExperience with large datasets and distributed computing (Hive/Hadoop, Spark)\\nExperience with statistical tools or packages (R / RapidMiner / Scikit)\\n\\nThis is a full-time, salaried position, located on-site at our office in New York City. We offer health insurance for you and your family, a 401k, team lunches, and a pet friendly office environment.\",\n",
       "  'Job Description: Looking for a new Data Science opportunity? This is great opportunity to work with large datasets along with building machine learning models from scratch. This is a great chance to work with various analytics problems for all aspects of the business. This role with have projects in NLP, Text Mining, AI and Statistical Analysis.\\n\\nQualifications:\\nMasters Degree or PhD Statistics, Data Science, or Computer Science\\nMust be Expert level in R and or Python\\nExperience working in Azure\\n2+ years data science experience\\nPreferred experience in Bayesian inference, Classification, Anomaly detection, Outliers analysis, Optimization, Trends analysis, Time-Series methods, and Markovian processes.\\n\\nKeywords: Python, R, Azure, machine learning, statistical modeling, anomaly detection, time-series analysis, classification, natural language processing, NLP, data mining, artificial intelligence, AI, statistical analysis, predictive modeling, statistics, data science',\n",
       "  \"Data Analyst and Data Scientist - New York, NY, Summit, NJ or San Francisco, CA\\n\\nAbout Charles River Associates\\n\\nCharles River Associates is a leading global consulting firm that provides independent economic and financial analysis behind litigation matters, guides businesses through critical strategy and operational issues to become more profitable, and advises governments on the economic impact of policies and regulations. Our two main services – litigation and management consulting – are delivered by practice groups that focus on specific areas of expertise or industries. Learn more about how CRA can help you Accelerateyour career.\\n\\nCRA's Life Sciencespractice helps pharmaceutical, biotechnology, diagnostics and medical device companies achieve optimum performance across key aspects of their business, including innovation, commercial success, organization, and reputation, all for the long-term benefit of patients, clinicians, employees, and shareholders. We work with life sciences businesses directly as well as with their legal counsel, industry associations, government organizations, and advocacy groups from the earliest stages of research into new therapy areas, through product development, to the delivery of products that will achieve commercial success and fulfill the expectations of our clients and their key stakeholders.\\n\\nEssential Functions\\n\\nAs part of Analytics Center of Excellence (COE), the Data Analyst and Data Scientist roles are important to CRA's project teams. You will work closely with internal business consultants and/or clients to solve interesting analytical problems for our clients in areas including commercial strategy, pricing & market access and litigation. In these roles, you will apply your coding and modelling skills, own and personally deliver components of the client engagement, contributing to the overall success of each project. At CRA, we offer a wide range of models/project types, e.g. Patient Journey, Physician Segmentation, Promotional Response Modelling, Forecasting, Web Scraping, etc. As you demonstrate strong analytic and develop sound project management skills, your responsibilities will grow rapidly.\\n\\nData Analyst\\nCandidates possessing a Masters degree or the equivalent work experience will be considered for the Data Analyst role\\nData Analysts will generally be key contributors to project teams, focusing on data manipulation, modeling, and delivery of analysis results\\nData Analysts who demonstrate strong performance and ability to take on additional responsibility may rapidly ascend to the Data Scientist role\\nData Scientist\\nCandidates with a Masters degree plus 2 years of relevant work experience OR a PhD may be considered for the Data Scientist role\\nData Scientists are generally key contributors to both the content and design of analyses, and will often lead project teams and manage key analytics work streams\\nCandidates with additional relevant experience may be considered for more senior positions, such as Senior Data Scientist\\nResponsibilities\\nImport, clean, manipulate and analyse large volume of data (>100MM records) to develop critical insights for our clients' issues, using SAS, SQL or other tools\\nBuild statistical and machine learning models to detect patterns and investigate important drivers, e.g. regression, Random Forest, Neural Network, clustering, Bayesian, etc., using SAS, R, Python or other specialized tools (e.g. LatentGold, SPSS)\\nPerform text mining and web scraping from online data sources using NLP-relevant packages in Python\\nApply critical perspective across multiple clients and industry data sources (PharMetrics, Truven MarketScan, XPONENT, etc.)\\nCommunicate findings, through visuals, data tables and models, from your analyses to internal team members and, over time, to clients\\nWork in a demanding but highly collegial and collaborative environment\\nThis is NOT a technical IT role\\nQualifications\\nMaster degree or above, preferably in Statistics, Mathematics, Machine Learning, Computer Science or mathematically-intensive field\\nAbility to proficiently process and manipulate data in SQL, SAS, R or Python\\nExperience in developing advanced models such as multi-variate regression, neural networks, support vector machines, decision trees, clustering and time series using tools such as R and Python\\nDemonstrate good written and verbal communication skills. Able to present information to various audiences\\nAbility to work collaboratively in a team environment and effectively with people at all levels in an organization\\nAbility to provide creative solutions to non-standard analytical problems\\nExperience in data visualization using Power BI, Tableau, D3.js, Qlik Sense, HTML/JavaScript is a plus\\nKnowledge or experience related to pharmaceutical industry and data (e.g. PharMetrics, Truven, Symphony, Optum, Xponent PlanTrak, DDD) is a plus\\nKnowledge of any of the following: Natural Language Processing & Text Mining, Experimental Design, Bayesian Networks, Network/Graph Mining is a plus\\nCandidates with different, specific expertise in broader analytics area (e.g. Data Engineering, Social Listening, Optimization, Simulation) are welcome to apply\",\n",
       "  'Two Sigma is a different kind of investment manager. Since 2001, we have used data science and technology to derive insights that forecast the future and discover value in markets worldwide. Our team of scientists, technologists and academics looks beyond traditional finance to understand the bigger picture and develop creative solutions to some of the world’s most challenging economic problems. Our work spans across markets and industries, from insurance and securities to private investments and new ventures.\\n\\nWhen you work with us, you tackle tough problems alongside other scientists and engineers. People who will challenge your ideas. Who you can really learn from, and collaborate with. And you’ll be doing work that matters to a lot of people, too. Our investors include some of the world’s largest retirement funds, research institutions, educational endowments, healthcare systems and foundations. We admire what they do, and we’re proud to serve these organizations.\\n\\nYou will take on the following responsibilities:\\nUse the scientific method to develop sophisticated investment models and shape our insights into how the markets will behave\\nApply quantitative techniques like machine learning to a vast array of datasets\\nCreate and test complex investment ideas and partner with our engineers to test your theories\\nAll the while, you’ll remain engaged in the academic community. As examples, you can:\\nJoin our reading circles to stay up to date on the latest research papers in your fields\\nAttend academic seminars to learn from thought leaders from top universities\\nThe internship program lasts 10 weeks in the summer and takes place at our Soho-based, New York City office. You will partner with an assigned mentor and work on a single project during the course of your time here, which will culminate in a final presentation at the conclusion of the program.\\n\\nYou should possess the following qualifications:\\nAre pursuing a degree in a technical or quantitative disciplines, like statistics, mathematics, physics, electrical engineering, or computer science with approximately one year remaining in your programs (all levels welcome, from bachelor’s to doctorate)\\nDemonstrate intermediate skills in at least one programming language (like C, C++, Java, or Python)\\nPerformed an in-depth research project, examining real-world data\\nAre an independent thinker who can creatively approach data analysis and communicate complex ideas clearly\\nYou don’t need a background in finance. It’s nice to have, but more than half of Two Sigma’s employees come from outside the finance industry. If you’ve got the quantitative skills, we can teach you the financial aspects of the job.\\n\\nWe are proud to be an equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics.',\n",
       "  'Job Title: Data Scientist\\n\\nLocation: New Jersey\\n\\nDuration: Long (Contract)\\n\\nRate: $60/hr.\\n\\nClient: DXC\\n\\nÂ\\n\\nWho are we looking for?\\n\\nLooking for Data Scientist resource - Who has Sound knowledge in the product support and implementation\\n\\nÂ\\n\\nTechnical Skills:\\n\\n5+ years of hands-on experience as a Data Scientist with â\\n\\nâ Undertaking data collection, preprocessing and analysis\\n\\nâ Building models to address business problems\\n\\nâ Presenting information using data visualization techniques\\n\\nâ Experience in data mining, business intelligence tools, data frameworks\\n\\nâ Understanding of machine-learning and operations research\\n\\nâ Knowledge of R, SQL and Python; familiarity with languages such Scala, Java or C++ would be an advantage',\n",
       "  'Job Description and Responsibilities\\n\\nInteractive Brokers Group (IBG) is looking for a junior Data Scientist. The successful candidate will execute analytical tasks on large data sets to support decision makers and provide insight and reports about our platforms. The data scientist role will use data to help us better understand who our clients are, how they engage with our products and services and how we can better serve them by identifying interesting and useful trends in our data. You will make an impact by using your passion for data and analysis to generate actionable insights that we will use to improve the experience for our clients. The position responsibilities include but are not limited to:\\nCollect and analyze various telemetry and behavioral data\\nProduce daily metrics and reports\\nClean and prepare data for Machine Learning and A.I.\\nAssist with designing and running A/B tests\\nAutomate frequently used reporting and data analysis workflows\\nWork closely with software engineers and architects to extract, transform and standardize data for optimal use for analytic tools\\n\\nQualifications\\n\\nBachelors in Computer Science, Mathematics, Statistics or related fields; Masters strongly preferred\\n2+ years\\' experience in a data science or data analysis role\\n2+ years of Java coding experience\\n2+ years mining and analyzing data sets to extract meaningful trends, producing meaningful and actionable reports\\nExperience using statistical programming languages, machine learning and other toolkits and techniques for analyzing large, complex datasets\\nTechnical proficiency with transforming structured and unstructured data sets\\nStrong analytical skills, attention to detail and accuracy\\nExpert problem solving skills and creative thinking ability\\nThis position is not sponsoring for employment authorization to work in the U.S.\\n\\nCompany Overview\\n\\nInteractive Brokers (\"IBKR\"), a subsidiary of publicly-traded Interactive Brokers Group, Inc., based in Greenwich, Connecticut (IEX: IBKR) is a low-cost provider of trade execution and clearing services for active traders, institutional investors, financial advisors and introducing brokers. IBKR\\'s premier technology provides electronic access to stocks, options, futures, forex, bonds, and funds worldwide from a single IBKR Integrated Investment account. IBKR is one of the largest online brokers by trade volume and is consistently ranked at the top of its field.\\n\\nOur employees are part of a dynamic, multinational, fast-paced, results-oriented team working to provide our customers with state-of-the-art trading technology, superior execution capabilities, worldwide electronic access, and sophisticated risk management tools.\\n\\nOur headquarters are in Greenwich, CT, USA. IBKR has offices in the United States, Canada, the United Kingdom, Switzerland, Hungary, Estonia, Russia, India, Hong Kong, China, Japan and Australia.\\n\\nIBKR is a member of NYSE, FINRA, and SIPC. Interactive Brokers Group brokerage affiliates are regulated by securities and commodities agencies around the world.\\n\\nCurious about our what it\\'s like to work at IBKR? Click the HERE to view a short message from current employees.\\n\\nFor more information, please visit www.ibkr.com/info',\n",
       "  \"About Us:\\n\\nAt Rent the Runway, our mission is to make women feel empowered and self-confident every single day by combining best in class technology, logistics, and customer service. Since our launch in 2009, we've developed proprietary technology, a one-of-a-kind reverse logistics operation, stores of the future, a viral brand, relationships with hundreds of fashion designers - and we are passionate about continuing to innovate our customer experience. We have pioneered the closet in the cloud and believe that every person globally will soon have a subscription to fashion. We are proud to be both a profitable and fast-growing business, with 11 million members of our community who believe that rental is the future.\\n\\nAbout the Job:\\n\\nRent the Runway has a complex data ecosystem that spans web and infrastructure logs, transactions and recurring subscription behavior, and tracking each inventory unit and throughput of our home grown reverse logistics rental operations.\\n\\nData systems power critical business decisions - from inventory assortment to operational efficacy, from marketing actions to personalized website features and customer experience enhancements - and we want to scale the data science function across all these vectors.\\n\\nWhat You'll Do:\\nMine the data ecosystem & find fruitful signals\\nDevelop and deploy performant algorithms powering applications\\nExamples of applications include consumer-facing recommendation engines, inventory allocation systems, and internal apps for inventory buying and customer experience\\nDo bleeding-edge work including optimizing how products fit customers (unsolved as of yet), using AI techniques for inventory buying, and real-time streaming/model computations\\nAbout You:\\nStrong academic background in Computer Science, Mathematics, Physics or a related quantitative/engineering field\\nExperience developing software or data products, particularly in Python, our go-to language\\nWe also use R for ad-hoc modeling and analysis. Some understanding of java, the main language for our back-end systems, is a plus\\nPassion for data and its ability to drive serious business impact\\nAbility to work independently and take responsibility in a function that is growing in strength but is also core to the company's growth\",\n",
       "  \"Prognos is an NYC-based healthcare startup whose moonshot is to Eradicate disease so that humanity can fulfill their potential. In order to achieve this goal we have curated the world's largest clinical lab dataset --covering almost 200M patients in the US-- and are currently deploying cutting-edge technology for predicting disease at the earliest possible time.\\n\\nThe Mission of the Data Science team at Prognos is to develop, deploy and maintain AI/ML pipelines within all of Prognos' products, addressing business-relevant problems in close collaboration with Engineering, Clinical and Product teams.\\n\\nWe are looking for a hands on (IC) Data Scientist to join the team, and help us move this mission-critical task forward. This position will have an initial focus of driving our natural language processing (NLP) capabilities forward, then expand into general predictive modeling.\\n\\nCandidates must have a degree in Statistics/Biostatistics, Computer Science, or a related quantitative field; Masters or higher is preferred. At least 3 years of hands on machine learning (ML) experience required.\\n\\nRequired Skills and Experience\\nStrong statistical analysis background with a deep understanding of a variety of ML algorithms and statistical modeling techniques\\nEnthusiastic about making sense of unstructured data using NLP or graph-based techniques\\nAbility to code a proof of concept in a short period of time (in a programming language of their choice)\\nHave at least 2 years of industry experience in NLP or sequence modeling (classic random-field or markovian techniques as well as modern attention-based ones)\\nWillingness to stay abreast of and evaluate recent advances in NLP and sequence modeling in general\\nExperience with relevant software tools, such as Python, PyTorch, Gensim, Spacy, scikit-learn, statsmodels, and/or others. You'll be free to use whatever tools you feel are appropriate\\nComfortable extracting data using SQL, Apache Spark or other distributed compute engine\\nAccustomed to working with GIT and shared codebases\\nExtensive data cleaning and textual data manipulation experience\\nStrong communication skills. Experience presenting experimental to people who are not Data Scientists\\nAbility to collaborate with the team and translate existing research into practical solutions and products\\nAbility to build and maintain relationships with various collaborators across the company, and take ownership of data science projects\\nPreferred Skills and Experience\\nExperience with complex, high dimension, sparse time series data\\nExperience with PyTorch Geometric, GPyTorch and more specifically PyTorch's NLP ecosystem\\nExperience with healthcare data and/or insurance data a plus\\nPrognos Values & Culture\\nBe Collaborative: Always do what is best for the client. Check your ego at the door. We deliver premium value at a premium price. Practice blameless problem solving. Create win/win solutions.\\nBe Courageous: Do the right thing, always. Look at the facts and don't assume.\\nBe Curious: Think big and start small. Be relentless about improvement. Be predictive. Be curious and always ask why\\nBe Enthusiastic: Celebrate success. Be enthusiastic and positive. Let's have fun. Have purpose and believe in the greater good.\\nBe Driven: Act with a sense of urgency. Take ownership and honor commitments. Either find a way or make one. Deliver results.\\nBe a Superstar: Make quality personal. Deliver remarkable client service. Be better than your previous self. Go above and beyond.\\nOur culture guide: http://bit.ly/2ISMzjl\\nAbout Prognos\\n\\nPrognos is a healthcare AI company focused on predicting disease to drive decisions earlier in healthcare in collaboration with payers, Life Sciences and diagnostics companies. The Prognos Registry is the largest source of clinical diagnostics information in 40 disease areas, with over 16B medical records for 185M patients. Prognos has 1000 extensive proprietary and learning clinical algorithms to enable earlier patient identification for enhanced treatment decision-making, risk management and quality improvement. The company is supported by a $42M investment from Safeguard Scientifics, Inc. (NYSE: SFE), Merck Global Health Innovation Fund (GHIF), Cigna (CI), GIS Strategic Ventures, Hikma Ventures, Hermed Capital, and Maywic Select Investments. For more information, visit www.prognos.ai.\\n\\nOur Mission\\n\\nTo improve health by driving the best actions learned from the world's data\\n\\nOur Vision\\n\\nTo prevail over disease and empower people everywhere to live life to the fullest\\n\\nSelected Perks\\nFree FreshDirect food, snacks, drinks, etc. delivered to the office weekly\\nFlexible work arrangements and unlimited PTO\\nMonthly Happy Hours with Leadership\\nSome of our benefits include: Health Insurance, Life Insurance, Short Term and Long Term Disability, Dental, Vision, 401k, HSA, FSA, Dependent care flexible spending, commuter benefits, free access to One Medical Group, Gym discounts, flexible work hours and locations, access to the WeWork network, a Health Advocate, Employee Stock Option Plan, and more\",\n",
       "  \"Please Note: this role is for experienced candidates only. Quantitative Research & Trading roles for campus candidates can be found by using the On-Campus Recruiting filter on our Careers Page.\\n\\n--\\n\\nTower Research Capital, a high-frequency proprietary trading firm founded in 1998, seeks experienced Quantitative Research Analysts.\\n\\nAs a member of one of Tower's trading teams, a Quantitative Research Analyst will be using Tower's in-house trading system—one of the fastest and most comprehensive in the world—to develop and deploy algorithmic trading strategies based on patterns in market behavior.\\n\\nResponsibilities\\nDesigning, implementing, and deploying high-frequency trading algorithms\\nExploring trading ideas by analyzing market data and market micro-structure for patterns\\nCreating tools to analyze data for patterns\\nContributing to libraries of analytical computations to support market data analysis and trading\\nDeveloping, augmenting, and calibrating exchange simulators\\nQualifications\\nA PhD from a top-tier university\\n1-3 years of research experience in high-frequency trading\\nA strong background in mathematics and statistics\\nProficiency in back-testing, simulation, and statistical techniques (auto-regression, auto-correlation, and Principal Component Analysis)\\nSolid data-mining and analysis skills, including experience dealing with a large amount of data/tick data\\nFamiliarity with signal generation and statistical models\\nStrong programming skills in C++, MATLAB, and R\\nBenefits\\n\\nTower is headquartered in New York City, and has offices around the globe. While we work hard, Tower's cubicle-free workplace, jeans-clad workforce, and well-stocked kitchens reflect the premium the firm places on quality of life. Benefits include:\\nCompetitive salary and performance-based bonuses\\n5 weeks of paid vacation plus paid holidays\\nFree breakfast, lunch, and snacks on a daily basis\\nReimbursement for health and wellness expenses\\nTower Research Capital is an equal opportunity employer.\",\n",
       "  \"Description: </br>\\nLooking for opportunities to use cutting edge technologies analyzing petabytes of data in a world class Hadoop cluster, generating insights to guide consumers in their journey to wellness and help them achieve their health ambitions, whether its running the Inca Trail Marathon or playing tackle football with their grandkids? Aetna's Member Analytics team is focused on delivering strategically-impactful programs and tools to help members across all life stages feel the joy of achieving their best health, in their own way.\\n\\nSUMMARY\\nThis position is for a seasoned data scientist role in the experimentation and campaign management platform team that will help to develop a world class experimentation, optimization and personalization platform. The candidate will work with a team of data scientists to develop analytics module with a full suite of A/B testing features, build machine leaning models to provide real time next best action for users and create a self learning optimization engine.\\nThis is a unique opportunity to work on new transformation initiatives with CVS Health as a result of the recent company merger\\n\\n60582\\n\\n</br></br> Fundamental Components: </br></br></br> Background Experience: </br>\\n2-4 years of relevant work experience within eCommerce, digital analytics, insurance, finance or other related fields\\nExperience with big data, preferably using Spark, Hadoop and Hive.\\n3+ years experience in statistical analysis, algorithms, building machine learning models\\nFamiliarity with experiment design\\nExperience with using R, Python. PySpark to manipulate large data sets and develop statistical models\\nExcellent problem solving skills, critical thinking and conceptual thinking abilities\\nSuperior skills to effectively communicate technical ideas and results to non-technical clients in written and verbal form\\nAbility to negotiate and manage interactions across the business and project stakeholders\\nExperience regularly working collaboratively with others to solve problem\\n</br></br> Potential Telework Position: </br>No</br></br> Percent of Travel Required: </br>0 - 10%</br></br> EEO Statement: </br>Aetna is an Equal Opportunity, Affirmative Action Employer</br></br> Benefit Eligibility: </br>Benefit eligibility may vary by position. Click here to review the benefits associated with this position.</br></br> Candidate Privacy Information: </br>Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.</br></br>\",\n",
       "  \"Doximity is transforming the healthcare industry. Our mission is to help doctors be more productive, informed, and connected. As a Data Analyst, you'll work within cross-functional delivery teams alongside other analysts, engineers, and product managers in discovering data insights to help improve healthcare.\\n\\nOur team brings a diverse set of technical and cultural backgrounds and we like to think pragmatically in choosing the tools most appropriate for the job at hand.\\n\\nAbout Us\\nHere are some of the ways we bring value to doctors\\nOur data stack run on Python, Snowflake, Spark, and Airflow\\nOur web applications are built primarily using Ruby, Rails, JavaScript (Vue.js), and a bit of Golang\\nWe have over 350 private repositories in Github containing our applications, forks of gems, our own internal gems, and open-source projects\\nWe have worked as a distributed team for a long time; we're currently about 65% distributed\\nFind out more information on the Doximity engineering blog\\nOur company core values\\nOur recruiting process\\nOur product development cycle\\nOur on-boarding & mentorship process\\nHere's How You Will Make an Impact\\nCollaborate with a team of product managers, analysts and other developers to define and complete data projects from data ingestion, to analysis to recommendations.\\nShow off your engineering skills by creating data products from scratch and automating code so they can be re-used continually.\\nLeverage Doximity's extensive datasets to identify and classify behavioral patterns of medical professionals on our platform.\\nPlay a key role in creating both product and client-facing analytics.\\nGrow into a presentation/communication-focused role or dive deeper into more-involved technical challenges - the choice is yours.\\nAbout you\\nB.S. or M.S. in quantitative field with 2-4 years of experience.\\nWorking knowledge of statistics and visualization.\\nExpert SQL skills with proven ability to create and to evaluate complex SQL statements involving numerous tables and complex relationships.\\nFluent in Python and experience using common modules (numpy, pandas, statsmodels, matplotlib) for EDA.\\nUnderstanding of Object Oriented principles and testing as it relates to Data and Python.\\nComfortable with UNIX command line interface and standard programming tools (vim/emacs, git, etc.)\\nExcellent problem solving skills and a strong attention to detail.\\nAbility to manage time well and prioritize incoming tasks from different stakeholders.\\nFast learner; curiosity about and passion for data.\\nPreferred Qualifications:\\nExperience with Amazon Web Services products (EC2, S3, Snowflake).\\nPrior exposure to workflow management tools (Airflow).\\nPrior exposure to machine learning techniques (regressors, classifiers, etc).\\nExperience leveraging Apache Spark to perform analyses or process data.\\nBenefits\\nDoximity has industry leading benefits. For an updated list, see our career page\\nMore info on Doximity\\n\\nWe're thrilled to be named the Fastest Growing Company in the Bay Area, and one of Fast Company's Most Innovative Companies. Joining Doximity means being part of an incredibly talented and humble team. We work on amazing products that over 70% of US doctors (and over one million healthcare professionals) use to make their busy lives a little easier. We're driven by the goal of improving inefficiencies in our $3.5 trillion U.S. healthcare system and love creating technology that has a real, meaningful impact on people's lives. To learn more about our team, culture, and users, check out our careers page, company blog, and engineering blog. We're growing steadily, and there's plenty of opportunity for you to make an impact.\\n\\nDoximity is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification. We also consider qualified applicants with criminal histories, consistent with applicable federal, state and local law.\",\n",
       "  \"The Role\\n\\nTripleLift is seeking an experienced data scientist to join our team full time. We are a fast growing startup in the advertising technology sector, trying to tackle some of the most challenging problems facing the industry. As a senior data scientist, you will responsible for exploring and leveraging our data in order to turn into actionable insights that help push the business forward for TripleLift and our customers. This includes optimizing our bidding algorithm, improving our computer vision logic, and planning and executing experiments to understand our real time bidding marketplace.\\n\\nAbout TripleLift\\n\\nTripleLift is building ads that people enjoy.\\n\\nAdvertising is an essential part of the internet. Yet most ads are either never noticed, or are interruptive. We make advertising that's helpful and effective.\\n\\nWe believe that by recreating the advertising experience to be seamless and focused on content, we can usher in a new era of content monetization. All day we are bombarded by ads online, many of which were created over 20 years ago. TripleLift is building the ad experience of the future.\\n\\nTripleLift is focused on delivering consumer-focused, digital advertising technology that seamlessly integrate native advertising into every channel of content consumption, including web, app, branded content, and connected TV. By utilizing our proprietary computer vision technology, TripleLift is able to dynamically assemble native ads that match publisher content that are accessible through the industry's first real-time, native programmatic exchange.\\n\\nTripleLift is one of the fastest growing companies in America, and has achieved a diversity of product lines, a leadership position in its industry and is regularly recognized as an innovator in its space - all while being consistently profitable.\\n\\nCore Technologies\\n\\nThe Data Science team employs a wide variety of technologies to accomplish our goals. From our early days, we've always believed in using the right tools for the right job, and continue to explore new technology options as we grow. The Data Science team uses the following technologies at TripleLift:\\nLanguages: Python, R, Java 8\\nFrameworks: Spark, Airflow, DataBricks\\nDatabases: MySQL, Redshift, MongoDB, S3/Parquet\\nAmazon Web Services to keep everything running\\nResponsibilities\\nOptimize our estimation models in order to improve the performance of our bidding algorithm and maximize performance for our customers;\\nPlan, execute, and analyze experiments to understand the dynamics of our real time bidding marketplace;\\nEagerly explore the dark corners of our data in order to push our business forward.\\nDesired Skills and Attributes\\nStrong Python skills, Spark is a plus;\\nProven experience building out predictive and forecasting models;\\nComfortable taking ownership of projects and showcasing key accomplishments;\\nStrives for continued learning opportunities to build upon craft;\\nExcellent organizational skills and attention to detail;\\nAbility to work quickly and independently with minimal oversight;\\nAbility to work under pressure and multitask in a fast-paced start-up environment;\\nDesire to accept feedback and constructive criticism;\\nExtremely strong and demonstrable work ethic;\\nProven academic and/or professional achievement.\\nEducation Requirement\\n\\nA Bachelor's degree in a technical subject is preferred, although candidates with relevant experience who hold other degrees will be considered.\\n\\nExperience Requirement\\n\\nAt least three years of working experience in a professional, collaborative environment.\\n\\nLocation\\n\\nNew York, NY\\n\\nBenefits and Company Perks\\nAmazing company culture\\nComprehensive Medical, Dental and Vision insurance\\nEquity options\\n401(k) program\\nSnacks on snacks on snacks\\nYoga, massages, and meals\\nOngoing professional development\\nTripleLift Awards\\nInc. Magazine's list of fastest-growing companies - 2017, 2018, 2019\\nCrain's Best Places to Work - 2015, 2016, 2017, 2018\\nForbes Next Billion Dollar Companies - 2018 Inc.\\nCrain's Fast 50, Fastest Growing Companies in New York - 2017, 2018\\nDeloitte Technology Fast 500 - 2017, 2018\\nCEO & Co Founder Eric Berry won the Ernst & Young Entrepreneur Of The Year 2019 New York Marketing and Advertising Award\\nNote: The Fair Labor Standards Act (FLSA) is a federal labor law of general and nationwide application, including Overtime, Minimum Wages, Child Labor Protections, and the Equal Pay Act. This role is a FLSA exempt role.\",\n",
       "  'This title is in a civil service competitive class that is subject to examination. Position is only open to applicants with permanent NYC civil service status as Education Analyst or candidates who apply for the Education Analyst civil service examination.\\n\\nThe filing period for the exam is 06/03/2020 - 06/23/2020. Information regarding NYC civil service exams can be found on the DCAS website below:\\n\\nhttps://www1.nyc.gov/site/dcas/employment/current-upcoming-exams.page\\n\\n(Those who previously applied need not re-apply)\\n\\nPosition Summary: The New York City school system is the largest in the country, composed of approximately 1.1 million students and 75,000+ teachers in over 1,800 schools. Within the Office of Policy and Evaluation, the Research and Policy Support Group (RPSG) conducts its own research and analyses to evaluate organizational structures, policies, programs and initiatives.\\n\\nThe Data Scientist will play a key role in EduStat, a research-based approach to school improvement that flattens our organization, elevates our values, and advances equitable outcomes for all students. The EduStat process enables our organization to monitor the progress and impact of teams’ strategic plans and is modeled on other municipal “Stat” models from NYC and other major U.S. cities.\\n\\nThe Data Scientist will be responsible for determining metrics and designing models at a Central-level to support DOE decision-making in a manner that improves organizational efficiency, promotes data transparency, and facilitates the improvement of educational initiatives. Working with the large datasets available from various sources, the Data Scientist will drive the management conversation to identify predictors of success and key early warning indicators to target interventions. While RPSG and OPE have extensive experience answering specific research and evaluation questions with a variety of analytical methods, the Department is looking to harness all the data available to identify key trends and indicators to focus leadership conversations. The Research and Policy Support Group (RPSG) plays a key role in this effort as the primary source of data analysis and research for the DOE. RPSG conducts its own research and analysis to inform DOE policy, as well as consulting on and supporting other analytic work with DOE data. Performs related work.\\n\\nReports to: Director, RPSG\\n\\nDirect Reports: N/A\\n\\nKey Relationships: Works closely with other RPSG staff; the Office of School Performance; Office of Talent Research and Data; Office of the First Deputy Chancellor, and other OSP staff, as well as with other DT&L staff, including SAS, SQL, R, and Python programmers, web developers, data quality managers, data and policy analysts and policy makers; and with the Division of Instructional and Information Technology (DIIT).\\n\\nResponsibilities\\n\\nIdentify the data analytics problems that offer the greatest opportunities for growth and improvement by building metrics and models that will comprise the basis of EduStat.\\nIdentify, evaluate, and refine metrics from other Department data systems and develop machine learning models to identify hidden patterns and trends to focus leadership conversations and support data-driven decision-making.\\nCollect and analyze large sets of structured- and unstructured data from various and distinct sources.\\nUtilize a combination of variables and data points to create metrics and cut points to identify key early warning indicators of adverse outcomes.\\nUtilize machine learning algorithms and statistics such as regression, simulation, scenario analysis, modeling, etc. to interpret data to feed EduStat reporting.\\nEvaluate metrics currently in use and identify meaningful cut points and metrics to drive attention and resources to areas of greatest opportunity.\\nCoordinate with a cross-functional team of analysts contributing to dataset creation and maintenance as well as back- and front-end designers.\\nWork closely with programmers and UX designers to support the development of business requirements and ensure relevant metrics are presented accurately, with fidelity, and in context.\\nCollaborate with DOE policymakers and educators to identify central research and analytic priorities, which can include the implementation of organizational procedures, structures, and programs, utilization of data systems and data collection, and performance standards.\\nDevelop a testing framework and test model quality; develop processes and tools to monitor and analyze model performance.\\nWork with stakeholders throughout the Department to identify opportunities to leverage student data to drive decision-making by school leaders and at the highest levels of the organization.\\nDraft communications based on updates to relevant policies and new product launches or improvements.\\nDesign and produce both ad hoc and regular reports to meet a variety of needs from diverse stakeholders which may include other teams within the Division of Teaching and Learning.\\nQualification Requirements:\\n\\nMinimum\\n\\nA master’s degree from an accredited college in economics, finance, accounting, business or public administration, human resources management, management science, operations research, organizational behavior, industrial psychology, statistics, personnel administration, labor relations, psychology, sociology, human resources development, political science, or a closely related field; or\\nA baccalaureate degree from an accredited college and two years of full-time satisfactory professional experience working with the budget of a large public or private concern in public administration, accounting, economic or financial administration, or fiscal or economic research; or in management or methods analysis, operations research, organizational research or program evaluation; or in educational, personnel or public administration, recruitment, position classification, personnel relations, labor relations, employee benefits, staff development, employment program planning/administration, labor market research, economic planning, fiscal management; or in a related area.\\n\\n\\nPlus\\n\\nProficiency with R, Python, SAS, Stata, or other statistical/data management programming languages.\\nPreferred\\n\\nExperience working with large datasets and experience with data science.\\nAn understanding of racial equity issues and a commitment to advancing racial equity.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nAbility to troubleshoot, problem-solve, and anticipate issues needing resolution.\\nAbility to clearly communicate and document technical work, including coding methodology.\\nExperience communicating research and analyses to non-researchers/analysts.\\nAbility to collaborate with programmers and analysts while also completing work independently.\\nAbility to use independent judgment to design, adapt, and modify analyses and programming to the needs of specific projects.\\nAbility to provide constructive feedback to others on their research and analytic work.\\nStrong attention to detail and well organized, with the ability to meet frequent and changing deadlines.\\nExcellent interpersonal skills in dealing with education and non-education personnel, internal and external to the DOE.\\nAbility to thrive in and enjoy working in a collaborative environment.\\nExperience or working knowledge of the public sector and/or demonstrated interest in public education.\\nInternal candidates preferred.\\nSalary: $77,407+\\n\\n(Internal candidates who are selected for this position and who currently hold comparable or less senior positions within the DOE will not make less than their current salary.)\\n\\nPlease submit a resume an cover letter with your application.\\n\\nApplications will be accepted through October 30, 2019 until 3:00 p.m.\\n\\nNOTE: The filling of all positions is subject to budget availability.\\n\\nThis position is open to qualified persons with a disability who are eligible for the 55-a program. Please indicate in your cover letter that you would like to be considered for the position under the 55-a program.\\n\\nAN EQUAL OPPORTUNITY EMPLOYER\\n\\nIt is the policy of the Department of Education of the City of New York to provide equal employment opportunities without regard to actual or perceived race, color, religion, creed, ethnicity, national origin, alienage, citizenship status, age, marital status, partnership status, disability, sexual orientation, gender (sex), military status, unemployment status, caregiver status, consumer credit history, prior record of arrest or conviction (except as permitted by law), predisposing genetic characteristics, or status as a victim of domestic violence, sexual offenses and stalking, and to maintain an environment free of harassment on any of the above-noted grounds, including sexual harassment or retaliation. For more information, please refer to the DOE Non-Discrimination Policy.\\n\\nRequired Skills\\n\\nRequired Experience\\n\\nJob Location\\nNEW YORK, US-NY',\n",
       "  'Description\\nThe Data Scientist will work closely with the operations leadership team of our innovative, app-based on-demand food delivery service organization to optimize every facet of our complex operation. You will be the epicenter of business intelligence, uncovering impactful insights that put us ahead of the curve. Your day-to-day will support aggressive growth efforts at a rapidly scaling, geographically distributed company.\\n\\nYou will tap into our rich data to help make informed decisions on inventory management, production levels, delivery van routing, hours of operation, staffing levels, etc. Your duties will take you to our markets to ensure you tangibly understand challenges and can support the in-market team members in addition to executing at the corporate level. You will become the go-to person for the operations team to make that team more effective.\\n\\nEssential Functions:\\nDevelop innovate data models to streamline the collection and synthesis of information. Use these models to recommend action items that maximize efficiency in areas such as inventory management, food production schedules, efficient routing, accurate delivery estimates, etc.\\nLeverage consumer analytics to understand customer purchasing and dining behavior. Identify key consumer segments within the customer base and summarize data-driven findings in a visual way that resonates with a variety of stakeholders and audiences.\\nAnalyze business performance data from a multitude of sources including labor tracking, route efficiency, and inventory management systems to identify opportunities to improve operations and recommend data-driven action plans.\\nLead the creation and development of key performance reporting to multiple stakeholders.\\nGenerate ad-hoc analyses to measure the outcome of short-term strategies. Work with cross-functional teams to define insights and analytics required to measure success.\\nFrequent meeting and collaboration with the leadership team to identify and track Food & Beverage KPIs and generate actionable insights to drive revenue.\\nTeaming with the leadership team to generate and support Tableau dashboards to provide actionable data to key stakeholders.\\n\\nQualifications\\n\\nBachelors degree required in Mathematics, Statistics, Accounting or Finance or another scientific discipline.\\nMinimum of 1-2 years of experience in an analytical function preferred.\\nSuperior analytical ability and spreadsheet modeling skills are required.\\nExcellent written and verbal skills and the ability to interface with multiple levels of management are essential.\\nStrong organizational, project management and time management skills\\nAbility to effectively communicate findings, analysis, implications, and recommendations to varied stakeholder audiences\\nMust be able to problem solve and adjust priorities with little advance notice to meet deadlines and requests.',\n",
       "  \"Etsy is looking for a Data Scientist to join our Data Science team in Brooklyn, New York. We are committed to advancing E-commerce related fields by building technologies that help Etsy buyers and sellers discover and celebrate handmade goods from all over the world. We are seeking individuals passionate in areas such as Computer Vision and Signal & Image Processing in general, with experiences in Image Classification, Object Detection, Visual Question and Answering, and Deep Learning.\\n\\nOur data scientists have the opportunity to make core algorithmic advances and apply their ideas in the dynamic world of E-commerce in strengthening Etsy’s global marketplace. Data scientists can publish their innovations at top tier conferences such as KDD, NIPS, ICML, ICLR, CVPR, ICCV, WWW, WSDM, SIGIR and etc. This position would be based in Brooklyn, New York.\\nAbout the Job\\nWhat we're working on:\\nComputer Vision\\nSignal and Image Processing\\nMachine Learning\\nDeep Learning\\nAbout You\\nYou share our values (below) and are looking for a company that has a solid mission.\\nYou have strong analytical and quantitative skills. You are familiar with techniques in Computer Vision and Signal & Image Processing in general, with experiences in Image Classification, Object Detection, Visual Question and Answering, and Deep Learning or related fields.\\nYou have a Ph.D. degree or Master degree in Computer Science, Machine Learning, or related fields.\\nYou have strong technical and programming skills. You are familiar with relevant technologies and languages (e.g. Python, Java, Scala, C++ and etc.). You have experience in or desire to learn Hadoop/Spark related Big Data technologies.\\nYou have demonstrated the capability to review and write technical papers.\\nYou can contribute to research that can be applied to Etsy products.\\nYou have the ability to quickly prototype ideas and solve complex problems by adapting creative approaches.\\nYou are a strong collaborator and communicator and you make the engineers around you grow and learn.\\nWhat’s Next\\n\\nInterested in working with us? Send us a cover letter and your CV or resume explaining why you’d be great for the job. We value your unique talents and point of view, so feel free to tell us what you are all about. And if you write, draw, craft, or contribute to something you’re proud of, we’d love to hear about it.\\nAt Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status.\",\n",
       "  \"Are you looking for a new role where you can use your Data Science and Machine Learning skills to help make the world a better place (and work for a fast-growing startup)? If that sounds like a good fit, then we want to hear from you!\\n\\nWhat's the Job?\\n\\nWe recently created an office here in New York, and we are building up our team here to be the foundation of our U.S. team. We are a well-funded pharmaceutical research startup that is a leader in using Machine Learning for drug research.\\n\\nYou will be using Machine Learning to graph human biology based upon vast data sets at our disposal. We will then run experiments, synthesize molecules then do Phase 1 and 2 trials, at which point we will let someone else take the drug to market. We have already found success and had a major breakthrough with a major disease.\\n\\nYou will be a very excited, energetic, curious Data Scientist who is excited about meeting the challenges we face. You should believe in our mission and want to spend your time helping to cure people of diseases.\\n\\nSkills We Are Looking For\\n\\n• PhD or Master's with a strong publication and academic record\\n\\n• You should have a specialty in an area such as Machine Learning, Deep Learning or AI\\n\\n• It would be ideal if you have a background in Biology and/or Chemistry\\n\\nCompensation\\n\\n• $150,000 - $200,000\\n\\n• 5 weeks paid vacation\\n\\n• Full health benefits\\n\\n• Startup Equity\\n\\nWhat's In It For You?\\n\\nAs one of the foundational members of our team, you will play an integral part in the future of our expansion. We are growing rapidly and you will have the opportunity to join a fast-growing startup with big expansion plans.\\n\\nFurthermore, our mission is to better the world through using Machine Learning and Data Science to cure diseases. This is an amazing opportunity to use your advanced skills to make the world a better place.\",\n",
       "  \"We are seeking a Senior Data Scientist to solve some of our hardest technical challenges. You will work closely with our clients, fellow engineers, deployment strategists, and others across the company to implement projects from start to finish, participating in data discovery, analysis, algorithm development, feature extraction, and delivery of results.\\n\\nYou have a real passion for using dirty, disparate data to optimize business processes, and highlight relevant metrics and insightful stories. You prefer asking the right question as opposed to offering the right answer. You appreciate ambiguity and are comfortable guiding others while obsessing over details.\\n\\nWe encourage people from underrepresented groups to apply.\\n\\nWhat You'll Do\\nExplore Data - Understand and identify preliminary signals in the data prior to deep processing. Quickly analyze the dataset to assess its usefulness for machine learning.\\nPrototype Models \\xad Develop features, uncover patterns, and build models.\\nExplore Techniques - From simple regressions to neural networks, we use a variety of techniques. You'll have the freedom to explore multiple methods to squeeze insights out of data.\\nProductize Models \\xad A pattern is good, but a prescriptive solution is better. Build products that help our customers get the most out of their data and workflows.\\nRequirements\\nMasters in a quantitative discipline (e.g. Stats, Math, Physics, Engineering, etc.)\\n5+ years of experience in data science/machine learning roles\\n3+ years of experience working with geospatial data together with spatial risk/catastrophe modeling and/or environmental modeling\\nAdvanced knowledge of machine learning methods and statistical principles, including experience in Bayesian statistics, anomaly detection, and/or time series forecasting\\nWell versed in Python or R (and willingness to continue to learn the Python ecosystem)\\nProven experience designing and delivering solutions using large, real-world datasets to support business decisions\\nBenefits\\nMission Driven - Some companies use AI to serve better digital ads and trade stocks, we seek to make our communities safer and more resilient.\\nTop Compensation - Competitive compensation package.\\nBest in Class Medical Coverage - 100% benefits and premiums paid.\\nPrime NoHo Location - Our office sits in the heart of NYC’s historic NoHo district and is just minutes away from the BDFM and 6 subway lines.\\nHealth Perks - Gym reimbursement and citibike membership.\\nStrong Culture - collaborative office focused on teamwork, humility, and hustle.\\nCatered lunch on Thursdays, plus a kitchen filled with snacks and drinks.\\nWe're an equal opportunity employer. All applicants will be considered for employment without attention to race, color, religion, sex, sexual orientation, gender identity, national origin, veteran or disability status.\",\n",
       "  \"POSITION RESPONSIBILITIES\\n\\n\\nResponsibilities:\\nThe Data Scientist will utilize their skills and competencies in big-data analytics and high performance computing and will collaborate with Montefiore Enterprise Data and Information Management teams to 1) build scalable, dynamic and enterprise analytics solutions for healthcare and population health management, 2) expand Montefiore data architecture with third party sources and linked open data when needed, 3) Enhancing Montefiore big-data collection procedures to include information that is relevant for building analytic systems, 4) develop automated and scalable data quality assurance processes for detecting anomalies, cleansing, and verifying the integrity of data used for analysis, 5) Move products through the full development process from research and validation through operational launch.\\nThe Data Scientist will utilize their strong statistical and mathematical background and foundational competencies and skills in big-data environments to develop accurate and scalable analytics functions and methodologies for\\nClassification and Regression, Deep Learning, Decision Trees and Ensembles (Random Forest, XGBoost, and more), Bayes and Probabilistic classifiers and learners, Case Based Reasoning, Topological Data Analysis, Advanced Visualization, Natural Language Processing, and more.\\nContribute to our intellectual property portfolio\\nDrive the rigorous data analysis effort and support group efforts toward high quality documentation.\\nWill work routinely and extensively with data analysis packages such as Pandas and scikit-learn.\\nQUALIFICATIONS\\n\\n\\nRequired Qualifications:\\nPh.D. in Data Science, Applied Math, Statistics, Computer Science, or other quantitative discipline\\nOutstanding quantitative modeling and statistical analysis skills (3+ years academic and/or industry experience)\\nAdept with at least one of the following data analysis software: Python, R, MATLAB\\nExcellent verbal and written communication skills with the ability to effectively advocate technical solutions to research scientists, engineering teams and hospital stakeholders\\nPreferred but not required:\\nExperienced with the python scientific ecosystem (pandas, scikit-learn, matplotlib, numpy, scipy)\\nWorked on software projects using a language such as Python, Java, C++, or Javascript\\nProficiency with databases, both distributed and single, and using a query language such as SQL, HIVE, or SPARQL\\nExperience with distributed analytical frameworks such as Spark, H20 or Dask\\nExperience with deep learning machine learning frameworks, like PyTorch or Keras\\nABOUT US\\n\\n\\nFounded in 1955, the Albert Einstein College of Medicine (Einstein) is one of the nation’s premier institutions for medical education, basic research and clinical investigation. A full-time faculty of some 2,000 conducts research, teaches, and delivers health care in every major biomedical specialty. The college has some 730 medical students, 193 Ph.D. students, 106 MD/Ph.D. students and 275 postdoctoral fellows.\\n\\nEinstein’s major strength, in addition to training physicians and scientists, is its science. During fiscal year 2015, the faculty’s consistently high level of scientific achievement resulted in the awarding of more than $150 million in peer-reviewed grants from the National Institutes of Health (NIH).\\n\\nEinstein is part of Montefiore Medicine Academic Health System, an integrated academic delivery system comprising seven campuses, including 8 hospitals, a multi-county ambulatory network, a new state-of-the art “hospital without beds”, a skilled nursing facility, school of nursing, home health agency, and the state’s first freestanding emergency department. As the University Hospital for the Albert Einstein College of Medicine, Montefiore is a premier academic health system, employing Einstein’s clinical faculty and training Einstein’s medical students, over 1,300 residents, 420 allied health students, and 1,600 nursing students annually.\\n\\nWe have also been recognized by Forbes as one of the country's best midsize employers in 2019. Forbes ranked Einstein in the top overall nationally among midsize employers. Einstein ranked 3rd within the education category in New York State and 9th nationally among all midsize education employers.\\n\\nThe Albert Einstein College of Medicine is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information. Einstein seeks candidates whose skills, and personal and professional experience, have prepared them to contribute to our commitment to diversity and excellence, and the communities we serve.\\n\\nWe are looking for a Data Scientist that will help us discover the insight hidden in vast amounts of clinical, biological, environmental, socio-economic, operations, and business data, and help us make smarter decisions to deliver even better health for our patients and population. Your primary focus will be in applying your mathematics and statistics skills and data mining techniques to build high quality analytics integrated with our delivery of care process and operational systems. This will include (but will not be limited to) developing big-data analytics solutions for “automated scoring using machine learning techniques”, “recommendation systems”, “improve and extend our existing platforms for machine learning and predictive modeling”, and developing “internal QA and validation procedures”.\",\n",
       "  'PulsePoint™, a global programmatic advertising platform with specialized healthcare expertise, fuses the science of programmatic targeting, distribution, and optimization with the art of brand engagement. The PulsePoint platform is powered by terabytes of impression-level data, allowing brands to efficiently engage the right audiences at scale while helping publishers increase yield through actionable insights.\\n\\nOur organization has a strong history of utilizing machine learning, contextualization, and targeting to distribute advertising to the right consumers at the right time and create real connections across the internet. We are now taking that knowledge and expertise to solve challenges within healthcare in order to create better health outcomes through Radical Health Personalization™.\\n\\nThe goals of the PulsePoint Data Science team:\\nOptimize and validate targeting mechanisms for specific health conditions\\nImprove and optimize our proprietary contextualization, and recommendation engines that handle hundreds of thousands of transactions per second, billions of times each month\\nCollaborate with internal Health experts to ideate and support rapid assessment, analysis, and prototyping of ideas for achievable commercialization.\\nWhat you will be tasked to do:\\nResearch and develop user profiling models to enhance our clinical trial recommendation engine to leverage both online and offline data.\\nCollaborate with Product teams on data-driven products to support clinical trial platform design and delivery.\\nSupport and enhance the existing work on health user profiling, prediction, and targeting tools.\\nContribute on future project on patient/physician identity for cross-device tracking, profiling and targeting.\\nSupport existing codebases for data integration and production support for our core models.\\nWhat you need to be successful in this role:\\n3+ years of full-time experience working as a Statistician/ Machine Learning Engineer/ Data Scientist\\nAdvanced knowledge of Big Data technologies such as Hadoop, Hive, and Impala\\nAdvanced knowledge of Python using the numpy/scipy/pandas/skilearn stack\\nMS/PhD in Astronomy, Physics, Applied Mathematics, Statistics, Machine Learning, Computer Science; or BS with several years of applied machine learning experience\\n** All applicants must submit a code sample or a GitHub link to be considered **\\n\\nAt PulsePoint™, data is at the core of everything we do and Data Science is a high profile and high impact team, focusing on creating innovative solutions that rely on predictive modeling and big data analytics. We are looking for \"A\" players that have a combination of drive, focus, speed, efficiency and quality to drive statistical modeling, optimization and/or machine learning. You will be given ownership and autonomy over the research and development of your projects and will be expected to execute well and on time. We work on challenging problems that will make ads matter for people with health problems. Your work will directly influence our trajectory as a company.\\n\\nWhat we offer:\\nSane work hours\\nGenerous paid vacation/company holidays\\nVacation reimbursement, sabbatical, pawternity leave, marriage leave, honeymoon bonus\\nComprehensive healthcare with 100%-paid medical, vision, life & disability insurance\\n$2,000 annual training and development budget\\nComplimentary annual memberships to One Medical, NY Citi Bike and SF Ford GoBike\\nMonthly chair massages\\nFree fitness classes (spin, yoga, boxing)\\nGym reimbursement, local gym membership discounts\\nOnsite flu shots, dental cleanings and vision exams\\nAnnual company retreat\\nPaid parental leave and a lot of new parent perks\\nEmergency childcare credits\\n401(k) Match and free access to a financial advisor\\nVolunteer Time Off and Donation Matching, ongoing group volunteer opportunities\\nTeam lunches, Sip & Social Thursdays, Game Nights, Movie Nights\\nHealthy snacks and drinks\\nAnd there\\'s a lot more!\\n\\nWant to peek inside the PulsePoint™ offices? Check it out here: https://www.themuse.com/profiles/pulsepoint',\n",
       "  \"SeatGeek operates a unique business model in a complicated, opaque market. Many of the hardest problems we face have never been tackled at scale and do not have clear questions, let alone answers. Moving forward requires critical thinking, rapid prototyping, and intellectual dexterity.\\n\\n\\nOur team members have varied backgrounds including an expert on natural language processing, a neuroscientist, a former math teacher, and a mathematician who previously specialized in traffic flow optimization. We share common views on experimental rigor, pragmatism, and software quality.\\n\\nWe want someone to join us who shares our excitement at providing data services to our colleagues and customers, someone proficient with at least one general-purpose programming language and who knows its scientific stack. We want someone who can take a messy dataset and make it clean and who can take a clean dataset and make it sing. Last but not least, we want someone who's committed not only to bettering themselves but to bettering their team, someone who values and invests in knowledge share and open communication.\\n\\nWhat You'll Do\\n\\n\\nAs a member of the SeatGeek data science team you will take the complex issues facing the business and make them simple. We aim to find meaning in the data we have, go out and get the data we don't. We leverage technology whenever possible, and we aim to build systems that anticipate the needs of tomorrow as well as solving the problems of today.\\n\\nHere are some things you might work on:\\nDesign and implement statistical tests for new KPIs in our A/B testing framework\\nMine user interaction data for operational efficiencies and product improvements that could save our customers time and boost our bottom line\\nDeliver a talk to engineers on your favorite Scala features or a class on epistemology to the rest of the business\\nEstimate how adding a hundred thousand tickets to our inventory or changing how we allocate marketing resources will impact revenue\\nUse machine learning to identify when a user has a problem before they contact us\\nBuild a model to predict the probability a particular ticket listing will sell at a given price\\nWhat You Have\\n\\n\\nYou have a passion for problem solving, experience working on open-ended projects, and a proven ability to come up with creative, elegant solutions to complex issues. Experience with specific tools is less important than aptitude and drive, but at a minimum we would expect:\\n5+ years of academic or professional experience in a quantitative role\\nExperience translating business problems into data problems and solving them\\nComfort turning ideas into code (bonus points for experience with Python or Scala)\\nCommitment to creating and sharing reproducible analysis\\nA passion for learning and teaching others\\nBonus points for candidates who have experience with or desire to learn any of the following:\\nStreaming data (Reactive Extensions, Spark Streaming, Akka-streams, Kafka, RabbitMQ)\\nAWS infrastructure (we use Redshift, S3, EMR, Kinesis, Lambda, and RDS)\\nBuilding distributed software (especially on top of Spark, Hadoop, etc.) in a production environment\\nUtilizing applied statistics or machine learning on large, complex, noisy datasets\\nScaling and turning statistical models into production-ready applications such as recommender systems\\nThe Tools We Use\\n\\n\\nWe do research and development work in a custom environment optimized for repeatability and collaboration. You absolutely do not need experience with all of these, but we thought you might be curious.\\nLanguages: Python for web services and product devlopment, R for analysis and prototyping\\nDatastores: MySQL, Redshift, Elasticsearch, Redis\\nMonitoring: Graphite/StatsD\\nVersion control: Git\\nPerks\\nA laid-back, fun workplace designed to facilitate collaboration and company wide events\\n$120/mo to spend on live events tickets\\nA superb benefits package that supports health/dental/vision\\nA focus on transparency. We have regular team lunches and Q&A panels where employees can chat openly with teams across SeatGeek, our co-founders, and external guests from the industry\\nAnnual subscriptions to Citibike, Spotify, and meditation services\\nSeatGeek is committed to providing equal employment opportunities to all employees and applicants for employment regardless of race, color, religion, creed, age, national origin or ancestry, ethnicity, sex, sexual orientation, gender identity or expression, disability, military or veteran status, or any other category protected by federal, state, or local law. As an equal opportunities employer, we recognize that diversity is a positive attribute and we welcome the differences and benefits that a diverse culture brings. Come join us!\",\n",
       "  'Who we are\\nAt Criteo, we are building the advertising platform of choice for the open Internet, an ecosystem that favors neutrality, transparency and inclusiveness. With more than 1.4 billion active shoppers and $600 billion in annual commerce sales, we deliver performance at scale. Founded in a Paris start-up incubator, Criteo now carries out our entrepreneurial spirit across 30+ global offices. Do you want to have an impact on more than half of the world’s internet users? Join us and be part of something big.\\n\\nWe are innovative, passionate and result-driven. We are an interdisciplinary team, leveraging the strengths of engineers and scientists.\\n\\nThe Data Science - Product Analytics team uses cutting edge technology, advanced statistics and machine learning to understand the most complex business problems at Criteo. We are committed to designing and building technical solutions to drive Criteo’s development and keep up with a fast-evolving product landscape.\\nWe serve as trusted partners to leadership and work closely with PMs, R&D, Finance or Business teams.\\nInside Data Science - Product Analytics:\\nFind new ways to optimize the bidding strategy\\nOptimize our capability to recognize users across all their devices and their interactions in the open-internet\\nDefine, drive and analyze results of A/B tests to assess whether online performance is in line with offline simulations\\nProvide data-driven insights to ensure Criteo remains one step ahead of competitive threats\\nIdentify and size development opportunities for new products\\nWhat will you do?\\nYou enjoy solving real-world problems with data.\\nYou will be assigned to one or several projects.\\nMine large data sets and turn them into understandable and actionable insights\\nBuild scalable analytic solutions using state of the art tools based on large and granular datasets\\nDesign and execute a stream of analysis and tests to measure the impact of your solutions\\nMaster our internal analytic datasets and reporting tools\\nWho are you?\\nLast year of Master’s degree or higher in a quantitative field (Mathematics, Computer Science, Physics, Engineering, Economics,etc.)\\nInternship of a minimum of 6 months\\nOutstanding analytical skills and creative thinking\\nFluency in the core toolkit of Data Science:\\nR/Python; SQL/Hive\\nManipulating large-scale data sets\\nBuilding data pipelines\\nDescriptive and predictive modelling\\nImplementing visualizations, dashboards, and reports\\nExcellent interpersonal and communication skills, pro-active and independent\\nFun to work with!\\nAt Criteo, we dare to be different. We believe that diversity fuels innovation and creates an energy that can be seen and felt all over Criteo. We champion different perspectives and are committed to creating a workplace where all Criteos are heard and feel a sense of belonging.\\n\\nCriteo collects your personal data for the purposes of managing Criteo\\'s recruitment related activities. Consequently, Criteo may use your personal data in relation to the evaluation and selection of applicants. Your information will be accessible to the different Criteo entities across the world. By clicking the \"Apply\" button you expressly give your consent.',\n",
       "  'Internship Overview\\n\\nOur Internship Program is a paid 10-week learning experience where you will be immersed in the daily environment of a thriving Fortune 250 global financial services company.\\n\\nYou will gain invaluable industry and organizational knowledge through daily business interactions and job assignments, in addition to engaging in projects that directly affect our business, interact with senior leaders in conversational settings, and network with employees and interns across our regional offices.\\n\\nLead by Vice President, Enterprise Analytics, this group is responsible for leading and innovating on the long-term data and analytics strategy for the organization. The team engages in highly visible and dynamic business projects partnering closely with technology, actuarial, marketing and others to develop an integrated approach to developing and implementing data-driven solutions. The team comprises of highly skilled data scientists from a cross-functional and academic background (computer science, mathematics, physics, technology, actuarial, MBA, etc)\\n\\nRequired Technical and Professional Expertise\\nIn process of receiving a UG or graduate degree in an analytical area such as Machine learning, Computer Science, Physics, Mathematics, Statistics, Engineering or similar.\\nExperience in Analytics or other quantitative disciplines.\\nExperience with modeling and analysis including machine learning, statistical analysis, operations research and management science and data mining.\\nStrong interpersonal and communication skills. Must be able to explain technical concepts and analyses implications clearly to a wide audience and be able to translate business objectives into actionable analyses.\\nExperience with SQL (any variations thereof), Python/PySpark and/or Scala preferred; though experience with other analytics software (SAS, STATA, MATLAB, Mathematica, R/SparklyR) is acceptable.\\nUsing SQL to transform and utilize data in the data warehouse by creating, automating and documenting new tables in a post-load process.\\nProven analytical and quantitative skills to use hard data and metrics to back up assumptions, develop business cases, and complete root cause analyses.\\nCapable of taking responsibility for an initiative and self-starter even when assignments are vague or undefined.\\nCreative in finding new solutions/designing innovative methods, systems, and processes.\\nReporting:\\n\\nRole will report to AVP, Analytics Translation, Enterprise Data Analytics',\n",
       "  \"Requisition Number 19-0190\\nPost Date 7/29/2019\\nTitle Associate Data Scientist\\nOrganization Name Macmillan\\nCity New York\\nState NY\\nDescription Macmillan Publishers is a global trade book publishing company with prominent imprints around the world, publishing a broad range of award-winning books for children and adults in all categories and formats. Macmillan Publishers is committed to our authors,\\nour employees, and to the environment.\\n\\nU.S. publishers include Celadon Books, Farrar, Straus and Giroux, Flatiron Books, Henry Holt & Company, Macmillan Audio, Macmillan Children’s Publishing Group, Picador, St. Martin's Press and Tor Books. In the UK, Australia, India, and South Africa, Macmillan\\npublishes under the Pan Macmillan name. The German company, Holtzbrinck Deutsche Buchverlage, includes among its imprints S. Fischer, Kiepenheuer and Witsch, Rowohlt, and Droemer Knaur.\\n\\nMacmillan Publishers is a division of the Holtzbrinck Publishing Group, a large family-owned media company headquartered in Stuttgart, Germany.\\n\\nWe are an Equal Opportunity Employer. We are actively seeking job applicants who reflect a broad representation of differences, including race, ethnicity, religion, sex, sexual orientation, gender identity/expression, physical ability, neurodiversity, age,\\nfamily status, economic background and status, geographical background and status, and perspective. We believe that the best companies reflect the incredible diversity in viewpoints, backgrounds, and identities of the world in their staffs, and are committed\\nto inclusive hiring across departments and levels. The successful candidate for this position will be an employee of Holtzbrinck Publishers, LLC.\\nRequirements\\nPosition Description:\\n\\nAs an Associate Data Scientist, you will play a part in shaping Macmillan’s future, helping it become a more data-driven company. In this role, you will help answer tactical and strategic questions from stakeholders across the company. To do that, you will\\nknow/learn how to frame business questions; write statistical code to collect data; extract signals from noise; and communicate quantitative information to a non-technical audience.\\n\\nMajor Responsibilities:\\n\\n• Builds and maintains in a reproducible manner sales and other relevant databases\\n\\n• Conducts statistical analyses by writing code that draws on millions of rows of data\\n\\n• Presents data in an accessible, often visual way\\n\\n• Communicates findings to senior management and through regular reporting\\n\\n• Collaborates with publishers, marketing, sales and Data Warehouse departments\\n\\n• Identifies new and useful data for use in analyses\\n\\n• Helps determine our data strategy by evaluating the impact of various projects\\n\\nRequired Skills/Knowledge:\\n\\n• Bachelor’s degree in a quantitative field (e.g., economics, statistics)\\n\\n• 1-3 years of experience\\n• Strong analytical skills\\n\\n• Knowledge of relevant software packages:\\n-Statistical programming (e.g., Stata, R)\\n-Database (e.g., Python, SQL, Cognos)\\n• Good written, verbal and interpersonal skills\\n\\n• Self-starter\",\n",
       "  \"Job Title:\\n\\nData Scientist\\n\\nJob Description:\\n\\nTeam Write-Up\\n\\n360i Advanced Analytics is about taking complex data to devise elegant yet simple, actionable solutions to our clients' business questions. Our clients see us as trusted advisors and depend on our subject matter expertise to drive enhanced decision making via our deep (and constantly evolving) marketing analytics capabilities.\\n\\nFor this role, we are looking for someone who is deeply interested in the application of advanced statistic and econometric methodologies in both theory and practice. Candidates should be intellectually curious with a passion for learning while simultaneously a pragmatic self-starter who sees projects through to completion.\\n\\nData Scientist\\n\\nResponsibilities\\nOperate 'in-the-weeds' on client analytics engagements, executing the below within Python or R:\\nCleanse, compile and process data\\nExecuting statistical/econometric frameworks in either\\nUnder direction of Manager/AD, develop custom analytical tools for ongoing media optimization\\nParticipate in stakeholder interview process for each project to frame analytics solutions through appropriate lens\\nAssist in discovery of new analytics methodologies, outline roadmap for tool development, and execute tool build\\nInterface directly with media team and client (when applicable) on interpretation of analytics outputs and how to implement results\\nQualifications & Requirements:\\nBachelors Degree in a quantitative field\\nUnderstanding of econometric and statistical frameworks, including but not limited to:\\nMultivariate linear regression, logistic regression, Bayesian networks, clustering, etc.\\n1-2 years working experience in analytics or other consultancy\\nPossess the ability to receive direction, take ownership, and operate independently to solve to client requests using a combination of critical thinking and initiative\\nStrong interpersonal skills or the demonstrated desire to develop client management ability\\nExperience with marketing response models and their applications a plus\\nFunctional working knowledge of R and/or Python (e.g., can process data and run statistical analyses)\\nExpert in Excel and PowerPoint\\nLocation:\\n\\nNew York\\n\\nBrand:\\n\\n360I\\n\\nTime Type:\\n\\nFull time\\n\\nContract Type:\\n\\nPermanent\",\n",
       "  'EFT Analytics, Inc. is looking for a Data Scientist to join our team! The Data Scientist will have the opportunity to work with a dynamic team to optimize and expand our advanced analytics platform. The successful candidate will be able to enhance and expand machine learning capabilities in the platform.\\n\\nStrong written and verbal communication skills are required.\\n\\nA Day In The Life Typically Includes:\\nResearch, design and develop machine learning algorithms as part of the product development team\\nPerform Root Cause Analysis leveraging data science on large industrial problems related to process control, process engineering, and discrete manufacturing environments\\nCollaborate with our process engineering teams as the expert in applied data science and machine learning techniques\\nUse industry technologies, tools, and data mining frameworks for data analytics including data visualization for analyzing, optimizing, developing hypotheses, and drawing conclusions\\nWhat You Will Need:\\n\\nBasic Qualifications:\\n2+ years of industry experience analyzing data sets and applying machine learning to assist decision making and solve problems\\nKnowledge of supervised and unsupervised algorithms\\nExtensive knowledge of a scientific computing language (e.g. Python or R)\\nExperience writing code in a collaborative environment\\nStrong knowledge of statistics and experimental design\\nThis role is not eligible for visa sponsorship\\nWhat Will Put You Ahead?\\n\\nPreferred Qualifications:\\nMaster’s degree or PhD in Mathematics, Computer Science, Computer Engineering, Physics, Data Science or Statistics from an accredited institution\\nExposure to programming languages: C#, Java, Scala, Julia\\nExperience deploying models to production and ensuring the model remains relevant in production\\n2+ years proficiency across a combination of academic and practical experience with Bayesian Statistics, Neural Networks, Regression Techniques, and other Machine Learning Applications\\nThe ability to communicate results clearly and a focus on driving impact\\nAdditional Information:\\nAbility to travel for work up to 15% of the time, by plane, car, etc.\\nWant to learn more about EFT Analytics?\\n\\nTwenty years ago, EFT Analytics set out to deliver real-time, data-driven insights. Today, Industry 4.0 is the advent of Internet of Things, smart manufacturing and industrial stewardship. Insights and results earn your data a seat in the boardroom. EFT is a subsidiary of Koch Industries, one of the largest private companies in the U.S. Koch has delivered results using its unique management philosophy across dozens of industries and hundreds of sites for decades. EFT strives daily to bring those time-tested fundamentals to how we operate and what we deliver.\\n\\nOur people and culture are just like you, totally unique. We come from vastly diverse backgrounds. We’ve taken our own paths. When there wasn’t a path, we blazed trails. Trails that led us here. Where we find ourselves unified around one mission: to be the best data analytics company on the planet. And we won’t stop until we get there.\\n\\nSalary and benefits commensurate with experience.\\nThis role is not eligible for work visa sponsorship.\\n\\nWe are an equal opportunity employer. Minority/Female/Disabled/Veteran\\n\\nThis employer uses E-Verify. Please visit the following website for additional information: www.kochcareers.com/doc/Everify.pdf\\n\\nFollow us',\n",
       "  'Location\\nNew York, New York\\nShift:\\nDay (United States of America)\\nDescription:\\n\\n\\nJoin the Artificial Intelligence team at New Yorks #1-ranked hospital.\\n\\nData Scientist - IT Transformation\\n\\nAt New York-Presbyterian, were leveraging the most innovative and transformative technologies to provide the highest quality care to our patients. This is your opportunity to provide world class technology solutions that will directly impact the quality of a patients life. Thrive in our fast-paced environment, as you lead enterprise-wide implementation of AI. Join and collaborate with a tight-knit team of creative and ambitious data analysts, engineers, and developers, working on the most important enterprise initiatives in our healthcare system. Ultimately you will be a critical part of building the hospital of the future, as we integrate A.I. into all aspects of healthcare delivery and make a huge positive impact on patient lives.\\n\\nOur Artificial Intelligence team is seeking an innovative, results-oriented data scientist to support, enhance, and expand NewYork-Presbyterians artificial intelligence program in redefining our healthcare delivery operations. Our mission is to enhance healthcare delivery through integration of machine learning, extracting knowledge from millions of data points from the electronic medical record and clinical documents to generate actionable insights for our clinical, operational, and enterprise teams. As a critical member of our A.I. deployment team, you will oversee machine learning algorithm generation and be responsible for establishing and maintaining a quality pipeline of models. Working with senior executives and clinical leadership, you will have a prominent voice in shaping strategic decisions and process redesign across the enterprise.\\n\\nPreferred Criteria\\nMasters in Computer Science, Bioinformatics, Physics, Mathematics, or other data science related field\\nExperience in neural network models in Keras and/or Tensorflow / Theano\\nExperience in MLaaS (AWS, Azure, GCP)\\nPrevious related experience in machine learning techniques and best practices\\nPython experience and related ML packages (scikit-learn, numpy, etc.)\\nProficiency in data analytics and engineering, able to clean, transform, and merge data for ML purposes\\nRequired Criteria\\nBachelor\\'s degree or equivalent experience\\nThree to five years of previous work related experience\\nJoin a hospital where employee engagement is at an all-time high. Enjoy competitive compensation along with benefits such as tuition reimbursement, hospital retirement contributions, and financial planning assistance. Start your life-changing journey today.\\n__________________\\n#1 in New York, \"America\\'s Best Hospitals 2019-2020.\" - U.S.News & World Report\\n2019 \"Best Places to Work: Employee\\'s Choice.\" - Glassdoor\\n2019 \"Employees\\' Choice: Top CEOs\" - Glassdoor\\n2019 \"America\\'s Best Employers in New York State\" - Forbes\\n2019 \"150 Top Places to Work in Healthcare.\" - Becker\\'s Healthcare\\n2018 \"Top-Rated Work Places: Best Hospitals\" - Indeed\\nDiscover why we\\'re #1 in New York and a best employer at: nyp.org/careers\\n\\nNewYork-Presbyterian Hospital is an equal opportunity employer.',\n",
       "  \"About the Company\\nMission Statement: Eyeview drives measurable, incremental sales, online and offline by creating and delivering millions of data-driven personalized videos locally to consumers at a national scale.\\n\\nEyeview is a video marketing technology company and the industry leader in outcome-based video marketing. Eyeview delivers superior return on investment through 1-to-1 video. Through our proprietary VideoIQ® technology, Eyeview easily leverages brand, product and consumer data to create and deliver 1-to-1 video ads to every consumer and ultimately drive sales. VideoIQ® provides an elemental knowledge of video variables that powers a results-driven decisioning engine, capable of making billions of decisions each day, delivering the most relevant message to every consumer across connected television, desktop, and mobile.\\n\\nHeadquartered in New York City, with offices in LA and Chicago, Eyeview serves the nation's top brands and was ranked as the 2nd fastest growing company in the Tri-State Area by Deloitte's Technology Fast 500TM.\\n\\nAbout the Role\\n\\nEyeview is looking for a Data Scientist to join our Data Science team working on research, development, and implementation of models, algorithms, and systems for our video personalization platform. This is an exciting opportunity to leverage our dynamic video rendering technology and create a real impact in the video advertising industry. In addition, the Data Scientist will also work with the Data Engineers and Campaign Analytics team to evolve an existing campaign modeling platform as well as participate in brainstorming sessions to create new product offerings. This role will sit in our New York HQ. The ideal candidate enjoys working across multiple disciplines and functional teams, has excellent written and verbal communication skills, and can maintain a business perspective.\\n\\nResponsibilities\\nWork with data engineers and analytics team on creating and evolving a modeling platform for scalable campaign modeling\\nWork with media managers to ensure campaign metrics are met\\nRequired Skills\\nMinimum 2 years experience experience as a hands on Data Scientist\\nAt least 1-2 years work experience in the video adtech industry\\nPhD degree or MS degree in a quantitative field or exceptional BS degree holders with 3-5 years work experience in analytics and data science roles will also be considered\\nStrong applied statistics background is required\\nAdept in: Linear and Non-Linear regression, Statistical tests, Bayesian methods, Monte Carlo methods, bootstrapping, Supervised and unsupervised learning\\nProgramming\\nPython or R along with experience in the various machine learning, scientific computing, and data visualization libraries (numpy, scipy, sklearn, pandas, matplotlib, seaborn, keras, etc)\\nAbility to work leverage Spark SQL\\nFamiliarity with AWS is strongly preferred\\nJava, Scala are a plus\\nFamiliarity with Big Data Technologies such as Spark, Hadoop, Hive\\nFamiliarity with ML pipeline such as MLFlow, SageMaker is a plus\\nFamiliarity with PFA/PMML is a plus\\nSoftware engineering experience is a plus\\nCapable of changing linguistic contexts --- can you explain Mahalanobis distance to a group of people with a diverse education background?\\nPerks & Benefits:\\nUnlimited Vacation Policy, Medical, Dental & Vision Benefits, 401K, Cell Phone Reimbursement, Employee Stock Purchase Plan, Free drinks & snacks, Sports teams, Weekly Catered Company lunches\\nEyeview provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, Eyeview complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\\n\\nEyeview expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of Eyeview's employees to perform their job duties may result in discipline up to and including discharge.\",\n",
       "  \"Summary:\\n\\nBNY Mellon Investment Management is one of the\\nworld's leading investment management organizations and one of the top U.S.\\nwealth managers. Our business encompasses BNY Mellon's affiliated investment\\nmanagement firms, wealth management organization, and global distribution\\ncompanies. Our goal is to build and\\ndeliver investment and wealth management strategies and solutions to meet our\\nclients’ needs.\\n\\nDrawing on deep expertise, we collaborate with our\\nclients to tailor our best ideas and resources to meet their specific\\nrequirements. Through our global network we have developed a significant\\nunderstanding of local requirements. We pride ourselves on providing dedicated\\nservice through our teams.\\n\\nWith extensive experience in anticipating and\\nresponding to the investment and financial needs of the world's governments,\\npension plan sponsors, corporations, foundations, endowments planned giving\\nprograms, advisors, intermediaries, individuals and families, and family\\noffices, BNY Mellon Investment Management can help our clients reach their\\ngoals.\\nThe Role:\\n\\nReporting\\nto the Head of Investment Management (IM) Data Solutions, the Data Scientist will\\nlead, design and architect solutions to complex data integration, data science,\\nand automation problems. This is a unique opportunity to join the IM Data\\nSolutions team, a small, multi-disciplinary team that is transforming the way information\\nis created, used, and communicated within BNY Mellon IM. The group works\\ndirectly with BNY Mellon’s Investment Management companies on\\nstrategic projects, creating valuable tools and insights for decision makers\\nacross portfolio management, sales, marketing, and other key areas of the\\nbusiness. The Data Solutions team capitalizes on opportunities across business\\nlines using the best tools and practices, like machine learning, econometrics,\\nagile development (scrum/kanban), and modern collaborative data platforms.\\nResponsibilities:\\nLead data science and/or data engineering projects to\\nsupport IM Investment Management companies\\nUse domain expertise to understand business requirements\\nand design the right solution\\nBuild or implement\\ndata pipelines, databases, visualizations, and other data tools (hands-on)\\nContribute to team\\nin a wide range of technical areas by instituting new practices and staying\\nabreast of the latest technical developments\\nTake initiative to\\ncontribute to overall team efforts in software development, data science, and\\ntechnical consulting\\nWork closely with\\nData Engineers to improve processes and enable faster insight-generation from\\ncomplex datasets\\nBA/BS Degree (advanced degrees and/or CFAs are great too)\\n5-7 years of general\\nbusiness experience, especially in financial services, asset management, and/or\\nmanagement consulting or similar environments\\n2-3 years of\\nhands-on data/coding experience\\nProficiency with Python (pandas, numpy, scikit-learn)\\nProficiency with\\nquery languages, databases, and large datasets\\nExperience with\\nmodern data pipelines and/or data operating platforms (e.g. Dataiku, Alteryx,\\nSpark)\\nStatistical competence\\nExperience with machine\\nlearning algorithms and techniques\\nAble to lead\\nad-hoc and structured product teams within an agile framework\\nExcellent\\ninterpersonal skills necessary to accomplish goals through others, including\\nemployees, peers, and other function/business areas of the company\",\n",
       "  \"About Influenster & Our Mission:\\nInfluenster is a digital destination where consumers discover products and reviews that enable them to make well-informed purchase decisions. With over 5 million members who have written over 40 million product reviews, we believe the consumer is the expert - everything we do and build as a company is centered around this philosophy.\\n\\nValues:\\nAt Influenster, we are user obsessed and value user-generated content and user experience above all else. We are proud of our collaborative work environment and practice unfiltered, professional, straightforward, and authentic communication. We are fast and innovative in our work, and we consistently set the bar high. The Influenster team is results driven, solution oriented, and resourceful. Our team sets the bar high to operate with integrity and respect. By empowering each other in the workplace and by empowering the Influenster community we serve, we achieve consistent success by guiding members to make informed, personalized purchases.\\n\\nWork environment:\\nWe are a dynamic fast paced work environment where each person has a strong voice. We give a high level of responsibility to all our team members and we’re proud of our award-winning collaborative work environment. We are always looking for ways to innovate and we value team members who bring fresh ideas to the table.\\n\\nAnalyst Role:\\nWe're are looking for a Data Analyst to help us with our Review Source product review source to streamline campaign set-ups, execution as well as reporting. The ideal candidate has a strong interest in user generated content, market research, sampling, and is comfortable using Excel.\\nResponsibilities:\\nCoordinate with sales and accounts to collect all the necessary specs in order to kick off reviewSource campaigns\\nIdentify and populate missing product pages needed for reviewSource campaigns\\nSet up reviewSource campaigns on the Influenster backend\\nTrack campaign performance, and update sales/accounts periodically, flag any issues with appropriate parties\\nDo ad-hoc analysis to trouble-shoot issues\\nPrepare post campaign reports to share with accounts/sales\\nCreate new snap questions and track and analyze existing snaps to maximize review yield\\nHelp Product Managers in the process of creating programmatic reviewSource solutions\\n\\nQualifications:\\n\\nVery familiar with Excel and SQL\\nSuperior organizational skills and attention to detail\\nExcellent verbal and written communication skills\\nAbility to work collaboratively with multiple teams\\nComfortable working in a fast-paced environment\\nProactive problem solver\\nInterest in the consumer product industry\\nExperience in the survey research industry is a plus\\n\\nPerks:\\n\\nOpportunity for growth\\nFast-paced, high growth work environment\\nFriendly, fun, and collaborative team-based work environment\\nDiversity of work projects\\nFrequent lunches and happy hours\\nDog friendly work environment\\nFully stocked fridge and cold brew\\nFun, creative, open, award-winning work space\\nHealth, dental, vision insurance\\nPaid vacation, flex time, holidays\\n401K\\nAbout Bazaarvoice\\nBazaarvoice connects brands and retailers to consumers, so that every shopping experience feels personal. From search and discovery to purchase and advocacy, Bazaarvoice’s solutions reach in-market shoppers, personalize their experiences, and give them the confidence to buy. Each month in the Bazaarvoice Network, more than a billion consumers view and share authentic content including reviews, questions and answers, and social photos across 6,000 brand and retail websites. Across the network, Bazaarvoice captures billions of shopper signals monthly - data that powers high-efficiency digital advertising and personalization with unmatched relevance.\\n\\nFounded in 2005, Bazaarvoice is headquartered in Austin, Texas with offices in North America, Europe, and Australia. For more information, visit www.bazaarvoice.com.\\nWhy join Bazaarvoice?\\nWe’re committed to client success: There are over 5K brand and retail websites in the Bazaarvoice network. Our clients represent some of the world’s leading companies across a wide range of industries including retail, apparel, automotive, consumer electronics and travel.\\nWe’re leaders in consumer-generated content: Each month, more than one-half billion consumers view and share authentic consumer-generated content, such as ratings and reviews, curated photos, social posts and videos, about products in our network. Last year, 135K reviews were submitted each day.\\nOur network delivers: Network analytics provide insights that help marketers and advertisers provide more engaging experiences that drive brand awareness, consideration, sales, and loyalty.\\nWe’re a great place to work: We pride ourselves on our unique culture. Join a company that values passion, innovation, authenticity, generosity, respect, teamwork, and performance.\\nCommitment to diversity and inclusion\\nBazaarvoice provides equal employment opportunities (EEO) to all team members and applicants according to their experience, talent, and qualifications for the job without regard to race, color, national origin, religion, age, disability, sex (including pregnancy, gender stereotyping, and marital status), sexual orientation, gender identity, genetic information, military/veteran status, or any other category protected by federal, state, or local law in every location in which the company has facilities. Bazaarvoice believes that diversity and an inclusive company culture are key drivers of creativity, innovation and performance. Furthermore, a diverse workforce and the maintenance of an atmosphere that welcomes versatile perspectives will enhance our ability to fulfill our vision of creating the world’s smartest network of consumers, brands, and retailers.\",\n",
       "  \"Using neuroscience-based assessments and machine learning algorithms, pymetrics is reinventing the recruiting industry by matching candidates to jobs where they are most likely to succeed. We are leading the charge in an evolving industry and growing our amazing team to support the mission of using data to unleash one's full potential.\\n\\nWe are looking for a talented data scientist with strong machine learning experience to join our core product development team. Data scientists on the core team contribute to the predictive modeling used throughout pymetrics to match job candidates to roles, audit algorithms for potential bias, and optimize solutions for faster, more robust analyses. You will work on an academically diverse team (with backgrounds in machine learning, statistics, behavioral science, and computer science) to test new methods and optimize existing solutions.\\n\\nThe ideal candidate would bring experience building scalable predictive algorithms in a production environment. We value an ability to communicate with a bright team from diverse backgrounds, and the initiative to tackle interesting problems independently and with scientific rigor.\\n\\nResponsibilities include:\\nDevelop machine learning solutions for new and existing data products relating to hiring\\nWrite production level code for practical application of data products\\nApply statistical and machine learning techniques to identify trends in data\\nWork with clients to apply custom data science solutions to unique problems\\nCollaborate with and train other members of the data science team\\nInteract with data scientists, analysts, engineers and product managers on other data products implemented at pymetrics, including language analysis, psychometric assessment, statistical analysis and data visualization\\nRequirements\\n\\nAbout you:\\nMS/PhD preferred\\n2+ years experience in data science/machine learning preferred\\nExperience with making predictions from both structured and unstructured data\\nPractical knowledge of Python and its scientific libraries SciPy, Scikit, Pandas, and NumPy, with extensive knowledge of programming best practices\\nSolid foundation in machine learning models, processes, and theories, with the ability to evaluate different algorithmic approaches\\nWorking knowledge of statistics and probability (statistical inference, Bayesian statistics)\\nEmbodiment of our core company values of motivation, positivity, curiosity, humility and integrity, and buy-in for our company mission of eliminating bias in hiring\\nWillingness to relocate or regularly travel to the NYC metro area\\nNice to Haves:\\n1+ years experience in deep learning, including familiarity with Keras, TensorFlow and/or PyTorch\\nComfort with SQL\\nKnowledge of psych/cogsci/neuroscience research\\nInterest in issues of transparency and fairness in AI\\nEye for data visualization and presentation of data-driven insights\\nWe are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\\n\\nBenefits\\nHealth care plan (medical, dental & vision)\\nFlexible paid time off\\nFamily leave (maternity, paternity)\\n401k\\nTeam budget for training & development\\nStock option plan\\nCommuter transportation reimbursement\\nDog-friendly workplace\\nFun, diverse and intellectually eager coworkers\",\n",
       "  'Job Description\\nData Scientist\\n\\nNew York, NY\\n\\nTarget Comp: USD $140,0000-$150,000\\n\\nTHE COMPANY\\n\\nAre you experience in using Data Science to detect fraud and anomalies? Are you a statistics and modeling whiz? Harnham are partnered with a global asset management company that uses alternative data, machine learning, and AI, to drive better business decisions in the investment banking industry.\\n\\nTHE ROLE\\n\\nAs a Data Scientist you will:\\nWork closely with research analysts to detect and understand threats to the business\\nPredict prices of commodities across the globe\\nImplement cutting edge machine learning and statistical models\\nWork with alternative data throughout the entire life-cycle\\nShare key insights with key members of investment teams and financial institutions\\nYOUR SKILLS & EXPERIENCE\\nExperience with data wrangling, pipe-lining, visualization, and modeling\\nWorking with alternative data sources\\nCommercial experience implementing machine learning and statistical models\\nExtensive experience in statistics and modeling\\nExpert in Visualizing results and presenting to stakeholders\\nMasters in a quantitative field\\nTHE BENEFITS\\n\\nWorking for a well-established company that works with industry leaders while growing within a small team that is breaking new ground in the AI space.\\n\\nHOW TO APPLY\\n\\nPlease register your interest by sending your CV to David Izso via the Apply link on this page.\\n\\nKEYWORDS\\n\\nAsset Management, Invest, Machine Learning, Statistical Modeling, Data Engineering, Investment Banking\\nCompany Description\\nData and Analytics recruitment is our core business and we’re proud to say, our customers believe we’re good at it. In our most recent customer satisfaction survey, 95% of respondents said that they would recommend Harnham.\\n\\nHarnham has actively chosen to focus on Data and Analytics, we’ve immersed ourselves in this market and are now an integral part of this business community.\\n\\nOur capability has grown to provide recruitment services and advice across the Marketing Analytics, Credit Risk, Data Science, Data and Technology and Digital sectors.',\n",
       "  'Background & Purpose of the Job\\n\\nAs a Data Scientist, you will be an integral part of the Information and Analytics team supporting the organization to enhance and drive overall top-line sales growth and profitability. This role will also interface with multiple key stakeholders and functional teams.\\n\\nWho You Are & What You’ll Do\\n\\nIn this role, you will be responsible for providing insights to the cross-functional business teams leveraging a variety of internal and external data sets, and advance analytic techniques to create value. This role will also interface with multiple stakeholders and teams on a consistent basis to deliver analytic outputs.\\n\\nYou’re a born leader: The ability to gain stakeholder consensus, build coalitions, and present a vision to the broader organization will be crucial for success.\\n\\nYou’re a dot connector: The ability to formulate hypothesis and draw inferences from different cross functional data sets and generate a story of insights is key.\\n\\nYou’re a story teller: You have ability to transform data into insights which provide very clear direction to stakeholders on what actions are needed to drive transformation in the business.\\n\\nYou’re a culture & change champion: Success is dependent upon fostering a highly collaborative and team-oriented environment\\n\\nExtensive experience solving analytical problems using quantitative approaches, a strong passion for empirical research and for answering hard questions with data. Analyze data, run descriptive statistics, generate prior distributions, explore relationships: correlation, covariance, causality, regression modeling, and build Bayesian Influence diagrams. Must be hands-on and must have worked on implementing data mining algorithms, work with cross-functional team members to identify and prioritize actionable, high-impact insights across a variety of core business areas. Strong experience in Data Visualization, Presentation and Story Telling. Ability to work independently as well as in a team environment. Partner with business, Data Scientists, and Engineers to create POCs/Pilots, demonstrate thought leadership and generate marketing insights.\\n\\nWhat You’ll Need to Succeed\\n\\nRequired:\\nMaster’s degree in a related field (e.g., Applied Science, Data Science, Computer Science, Mathematics, Statistics, or another analytical field)\\n2+ years data science experience\\nExpertise in Machine Learning (ML), Data Science, AI, and Data Mining is required.\\nExpertise in Data Wrangling, Engineering, Exploratory data analysis using statistics and programming\\nExpertise in ML algorithms such as PCA, LASSO, RIDGE, SVM, Bayesian, Random Forest, Boosting, Clustering, ARMA, Time Series modeling, Generalized Linear Models, Regression Models, Neural Networks, RNN, CNN, and LSTM is required\\nExpertise in building models using Python, Spark, Scala, SQL, R\\nActive listening skills and deep analytical ability are required\\nA passion for excellence and exceeding customer expectations is required\\nPreferred:\\nPhD in a related field (e.g. Applied Science, Data Science, Computer Science, Mathematics, Statistics, or another analytical field)\\nConsumer goods experience\\nPreferred experience in Classification, Forecasting, Anomaly detection, Outliers analysis, Optimization, Trends analysis, Bayesian inference, Time-Series methods (ARMA/ARIMA), Markovian processes\\nPreferred experience in Java, C++, SAS, SPSS, Azure, Big Data, AWS\\nPreferred experience in Text Mining and Natural Language Programming (NLP)\\nWhat We Can Offer You\\n\\nCulture for Growth | Top Notch Employee Health & Well Being Benefits | Every Voice Matters | Global Reach | Life at Unilever | Careers with Purpose | World Class Career Development Programs | Check Out Our Space | Focus On Sustainability\\n\\n------------------------------------\\n\\nUnilever is an organization committed to diversity and inclusion to drive our business results and create a better future every day for our diverse employees, global consumers, partners, and communities. We believe a diverse workforce allows us to match our growth ambitions and drive inclusion across the business. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, age, national origin, or protected veteran status and will not be discriminated against on the basis of disability. For more information, please see Equal Employment Opportunity Posters\\n\\nEqual Opportunity/Affirmative Action Employer Minorities/Females/Protected Veterans/Persons with Disabilities\\n\\nEmployment is subject to verification of pre-screening tests, which may include drug screening, background check, credit check and DMV check.\\n\\nIf you are an individual with a disability in need of assistance at any time during our recruitment process, please contact us at 1-855-239-5459 or NAAccommodations@unilever.com . Please note: These lines are reserved for individuals with disabilities in need of assistance and are not a means of inquiry about positions or application statuses.',\n",
       "  'The individual will be a hands-on data scientist being part of a team responsible for product innovation. You will be instrumental in working closely with product teams, marketing teams, engineering teams, and technical communities in incorporating data science and modern analytics into products and take an active role in effectively demonstrating how data science concepts are translated into viable solutions.\\n\\nResponsibilities: (1) Advocate production-centric & end-to-end development mentality and culture changes (2) Build analytics models to analyze and derive practical insights from large volumes of data that require the expertise of data analysis & generation and feature engineering. (3) Design, develop and deploy state of the art, data-driven predictive models.(4) Develop optimization and simulation models to solve business problems using latest technologies in data mining, statistical Modeling, pattern recognition, and optimization across the chosen industry and functional area. (5) Develop proofs-of-concepts and proofs-of-technology to recommend and push optimal solutions and right technologies to production (6) Develop intellectual properties including patents and publications. (7) Advise clients & internal teams on the right technology environment, analytic platforms and approaches to take in addressing complex, open-ended business problems.(8) Convert code base from Python/R to production languages e.g. Java to streamline productionization.\\n\\nQualifications: (Please list all required qualifications) (1) MS or PhD with substantial experiences in highly quantitative fields.(2) Expertise in some of the following technical domains: AI/ML, Deep Learning, Cognitive Computing, Simulation, Optimization, Security, and Blockchain. (3) Experienced in at least one of the following business domains: Banking and Financial Services, Retail, Communications, Intelligent Infrastructure, Smart City, Digital Twin. (4) The qualified candidate will have a proven history of driving critical research and development and developing in-depth analytical understanding of systems. (5) 3+ years applied experience devising, deploying and servicing statistical and machine learning models on massive datasets; extensive experience in wrangling data from its raw forms to a state suitable for application of advanced algorithms. (6) 3+ years hands-on experience in optimization modeling, simulation and analysis. (7) Fluent in using Python, Java as a must. Good to master in C, C++, Scala, R. (8) Experienced in statistical tools including SAS, Matlab, SPSS and so on. (9) Hands-on Experience in modern AI/ML/DL platforms and tools including Spark, Tensorflow etc. (10) Strong team player - invested in the collective success of the team and project outcomes (11) Self-sufficient with an ability to thrive in an environment of autonomy amidst ambiguity.\\n\\n- provided by Dice',\n",
       "  \"We’re a data-driven organization, which makes our Analytics and Data Science teams the brains of our operation. On the cutting edge of customer and business analytics, they make sure all our decisions and innovations are based on the latest insights.\\n\\nPriceline is looking for a Sr. Data Scientist to be a key member of our center of excellence for pricing. The person in this role will be responsible for building pricing models for machine assisted pricing platform initiative by leveraging advanced analytics, statistical, econometrics, and quantitative models while striving to serve the company’s mission to be the best travel deal maker in the world.\\n\\nResponsibilities:\\nSupports the company's product, sales, leadership, operations and marketing teams with insights gained from data analysis.\\nUses large data sets to uncover opportunities for product and process optimization while using models to test the effectiveness of different courses of action.\\nDiscovers solutions hidden in large data sets and works with stakeholders in improving business outcomes.\\nAdditional responsibilities include developing custom data models and algorithms to apply to data sets; using predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.\\nCoordinates with different functional teams to implement models and monitor outcomes.\\nDevelops processes and tools to monitor and analyze model performance and data accuracy.\\nAssesses the effectiveness and accuracy of new data sources and data gathering techniques.\\nStrong problem-solving skills with an emphasis on product development.\\nCoding ability using several coding languages: C, C++, Java; querying databases and using statistical computer data languages.\\nMentors various product team members with experiment design and decisioning around experiments while providing oversight to all methodologies within the product teams, especially within the company's A/B testing system.\\n\\nRequirements:\\n5+ years of experience in data science.\\nExperience using statistical computer languages: R, Python, SQL, etc. to manipulate data and draw insights from large data sets.\\nExperience working with creating data architectures.\\nKnowledge of a variety of machine learning techniques: clustering, decision tree learning, artificial neutral networks, etc. and their real-world advantages/drawbacks.\\nKnowledge of advanced statistical techniques and concepts: regression, properties of distributions, statistical tests and proper usage, etc. and experience with applications.\\nKnowledge and experience in statistical and data mining techniques such as GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\\nExperience analyzing data from third party providers such as Google Analytics.\\nBachelor’s Degree in Mathematics.\\nDemonstrated history of living the values important to priceline: Customer, Innovation, Team, Accountability and Trust. Unquestionable integrity and ethics.\\nPriceline is an equal opportunity employer in accordance with all applicable federal, state and local laws.\\n\\nWe're excited that you are interested in a career with us. For all current employees, please use the internal portal to find jobs and apply.\\n\\nExternal candidates are required to have an account before applying. When you click Apply, returning candidates can log in or new candidates can quickly create an account to save/view applications.\",\n",
       "  \"One of top 10 Analytics Consulting and Operations Management firms in the world is actively hiring Data Scientist in Jersey City, NJ with strong experience in NLP who can develop state of the art, scalable, and self-learning systems for enterprises.Please find below the job description for your perusal.\\n\\nJob Title: Data Scientist (NLP)\\n\\nLocation: Jersey City, NJ\\n\\nDuration: Full Time\\n\\nPosition Overview:\\n\\n• The client developing advanced analytics solutions for some of the most challenging business problems in the industry by leveraging AI, NLP, Computer Vision and Speech. If you are interested in working on cutting-edge technologies and solutions, we have an excellent opportunity for you.\\n• We are looking for Data Scientists in the field of NLP who can develop state of the art, scalable, and self-learning systems for enterprises. Primary responsibility will be to develop advanced analytics based NLP solutions for real world business problems using machine learning, deep learning as well as other relevant NLP capabilities.\\n\\nResponsibilities:\\n\\n• Design, develop and implement solutions for a wide range of NLP use cases across different domains involving classification, extraction and search on unstructured text data\\n• Create and maintain state of the art scalable NLP solutions for multiple business problems. This involves:\\nChoosing the most appropriate NLP technique based on business needs and available data\\nPerforming data exploration and preprocessing\\nTraining and tuning a variety of NLP models/ solutions which include regular expressions, traditional NLP and/or deep learning models\\nAnalyzing and reporting solution output, performance and accuracy\\nDeploying the solution on premise or on cloud\\nMonitoring the solution outcome\\n• Collaborate with ML engineering team to deploy NLP solutions in production- both on premise as well as cloud deployment\\n• Interact with clients and internal business teams to perform solution feasibility as well as design and develop solutions\\n• Present solutions to clients and internal stake holders\\n\\nQualifications:\\n\\n• 3+ years of hands on experience in developing and deploying NLP solution leveraging Regular Expressions and/or Machine Learning\\n• Education: Bachelor's/ Masters degree in quantitative fields (Statistics, Operations Research, Mathematics, Econometrics, Computer Science, Engineering etc.). An advanced degree is preferred.\\n• Proficiency in one or more of programming languages like Python/ Java/ Scala\\n• Proficiency in NLP packages that perform regular expressions based search and extraction, named entity recognition and extraction\\n• Experience with document data bases like MongoDB, Elastic Search etc.\\n• Experience with one or more of deep learning frameworks like TensorFlow, Torch, Keras, Caffe, MXNet, Theano is a plus\\n• Ability to learn emerging NLP, Machine Learning and Deep Learning techniques and apply them to solve business problems\\n• Experience with model optimization on GPUs is preferred\\n• Experience with big data analytics technologies like Apache Spark, Kafka, Flume, ElasticSearch is a plus\\n• Organized, self-motivated, disciplined and detail oriented\\n• Superior verbal and written skills\\n\\nTo discuss further on this opportunity, please connect with Rahil Saiyed at 847-258-9627 or e-mail him at rahil.saiyed @apideltech.com.\\nprovided by Dice\",\n",
       "  'Bayer is a global enterprise with core competencies in the Life Science fields of health care and agriculture. Its products and services are designed to benefit people and improve their quality of life. At Bayer you have the opportunity to be part of a culture where we value the passion of our employees to innovate and give them the power to change.\\nData Scientist\\n\\nYOUR TASKS AND RESPONSIBILITIES\\n\\nBayer U.S. LLC’s Whippany, NJ, office seeks a Data Scientist to enable scientists to identify data driven answers by building diagnostic and predictive models using first and third party data.\\n\\nThe primary responsibilities of this role, Data Scientist, are to:\\nleading design, development, and maintenance of Artificial Intelligence systems;\\nApplying feature engineering and iterative analytics on data to extract valuable insights from clinical and electronic health data;\\nadvising scientists in regards to patterns and relationships in data to recommend business direction and outcomes;\\nresearching and developing advanced machine learning algorithms which provide insights into data beyond human judgement;\\ntranslating requirements from business into advanced analytics models.\\nWHO YOU ARE\\n\\nYour success will be driven by your demonstration of our LIFE values. More specifically related to this position, Bayer seeks an incumbent who possesses the following:\\nMaster’s degree (or foreign equivalent degree) in Information Technology, Data Analytics Engineering, Data Science, Machine Learning, or a directly related field;\\nthree (3) years of experience in a related position;\\nscripting languages such as R, Python and SQL\\ntransforming data into insights with visualization tools such as Tableau and Ggplot/Matplotlib\\nworking with databases, e.g. SQL Server, Oracle and structured data from multiple CSVs;\\nworking in a cloud environment (e.g. AWS, Azure);\\nbuilding predictive models, regression analysis, A/B Testing, statistical modeling and data simulation, time-series forecasting, NLP and text analysis, classification algorithms, supervised and unsupervised machine learning models (e.g., SVM, Random Forest, XGBoost, Neural Networks, and KNN & K-means clustering);\\nresearch and development in data science and machine learning problems;\\nawareness of relevant statistical measures and machine learning performance metrics, e.g. AUC/ROC;\\nBayesian statistics;\\nanomaly detection algorithms, e.g. Auto-encoders, DB-Scan;\\nworking with/in a Unix/Linux environment and Git version control; and (xi) Agile and Scrum methodologies.\\nExperience can be concurrent.\\nYOUR APPLICATION\\nBayer offers a wide variety of competitive compensation and benefits programs. If you meet the requirements of this unique opportunity, and you have the \"Passion to Innovate\" and the \"Power to Change\", we encourage you to apply now. To all recruitment agencies: Bayer does not accept unsolicited third party resumes.\\nBayer is an Equal Opportunity Employer/Disabled/Veterans\\n\\nBayer is committed to providing access and reasonable accommodations in its application process for individuals with disabilities and encourages applicants with disabilities to request any needed accommodation(s) using the contact information below.\\nLocation: United States : New Jersey : Whippany\\nDivision: Corporate Functions\\nReference Code: 61292\\n\\nContact Us\\n+ 1 888-473-1001, option #5',\n",
       "  \"OpenX is seeking a Data Scientist to be responsible for executing critical R&D projects on a petabyte-scale dataset. This isn't an advanced analytics role. Simple linear & logistic regression won't cut it for most of the of problems we face. We're talking about cutting edge applications of Neural Networks, bagged & boosted decision trees, as well as highly customized optimization algorithms. We're looking for a Senior Data Scientist who:\\nCan translate requests from business and product into Data Science/ML problems\\nHas a track record of taking state-of-the-art advances from the literature and incorporating them into production systems\\nWorks well independently and as part of a team of people with diverse backgrounds\\nKey Responsibilities\\nRapidly prototype models, including creation, training, and evaluation to drive business objectives, including:\\nOptimizing aspects of OpenX's programmatic ad exchange to maximize value for our buyers and sellers\\nLeveraging big data to unlock the potential of people based marketing\\nImplement production training and real-time model evaluation architectures in Google Cloud Platform\\nDesign and execute experiments to assess model performance\\nServe as a consultant for analysts across the organization\\nRequired Qualifications\\nD in Computer Science, Statistics, Physics, Mathematics, Operations Research or related field, or MS with 3+ years of experience\\nPractical experience with machine learning languages and tools such as python/pandas and tensorflow/pytorch\\nSolid background in machine learning/data science, control theory, signal processing, or image processing\\nFamiliarity with big data technologies such as SQL, Hadoop, Hive, Impala, Scala, Spark, etc.\\nDesired Characteristics\\nExperience with machine learning on GCP, AWS or Azure\\nExperience reporting findings to a broad audience\\nCompany at a Glance\\n\\nOpenX is focused on unleashing the full economic potential of digital media companies. We do this by making digital advertising markets and technologies that are designed to deliver optimal value to publishers and advertisers on every ad served across all screens.\\n\\nAt OpenX, we have built a team that is uniquely experienced in designing and operating high-scale ad marketplaces, and we are constantly on the lookout for thoughtful, creative executors who are as fascinated as we are about finding new ways to apply a blend of market design, technical innovation, operational excellence, and empathetic partner service to the frontiers of digital advertising.\\n\\nOpenX Values\\n\\nOur five company values form a solid bedrock serving to define us as a group and guide the company. Our values remind us that how we do things often matters as much as what we do.\\n\\nWe are one\\n\\nOne team. No exceptions. We are a group of strong and diverse individuals unified by a clear common purpose.\\n\\nOur customers define us\\n\\nWe know our business flourishes or dies because of our customers.\\n\\nOpenX is mine\\n\\nWe are all owners of OpenX. We stake our personal and professional reputations on the excellence of our work.\\n\\nWe are an open book\\n\\nWe are eager to teach and share what we know with others.\\n\\nWe evolve fast\\n\\nWe take risks and confront failure openly. We recognize and repeat success aggressively. We actively seek out and provide constructive criticism. Defensiveness is for weaklings!\",\n",
       "  'By selecting Apply above, you indicate you have read and acknowledge the Dow Jones Applicant Privacy Notice and Dow Jones Cookie Policy , including data transfers as described in the Policy.\\n\\nJob Description:\\n\\nWe are looking for a data scientist to join the Data Science & Analytics team for The Wall Street Journals Membership group. In this role, you will lead our efforts in machine learning and product analytics, working on the development and refinement of critical models for our businessincluding the propensity model behind WSJs dynamic paywall. Additionally, you will serve as an evangelist for analytics within our organization and develop metrics for measuring engagement within our products.\\n\\nThe role will require a strong understanding of data wrangling, log-level usage data, and production models. You will need strong organization skills, and will oversee the execution of large scale data projects. An interest in best-in-class design and engineering, as well as visualization and storytelling, will also be critical to your success in this role.\\n\\nRequirements:\\nYou have at minimum bachelors degree in information systems, computer science, mathematics, data analytics, data science, or a related quantitative discipline. Applicants with strong technical experience and a professional background in media are also encouraged to apply.\\nAt least 2-5 years of experience working in a data-related role is preferred.\\nYou have strong coding skills in R and/or Python and understand version control software (git/GitHub).\\nYou have a mastery of analytics tools (SQL, Tableau, Adobe/Google Analytics).\\nYou are well versed in the fundamentals of statistics and have a working understanding of machine learning algorithms and their applications.\\nYou have an entrepreneurial attitude toward work and sweat the details.\\nYou have an interest in continued learning and stay well versed in new technologies that can be applied to your work.\\nDow Jones , Making Careers Newsworthy\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, or disability status. EEO/AA/M/F/Disabled/Vets .\\n\\nDow Jones is committed to providing reasonable accommodation for qualified individuals with disabilities, in our job application and/or interview process. If you need assistance or accommodation in completing your application, due to a disability, please reach out to us at TalentResourceTeam@dowjones.com . Please put Reasonable Accommodation\" in the subject line.\\n\\nBusiness Area: WSJ MEMBERSHIP GROUP\\n\\nJob Category: Sales Ops/Planning/Support Group\\n\\nAbout Us\\n\\nDow Jones is a global provider of news and business information, delivering content to consumers and organizations around the world across multiple formats, including print, digital, mobile and live events. Dow Jones has produced unrivaled quality content for more than 125 years and today has one of the worlds largest news gathering operations globally. It produces leading publications and products including the flagship Wall Street Journal, Americas largest newspaper by paid circulation; Factiva, Barrons, MarketWatch, Financial News, DJX, Dow Jones Risk & Compliance, Dow Jones Newswires, and Dow Jones VentureSource.Dow Jones is a division of News Corp (NASDAQ: NWS, NWSA; ASX: NWS, NWSLV).\\n\\nIf you are a current employee at Dow Jones, do not apply here. Please go to the Career section on your Workday homepage and view \"Find Jobs - Dow Jones.\" Thank you.\\n\\nReq ID: 18133',\n",
       "  'Job Description\\n\\nAFC AML Financial Crime Investigations (FCI) – Analytics & Intelligence Team\\nAFC AML – Model Development\\n\\nThe purpose of the AML Financial Crime Investigations team is to ensure that AML coverage assessment, model review and validation is completed annually in accordance with the Anti-Money Laundering/ Sanctions model governance framework within the Americas. Reporting to the Model Development VP in New York and as part of the wider Americas AML and Anti Financial Crime Team. As part of team, your main focus is supporting and performing key responsibilities for model development and implementation process.\\n\\n\\uf0a7 Hands-on development and programming\\n\\uf0a7 Can work independently when task is assigned and deliver a quality product\\n\\uf0a7 Have good understanding of end-to-end development and implementation methodologies\\n\\n\\uf0a7 Hands on programming experience in Python, Spark, R, HQL\\n\\uf0a7 Experience with Big data analytics and Business Intelligence and industry standards tools integrated with Hadoop ecosystems e.g. R, Python\\n\\uf0a7 Experience in Database concepts\\n\\uf0a7 Experience in Hadoop stack technologies – Hive, Impala, Spark, Oozie, MapReduce\\n\\uf0a7 Experience in source control technologies like SVN/Git\\n\\uf0a7 Experience in continuous integration software like Control M, CruiseControl\\n\\uf0a7 Knowledge of Java or other OOP programming is plus\\n\\uf0a7 Good to have experience in Machine Learning, AI aaplications\\n\\uf0a7 Develop regression models and machine learning models\\n\\uf0a7 Perform end-to-end model development including data cleansing, transformation, variable selection, parameters tuning, validation, etc.\\n\\uf0a7 Excellent written and communications skills',\n",
       "  'Description\\nSenior Scientist Position available in our NY, NY office.\\nDuties:\\nModel and predict protein viscosity.\\nDevelop biologics (large proteins), including therapeutic agents.\\nDevelop accurate methods for virtual screening.\\nAutomate input structures for free energy calculations.\\nServe as a computational protein scientist to support the biologics team.\\nDevelop novel algorithms and tools to enhance capabilities in state-of-the-art structure-based drug design by utilizing expert knowledge in protein biochemistry and protein engineering.\\nAddress complicated protein liability issues, such as aggregation and viscosity, centered on an in-depth protein-protein interaction analysis and support the development of the computation framework for use in drug design models.\\nUtilize knowledge of method development, sound programming, and familiarity with the algorithms and computational methods that aid modern protein engineering efforts.\\nDevelop computational tools to calculate a large and diverse set of protein properties that can be used to delineate prediction models based on statistical techniques.\\nCreate highly improved prediction models based on quantitative structure activity relation (QSAR) techniques.\\nSupport methodology development rooted in sound physical theorems in order to gain a better understanding of the causes of aggregation and viscosity.\\nCollaborate with inter-disciplinary groups, including academic labs and industrial organizations in the on-going development and application of drug design.\\nUtilize Python and C/C++ programming languages.\\nDevelop calculation workflows for target structure preparation and protein system analysis.\\nWork with large and orthogonal protein descriptor sets with the ability to capture protein aggregation and viscosity problems.\\nDevelop QSAR predictive models that support the continuous regression and classification schemes and vector machine techniques.\\nPerform analysis of statistical data and develop calculation frameworks centered on the physical principals of the osmotic second virial coefficient (B22) and the Zeta potential.\\nWork with statistical mechanics, molecular dynamics, protein science, and condensed phase physics.\\n\\nMinimum Requirements:\\nPh.D. degree in Physics, Biophysics, Chemistry, or a related field of study.\\nOne (1) year of experience with statistical mechanics, molecular dynamics, protein science, and condensed phase physics.',\n",
       "  'We are conducting search for Data Scientist for one of the major Banks. This will be an on-site position based in Jersey City, NJ.\\n\\nPosition: Data Scientist\\n\\nLocation: Jersey City, New Jersey\\n\\nJob Length: 1 Year\\n\\nPosition Type: W2/C2C\\n\\nRequirements:\\nHelp create the best in class data science models for various business use cases using NLP,NLTK, Spacy, Word2Vec, and other Statistical and Optimization techniques.\\nHelp us to research and explore the new techniques and solve the business problems related to text, language, word vectorization, word embeddings, string matching, context from the text, etc\\nClosely work with the Data Engineering team to integrate the data science models into the business applications\\nTesting, validation and monitoring the data science models and reporting/publishing the performance of the models at a frequent regular interval\\nHave solid understanding about the frameworks like Sklearn, Tensorflow, Spacy, PyTorch, Bert,etc\\nHave solid understanding about the Text/Language analytics esp. de-coding the human language into the readable corpus. Preferable if experienced in the voice to text, Text to Vectors models/techniques\\nHave good understanding about the Active Learning Agent/Multi Agent Learning, etc\\nA mid-high understanding about the sampling techniques along with Hypothesis techniques\\nGood understanding about extrapolating the sampling results to match the overall population by maintaining the same level of performance metrics\\nGood understanding about algorithm development including performance optimization, resource distribution, effective function handling, etc\\nTechnical skills Python, PySpark, PyTorch, R, GCP/AWS Sage Maker, Keras, Big Data Tools Hadoop, AWS, Spark, Tensorflow\\nPreferable Skills Hive/Impala, Airflow/Scheduling\\nQualification\\nTeam spirit\\nStrong Programming Background\\nBS in Computer Science, Engineering or relevant field\\nRegards,\\nPrinceton IT Services\\nwww.princetonits.com\\n\\n- provided by Dice',\n",
       "  \"Job Description The Data Scientist is a key member of the Analytic Tools & Solutions (ATS) group in MIS. ATS is responsible for developing the quantitative models and analytical tools used in the rating process and across the rating agency, as well as certain MIS technology innovation activities, including advanced capabilities in machine learning and artificial intelligence. The Data Scientist will be part of a team of individuals responsible for applying the latest techniques in Machine Learning and Distributed Computing to drive business value particularly related to Blockchain and digital assets. A successful data scientist will not only be technically competent, but will be able to work collaboratively with business stakeholders to increase analytical efficiency, drive new business insights, and develop new products.\\n\\nThe duties of the Data Scientist include:\\nDevelopment and Deployment of Machine Learning / Statistical Learning models for developing analytics and tools largely related to Blockchain and Digital / Crypto assets\\nFull cycle data management from collection and cleaning to processing\\nIdentify potential data sets (internal and external) which could be used to enhance analytics\\nStay up to date with the latest in statistical and machine learning methods\\nApplying sound software and architectural development practices in development and deployment of models as software products.\\nUsing cloud and distributed computing platforms for model development and deployment\\nCommunication of results to business stakeholders and decision makers\\nQualifications\\nMaster's Degree in Computer Science, Statistics, Applied Math, specialized Machine Learning program, or related field\\n3+ years' practical experience in Machine Learning or statistics and/or distributed computing\\nWorking knowledge of Machine Learning techniques include Neural Networks, Tree-based Models, Linear models\\nIndustry knowledge of Blockchain and Digital / Crypto assets is highly preferred\\nExperience with one or more of the following programming languages is highly preferred: Python, R, and SQL\\nMoody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com.. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.\\n\\nFor San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance. For New York City positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the New York City Fair Chance Act. For all other applicants, qualified applicants with criminal histories will be considered for employment consistent with the requirements of applicable law.\\n\\nClick here to view our full EEO policy statement. Click here for more information on your EEO rights under the law.\\n\\nCandidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.\",\n",
       "  'About Cubist\\nCubist Systematic Strategies is one of the world’s premier investment firms. The firm deploys systematic, computer-driven trading strategies across multiple liquid asset classes, including equities, futures and foreign exchange. The core of our effort is rigorous research into a wide range of market anomalies, fueled by our unparalleled access to a wide range of publicly available data sources.\\nJob Description\\nWe are passionate about data. We collaborate to build elegant, effective, scalable and highly reliable solutions to empower predictive modelling in finance.\\nCubist’s Data Services group is looking for a Data Analyst to join our dedicated team. Our group is responsible for the timely delivery of comprehensive and error-free data to some of the most demanding and successful systematic Portfolio Managers in the world.\\nThis exceptional individual will be a member of a small team of Data Scientists/Analysts who play a vital role in ensuring the smooth day-to-day implementation of a large research infrastructure, and the live production trading of billions of dollars of capital across global capital markets, including equities, futures, options and other financial instruments.\\nJob Responsibilities\\nIdentifying potential data sources\\nCoordinating with compliance team and legal team on new vendor trial/subscription process\\nAssisting with collecting and maintaining vendor overviews and content offerings\\nAssisting with data questions and requests from investment teams\\nSetting up feed downloads and monitor check-in database\\nMonitoring the automated data collection and cleansing infrastructure\\nAssisting with scheduling meetings and conference calls between data users and experts\\nAssisting Data team with interoffice travel arrangements, expenses, etc.\\nAssisting in organizing presentations\\nManaging data trial repository permissions\\nDownloading trial datasets from vendor FTP sites or other delivery mechanism\\nAssisting Data Team with manual data processing as required\\nMaintaining Cubist Data Wiki contents\\nDesirable Candidates\\nBachelor degree required\\nBasic understanding of SQL.\\nProgramming skills in Python and at least one of C#, C++, or Java are a plus\\nProficient knowledge of Microsoft Outlook, Excel, and Word\\nFinancial industry experience is preferred but not required\\nStrong organization, communication and interpersonal skills\\nAttention to detail and a love of process\\nStrong oral and written communication skills\\nAbility to exercise sound judgment in assessing and determining how to handle queries, calls and issues\\nAbility to multitask and prioritize assignments',\n",
       "  'DS/ML stack:\\nLanguages: Python, PySpark, SQL\\nData Tools: Spark, AWS RedShift, AWS Athena, pandas, numpy, scipy, parquet\\nModeling Tools: SparkML, scikit-learn, Tensorflow, Tensorflow-Serving, Keras (Tensorflow and Theano backend)\\nAlgorithms: Classifications, Regressions, Neural Networks, Time series, Graphs\\nVisualization: Tableau or similar\\nInfrastructure: AWS (including S3, EMR, EC2, Lambda)\\n\\nWhat will you do?\\nUnderstand business decisions that need to be supported by data e.g. risk of readmission to hospital\\nIdentify relevant internal or external data sources for various business needs\\nResearch the state of a problem and existing solutions then quickly summarize research\\nHelp identify new business opportunities and value propositions from existing data\\nUtilize raw or aggregated data to build predictive models in SPARK, sql, or Python\\nUnderstand quality of models and impact on business problems\\nCommunicate summaries of analyses and predictive modeling efforts to product and business teams\\nCommunicate insights to stakeholders in data engineering, product and clinical teams.\\nBecome expert on projects to help strategize plans of attack in terms of technology and team education.\\nEnsure that all security procedures within their area of responsibility are carried out to achieve compliance with security policies and standards.\\nLeading multiple initiatives across both existing and innovative work\\nProvide continuous mentoring to junior resources\\nWe are looking for someone with:\\n4+ year’s professional experience as a data scientist or machine learning engineer\\n4+ year’s professional experience working in quantitative computational role\\n2+ year’s professional experience working with big data and relational databases\\nVery strong knowledge of advanced applied data science (machine learning, neural networks, etc.), mathematical modeling, computational, statistical, data mining techniques (regression, decision trees, clustering etc.), as well as dimensionality reduction techniques\\nStrong hands-on modeling experience in a business environment with a goal of productionalizing models.\\nStrong experience using machine learning and deep learning packages\\nStrong experience with data manipulation, analysis and visualization\\nProven track record of fully understanding the scope, commitment to quality, and end-to-end ownership to meet upon agreed timelines.\\nQuick learner that can manage multiple projects at the same time successfully\\nDeveloped and designed real-time prediction software\\nStrong experience mentoring junior colleagues\\nExcellent written and verbal communication skills\\nAdvanced degree in physics, applied mathematics, statistics or related field is preferred\\nHealthcare industry experience is a plus',\n",
       "  'Job Description\\nWill work with Head of Reference Data and Data Warehouse Application Development and front office groups.\\nLooking for candidates with exposure to machine learning technologies\\nIdeal candidate coming from a hedge fund, or an investment bank. Needs to be familiar with our asset classes(Capital Markets, Credit, Liquidity risk) and market data\\nMust have at least 10 years in SQL, data warehouse and ETL tools\\nMust have at least 3 – 4 years in Python, Pandas, Scikit learn and Apache Spark',\n",
       "  'Job Description\\n\\nOur client is looking for a Data Scientist/ Statistician to join their New York office.\\n\\nResponsibilities:\\nServe as expert statistician/data scientist on discretionary investment team\\nTest research hypotheses and assumptions of portfolio manager and investment analysts by designing methodology and writing necessary code to perform backtesting, cross-validation, event studies, Bayesian data analysis, etc.\\nTest and identify the most predictive and robust statistical, machine learning, and deep learning methods to forecast factors, features, metrics, drivers, etc. relevant to portfolio manager and investment analysts in making investment decisions\\nWith guidance from and in collaboration with portfolio manager and investment analysts help select relevant data sets\\nWork closely with team’s data engineer/systems developer to help specify best sources, format, and delivery mechanism of data\\nWrite efficient, modular, and dependable code, packages, libraries, and scripts\\nEnsure code is written so it is easily understood by teammates when reviewing\\nDocument all work extensively and train teammates on use of work products (e.g., custom Python libraries or R packages)\\nWork closely with team’s data engineer/systems developer and portfolio manager to design research (e.g., backtesting software) and production (e.g., trade file creation) processes\\nCollaborate regularly with firm’s Big Data group (Aperio) and other firm resources\\nStay abreast of new research in statistics, machine learning, and deep learning\\n\\n\\nRequirements:\\n3+ years of working experience in a role that requires advanced statistical analysis\\nBe an expert in statistics (e.g., doctoral level) including time-series analysis\\nHave strong programming skills in SQL/NoSQL and at least one of the following Python, R, or C++\\nGood communication skills\\nEducation:\\nAdvanced degree in quantitative discipline (or 6+ years of full-time work experience using advanced statistical analysis)\\nIf you would like to be considered for the position of Data Scientist / Statistician or wish to discuss the role further then please leave your details below. Your resume will be held in confidence until you connect with a member of our team',\n",
       "  \"About Us:\\n\\nSmartAsset is an award-winning financial technology company pursuing the singular mission of empowering people to make smart financial decisions. Recently named one of Y Combinator's Top 100 companies of all time, we have raised more than $50 million in venture capital. Our personal finance tools, calculators and content reach more than 45 million people each month.\\n\\nWe've been featured in hundreds of publications including The Wall Street Journal, CNN, Mashable, TechCrunch, The New York Times, The Washington Post, US News & World Report, TIME, Reuters, Businessweek, Barron's and many more.\\n\\nAbout You:\\n\\nWe're looking for a data scientist who can help take our marketing to cutting-edge levels of data sophistication. Data-driven decision making is at the heart of the SmartAsset and marketing team cultures already. This is an opportunity to bring advanced analytics to the team to super charge our efficiency and growth. Join a very talented, scrappy team of marketers who have already driven amazing growth.\\n\\nResponsibilities:\\nOptimizing bidding strategies through various ad platforms\\nDeveloping a high-volume creative testing framework\\nModeling impact of offline advertising channels\\nOptimizing profitability of marketing efforts at a granular level by syncing with inventory availability\\nDevelop segmentation models to inform creative and targeting\\nAutomating analyses to improve team efficiency\\nImplementing API endpoint to operationalize models\\nRequired Skills and Experience:\\nBachelor's Degree\\n3+ total years experience with 2+ years as a data scientist\\nKnowledge of key digital marketing platforms (DCM, DMPs, Google Ads, Facebook, etc)\\nStrong knowledge of SQL, Python, and analytics packages (pandas, scikit-learn, etc)\\nStrong knowledge of statistics\\nStrong communication (oral, written) skills\\nDesire to work collaboratively and cross-functionally\\nA desire to learn about all facets of the SmartAsset business\\nPreferred Skills and Experience:\\nAdvanced degree in math / statistics / science / engineering\\nWorking front end knowledge - HTML/CSS/jQuery\\nExperience with AWS services/EC2\\nExperience with BI tools - Looker, Tableau etc.\\nPerks:\\nCatered lunch three times per week\\nCasual dress code\\nWeekly happy hours\\nRegular team social activities such as a company book club, sports leagues, community service opportunities and more\\nBenefits package including medical, dental and 401(k)\\nProfessional education reimbursement program\",\n",
       "  'Data Scientist - Data Engineer\\n\\n\\nREF#: 32836\\n\\nCBS BUSINESS UNIT: Simon & Schuster\\n\\nJOB TYPE: Temporary / Per Diem / Freelance\\n\\nJOB SCHEDULE: Full-Time\\n\\nJOB LOCATION: New York, NY\\n\\nABOUT US:\\n\\nSimon & Schuster, a part of CBS Corporation, is a global leader in the field of general interest publishing, dedicated to providing the best in fiction and nonfiction for consumers of all ages, across all printed, electronic, and audio formats. Its divisions include Simon & Schuster Adult Publishing, Simon & Schuster Childrens Publishing, Simon & Schuster Audio, Simon & Schuster Digital, and international companies in Australia, Canada, India and the United Kingdom.\\n\\nDESCRIPTION:\\n\\nSimon & Schuster has an exciting role for a data engineer to join a fast-paced, leading-edge team working to help advance its publishing business. In this Freelance or Independent Contractor role, you will be working with a small team of data scientists and analysts to rapidly prototype data applications and analytics tools to serve our fast-growing imprint.\\n\\nThe full-time, contract position will be responsible for developing, testing and maintaining data flows and information architecture. This includes aggregating data from multiple databases in our data warehouse as well as external sources, establishing data pipelines, designing data models, deploying machine learning models to production and building robust data visualizations and reporting tools that align with the business needs of our team.\\n\\nThis is an exciting opportunity for the right candidate to build a robust data science and analytics environment from the ground up. Youll be working with a small team of professionals based remotely. Hours are flexible. Two scheduled check-ins/code reviews are required weekly.\\n\\nResponsibilities\\nCollaborate with team to align architecture with business requirements\\nStand up, test and maintain databases and information architecture\\nBuild tools to automate data acquisition and aggregation\\nDevelop data models and table schemas\\nIdentify ways to improve data reliability, efficiency and quality\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nPrepare data for predictive and prescriptive modeling\\nUse data to discover tasks that can be automated\\nDevelop data visualizations, dashboards, reports and notification tools that respond to business needs\\nDesign analytical models that support analysis and inference.\\nQUALIFICATIONS:\\n\\nSkills and experience:\\nComputer science degree or equivalent with 2+ years data engineering and/or full stack developer experience\\nBackground in data science or equivalent continuing education\\nStrong knowledge of Python 3\\nExperience in developing and maintaining SQL databases and writing SQL queries; NoSQL (MongoDB) experience preferred but not required\\nExperience establishing and maintaining AWS, Google Cloud, Heroku, Tableau or similar cloud-based environments\\nKnowledge of HTML, CSS and JS and lightweight web frameworks such as Django, Flask\\nFamiliarity with database administration tools and best practices\\nBackground in extracting and storing data from APIs preferred but not required\\nBackground in deploying machine learning models to production preferred but not required\\nKnowledge of Git/GitHub and software development workflows\\nStrong written and verbal communication skills.\\nAbility to summarize key findings into clear and actionable recommendations, and to interact with individuals at all levels.\\nStrong project management skills and ability to plan and prioritize work in a fast-paced environment.Please send a link to your work samples on Github\\n\\nEEO STATEMENT:\\n\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled',\n",
       "  'Title: Data Science and Analytics\\n\\nLocation: New York\\n\\nDuration: 3 Months\\n\\nRequired Skills:\\n\\n• Website/mobile (Adobe Analytics)\\n\\n• Data Visualization: advanced Excel, Tableau, Power BI\\n\\n• Marketing Automation\\n\\n• Adobe Test & Target\\n\\nPreferred Skills:\\n\\n• Adobe Analytics\\n\\n• Adobe Test & Target\\n\\n• Marketing Automation\\n\\nWho are you?\\n\\n• Bachelor’s degree, in Marketing, Business,\\n\\n• 8-10+ years of work experience in a quantitative business environment with 5+ years in marketing analytics\\n\\n• Passion for digital analytics, working with data and deriving insights to answer client business questions\\n\\n• Experience leading and delivering marketing analytics engagements\\n\\n• Strong knowledge of the digital space and digital analytics\\n\\n• Strong communication and client service skills\\n\\n• Strong organizational skills and attention to detail\\n\\n• Strong consultative skills and the ability to challenge status quo and gain adoption\\n\\n• Comfortable with working in and contributing to a fast-paced, team-based environment\\n\\n• Management or mentorship experience - providing training, project support, career advice and delegated responsibilities\\n\\n• Proven ability to influence stakeholder and senior leaders, and manage organizational change\\n\\n• The candidate should be a hybrid of analyst, but also storyteller.\\n\\n• Being able to distill data into keen, actionable insights.\\n\\n• They need to be able to synthesize the data. So we need someone who is an excellent verbal and written communicator',\n",
       "  \"Small Teams; Big Data\\n\\n\\nWe are looking for a Senior Data Scientist with project leadership responsibilities to research and develop state-of-the-art algorithms and build models to support Tapad's mission to creating the largest identity graph in the world. Tapad is home to the team that cracked the code on cross-device marketing technology. Our groundbreaking, proprietary tech assimilates trillions of data points to find the relationship between smartphones, desktops, laptops, tablets, connected TVs and game consoles.\\n\\nAs part of the Data Science team, you will work on complex problems related to our suite of identity resolution and targeting products using cutting-edge machine learning, natural language processing, and advanced statistical and clustering techniques to build models that make their way into production. Additionally, you will perform advanced analyses that supports model improvements, product enhancements, and identify and assess new opportunities to innovate and create new solutions for clients and the Tapad business. As a Tapad Data Scientist, you are an integral partner in the development of Tapad's products. You will work with Engineers and Product Managers to deliver solutions into production, and partner with Platform Solutions, Marketing, and Sales Associates to understand client needs and identify new opportunities for Tapad and our clients.\\n\\nWe value transparency, open knowledge exchange, excellence, and creativity in our work. When you work with us, you matter. We ask our employees to make an impact and feel it is only right to give a lot in return. We offer every employee a 401k with matching, generous parental leave, and unlimited PTO. We believe if you're sick, feel like you're getting sick, or just need a personal day, you should take that time to get better. We have free lunch every Wednesday, free bagels every Friday, and a free and open hierarchy every day. We make sure our office is full of individuals who can teach and learn from one another.\\n\\nTechnologies we use at Tapad:\\nScala, SBT, Play!, Akka\\nScalding, Spark, Python\\nKubernetes, BigTable, BigQuery\\nTypeScript, Angular, Node.js, Hapi.js, Postgres, MySql\\nIn this role, you will be using:\\nPython, SQL\\nSpark, SparkML, AutoML, SciKit Learn, etc.\\nGoogle Cloud Platform (GCP)\\nResponsibilities:\\n\\n\\nUse machine learning, clustering, statistical modeling, and other advanced techniques to infer device, individual, and household connections from petabyte sized probabilistic internet signals\\nEmploy predictive modeling, data mining, graph algorithms, and other data science techniques with the end goal of increasing our graph product's ability to drive customer value Design experiments and integrated quality statistics to measure the impact of our models on the overall performance of our device graph\\nPartner with engineers to productionalize various machine learning and clustering solutions that enhance and strengthen connections within our device graph\\nCollaborate with product managers and business partners to understand client needs and help identify opportunities for Tapad and our clients\\nQualifications:\\n\\n\\nPh.D. in a Quantitative Discipline (e.g. Physics, Mathematics, Statistics, Neuroscience, Chemistry, Engineering, etc.)\\n6+ years of experience as a Data Scientist, especially in applying Machine Learning techniques, advanced analytics, and statistical modeling\\nExperience in working on large-scale, distributed system environment(s)\\nExperience manipulating large data sets, using SQL, Hadoop, MapReduce, Spark, etc.\\nUnderstanding of programming concepts and experience with programming languages, such as Python, MATLAB, R, Java, Scala or C+\\nTapad Perks:\\nGenerous PTO - no accruing necessary\\n401k matching\\nOn-site medical clinic (we bring the doctor in-house once per quarter, so you can make appointments at your convenience)\\nScala School (we'll teach you!), Coursera, LinkedIn learning, peer-lead professional development, and an abundance of resources to help you stay sharp\\nUnlimited snacks and beverages\\nCollaboration catered lunches\\nDiscounts on gym memberships\\nFussball, ping pong, diversity and inclusion group, book club, Tough Mudder, push-up challenges, and tons of other extra-curricular activities that will make you feel like part of the Tapad family\\n\\n\\nTapad is proud to be an equal opportunity employer and will consider all qualified applicants regardless of age, sex, race, religion, national origin, sexual orientation, gender identity, marital or family status, disability, or any other legally protected status. Tapad does not accept resumes from unsolicited search firms nor recruiters. In no event shall fees be paid to any unsolicited search firms nor recruiters, regardless of whether the candidate is made an offer or accepts a placement at Tapad. All resumes received through any channels will be considered the sole property of Tapad.\",\n",
       "  'Position: Data Scientist\\n\\nLocation: Hoboken/Atlanta/Remote\\n\\nNICE Actimize is comprised of talented, creative and dedicated individuals with a passion for delivering innovative solutions to the market. At NICE Actimize, we recognize that every employee’s contributions are integral to our company’s growth and success. To find and acquire the best and brightest talent around the globe, we offer a challenging work environment, competitive compensation and benefits, and rewarding career opportunities. Come share, grow and learn with us – you’ll be challenged, you’ll have fun and you’ll be part of a fast growing, highly respected organization.\\n\\nNICE Actimize is currently seeking an experienced Data Scientist to join our dynamic and growing Fraud & AML Analytics Services team.\\n\\nResponsibilities\\nPerform analysis to support the deployment of fraud prevention analytical models\\nAnalyze fraud cases obtained from clients\\nResearch data patterns in order to find patterns predictive of fraud\\nImprove the quality and actual implementation of computational algorithms and tools\\nOptimize the detection performance of NICE Actimize Fraud products and improve customers’ experience with our Fraud solutions\\nDefine product requirements for analytics and provide feedback to the product team on ways in which product may be improved\\nDevelop and enhance our solution-specific risk scores\\nMeasure the quality of the analytical performance of Fraud Products\\nDevelop tools to support model tuning, performance tracking and automation\\nDevelop custom detection logic for specific clients\\nQualifications\\nTwo years research and analytical experience in large-scale data analysis\\nKnowledge of statistics and computational algorithms\\nProven record of large-scale data analysis\\nExperience with SQL\\nExperience with Data Mining/designing analytical models\\nExperience with programming languages such as Python\\nExperience with statistical packages such as SAS or R\\nTeam-oriented and yet able to work independently\\nAble to thrive in a fast-paced environment and learn quickly\\nFinancial services experience (banking, brokerage or insurance) – a plus\\nKnowledge of fraud or AML prevention software systems – a plus\\nBackground in software development – a plus\\nCustomer facing experience – a plus\\nAbility to travel 20% of the time\\nMS in Computer Science or a directly related computational field preferred\\nWill also accept BS degree in Computer Science or other computational related fields plus five (5) years research and analytical experience in large-scale data analysis\\nNICE Actimize is offering a competitive base salary, bonus and a full benefit package. Benefits include: medical, dental, 401K, vision, Life, STD, LTD, and a Flexible spending account.\\n\\nNICE Actimize Inc. is an Equal Opportunity Employer and does not discriminate on the basis of age, gender, race, color, religion, creed, national origin, sexual orientation, marital status, disability, disabled veteran status or Vietnam era veteran status, or other legally protected categories',\n",
       "  'Business Intelligence Analyst\\n\\n\\nREF#: 35155\\n\\nCBS BUSINESS UNIT: CBS Interactive\\n\\nJOB TYPE: Full-Time Staff\\n\\nJOB SCHEDULE:\\n\\nJOB LOCATION: New York, NY\\n\\nABOUT US:\\n\\nCBS Interactive is the premier online content network for information and online operations of CBS Corporation as well as some of the top native digital brands in the entertainment industry. Our brands dive deep into the things people care about across entertainment, technology, news, games, business and sports. With over 1 billion users visiting our properties every quarter, we are a global top 10 web property and one of the largest premium content networks online.\\n\\nCheck us out on [1] The Muse, [2] Instagram and [3] YouTube for an inside look into \\'Life At CBSi\\' through employee testimonials, office photos and company updates.\\n\\nReferences\\n\\nVisible links\\nhttps://www.themuse.com/companies/cbsinteractive\\nhttps://www.instagram.com/cbsinteractive/?hl=en\\nhttps://www.youtube.com/channel/UCAvGapyifCtUlmNTagAl_sQ\\nDESCRIPTION:\\n\\nRole Details:\\n\\nInterest and ability to take on highly technical analyses, which require researching, reporting, presenting and ultimately providing decision support services and actionable insights to the business. Sources of data include traffic/video/ consumption data from Adobe Analytics tools (Omniture), purchase/ subscription data via SQL queries on a data warehouse, audience data from syndicated audience measurement services (comScore/ Nielsen), Ad revenue data from Doubleclick, and many more\\n\\nYour Day-to-Day:\\nBuild and manage reports showing the health of key business metrics across platforms like desktop web, mobile web, mobile apps, connected over-the-top devices, and subscription video services\\nWrite SQL to query a data warehouse and join a customer table to a product-purchase table in order to report the number of customers in a particular city/ zip code that purchased an online video subscription service last month\\nCreating reports that spotlight the lifetime value of a person that downloaded the CBS Sports App (iOS or Android)\\nScrape twitter to pull in tweets from a specific hashtag from yesterdays CBS-broadcasted game. Use Python to perform exploratory data analysis on the data and process the tweets via a sentiment analysis engine to gauge how viewers felt about the sports event. Uncover data stories and insights and present your findings to the executive team\\nBuild dashboards and reports that illustrate the fall-out rate of customers as they engage in a product purchase funnel. Make recommendations to the business from the data\\nArming the editorial staff with insights around the types of stories and content that resonated with users during the NFL Draft, so they can best focus their time covering the areas of highest user interest\\nLeverage insights found via 3rd-party research companies and syndicated tools (comScore) to uncover areas of strength versus sports competitors. Socialize and present findings to the Sales team to aide in their efforts\\nKey Projects:\\nBuild dashboards (using Tableau, Python and other BI tools) that illustrate how key performance indicators (KPIs) and metrics are changing over time. Provide \"color\" and insights from the findings and make recommendations to the business.\\nCreate a lifetime value report (LTV) by signup month cohort for a paid online content subscription product in order to understand how much we can spend to acquire new customers\\nCreate a user-based analysis to understand how different consumers, broken down by behavioral segments, contribute to overall revenue. The ultimate goal is to be able to use this analysis to encourage profitable user behavior or to convert users for one segment to a more profitable one\\nQUALIFICATIONS:\\n\\nWhat you bring to the team:\\n\\nYou have -\\nBA/BS\\n2+ years experience in web analytics, online media industry.\\nStrong experience in web analytics, video analytics (Adobe Analytics, Google Analytics) and 3rd-party syndicated audience measurement services (comScore/ Nielsen)\\nUnderstanding of web, native app, video measurement technology and standards\\nProficient with analytical tools and languages supporting data analysis, reporting, and visualization - Excel, Microsoft Office, Google Collaborative Apps (Docs, Sheets, Slides), Tableau, R, Python\\nExperience extracting data from databases using SQL\\nStrong detail-orientation with a penchant for data accuracy and good grammar\\nExceptional communicator across all levels of the organization; able to effectively tell stories with data and present findings to a non-technical audience\\nYou might also have -\\nStrong mathematics, statistics background\\nExperience using project management tools like those from Atlassian (JIRA, Confluence)\\nExperience using Google Cloud Platform (BigQuery)\\nEEO STATEMENT:\\n\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled',\n",
       "  'About Us\\n\\nJust as the field of Data Science is growing and ever-changing, so too is our team: we’re on the hunt for instructors who are passionate about what they do and want to make a difference in the education of the world’s up-and-coming data scientists — our students.\\n\\nThe NYC Data Science Academy prides itself on housing the most comprehensive 12-week intensive bootcamp in data science methodologies, providing theoretical, practical, and hands-on knowledge to our scholars. We adapt faster than the quickest machine learning algorithms out there — with content that reflects research and application in the growing market and teaching expertise that is beyond ordinary.\\n\\nThat’s where you come in.\\n\\nFrom our part-time weekend/evening classes, to our full-time boot camp, our courses are both designed in-house and taught by our robust team of data scientists and engineers. Instructors have the opportunity of taking part in corporate training and consulting projects, building both data science and big data solutions. We encourage collaboration and positive change in not only our students and clients but also in our team. Nerding out is also highly encouraged.\\n\\nResponsibilities\\nDevelop, evaluate, deliver, and maintain superb content on the cutting-edge of technology for training experiences relevant to audiences with a diverse set of backgrounds.\\nParticipate in planned tailoring of teaching content for both corporate and client audiences.\\nProvide feedback to product and content development teams.\\nStand as an instructional mentor for students of all levels.\\nAbout You\\nMinimum of a Master’s Degree in a Science, Technology, Engineering, or Mathematics-related field; Computer Science and Statistics preferred.\\nProficiency in statistical computing and/or programming in R and/or Python.\\nTaught undergraduate or graduate level coursework in STEM, required.\\nDemonstrated knowledge of Statistics and Machine Learning from both a theoretical and applied perspective.\\nExperience in developing a curriculum or providing training in a client-facing endeavor.\\nPerks\\nCompetitive salary, adjustable hours, and flexible vacation policy.\\nOpportunity to train, research, and learn in the field of data science on-the-job; chance to interact with active data science communities through conferences, Meetup events, etc.\\nCompletely stocked snack pantry.\\nHigh-quality computational equipment.',\n",
       "  \"Medidata: Conquering Diseases Together\\n\\nYour Mission:\\n\\nYou will play a key role in developing and delivering analytical insights on client engagements as part of the Intelligent Trials team of Acorn AI, a Medidata company powered by data from over 17,000 clinical trials and provide actionable insights to the front lines of life sciences decision-making, from R&D to commercialization.\\n\\nYou will work with a dynamic team of predictive modelers, computer scientists, UX designers, data wranglers, and business experts. You will support the productization of analytics into cloud based software.\\n\\nYour Commitments:\\nDevelop analyses for and support management of client engagements involving large amounts of data, statistical modeling / machine learning, prototyping and cloud based software products\\nWork across an interdisciplinary team to prioritize and coordinate tasks, monitor progress and issues, facilitate team meetings\\nPerform and share analyses to guide internal and external decision making\\nCoordinate and assist in the testing of new products and features\\nHelp develop external communication materials, translating and summarizing technical information for business audiences and developing key messaging\\nYour Education & Experience:\\n\\nMust have(s)...\\nBachelor’s degree (in a quantitative or business discipline (business administration, management information systems, computer science, engineering, statistics or a related field)) and a minimum of 4 years of related experience; or an advanced degree and 2 years of work experience; or equivalent work experience.\\nDemonstrated creative problem solving, analytical and organizational skills\\nAbility to manage work in teams of widely varying skills and levels\\nFamiliarity with analytics concepts such as databases, programming, statistical modeling, machine learning, data visualization\\nExcellent verbal and written communication skills, great attention to detail\\nAdvanced skills in MS Office, specifically PowerPoint, Excel and Word\\nNice to have(s) but not required…\\nExperience as an analytics consultant or business analyst on a data science team\\nFamiliarity with Agile development and JIRA\\nWork experience with SQL, exposure to R or SAS\\nOur Industry: Where we play\\n\\nMedidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development. Our work ensures that life-saving drugs and medical devices get to market faster. We are a certified “Great Place to Work” with highly engaged employees focused on improving the health outcomes of patients across the globe.\\n\\nWe develop cloud-based enterprise products and services and are a world leader driving the convergence of the Technology and Life Sciences industries, one of the most exciting areas for innovation globally. With annual revenue in 2016 of nearly $500 million, we are publicly traded (MDSO) with over 850 customers and customer retention rates above 99%. The Medidata Clinical Cloud® is the primary technology solution powering clinical trials for 18 of the world's top 25 global pharmaceutical companies and is used by 18 of the top 25 medical device developers—from study design and planning through execution, management and reporting. Our customers include global pharmaceutical companies, biotech, diagnostic and device firms, leading academic medical centers, technical partners and contract research organizations.\\n\\nBy automating over 12,000 clinical trials to date, Medidata has the largest collection of clinical trial data in the world. Today, Medidata pioneers innovative, advanced applications and intelligent data analytics, bringing a new level of quality and efficiency to clinical trials. That means better treatments reach waiting patients sooner.\\n\\nWe know that diverse teams win. It is our diversity and inclusiveness that fuels innovation and sparks our passion and commitment to patient health. We are still led by our Co-founders, Tarek Sherif and Glen de Vries, and have global operations in US, Europe and Asia with over 1900 employees.\\n\\nOur Culture: Who we are\\n\\nWe know that creativity doesn't happen on-demand. Developing cutting-edge cloud technology takes great minds and talented people working together in a collaborative environment. That is why we are committed to fostering an innovative, agile company culture. We encourage our teams to come together and experiment with new concepts, research new approaches and test out new technologies. We believe that being part of our team will make a difference in the world.\\n\\nOur Mission: Powering smarter treatments and healthier people.\\n\\nOur Vision: To be the most innovative cloud company in Life Sciences.\\n\\nOur Principles: Integrity, Partnership, Inventiveness, Humility, Nimbleness, Tenacity, Inclusiveness, and Caring.\\n\\nOur Leadership Drivers:\\nTHINK: Inspires purpose, articulates strategy, and simplifies complexity\\nTEAM: Communicates effectively, builds relationships and collaborates with others\\nDO: Plans ahead, scales for growth, ensures accountability\\nLEARN: Self-aware, values difference, strives to learn\\nTEACH: Inspires work, coaches others, builds teams\\nEEO Statement\\n\\nUS:\\n\\nMedidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.\\n\\n*LI-AS1\",\n",
       "  'The Senior Data Scientist will be a part of the S&P Global Market Intelligence (SPGMI) Data Science team.\\n\\nThe Role:\\n\\n• Discover insights and identify opportunities through the use of statistics, algorithms, data mining and visualization techniques\\n• Build and evaluate models, make predictions, gather results, and communicate findings to stakeholders\\n• Use advanced business knowledge and advanced machine learning techniques to acquire, combine & transform multiple datasets to solve a business use case\\n• Collaborate with engineering and product teams to create and build strategic models that drive product improvements while maintaining cost efficiency\\n• Help in building and maintaining re-usable machine learning and model validation procedures for the rest of the team to use\\n\\nExperience and qualifications:\\n\\n• Bachelors Degree in Mathematics, Statistics, Computer Science, Engineering, Operations Research or related fields preferred (Masters degree an advantage)\\n\\n• 2+ years practical experience with statistical analysis and creating complex models, preferably in the financial services sector\\n\\n• Excellent analytical and problem solving skills\\n\\n• Advanced experience in at least one data analysis/data transformation package (R, Python, Alteryx)\\n\\n• Exposure to one or more data discovery, data visualization tools\\n\\n• 2+ years of hypothesis testing, web analytics and python scripting\\n\\n• Experience with Machine Learning\\n\\n• Ability to remain focused and to think logically in a fast-paced environment\\n\\nGrade 10 (For Internal Purpose)\\n\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.',\n",
       "  '7Park Data, a Vista Equity portfolio company, is the world’s leading alternative data intelligence firm. We have access to some of the most coveted alternative datasets – such as clickstream, geolocation, mobile app usage, credit and debit card, email receipt, and shipping cargo data – and are constantly acquiring more.\\n\\nYou will be joining other extremely passionate data scientists, product managers, and engineers that share a common interest in tackling some of the most difficult data science and machine learning problems today. With the data products and machine learning systems you design, 7Park will arm influential decisionmakers at financial and corporate giants with critical information they need to make smart, data-driven decisions. And, along the way, you will help advance the current work in artificial intelligence and predictive modeling.\\n\\nThe data science team has two major goals: to create and build data products that provide intelligence on real-time economic activity to our financial and corporate clients and to research and develop machine learning systems to extract information from large volumes of structured and unstructured text.\\n\\nWe are looking for a talented and creative Data Scientist to join our data science team. As a Data Scientist, you will be responsible for the following:\\nConduct research on some of the world’s most interesting alternative datasets\\nPlan, develop, and apply cutting-edge machine learning systems and statistical modeling to extract insight from vast amounts of data at scale\\nWrite production-ready code to analyze, structure, and make accurate and timely predictions\\nDesign systems to monitor the results of models in productions, discover and address anomalies, and ensure the robustness and reliability of these models\\nLead projects from start to finish, collaborating with 7Park’s senior management, product managers, engineers, external data partners, and clients\\nAn ideal candidate will be passionate about building machine learning systems on real world data and have several years of industry experience and/or a Masters or PhD in computer science, mathematics, statistics, linguistics, physics, computational finance, or a similar quantitative field.\\n\\nIt is also very important that you enjoying working in a lean, tight-knit, and highly entrepreneurial startup that marries the creative, experimental problem-solving found in academia with the hacker ethos of shipping products quickly and often.\\n\\nCompensation package: includes highly competitive salary, bonus and 401(k) plan.\\n\\nRequirements\\nAt least 5 years of relevant professional experience and/or a Masters or PhD in computer science, mathematics, statistics, linguistics, physics, computational finance, or similar quantitative field.\\nStrong knowledge of machine learning, computer science, mathematics, and statistics.\\nStrong programming skills in Python, R, and/or Scala.\\nExperience with NumPy, SciPy, Pandas, Scikit-Learn, TensorFlow, and Keras. PyTorch is also acceptable.\\nExperience with building distributed machine learning systems using Apache Spark and a working knowledge of MLlib.\\nExperience with several of the following concepts: decision trees, random forests, and gradient boosting; linear regression; logistic regression; linear and non-linear dimensionality reduction using PCA, kernel methods, and dictionary learning; clustering with K-means, hierarchical clustering, and DBSCAN; autoencoders; generative models; and sequential data modeling.\\nBonus\\nPublications in communities such as NIPS, ICML, or related.\\nGitHub projects demonstrating your creative drive.\\nKaggle wins demonstrating your competitive drive.\\nExperience running or working at data-centric startups.\\nExperience with knowledge graphs.\\nWorking knowledge of GraphX and Spark Streaming.',\n",
       "  \"Company Description\\n\\nIngrooves is a leading global music marketing and distribution company. We provide labels and artists with a global team of experts and a powerful technology framework, giving them transparent distribution tools and marketing solutions to maximize revenue in today’s dynamic music marketplace. By embracing data science and continually updating its offerings, Ingrooves is redefining the modern music company.\\n\\nJob Description\\n\\nThis is an execution and delivery-oriented, highly visible role in the Insights & Analytics team with primary responsibility for building, testing, and validating scalable machine learning models that drive business-relevant insight generation. This includes the development of project plans, handling of potential issues and risks, collaborating constructively with project team members, providing effective cross-functional communication, and advancing multiple projects at once.\\n\\nThe successful candidate will report directly to the Chief Analytics Officer and will be based at our New York, New York office.\\n\\nKey Responsibilities:\\nDesign, build, and deploy machine learning models at scale\\nCollaborate closely with product teams to provide high-level AI-driven services for music content owners\\nIntegrate with external and internal APIs and data sources to augment music consumption data to uncover shifting market trends, new opportunities, operational efficiencies and revenue streams\\nCreate data-rich informative visuals and documents to articulate and convey complex concepts to executive leadership team and label partners\\nContribute to and enable a positive work environment which highly encourages creative input and constructive criticism from all team members\\nOther duties as determined by the Chief Analytics Officer\\nQualifications\\n\\nMinimum Requirements:\\nEarned graduate degree (Ph.D.) in a highly quantitative discipline (e.g., machine learning, statistics, computer science, applied mathematics, theoretical physics, physical chemistry, econometrics)\\nStrong record of peer-reviewed scientific publications and/or patents, with at least one publication and/or patent involving the development or application of machine learning algorithms\\nKey Qualifications & Experience:\\n3+ years of experience developing machine learning models to deliver robust predictions and inferences\\nDemonstrated experience scaling machine learning up to logistically challenging data sets (e.g. leveraging the Hadoop ecosystem, distributed deep model training, etc.)\\nStrong software development skills, with demonstrated ability to implement inferential and predictive models. Experience with Python + relevant data science and deep learning libraries preferred\\nExperience with AWS (i.e., EC2, S3, RDS, Redshift, EMR) or similar cloud computing services\\nExcellent communication and exposition skills with the ability to explain and present complex analyses and machine learning concepts to a broad audience, both technical and non-technical, in English\\nDemonstrated ability to work creatively and deliver within highly collaborative work environments while remaining product-focused\\nAdditional Information\\n\\nSuccess at Ingrooves\\n\\nBusiness Acumen\\n\\nThe successful candidate aligns with the Ingrooves culture by being solution-oriented, collaborative, leveraging best practices, and possessing a passion for both technology and music.\\n\\nWhat this means: You care about success and moving your team forward.\\n\\nInterpersonal/Communication Skills\\n\\nThe successful candidate will be open minded, with a natural curiosity of their internal and external customers, delivery oriented with an understanding of the product development process. Builds and fosters strong relationships and effective partnerships with cross-functional teams.\\n\\nWhat this means: You care about your teammates and clients.\\n\\nExecuting for Results\\n\\nThe successful candidate will demonstrate the ability to generate and translate strategic plans into actions with timely execution and accountability.\\n\\nWhat this means: You have a strong work ethic.\\n\\nLeadership/Collaboration\\n\\nEstablishes and maintains positive working relationships, operates with integrity, influences and supports others, and remains open to ideas.\\n\\nWhat this means: Your teammates care about you.\\n\\nProblem Solving\\n\\nThe successful candidate will be tenacious and self-motivated and have a demonstrable record of resolving issues and providing effective solutions. Demonstrates an eagerness and ability to learn quickly and leverages a flexible mindset in response to shifting dynamics, adversity and/or change.\\n\\nWhat this means: You're efficient and intelligent.\\n\\nWhy work with us?\\n\\nWe are passionate about developing dynamic solutions in a relaxed and engaged environment. Our people matter! And we have a fun, relaxed working environment (with great views, among other perks)!\",\n",
       "  \"Who We’re Looking For:\\n\\nNational Debt Relief (NDR) is currently seeking an inquisitive, highly motivated, and creative Data Scientist who is passionate about helping customers get out of debt. The ideal candidate will have hands on experience transforming unique data into amazing products. At scale.\\n\\nAt NDR you will have access to an enormous amount of high-value business activity data including unstructured and semi-structured records around the sales process as well as post sales customer activity. You will participate in the end-to-end processes of machine learning, from proof of concept to deploying models in production. You will be asked to experiment, and conduct research work geared towards new product development.\\n\\nPrinciple Duties & Responsibilities:\\n\\n· Work with large, complex datasets and solve difficult, non-routine analysis problems, applying advanced analytical methods as needed. Conduct end-to-end analysis that includes data gathering and requirements specification, data processing, analysis, and deployment to production.\\n\\n· Research and develop models to improve the quality of NDR user facing products; example application areas include lead scoring and end-user behavioral modeling.\\n\\n· Make business recommendations with effective presentations of findings at multiple levels of the business through visual displays of quantitative information.\\n\\n· Develop processes and tools to monitor and analyze model performance and accuracy\\n\\n· Interact cross-functionally with a wide variety of leaders and teams and work closely with Engineers and Product Managers to identify opportunities for improvement.\\n\\n· Be fiercely competitive and maintain a sense of urgency, creativity, and curiosity for how to continue to improve internal and customer facing processes.\\n\\nQualifications:\\n\\n· BA/BS in a quantitative discipline (Computer Science, Statistics, Bioinformatics, Math, Physics, Engineering) or an equivalent practical skillset.\\n\\n· Industry experience writing code (e.g., Python, Pytorch, SQL) and taking ML models/ algorithms to production.\\n\\n· 3+ years of expertise using advanced machine learning algorithms and statistics: clustering, decision tree learning, ensemble methods, regression, etc. on large data sets as well as a strong understanding of their real-world advantages/drawbacks. The successful candidate will have regularly used Python and SQL to extract data, design ETL flows and derive insights.\\n\\n· A love for data - this is what we do. We are looking for people who are excited about different and unique data sets, and all the ways that they could be used to improve our business.\\n\\n· Demonstrated skill in selecting the right statistical tools given a data analysis problem.\\n\\n· Excellent written and verbal communication skills for coordinating across teams\\n\\n· Startup experience while not essential is preferred.\\n\\nWhat We Offer:\\n\\nWe believe in a team-first culture, full of rewards and recognition for our employees. We are dedicated to our employees’ success and growth within the company, through our employee mentorship and leadership programs.\\n\\nOur extensive benefits package includes:\\nGenerous Medical, Dental, and Vision Benefits\\n401(k) with Company Match\\nPaid Holidays, Volunteer Time Off, Sick Days, and Vacation\\n10 weeks Paid Parental Leave\\nPre-tax Transit Benefits\\nDiscounted Gym Membership\\nCiti Bike Annual Membership Discounts\\nNo-Cost Life Insurance Benefits\\nVoluntary Benefits Options\\nASPCA Pet Health Insurance Discount\\nAbout National Debt Relief:\\n\\nNational Debt Relief is one of the country’s largest and most reputable debt settlement companies. We are made up of energetic, smart, and compassionate individuals who are passionate about helping thousands of Americans with debt relief. Most importantly, we’re all about helping our customers through a tough financial time in their lives with education and individual customer service.\\n\\nWe are dedicated to helping individuals and families rid their lives of burdensome debt. We specialize in debt settlement and have negotiated settlements for thousands of creditor and collections accounts. We provide our clients with both our expertise and our proven results. This means helping consumers in their time of hardship to get out of debt with the least possible cost. It can also mean conducting financial consultations, educating the consumer, and recommending the appropriate solution. Our core services offer debt settlement as an alternative to bankruptcy, credit counseling, and debt consolidation. We become our clients' number one advocate to help them reestablish financial stability as quickly as possible.\\n\\nNational Debt Relief is a certified Great Place to Work®!\\n\\nNational Debt Relief is an equal opportunity employer and makes employment decisions without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability status, or any other status protected by law.\",\n",
       "  'Company Description:\\nSagence is a management advisory firm dedicated to helping our clients optimize the value of their data assets. From thinking to doing, Sagence works with leading institutions in the acquisition, evaluation, development and management of their critical data assets and in the application of analytics to discover new insights, shorten time-to-value, and drive competitive advantage.\\nJob Overview:\\nSagence is hiring experienced, hands-on Data Modelers to help us build and enhance our client’s data capabilities and create a competitive advantage. This position requires active participation in the modeling techniques for conceptual, logical and physical data models as well as an eagerness to remain hands-on from a project’s start to finish. Listed below is an overview of the position requirements as well as the characteristics of our successful professionals.\\nSkills & Requirements:\\nMust be hands-on. All candidates should be technologically proficient in performing development work, have the ability to install & configure tools and platforms, the desire to rapidly build up knowledge on new tools, and possess the following skills and knowledge:\\nAdvanced knowledge of modeling techniques for conceptual, logical and physical data models\\nAdvanced knowledge of modeling normalized data structures and manipulating data\\nFamiliarity with multiple database platforms such as Oracle, DB2 and SQL Server\\nUnderstanding of the different data storage/access models (transactional, data staging, operational data store, data warehouse, data mart, data federation) and the ability to choose the model(s) appropriate for the client.\\nAbility to work with data modeling and repository toolsets (including ERwin and other metadata management toolsets) to create work products\\nUnderstanding of the characteristics of various data types (structured versus unstructured)\\nKnowledge of data format standards, including industry standards (e.g. XMI for interchange of XML)\\nExperience with Agile methodologies\\nAbility to define business requirements\\nAbility to mentor and perform knowledge transfer to support team\\nPrior professional experience in an IT management, management consulting, or client facing role is preferred\\nKnowledge of the Financial Services, Insurance, Healthcare or Retail industries is preferred\\nDemonstrated ability in engaging, communicating, and presenting to stakeholders across both business and technology functions\\nTools and Technology:\\nProficient at leveraging tools and technology to drive value for clients. Illustrative tools include the following;\\nData Modeling tools - e.g., Toad, ERWin, ER/Studio, PowerDesigner, IBM Data Architect, or similar\\nDatabase Management Tools:\\nRelational – e.g. Oracle, MySQL, Microsoft SQL Server, PostgreSQL or similar\\nNoSQL – e.g. MongoDB, Apache Cassandra, Couchbase or similar\\nCloud-based – e.g. AWS (Redshift or DynamoDB), Azure (SQL Server or Cosmos DB) or similar\\nGeneral:\\nMust be currently authorized to work in the US on a full-time basis. Sagence does not provide sponsorship for work visas.\\n3+ years of professional experience working in a related role\\nMust be collaborative, innovative, curious, and resourceful, and exhibit a positive attitude\\nStrong desire to work on interesting projects with smart and creative people\\nWillingness to travel to client sites as needed\\nChicago or New York area candidates preferred, but will consider candidates in other parts of U.S.\\nOur Culture:\\nPassionate, diverse, creative, genuine, flexible, hands-on…these are just a few of the words that describe our culture. Our Partners are deeply involved in the client work on a daily basis. We have a high-energy workplace with a focus on producing high-quality, impactful results. We are committed to equality of opportunity, fairness, work and lifestyle balance, and mutual respect. We promote an entrepreneurial spirit by encouraging individual initiative and foster a collaborative culture and work environment which includes open communication and on-going learning. We build teamwork through small, dedicated teams who continuously teach each other and learn from one another. We strongly believe these characteristics enable our employees to develop to their fullest potential. To learn more, please visit us at www.sagenceconsulting.com',\n",
       "  'Requirements:\\n\\nExperience in relevant areas of computer science, including data engineering, applied machine learning, deep learning, NLP or related disciplines.\\nStrong foundation in coding skills relevant to data science/engineering, e.g., BigQuery, Python, SQL, Python, Tensorflow, CloudML, Spark, Kubernetes etc.\\nFamiliarity with modern data pipelines and ETL practices e.g. Airflow.\\nExtensive experience solving analytical problems using quantitative approaches.\\nExperience developing production-quality data products using the results of quantitative research.\\nExperience with open source machine learning platforms e.g. TensorFlow, sci kit-learn.\\nExperience with cloud e.g. Google Cloud Platform.\\n--\\nRiVi Group',\n",
       "  'About Cubist\\nCubist Systematic Strategies is one of the world’s premier investment firms. The firm deploys systematic, computer-driven trading strategies across multiple liquid asset classes, including equities, futures and foreign exchange. The core of our effort is rigorous research into a wide range of market anomalies, fueled by our unparalleled access to a wide range of publicly available data sources.\\nJob Description\\nWe are passionate about data. We collaborate to build elegant, effective, scalable and highly reliable solutions to empower predictive modelling in finance.\\nCubist’s Data Services group is looking for a Data Support Analyst to join our dedicated team. Our group is responsible for the timely delivery of comprehensive and error-free data to some of the most demanding and successful systematic Portfolio Managers in the world.\\nThis exceptional individual will be a member of a small team of Data Scientists and Analysts who play a vital role in ensuring the smooth day-to-day implementation of a large research infrastructure, and the live production trading of billions of dollars of capital across global capital markets, including equities, futures, options and other financial instruments.\\nJob Responsibilities\\nProvide day-to-day support for various operational procedures\\nProject lifecycle management to help the team support PM teams seamlessly\\nManage calendar and conference call schedule, make appointments, organize meetings and occasionally take and maintain meeting minutes\\nAssist Data team with interoffice travel arrangements, expenses etc.\\nAssist in organizing presentations\\nInteract with vendors on a daily basis\\nCoordinate with compliance team and legal team on new vendor trial/subscription process\\nAssist with collecting and maintaining overviews and vendor content offering\\nDownload trial datasets from vendor FTP sites or other delivery mechanism\\nDesirable Candidates\\nBachelor’s degree required.\\nStrong PowerPoint, Visio, Word, and Excel skills. Access is a plus.\\nFinancial industry experience preferred but not required.\\nStrong oral and written communication skills.\\nExcellent interpersonal skills; we seek a demonstrated ability to build relationships both internally and externally.\\nAttention to detail and a love of process.\\nAbility to exercise sound judgment in assessing and determining how to handle queries, calls and issues.\\nAbility to independently manage multiple long-term and short-term projects simultaneously.',\n",
       "  'Do you love numbers and finding the story in the numbers? Does the thought of tackling a complex data issue make you smile? Have you got a knack for solving problems? Do you want to help drive the results of a multi-million dollar business? If you have answered “yes” to these questions, the Lead Data Scientist position at Strategic Financial Solutions may be the right fit for you.\\nStrategic is looking for an experienced Senior Data Scientist with extensive machine learning experience to join our Data Science Team, which already produces cutting-edge models for predictive and prescriptive analytics. The person in this role would be responsible for developing predictive models leveraging data science and machine learning to solve various business use cases, including marketing intelligence, customer segmentation, and predictive models for operations. We are looking for a Data Scientist who will support our product, sales, leadership and marketing teams with insights gained from analyzing company and external data. The ideal candidate is adept at using large data sets to find opportunities for product and process optimization and using models to test the effectiveness of different courses of action. They must have strong experience using a variety of data mining/data analysis methods, using a variety of data tools, building and implementing models, using/creating algorithms and creating/running simulations. They must have a proven ability to drive business results with their data-based insights. They must be comfortable working with a wide range of stakeholders and functional teams. The right candidate will have a passion for discovering solutions hidden in large data sets and working with stakeholders to improve business outcomes.\\nResponsibilities\\nWork with stakeholders to identify opportunities for leveraging company data to drive business analytics solutions.\\nResearch and develop statistical learning models for data analysis\\nCommunicate results and ideas to key decision makers\\nIdentify valuable data sources and automate collection processes\\nUndertake preprocessing of structured and unstructured data\\nAnalyze large amounts of information to discover trends and patterns\\nBuild predictive models and machine-learning algorithms\\nPresent information using data visualization techniques\\nCollaborate with sales, marketing and senior executive teams\\nQualifications\\n~3-5 years of relevant experience\\nGraduate degree in Statistics, Data Science, Applied Math, Operations Research, or Computer Science.\\nStrong problem-solving skills with an emphasis on sales and marketing predictive analytics.\\nExperience using statistical computer languages (R, Python, SQL, etc.) to manipulate data and draw insights from large data sets.\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and their real-world advantages/drawbacks.\\nKnowledge and experience in statistical and data mining techniques: GLM/Regression, Random Forest, Boosting, Trees, text mining, social network analysis, etc.\\nKnowledge of advanced statistical techniques and concepts (regression, properties of distributions, statistical tests and proper usage, etc.) and experience with applications.\\nExcellent written and verbal communication skills for coordinating across teams.\\nA drive to learn and master new technologies and techniques.\\nExperience using business intelligence tools (e.g. PowerBI, Tableau) and data frameworks (e.g. Hadoop)\\nAbout Strategic:\\nStrategic Financial Solutions is a leading consumer finance company that specializes in helping people that have too much credit card debt. We were recently named the 21st Best Company to Work for in New York by Best Companies to Work For and have been certified as a Great Place to Work 4 times. Additional honors include being named, two times, as one of the 50 fastest growing companies in New York City and to the prestigious Inc. 500 list as one of the 500 fastest growing companies in the United States.',\n",
       "  'The Senior Data Scientist is an upper-level position in 605’s data science group, and is focused on the statistics-heavy, technical/backend aspects of data analytics. At 605, the Data Science and Client Analytics teams share a baseline knowledge of statistics and machine learning, R and python, and familiarity with relational databases and the scale/variety of 605’s data assets. In contrast to the Client Analytics team, Data Scientists bring to the table some deeper technical skills, including:\\nHands-on experience with industry-standard predictive modeling solutions such as scikit-learn, xgboost, Spark ML, and H2O.ai in a production setting\\nFamiliarity with diverse methods for supervised learning, unsupervised learning, ETL pipelines in general\\nA more nuanced understanding of cloud-based (Amazon Web Services) computing resources/infrastructure, especially the need for and methods of parallelization for analytics tasks\\nExperience in designing and implementing wrapper/tool/utility functions for automated tasks, to be used by less-technical Analytics team members, and organizing them into distributable, regularly maintained and tested R/python packages\\nProficiency with source code management (git) and related code development/review workflows, as well as continuous integration tools like Travis and/or Jenkins\\nData Scientists at 605 are generally involved in at least two different projects at any given time and work alongside other data scientists or analysts. Projects produce both client-facing deliverables as well as internal tools/datasets consumed by other various teams at 605. This role could potentially be based out of one of 605’s offices (in New York City or Pasadena, CA), or it could be full-time remote, dependending on the circumstances\\n\\nRequirements\\nMasters degree in a quantitative, scientific, or engineering field and at least 2 years experience in a data science industry position\\nAdvanced-level proficiency in either R or Python (baseline proficiency in both)\\nIntermediate-level proficiency with Apache Spark (via Python, R, Scala, and/or Java), with application to machine learning and/or ETL pipelines\\nKnowledge of diverse modeling algorithms for supervised learning, including most of the following: scikit-learn, xgboost, Spark ML, H2O.ai\\nExperience with most of the following AWS services: Redshift, S3, EC2, EMR, and Glue\\nAdvanced-level proficiency with Linux/Unix operating system, command-line/shell environments, accessing remote machines and using Docker containers\\nExperience with git and github.com\\nPreferred Skills\\nDoctoral degree in a quantitative, scientific, or engineering field and/or at least 4 years experience in a data science industry position\\nPast work with household-level or person-level data sets including demographic, CRM, and/or self-reported (survey) data\\nPast work with time-stamped video consumption/viewing data or device usage data\\nAdvanced-level proficiency in both R and Python (including class/function structure, package design and management)\\nAdvanced-level proficiency with Apache Spark (via Python, R, Scala, and/or Java), including optimization for local and cluster scale applications\\nExperience with all of the following AWS services: Redshift, S3, EC2, EMR, and Glue\\nKnowledge of information security best practices\\nBenefits\\nComprehensive health and dental insurance for employees and their families\\nLife insurance\\n401k with match, eligible for match after one year\\nPre-tax flexible compensation plan for medical, transit, parking or dependent care expenses\\nPTO & Sick days—if you’re sick, you stay home\\nWork-from-home Fridays\\nA kitchen stocked with sodas, snacks, yogurt and other goodies\\nA tight-knit startup community who likes to eat! We celebrate everyone’s birthdays, have frequent team lunches, and do events in and out of the office\\n605 is an active participant in conferences',\n",
       "  'JOB RESPONSIBILITIES\\n\\nThink strategically about data as a core enterprise asset and assist in all phases of the advanced analytic development process\\nSupport advanced analytical and data mining efforts which could include but not limited to clustering, segmentation, logistic and multivariate regression, decision/CART trees, neural networks, time-series analysis, sentiment analysis, topic modeling, random forests, and Bayesian analysis\\nThe scope of work includes Forecast, Prediction Models, Outlier Reporting, Payment Integrity violation identification, Adhoc analysis\\nImplementation of Supervised and Unsupervised model development techniques\\nWork with Data engineers to supervise and help institutionalize models and dashboards for Analytics team of leading Healthcare client\\nDevelops ML models using identified features and packages\\nResponsible for maintenance and performance monitoring of the production environment for the Advanced Analytics team\\nLead the design of complex and large-scale datasets to be used for statistical modeling and data mining.\\nSlice and dice through the database and come up with actionable analytical insights\\nSolid communication skills with exposure to direct client communication is preferred\\nREQUIRED COMPETENCIES\\n\\nFacility with one or more quantitative data analysis languages such as R, SciPy, NumPy, SQL, Python, SAS, SPSS\\nExperience with relational database management systems (Oracle, Teradata, SQL Server, DB2..)\\nWorks with key stakeholders to generate and test hypotheses\\nExperience with contemporary big data technologies (Hadoop, HIVE, PIG, MapReduce)\\nFacility with one or more data analytical methods such as regression, decision trees, experimental designs, support vector machines, machine learning and text mining\\nProficiency with Microsoft Office Suite\\nWork in a dynamic and fast-paced environment without compromising the quality\\nConduct explanatory data analysis and prepare data sources to be analyzed.\\nStrong domain knowledge on US Healthcare is preferred\\nEDUCATION:\\n\\nBachelors or Master s degree with specialization in statistics, applied mathematics, economics, finance, computer science or Information systems/science; Preference given to candidates with demonstrated academic achievement in core subjects and proficiency in quantitative subject matter (Advanced Statistics coursework, Predictive Modeling projects). Familiarity with PBM or Healthcare industry.\\nprovided by Dice',\n",
       "  'At OnDeck, we make small business a big deal. We\\'re improving the world\\'s economic landscape by changing the way small businesses access capital. We care intensely about each other, our company and the customers we serve, and are committed to making every day count.\\n\\nThe vision of our Sales, Marketing, and Partnerships team is to drive long-term revenue with small businesses by creating meaningful and value-added engagement at every touch-point, developing the foundation for an ongoing business relationship.\\n\\nThe Revenue Operations (Rev Ops) team works cross functionally and collaboratively across the organization to drive growth. We provide insights, support, analytics, forecasting, and change management to help align the goals of the company.\\n\\nThe Business Intelligence Analyst will oversee managing third party incentives and assist in developing new strategies while maintaining compliance and overseeing controls.\\n\\nAs a Business Intelligence Analyst at OnDeck on the RevOps Team, you will:\\nAssist in financial forecasting and planning\\nCreate monthly and quarterly business reports\\nUse Tableau to create self-serve reports in Tableau Server\\nWork independently to solve business questions using data to drive decision making\\nWork cross functionally to drive collaboration and results for the broader business\\nUpdate existing reports as needed\\nHelp create new views to better access data across the organization\\nNecessary qualifications for success:\\nMajor in Business, Economics, Finance, Accounting or Computer Science\\nexperience working in a BI function previously\\nExperience running important company processes and working within deadlines\\nProficient with SQL and able to run and edit queries\\nMust have experience with Excel and handling large datasets\\nStrong problem solving and project management skills\\nComfortable working cross-functionally and with confidential information\\nNice-to-haves (not required):\\nProficiency with Salesforce.com\\nExperience with Sarbanes-Oxley processes and controls\\nAbout OnDeck:\\n\\nAs the largest online small business lender in the U.S. and a leading online lender in Canada serving more than 700 different industries, we have been trusted by approximately 100,000 small businesses by providing them with financing to help them build growing and thriving enterprises. Since 2007, we\\'ve issued over $10 billion in capital.\\n\\nJoin us as we enable small businesses to achieve their goals. At OnDeck, we\\'re reinventing small business financing. We care intensely about each other, our company and the customers we serve, and are committed to making every day count. We are small enough to be nimble and strong enough to make a big impact.\\n\\nOnDeck believes that each and every team member plays an important role in our company\\'s success. That\\'s why we strive to provide you and your family with a competitive and comprehensive benefit program with a variety of options and opportunities. We offer:\\nGenerous Vacation\\nComprehensive Healthcare\\nEducational Reimbursement\\n401k Matching\\nParental Leave\\nSports Teams\\nStocked Kitchens\\nLoan Consolidation\\nWe are going to ask you to talk about your accomplishments. Here are some of ours:\\nBuilt in Colorado, Top 100 Digital Companies in Colorado, 2015, 2016, 2017\\nBuilt in NYC\\'s 100 Best Places to Work, 2019\\nColorado SHRM Best Companies to Work For in Colorado, 2015\\nCrain\\'s New York Best Places to Work, 2013, 2014, 2015\\nCrain\\'s New York Business Fast 50, 2013, 2014, 2016, 2017\\nDenver Business Journal Largest Technology Employers in Denver, 2019\\nDenver Business Journal Best Places to Work, 2019\\nFinTech Breakthrough Award – Best Overall LendTech Company, 2018\\nFortune 50 Best Workplaces for Diversity, 2016\\nFortune 50 Best Small and Medium Companies to Work For, 2016\\nFortune 30 Best Workplaces in Finance and Insurance, 2016\\nFortune.com and Great Place to Work 100 Best Workplaces for Millennials, 2015\\nFortune/Great Place To Work Great Rated! People\\'s Picks: 20 Great Workplaces in Financial Services, 2015\\nForbes\\' America\\'s Most Promising Companies, 2013, 2014\\nGreat Place to Work Certification, 2017, 2018, 2019\\nInc. 500|5000, 2013, 2014\\nInc. Hire Power, 2013\\nLending Tree\\'s Top Rated Customer Satisfaction, Q1 2018\\nSelling Power Magazine Best Company to Sell For, 2013, 2014, 2015, 2016, 2017, 2018, 2019\\nUS News & World Report, \"Best Unsecured Business Loans of 2018\" – Best for Term Loans\\nWashington Post Top Places to Work, 2019\\nWorldatWork, 2017 Seal of Distinction\\nAs part of our dedication to maintaining an inclusive and diverse workforce, OnDeck provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability or genetics. In addition to federal law requirements, OnDeck complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.\\n\\nOnDeck expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status. Improper interference with the ability of OnDeck\\'s employees to perform their job duties may result in discipline up to and including discharge.\\n\\n**No external recruiters or agents, please.**',\n",
       "  \"Returning Candidate? Log back in to the Career Portal and click on 'Job Browsing/History' and find the job you're looking for.\\n\\n2018-021-OIC: Machine Learning Research Scientist\\n\\nDirectorate Open Innovation Center\\nLocation New York, NY\\nRiverside Research is seeking a motivated Machine Learning Research Scientist to join our Open Innovation Center’s Machine Learning group.\\nThe Research Scientist will work on ground-breaking research to solve pressing problems in the realm of Machine Learning and Artificial Intelligence. The position will support both internally and externally funded research efforts. Ongoing research in the group includes machine learning for second and third generation applications and research.\\n\\nPosition is available at our New York City Research Office.\\nOther locations available include\\nBoston, MA\\nDayton, OH\\nNational Capital Region (Crystal City or Centreville, VA).\\n\\nAll Riverside Research opportunities require U.S. Citizenship.\\n\\nJob Responsibilities:\\n• Assess customer systems and processes for application of machine learning / artificial intelligence technologies to support their needs\\n• Create and prototype machine learning approaches to demonstrate efficacy\\n• Support algorithm implementation in Von Neumann and neuromorphic computing environments\\n• Coordinate with external agencies and research partners on emerging methods and technologies for the application of ML/AI to their needs\\n• Communicate advanced ML/AI concepts to senior leadership to facilitate decision making\\n• Perform basic and applied research in ML; document and present research results and the status of ongoing or emerging projects\\n• Author proposals and conference/journal papers\\n• Occasional travel may be required\\n• Other duties as assigned\\n\\nQualifications:\\n• U.S. citizenship required\\n• Eligibility to obtain and maintain a DOD Top Secret with SCI clearance\\n• Bachelor's (Master's or Doctorate preferred) in a physical science. 8 years related experience in an engineering or scientific function, including research during academic studies\\n• Experience in machine learning/statistical analysis, neural networks, and data mining\\n• Experience developing and implementing machine and deep learning algorithms\\n• Candidates with Masters degree and relevant AIML deep learning experience or academic studies will also be considered.\\n• Superior communications skills, both written and verbal\\n\\nDesired Qualifications:\\n• Current Top Secret with SCI clearance\\n• MS in Electrical Engineering, Physics, Computer Science, or equivalent; PhD strongly preferred.\\n• Fluency in C/C++, Unix, Matlab, Python, FPGA programming, Linux\\n\\nRiverside Research strives to be one of America’s premier providers of independent, trusted technical and scientific expertise. As we continue to add experienced, technically astute staff, we are looking for highly motivated, talented team members that can help our DoD and Intelligence Community (IC) customers continue delivery of world class programs. As a not-for-profit, technology-oriented Defense Company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.\\n\\nAll positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.\\n\\nThis contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability, and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.\\n\\nThis contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans, and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.\\n\\nApply Now\",\n",
       "  'TECHNOLOGY\\nSenior Data Scientist\\nNew York, NY and Charlotte, NC, USA\\n\\nApply\\n\\nJob Description\\n\\nThe Data Science team is looking for a seasoned data scientist with a strong analytics, programming, and machine learning background. The ideal candidate will work closely with top tier financial services organizations to formulate solutions to complex business problems using multiple analytical tools and frameworks, including statistical analysis, natural language processing, machine learning, and deep learning. The ideal candidate will be able to identify client needs, propose effective solutions, and implement the solutions as part of a team.\\n\\nSynechron’s collaborative culture and global network of Tier One banking clients provides an incredible environment for a data scientist to work on the most cutting-edge use cases in machine learning and artificial intelligence in finance. You will have the opportunity to work with some of the best data scientists anywhere in an environment which truly values innovation, creative thinking, and a proactive approach to problem solving. Exciting projects include building machine learning platforms for investment research, private wealth management, IPO underwriting, customer analytics, social AI, and retail and commercial banking. As a senior member of the Data Science team you will have the opportunity to lead teams and projects and to mentor junior data scientists in their development.\\n\\nResponsibilities\\nDesign and develop scalable solutions to complex problems using statistical modeling, machine learning, Natural Language Processing, Optical Character Recognition, and deep learning;\\nImplement solutions in Python in multiple environments including Spark, AWS, and Azure;\\nManage junior data scientists in the implementation of machine learning solutions;\\nMeet with clients to understand business use cases, identify needs, and formulate end-to-end solutions encompassing data ingestion, cleansing, modeling, and predictive and prescriptive analytics;\\nManage the ingestion and cleansing of large data sets using big data tools such as Spark;\\nTranslate solutions into business requirements;\\nIndependently formulate new ideas for solving client problems or providing clients with new tools for growing revenue, reducing cost, improving customer acquisition and retention, and automating business processes.\\n\\nRequirements\\n\\nBasic Qualifications\\nHold a MS or PhD degree in a quantitative discipline: computer science, applied mathematics, statistics, operations research, management of information systems, engineering, economics, social sciences or equivalent;\\nProgramming skills in Python;\\nPresentation skills including experience presenting complex ideas to clients and prospects;\\nThe ability to independently formulate solutions to complex problems or to otherwise think of potential solutions without guidance;\\n8+ years of total experience including 4+ years of experience developing machine learning systems in Python, Java, or a related programming language;\\nHands on experience with data mining, machine learning, econometric analysis or equivalent;\\nExperience in Hadoop or other MapReduce paradigms and associated languages such as Spark;\\nExperience with Unix/Linux environment for automating processes with shell scripting.\\nPreferred Qualifications\\nStrong Python programming skills;\\nKnowledge of deep learning and related open source libraries such as TensorFlow;\\nFinancial Services experience and knowledge of the FS industry;\\nDeep knowledge of Natural Language Processing;\\nExperience with Spark and machine learning libraries such as MLlib;\\nExperience in marketing and sales, being an innovator and disruptor is also a plus;\\nBeing self-motivated, creative and collaborative;\\nStrong presentation and communications skills.',\n",
       "  'As a Senior Application Developer for the Market Intelligence Data Science Team, you will take part in building out new functionalities and enhance the same using the big data, spark and microservices stack on Machine Learning Algorithms and building out ML Services Stack using Docker and Kubernetes. In addition, you will be supporting the existing applications from your team. You will also work very closely with cross-functional teams in the Market Intelligence space to build platform agnostic Machine Learning Services. Our team currently uses these skills/tools: Java, SQL, NoSQL, Distributed Computing Platform (HDFS, Apache Spark), REST services, Python, Jupyter, AWS SageMaker, Docker, Kubernetes, Git, Shell Scripting, and Python.\\n\\nBasic Qualifications:\\n\\nBachelor Degree in Computer Science or equivalent. MS in Computer Science is preferred. Candidates without Computer Science or Information Technology degree should have significant work experience\\nAt least 5+ years of hands on development in building enterprise applications to cloud [AWS / Azure / GCP];\\nMust have Hands-on experience with Machine Learning and Deep Learning Algorithms [ like RNN, CNN, LSTM, Logistic Regression, XGBoost etc] , Core Java 8(Collections, Concurrency, Multi-Threading, Locks), Java 8 Streams, Spring Boot, Docker, Kubernetes, SQL and NoSQL databases, Git, Shell Scripting\\n\\nPreferred Qualifications:\\n\\nFamiliarity in building microservices using Zuul, Ribbon, Hystrix is desirable.\\nExperience in working with REST APIs and Cassandra is highly desirable\\n2-3 years of experience with Docker and Kubernetes is a must\\n2-3 years of building and implementing ML Algorithms [Logistic Regression, Neural Networks, Random Forest and Gradient Boosting Algorithms]\\nAny prior experience in financial industry is a big plus\\nExposure to TDD and Agile Methodologies and working in a Scrum Team\\nKnowledge and experience in Distributed Computing Platforms like HDFS & Apache Spark are a must\\n\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.',\n",
       "  \"Returning Candidate? Log back in to the Career Portal and click on 'Job Browsing/History' and find the job you're looking for.\\n\\n2019-004-BIO: Ultrasound Research Scientist\\n\\nDirectorate Biomedical Engineering\\nLocation New York, NY\\nInternationally recognized research group located in New York City seeking an Ultrasound Research Engineer to support and collaborate on ongoing innovative projects and ultimately develop new research areas in biomedical ultrasound, acoustics, and related fields. Current projects include high-speed-ultrasound imaging of blood flow and detection of acoustic nanodrops in microcirculation. The Research Engineer will be responsible for advancing projects in various states of maturity including acquiring data, analyzing data and presenting results via peer-reviewed papers and scientific conferences.\\n\\nAll Riverside Research opportunities require U.S. Citizenship.\\n\\nJob Responsibilities:\\n• Assume responsibility for currently funded and/or internal research efforts\\nCollect ultrasound data with new and existing experimental systems\\nPlan collection and analysis of experimental data\\nWork independently as needed and with collaborative teams\\nProcess and analyze data\\nModel acoustic fields from transducers\\nInteract with collaborators at outside institutions\\nPresent results at scientific conferences and publish results in peer-reviewed journals\\nDevelop new research projects and apply for external funding\\nHelp maintain laboratories\\nImplement enhancements to experimental systems and data processing tools\\nQualifications:\\n• Masters degree with 3-5 years experience in ultrasound related area (Biomedical Eng., Mechanical Eng, Electrical Eng, etc)\\nCritical thinking, problem solving skills, and ability to identify inconsistencies in results\\nAbility to acquire ultrasound data using experimental and clinical ultrasound systems\\nProficiency with standard laboratory equipment (oscilloscopes, etc.)\\nTechnical writing skills as evidenced by peer-reviewed publications\\nAbility to develop new instrumentation including data acquisition, parts fabrication and system control via high-level user interfaces\\nExpertise programming with MATLAB, LabVIEW or other scientific programming languages\\nExperience presenting research results at scientific conferences\\nEvidence of collaborative research skills (communication, team building, conflict resolution etc.)\\nAbility to develop new research collaborations\\nDesired Qualifications:\\nPhD with 0-2 years experience in ultrasound related area (Biomedical Eng., Mechanical Eng, Electrical Eng, etc)\\nCAD/CAM related parts fabrication\\nTransducer design and fabrication\\nStrong image and signal processing skills\\nExperience with small-animal experiments using ultrasound\\nKnowledge and experience with photoacoustic instrumentation\\nExperience modelling ultrasound transducers with FEM software\\nKnowledge and experience with high-speed plane-wave ultrasound imaging\\nKnowledge of the physics and experimental use of acoustic contrast agents\\nRiverside Research strives to be one of America’s premier providers of independent, trusted technical and scientific expertise. As we continue to add experienced, technically astute staff, we are looking for highly motivated, talented team members that can help our DoD and Intelligence Community (IC) customers continue delivery of world class programs. As a not-for-profit, technology-oriented Defense Company, we believe service to customers and support of our staff is our mission. Our goal is to serve as a destination company by providing an industry-leading, positive, and rewarding employee experience for all who join us. We aspire to be a valued partner to our customers and to earn their trust through our unwavering commitment to achieve timely, innovative, cost-effective and mission-focused solutions.\\n\\nAll positions at Riverside Research are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.\\n\\nThis contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability, and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.\\n\\nThis contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans, and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans.\\n\\nApply Now\",\n",
       "  \"Company Description\\n\\nISO, a Verisk business, has been a leading source of information about property/casualty insurance risk since 1971. For a broad spectrum of commercial and personal lines of insurance, ISO provides statistical, actuarial, underwriting, and claims information and analytics; compliance and fraud identification tools; policy language; information about specific locations; and technical services. ISO serves insurers, reinsurers, agents and brokers, insurance regulators, risk managers, and other participants in the property/casualty insurance marketplace. To learn more about ISO please visit us at: www.verisk.com/iso. We are proud to be a part of the Verisk family of companies!\\n\\nWith a history of impressive growth, an innovative culture, and offering industry-leading solutions, Verisk Analytics is an amazing place to work and make a difference. In 2018, Forbes magazine named Verisk to its World’s Best Employers list and, in 2017, to its World’s Most Innovative Companies list for the third consecutive year. We also earned the Great Place to Work® Certification for the third consecutive year in recognition of our outstanding workplace culture.\\n\\nVerisk is a leading data analytics provider serving customers in insurance, energy and specialized markets, and financial services. Using advanced technologies to collect and analyze billions of records, Verisk draws on unique data assets and deep domain expertise to provide first-to-market innovations integrated into customer workflows. We’ve been delivering predictive analytics and decision support solutions to our customers for nearly 50 years, helping them protect people, property, and financial assets. At Verisk, you’ll be part of an organization that’s committed to serving the long-term interests of our stakeholders, including the communities where we operate.\\n\\nAt Verisk, you can build an exciting career with meaningful work; create a positive and lasting impact on the business; and find the support, coaching, and training you need to advance your career. Our culture of innovation means your ideas on how to improve our business will be heard. As key contributors to our success, our team members enjoy working in a business-casual, collaborative environment that offers state-of-the-art resources, advanced technologies, and an excellent benefits package.\\n\\nJob Description\\nLeads a team of diverse team of data scientists\\nCoaches team members, and manages team development; knowledge transfer to team members.\\nPartner with project managers, directors, and other internal project leaders.\\nActively supports innovation, and the integration of new methods and technologies.\\nSupports and/or tests new methodologies, software and data sources for continual improvement of quantitative solutions.\\nUtilizes statistical, mathematical, and machine learning theory and methodologies to design analytic architecture and solutions.\\nSupports and/or implements techniques to create high-performing models that comply with regulatory and privacy requirements and address business objectives and client needs.\\nExecute and monitor project plans for timely project completion; supporting all stages of the analytic pipeline.\\nSupports and/or participates in the creation of lucid documentation and reports (technical and non-technical) for internal and external clients.\\nPresents analysis ideas, progress reports and results to internal managers, project owners, and executives.\\nQualifications\\nGraduate-level degree with concentration in a quantitative discipline such as statistics, mathematics, economics, operations research, computer science or aligned discipline. MS or higher preferred.\\n7+ years of insurance experience, focusing on the personal lines.\\nTeam-oriented with a track record of building strong internal and external unit partnerships.\\nLead cross-functional projects using advanced data analysis techniques.\\nStrong logical, evidence-based problem solving, and critical thinking skills that support innovative, creative solutions.\\nProficient across the stages of the data analytic pipeline, from ETL to results communication, supporting knowledge transfer for team growth.\\nBreadth and depth in the areas of feature engineering and selection methodologies, and machine learning methodologies.\\nExperience using statistical computer languages (e.g. R, Python, SLQ), and with the management of large, complex datasets.\\nExcellent verbal and written communication, and presentation skills (technical and non-technical)\\n#LI-YD1\\n\\nAdditional Information\\n\\nVerisk Analytics is an equal opportunity employer.\\n\\nAll members of the Verisk Analytics family of companies are equal opportunity employers. We consider all qualified applicants for employment without regard to race, religion, color, national origin, citizenship, sex, gender identity and/or expression, sexual orientation, veteran's status, age or disability.\\n\\nhttp://www.verisk.com/careers.html\\n\\nUnsolicited resumes sent to Verisk, including unsolicited resumes sent to a Verisk business mailing address, fax machine or email address, or directly to Verisk employees, will be considered Verisk property. Verisk will NOT pay a fee for any placement resulting from the receipt of an unsolicited resume.\\n\\n</br>Apply now\",\n",
       "  'Job Description\\nAs Data Scientist, you will be responsible for developing machine learning models to support product features in the roadmap. You’ll analyze and prepare data for testing different models to find the best fit that will support various product features. You’ll work closely with the product and engineering team stakeholders in supporting the product roadmap.\\n\\nAs Data Scientist in a fast-paced growing startup, you should be a self-starter in taking a complex problem statement, breaking it down to solutions and develop or use different models and algorithms to find the optimal solution. Along with technical skills, you also possess the ability to propose and apply different techniques and algorithms and present the findings to all the stakeholders. Sr. Data Scientist is a key component of the product strategy at HR Acuity® and will ensure the delivery of our product roadmap.\\n\\nResponsibilities:\\nResearch and develop machine learning models and algorithms to solve various business problems using structured and unstructured data\\nImplement scalable and efficient modeling algorithms and modeling pipelines that can work with real world data in production systems\\nUse raw data and prepare for various modeling scenarios, execute the models, and present the outcome\\nWork with product management and engineering groups to develop new products and features\\nData Scientist Qualifications:\\nBS and master’s degrees in computer science, data science, engineering, or related technical/scientific field\\n3+ years of professional experience in a business environment\\nExperience in using Python, R, or other programming environments\\nPhD degree in computer science, data science, engineering, or related technical/scientific field (\\nDeep understanding of statistical modeling, machine learning, or data mining concepts, and a track record of solving real world problems with these numerical modeling methods.\\nThorough understanding of NLP and linguistics modeling, machine learning, and a track record of solving real world problems with structured and unstructured data modeling methods.\\n5+ years of professional experience in using and developing various statistical, machine learning\\nKnowledge and experience of working with relational databases and SQL\\nStrong communication and data presentation skills\\nStrong attention to detail\\nComfortable working in a fast paced, highly collaborative, dynamic work environment\\nAbility to think creatively and solve problems\\n\\nCompany Description\\nHR Acuity® is the leader in employee relations management solutions. Our Software as a Service (SaaS) technology solution helps achieve consistency in how organizations track, investigate and analyze employee issues.',\n",
       "  'DISH is a Fortune 250 company with more than $13 billion in annual revenue that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service – Sling TV – that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television.\\nNow we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers.\\nWe are driven by curiosity, pride, adventure, and a desire to win – it’s in our DNA. We’re looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story.\\nOpportunity is here. We are DISH.\\n\\nDISH Media Sales, the advertising sales division of DISH Network, provides smart and effective media solutions that complement those of traditional national cable, and is headquartered in New York, with offices in Chicago, Denver and Los Angeles. We are an innovative, passionate and fun group working hard and finding reward in radically changing TV advertising.\\nThe Data & Analytics team within Media Sales works closely with the corporate DISH Viewer Measurement team to combine powerful household level reporting data, analytics and expertise that helps our sales staff succeed in a rapidly evolving environment. Constructing automated processes through scripting languages, using data to identify and create models of an advertiser’s target audience, measuring the success of an advertiser’s campaign and providing visualization through self-service tools are just a few of the day to day tasks this team is responsible for.\\nWe are now seeking a Data Scientist that will support the on-air addressable Media Sales team and Data Sales efforts through:\\nDeveloping critical analytics and propensity models using key DISH data sources including the viewer measurement data and DISH CRM data.\\nWorking with the Account Executives and clients to develop client proposals based on conducted research.\\nDemonstrating thought leadership by writing papers, attending conferences and promoting yourself as subject matter expert on topics such subscriber and network viewership trends.\\nDeveloping key relationship with Analytics heads at advertising agencies, data partners, internal divisions and other key industry players.\\nArchitect and develop new advertising products using state of the art machine learning algorithms either in a big data environment such as Hadoop or in a cloud-based environment such as AWS.\\nWrite code in R, Python and SQL producing visualizations in Excel or Tableau.\\n\\nA successful Data Scientist will have the following:\\nBachelor’s Degree from a four-year college or university; advanced degree in a STEM field preferred.\\n3-5 years of working experience utilizing data modeling techniques.\\nFamiliarity with big data projects, tools, and technique, segmentation and ad targeting.\\nExperience with SQL and scripting tools; willingness to be hands on where needed.\\nExcellent communication skills and have the proven ability to manage multiple projects and meeting deadlines.\\nExperience working with IT and / or DBAs in respect to projects involving large databases.\\nStrong work ethic; personable and must be able to prioritize tasks and deliverables in a fast-paced Sales environment.',\n",
       "  \"What you’ll be doing...\\n\\nThis role requires experience building, analyzing and maintaining supervised learning models, as well as familiarity with malware detection and binary analysis.\\n\\nYou are expected to analyze model results and to develop and to develop and implement automated tools for data mining.\\n\\nIn this role, you’ll be responsible for:\\nExperience in applying a wide variety of unsupervised, semi-supervised, and supervised machine learning techniques, and the ability to turn big data into actionable intelligence.\\nAbility to analyze, retain and improve machine learning models.\\nAbility to provide and receive scientific critiques and work towards data driven solutions.\\nFamiliarity with malware, host forensics, or network traffic analysis.\\nExperience with reverse engineering malware.\\nExperience using relational and non-relational databases.\\nExtracting and transforming data using python.\\nApplying big data analytics tools to large, diverse sets of collection data assess risk of adverse threat activities.\\nDeveloping scripts to support cyber threat detection that outputs results in a variety of formats.\\nWhat we’re looking for...\\n\\nYou’ll need to have:\\nBachelor’s degree or four or more years of work experience\\nFour or more years of relevant work experience\\nSix or more years of experience as a Data Scientist within Threat Intelligence\\nExpertise in Automation tools such as Terraform, CloudFormation, Puppet, etc\\nExperience with Hadoop and Spark\\nProficiency in Python\\nWillingness to travel\\nEven better if you have:\\nA Master of Computer Science or related fields\\nAbility to work in a team environment\\nUnderstanding of adversarial TTPs\\n#ProfessionalServices; 22CyberVES; 22CyberOPS\\n\\nWhen you join Verizon...\\n\\nYou’ll have the power to go beyond – doing the work that’s transforming how people, businesses and things connect with each other. Not only do we provide the fastest and most reliable network for our customers, but we were first to 5G - a quantum leap in connectivity. Our connected solutions are making communities stronger and enabling energy efficiency. Here, you’ll have the ability to make an impact and create positive change. Whether you think in code, words, pictures or numbers, join our team of the best and brightest. We offer great pay, amazing benefits and opportunity to learn and grow in every role. Together we’ll go far.\\n\\nEqual Employment Opportunity\\n\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.\\n\\n]]>\",\n",
       "  'The Marketing Analytics Manager is responsible for developing the strategic marketing analytic capabilities within the organization, influencing decision making, identifying KPIs & objectives, storytelling with data and insights, and creating measurement tools and frameworks. This individual plays a pivotal role in synthesizing robust data into actionable insights and narratives.\\n\\nThe responsibilities of this position include:\\nProvides marketing intelligence, reporting, and analysis/synthesis in the areas of paid & owned media (tv, experiential, digital, social, website, etc). Specifically:\\nEvaluate and define metrics and frameworks\\nUnderstand root causes of changes in metrics and communicate insights effectively\\nIdentify levers to move key metrics\\nBuild and automate dashboards and reports\\nSynthesizes data from a variety of sources into actionable marketing recommendations and narratives at the organizational, campaign, channel and tactic levels\\nReports out to executives on marketing performance on a bi-weekly basis; partners with internal teams and/or vendors to build dashboards so team members can access performance data in real time\\nLiaises with key data partners including media agency, marketing cloud partner, and data onboarding partner\\nPartners with cross-functional marketing teams (i.e., insights, shopper marketing, channel marketing, PR, etc.) to measure marketing metrics, correlation and, where possible, direct attribution between each tactic within the channel marketing mix and business performance\\nIdentifies and ideates new ways to measure qualitative brand metrics\\nTranslates complex business challenges into hypotheses for testing; leverages first and third party data to draw actionable conclusions\\nEmpowers marketing teams to leverage marketing performance data, evangelizing data-driven decision making across the organization\\nCollaborates with business owners and leaders to uncover challenges and turn them into data driven solutions\\nStays up to date on all marketing technology and industry trends\\nAssists with special marketing projects as needed\\nThe requirements of this position include:\\nBachelor’s degree with 4 - 6 years of experience in analytics, marketing analytics, media analytics, consumer insights or similar field\\nExperience with marketing analytics tools including marketing mix analysis, multi touch attribution, data management platforms, first party data onboarding, and brand lift studies\\nExperience working with syndicated sales data such as Nielsen or IRI data\\nProficient user of Google Analytics\\nAdvanced Microsoft Excel skills including advanced formula writing, experience with pivot tables, and dynamic chart building\\nAbility to translate big data sets into simple, actionable insights\\nAbility to think critically and creatively\\nStrong organizational skills\\nStrong interpersonal communication skills\\nInnate curiosity – must be able to think outside of the box\\nStrong desire to collaborate and connect with others\\nBe unafraid to contribute ideas and spark inspiration\\nBe entrepreneurial in spirit\\nProven ability to be nimble, dynamic, and able to adjust to changing business needs\\nMust possess a strong work ethic and be an enthusiastic team player\\nAbout Us:\\n\\nSince our founding 10 years ago, we’ve always been a different kind of company. After moving to New York from his native Turkey, our CEO Hamdi Ulukaya found that in America, yogurt just wasn’t as delicious or widely available as it was back home. He thought everyone deserved better options, so he set about making delicious, nutritious, natural, and accessible Greek Yogurt right here in the U.S.\\n\\nOur mission since day one has been to provide better food to more people. And now as the No. 1-selling Greek Yogurt brand in America and the second largest overall yogurt manufacturer, we believe every food maker has a responsibility to provide people with better options, which is why we’re so proud of the way our food is made.\\n\\nOur food philosophy of crafting quality products with simple ingredients is what makes Chobani a different kind of yogurt. Our belief that business done right has the ability to change lives and strengthen communities is what makes Chobani a different kind of company. From the way we source our ingredients to how we treat our employees, Chobani strives to make universal wellness happen sooner with everything we do. Certified as a Great Place to Work®, our culture is built on shared passion, dedication, and a commitment to doing what is right. Together, the Chobani family has created something unlike what any company has done before. The possibilities are endless.\\n\\nChobani is an equal opportunity employer. Chobani will not discriminate against any applicant for employment on any basis including, but not limited to: race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, military and/or veteran status, marital status, predisposing genetic characteristics and genetic information, or any other classification protected by federal, state and local laws.',\n",
       "  'Eisai Inc. was established in 1995 and began marketing its first product in the United States in 1997. Since then we have rapidly grown to become a fully integrated pharmaceutical business. Eisai’s areas of commercial focus include neurology, gastrointestinal disorders and oncology/critical care. The company serves as the U.S. pharmaceutical operation of Eisai Co., Ltd., a research-based human health care (hhc) company that discovers, develops and markets products throughout the world.\\n\\nEisai has a global product creation organization that includes U.S.-based R&D facilities in Massachusetts, New Jersey, North Carolina and Pennsylvania as well as manufacturing facilities in Maryland and North Carolina. The company’s areas of R&D focus include neuroscience; oncology; vascular, inflammatory and immunological reaction; and antibody-based programs.\\n\\nEisai recognizes that we are the Company’s greatest asset. To this, Eisai is committed to providing us an enabling and empowering work environment that allows us to grow and thrive. In our diverse organization, we adhere to a strict commitment to our values of Integrity, Respect, Professionalism, Quality and Teamwork. We therefore invite you to explore this job opportunity or any others that may match your qualifications and interests.\\n\\nUnder the guidance of the Senior Manager, Clinical Safety Data Review in the Oncology Business Group, the Medical Data Scientist performs review of Safety Data by reconciling the information from Safety report with Clinical Database. Ensures medically relevant information is captured in the Clinical database data entry fields. Generates Clinical Summaries for the subjects meeting the criteria of serious adverse events, adverse events that led to study discontinuation and clinically significant events determined by the study.\\n\\nResponsibilities include but not limited to perform review of safety data within Clinical database by generating clinical summaries. Ensuring queries are posted and actioned within the expected timelines.\\n\\nDiscusses difficult cases and data discrepancies with Senior Manager or above for resolution. Collaborates with Project Data Manager for timelines and with Data Operations to ensure the tracking /auto-generated template of Subjects meeting the criteria for review is accurate and in time for any milestone.\\n\\nWrite Clinical Summaries with quality, accuracy and relevant medical information. Observes the Eisai writing guidelines and study specific writing guidelines. Collaborate with other study teams across therapeutic areas on safety related matter especially SAE reconciliation.\\n\\nConsults with Study Directors to resolve safety data review issues/queries. Interface with Data Operations for System issues.\\n\\nKey Responsibilities\\nPerform review of safety data within Clinical database by reconciling safety report with data information in the clinical database.\\n\\nWrite Clinical Summaries with quality, accuracy and relevant medical information. Observe the Eisai writing guidelines and study specific writing guidelines\\n\\nEnsuring queries are posted and actioned within the expected timelines\\n\\nParticipate in the development and implementation of department standards and documents as needed\\n\\nJob Qualifications\\n\\nAdvanced health care degree, Masters degree in medical science, Nursing, Pharmacy or relevant biological science or healthcare field plus 3 years of experience in job offered.\\n\\nRequires 2 years of medical/clinical data review, use of I/J Review, Inform, and other tools collecting clinical data and Word or Excel is a plus\\n\\nRequires 2 years of experience supporting Oncology studies and reviewing data in Case Report Forms; managing CRO global team and supporting Oncology studies).\\n\\nJob Qualifications\\nAdvanced health care degree, Master’s degree in medical science, Nursing, Pharmacy or relevant biological science or healthcare field plus 3 years of experience in job offered.\\nRequires 2 years of medical/clinical data review, use of I/J Review, Inform, and other tools collecting clinical data and Word or Excel is a plus\\nRequires 2 years of experience supporting Oncology studies and reviewing data in Case Report Forms; managing CRO global team and supporting Oncology studies).\\nHere at Eisai, we are rewarded with highly competitive salaries, incentive awards, a comprehensive benefits package (Medical, Dental, Prescription and Vision Plans) 401k, Flexible Spending Accounts, Life and Disability Insurance, and other great programs.\\n\\nEisai is committed to a policy of equal employment opportunity for all employees and applicants. The Company also strives for a work environment free from discrimination and harassment. It is Eisai’s policy to comply with all applicable federal, state and local laws and regulations regarding nondiscrimination in employment and not to discriminate against any employee or applicant for employment on the basis of any protected status, including, but not limited to, race, color, national origin, religion, sex, age, disability, pregnancy, ancestry, creed, alienage or citizenship status, sexual orientation, gender identity and expression, marital status, military/veteran status, genetic information, or any other protected characteristic as established by law.\\n\\nEisai is an Equal Opportunity Employer Minority / Female / Disability / Vet from tblHiringOrgDefaults',\n",
       "  \"Tapad is looking for outstanding data-driven interns to join our Data Science team this summer. Our ideal candidate is passionate about big data and eager to apply Machine Learning techniques to real-world problems.\\n\\nWe are a small, talented team of friendly, pragmatic, productive data scientists. We have created a culture of transparency and accountability, which minimizes bureaucracy and meetings to maximize developer freedom, autonomy, and creative development time. We are looking for interns who are driven, creative, and innovative thinkers. You will experience what it's like to be a data scientist in a fun, technology-first startup atmosphere. You will be paired one-on-one with a data scientist mentor for guidance and advice.\\n\\nWe are looking for a Ph.D. candidate who meets some of the following qualifications:\\nExperience in applied Machine Learning techniques, advanced analytics, and statistical modeling.\\nUnderstanding of programming concepts and experience with programming languages, such as Python, MATLAB, R, Java, Scala or C++\\nExperience manipulating large data sets, using SQL, Hadoop, MapReduce, or Hive\\nHas a background in a Quantitative Discipline (e.g. Physics, Mathematics, Statistics, Engineering, etc.)\\nA day in the life as a Data Science Intern:\\nDesign experiments and create case studies demonstrating the effectiveness of our research and development.\\nEmploy predictive modeling, data mining, graph algorithms, and other data science techniques to your internship project\\nYou will work towards increasing our platform's ability to drive customer value while sustaining healthy margins.\\nCommunicate your results with your mentor, along with appropriate engineering and business units to deploy research into production.\\nTechnologies we use at Tapad:\\nScala, SBT, Play!, Akka\\nScalding, Spark, Python\\nKubernetes, BigTable, BigQuery\\nTypeScript, Angular, Node.js, Hapi.js, Postgres, MySql\\nIn this role, you will be using (don't worry, we'll teach you):\\nPython, SQL\\nSpark, SparkML, AutoML, SciKit Learn, etc.\\nGoogle Cloud Platform (GCP)\\nTapad Intern Benefits:\\nGain valuable experience working in a cutting edge and data-driven environment using state of the art technologies\\nDynamic and fast paced well-established start-up\\nA designated mentor to help guide you through the 10-week internship program\\nOngoing training, which will include Scala School, access to Coursera, peer-led professional development, and an abundance of resources to help guide you through your internship\\nCatered lunches and unlimited snacks and beverages\\nLeadership lunches - sit with the CEO, CTO, and Chief Product Officer, and ask them anything your heart desires\\nFussball, ping pong, diversity and inclusion group, book club, and tons of other extra-curricular activities that will make you feel like part of the Tapad family\\nAbout Tapad:\\n\\nTapad is home to the team that cracked the code on cross-device marketing technology. Our groundbreaking, proprietary tech assimilates billions of data points to find the human relationship between smartphones, desktops, laptops, tablets, connected TVs, and game consoles. With 91.2% data accuracy confirmed by Nielsen, Tapad offers the most substantial in-market opportunity for marketers and technologies to address the ever-evolving reality of media consumption across devices.\\n\\nTapad is proud to be an equal opportunity employer and will consider all qualified applicants regardless of age, sex, race, religion, national origin, sexual orientation, gender identity, marital or family status, disability, or any other legally protected status. Tapad does not accept resumes from unsolicited search firms nor recruiters. In no event shall fees be paid to any unsolicited search firms nor recruiters, regardless of whether the candidate is made an offer or accepts a placement at Tapad. All resumes received through any channels will be considered the sole property of Tapad.\",\n",
       "  'Data Scientist\\n\\nLocation: 99 Jefferson Rd, Parsippany, NJ 07054\\n\\nThe Data Scientist will be responsible for deriving actionable insights and recommendations to measurably improve client experience and operational efficiency across the enterprise by leveraging various elements of data both internal and external to ADP and its clients. Their focus will be on the exploration and visualization of patterns within data obtained and assembled from multiple sources, including application of statistical modeling and leading edge analytical approaches including predictive modeling and machine learning, to identify key drivers of client experience related metrics.\\n\\nThis individual will be a senior member of the Global Shared Services Analytics, a highly collaborative team, which provides data science and deep analytics expertise to solve the most challenging business problems here at ADP.\\n\\nQualifications:\\n5-10 years of progressive experience building statistical models, with preference to those with prior experience in client experience related roles.\\nExceptional quantitative and data analysis skills\\nMust have deep expertise with SAS, R for statistical programming\\nData visualization platforms such as Tableau\\nDeep understanding of Excel, SQL, Oracle for data extraction/transformation.\\nHadoop or similar preferred, but not required\\nPython for text manipulation preferred, but not required\\nResults-oriented, self-starter with ability to learn quickly and adapt to changing business priorities\\nEssential Duties and Responsibilities:\\nApply appropriate data visualization and analytical methods to identify key drivers of client experience related metrics and to provide insightful, innovative and actionable recommendations to improve client experience and business outcomes.\\nBuild deep partnership with business and technology stakeholders from across the enterprise to understand client experience concerns and existing available data sources\\nEstablish credibility as a trusted advisor to key stakeholders across business units in order to promote and elevate statistical and predictive modeling standards.\\nExtract, merge, clean and normalize data sources as necessary for subsequent analytical studies using a combination of Excel, SQL, Oracle and Hadoop.\\nUse R, SAS statistical platforms to design and build appropriate statistical and machine learning models, including but not limited to regression, clustering and decision trees, to identify main drivers of client experience related metrics.\\nCreate appropriate data dashboards/visualizations (Tableau server) and presentations (PPT) to highlight findings and recommendations for both executive and technical audiences.\\nUse good business and financial acumen to assess business cases.\\nImplement appropriate tracking methods to measure business impacts of recommendations.\\nEducation & Certification:\\nBachelor\\'s degree in Statistics, Applied Mathematics, CS, Engineering or related field preferred.\\n____________________________\\n\\n#LITECH\\n\\nReq 181601\\n</P>\\n\\nWe\\'re designing a better way to work, so you can achieve what you\\'re working for. Consistently named one of the \\'Most Admired Companies\\' by FORTUNE® Magazine, and recognized by DiversityInc® as one of the \\'Top 50 Companies for Diversity,\\' ADP works with more than 740,000 organizations across the globe to help their people work smarter, embrace new challenges, and unleash their talent. \"Always Designing for People\" means we\\'re creating platforms that will transform how great work gets done, so together we can unlock a world of opportunity.\\n\\nAt ADP, we believe that diversity fuels innovation. ADP is committed to equal employment opportunities regardless of race, color, genetic information, creed, religion, sex, sexual orientation, gender identity, lawful alien status, national origin, age, marital status, or protected veteran status. We support an inclusive workplace where associates excel based on personal merit, qualifications, experience, ability, and job performance.',\n",
       "  'About Qapital\\n\\nQapital is a new kind of banking experience that empowers people to maximize their happiness by saving, spending, budgeting, and investing with their goals in mind. With over 1.5 million members, Qapital blends behavioral economics with technology, providing people with the tools they need to make managing money easy and fun.\\n\\nRecognized as a pioneer in fintech, Qapital was awarded Most Innovative App of the Year, 2017 by Google, featured by Fast Company as one of The 25 Best New Apps of 2018, and included by CNET Download on its list of The best money-saving apps to help stay within a budget in 2019. We also have over 40,000 5-star reviews and were listed as one of 2018s best apps for saving on the Apple App Store.\\n\\nWe are a team of designers, behavioral scientists and financial technologists on a mission to build a new banking experience from scratch. Join us, and be part of a team that is changing the way people think about everyday banking.\\n\\nAbout the Role\\n\\nAs an Analytics Manager at Qapital, you will enable us to utilize data to drive key strategic decisions. You will develop and maintain dashboards for key product and user analytics using Looker, and build models that help marketers and our leadership understand and guide Qapitals growth. You will create and deploy tools to measure and monitor our progress against business-critical goals and perform ad hoc analyses diving into the impact of individual campaigns, investments, and initiatives. You will collaborate and align with marketing, product, and executive stakeholders to translate end-customer requirements into analytics deliverables.\\n\\nFurthermore, you will:\\nDevelop a deep understanding of available data and how it can be used for different analyses to answer questions and solve problems.\\nUse advanced data modeling and analysis techniques to discover insights that will guide strategic decisions and uncover optimization opportunities.\\nLead analytics for integrated marketing campaigns, including establishing consistent tracking and dashboards, plus develop insights into campaign impact on product awareness, user acquisition and lifecycle, product usage, and more.\\nDrive interpretation, understanding, and adoption of analytics insights.\\nWork cross-functionally with developers and data scientists to identify and address analytics tracking, tooling, attribution modeling, and infrastructure challenges.\\nSkills and Requirements\\nBachelors Degree, with coursework in statistics.\\n3+ years experience in analytics; experience in marketing, revenue, web, or growth analytics is a plus.\\nExperience writing and optimizing SQL queries and working with large data sets.\\nData visualization and dashboarding experience using Looker.\\nExperience with either Python or R for more advanced statistical analysis and visualization.\\nExcellent communication and presentation skills.\\nResults-driven mindset and ability to handle multiple projects.\\nStrong interpersonal and time-management skills.\\nExperience with digital subscription-based models is a plus.\\nFamiliarity with lifecycle marketing programs and tactics for integrated campaigns and lead gen is a plus.\\nDiversity Commitment\\n\\nQapital is an equal opportunity employer committed to creating a diverse workforce. A diverse team is best positioned to truly serve our market by offering a broad range of ideas, abilities and perspectives. We consider all qualified applicants without regard to race, religion, color, sex, national origin, age, disability, veteran status, or any other classification.',\n",
       "  \"Data Scientist\\nWe are seeking a smart and engaged Data Scientist (or Senior Data Scientist) to work with our large data sets in the ed-tech and advocacy space! We are a developing a platform that supports data aggregation for our partner organizations to efficiently and effectively campaign for the increased quality and quantity of education in this country. Our current team is growing and needs to add a Senior Data Scientist at the individual contributor level to come on board to help us refine our predictive modeling algorithms.\\n\\nIdeally, this person will be passionate about Data Science and predictive modeling and will be published, have conference speaking experience, a personal Git-Hub repository, and experience working in advocacy, education, (and/) or politics.\\nWhat You Need for this Position\\n- heavy data aggregation and analytics\\n- machine learning development and deployment\\n- Python\\n- R\\n- SQL (Postgres / Redshift)\\n- AWS cloud computing\\nWhat You Will Be Doing\\n- Developing algorithms and statistical modeling techniques for pattern recognition problems.\\n- Developing code in Python, R, and SQL, or other object oriented languages for modeling optimization, simulation, and statistical analysis.\\n- Building models that maximize performance and accuracy.\\n- Strategic planning and external client interaction.\\nWhat's In It for You\\n- An opportunity to join a very smart, motivated team and to directly impact the future of education.\\n- Strong base salary depending on experience.\\n- Excellent benefits and vacation policy.\\n- Growth potential and a great working environment.\\nIf you are a Data Scientist looking to be a core player at a growing startup, please apply today!\\n-\\nApplicants must be authorized to work in the U.S.\\n\\n\\nCyberCoders, Inc is proud to be an Equal Opportunity Employer\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, protected veteran status, or any other characteristic protected by law.\\n\\nYour Right to Work In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.\",\n",
       "  \"Job Description\\nWarby Parker is on the lookout for a Senior Data Scientist who will help Warby Parker use data more efficiently to work more effectively. To make this possible, our Data Science team works closely with all departments across the company: Customer Experience, Supply Chain, Strategy, Product Strategy, Technology, Applied Research, Brand, Real Estate, Marketing, Digital Product, Retail, and more.\\n\\nAs a member of our team, you’ll see that our work can take many forms. At times, we're producing polished decks to be presented to the co-CEOs. Others, we're building simulation tools that output interactive maps or spreadsheets to be crunched by our Analysts. Sometimes we're even constructing predictive models built into a microservice to be used by systems throughout the company. We also help develop useful frameworks for thinking about risk and uncertainty, giving stakeholders more confidence when making multimillion-dollar decisions.\\n\\nIn this role, you'll use a wide variety of mathematical and technical tools, including mixed-integer programming, statistical modeling, and machine learning. We also help run research studies and experiments. These methods have allowed our team to validate new products, like the Prescription Check app.\\n\\nOver the years, our team's efforts have been featured in Inc. and have helped earn Warby Parker a Webby award. (The latter was for Virtual Try-On, a feature in our an e-commerce iOS app.) At the end of the day, when we help our company succeed we're also supporting our incredible Buy a Pair, Give a Pair program. Good work leads to good outcomes.\\n\\nOur team is geographically distributed with a quorum in our New York City headquarters, complete with a beautiful, quiet library space and bountiful snacks. Our company benefits include trans-inclusive healthcare, parental leave for birth and non-birth parents, and a slew of other perks (like free glasses, of course!). If all of this sounds right up your alley, read on!\\n\\nWhat you’ll do:\\nAsk thoughtful questions and listen carefully to get to know your stakeholders’ vocabulary, problems, goals, and constraints\\nPartner with our Product Manager to scope out and prototype potential solutions to make sure your output will be useful for stakeholders and/or customers\\nSelect and apply appropriate mathematical tools, then write the code to implement your solution\\nRegularly present your progress to stakeholders and iterate it based on their feedback\\nPresent final results to stakeholders, executives, and the broader company as needed\\nDeploy final code to production (for applicable projects)\\nGive helpful feedback on teammates’ designs, plans, and code\\nLearn from and teach colleagues on the Data Science team, while also supporting Analysts on teams throughout the company\\nWho you are:\\nBacked by 3+ years of experience in related stakeholder- or customer-focused data science work\\nAble to exercise independent judgment on all aspects of project execution while also soliciting regular input from colleagues and stakeholders\\nA kind, empathetic communicator (in writing and in person) with excellent listening skills\\nSomeone who works proactively and effectively with people of various skill sets\\nEquipped with a good numerical sense and able to think probabilistically\\nAble to write clean, organized queries in SQL and code, in either R or Python\\nComfortable working with the tools of modern software engineering, such as working at the command line, using version control, and writing tests\\nNot on the Office of Inspector General’s List of Excluded Individuals/Entities (LEIE)\\nExtra credit—definitely not required, but if you have these skills we can put them to good use!\\nA good understanding of visualizing data—you know how to do it and how your work will likely be understood by non-experts\\nA background in teaching, mentoring, tutoring, or lecturing\\nExperience deploying and maintaining production code\\nExperience designing, running, or analyzing research studies\\nAbout us:\\nWarby Parker was founded with a lofty objective: to offer designer eyewear at a revolutionary price while leading the way for socially conscious businesses. By circumventing traditional channels and designing our frames in-house, we’re able to offer top-quality glasses and sunglasses (plus an uncommonly delightful shopping experience) at a fraction of the traditional going price.\\nSince starting out in 2010, we’ve set up headquarters in New York City and Nashville, built our own optical lab, and opened retail locations all around the U.S. and Canada. As we grow, we’re committed to proving that businesses can scale and be profitable while doing good in the world. For every pair of glasses we sell, a pair is distributed to someone in need—to date, that’s over five million pairs.\\nOf course, all work and no play makes a dull workplace. Who likes that? At Warby Parker, you can look forward to company outings, volunteering and learning opportunities, and just great company. Teammates can also connect around common interests, backgrounds, and identities, no matter their home base, through our various employee resource groups. (We’re happy to say that the Human Rights Campaign has named us a Best Place to Work for LGBTQ+ employees!) That sense of community keeps us excited to walk through the door every day. Good work, good people.\\n\\nSome benefits and perks of working at Warby Parker:\\nHealth, vision, and dental insurance\\nFlexible “My Time” vacation policy\\nRetirement savings plan with a company match\\nParental leave (non-birthing parents included)\\nCell phone plan reimbursement\\nA health-and-wellness stipend\\nFree eyewear, plus discounts for friends and family\\nAnd more—just ask!\",\n",
       "  'We are seeking a highly-experienced ML Engineer to join our team building advanced Business Intelligence, Machine Learning, and Data Processing applications.\\n\\nYour Impact\\n\\nAs a Backend Engineer, you will develop advanced tools that allow our customers to build highly-sophisticated Business Intelligence applications for their stakeholders. You will work as a member of an engineering “feature team,” responsible for delivering value to our customers. In this role, you will design, implement, and deliver enterprise-grade machine learning applications for both internal- and external-facing consumers. You will be part of a dynamic team of engineers who are developing cutting edge technology that fuels the premier BI platform in the industry. Your work will be deployed to production and used by dozens of major corporations such as FedEx, US Bank, and Mastercard to help drive their business goals and service their customers.\\n\\nWhile mostly focusing in ML models productizing and data processing you will be expected to contribute to the model development. You believe that good design is the key to good coding -- “measure twice, cut once.” You write excellent-quality code (if you do say so yourself), and understand how to best practices of SW architecture, development and testing.\\n\\nResponsibilities\\nCollaborate with the product owner, technical lead, product designer, and other stakeholders to design, prototype and develop enterprise-class data intensive applications.\\nMaintain existing code and make improvements to increase maintainability, performance, and scalability.\\nSupport software rollouts to production.\\nConstantly improve code quality and test coverage.\\nUnderstand full-stack dependencies to minimize regressions and attain improved designs.\\nGuide and mentor junior engineers. Serve as team lead if appropriate.\\nQualifications\\nBS/MS degree in Computer Science, Computer Engineering, or a related subject.\\n5+ years of demonstrated experience in Python.\\nIn-depth knowledge of Python data processing and machine learning libraries.\\nExperience with and understanding of the Python ML frameworks such as TensorFlow and PyTorch\\nExperience with API design & development\\nUnderstanding of the data layer integration (both SQL and no-SQL)\\nExperience with cloud deployments is a plus\\nUnderstanding AWS / Azure / GCP data ETL capabilities is a plus\\nExperience with C/C++, Java and/or Scala plus.\\nPassion for writing well structured, testable code with a focus on readability and maintainability.\\nExperience with open source CI tools is a plus.\\nData modeling experience is a plus.\\nExcellent communication skills.\\nInformation Builders helps organizations transform data into business value. Our software solutions for business intelligence and analytics, integration, and data integrity empower people to make smarter decisions, strengthen customer relationships, and drive growth. Our dedication to customer success is unmatched in the industry. That’s why thousands of leading organizations rely on Information Builders to be their trusted partner. Founded in 1975, Information Builders is headquartered in New York City, with offices around the world, and remains one of the largest independent, privately held companies in the industry.\\n\\nInformation Builders, Inc. is an Equal Opportunity Employer: All qualified applicants will receive consideration for employment and will not be discriminated against based on their race, gender, disability, veteran status, or other protected classification.\\n\\n#LI-LO1',\n",
       "  \"Job Description\\nMy name is Chad and I represent PeopleMakeUs. We are a national staffing firm seeking a Data Scientist for one of our Fortune 500 clients. It’s offering competitive pay! The position is located in Summit, NJ. Description below:\\n\\nWe're currently looking for an experienced Data Scientist with deep technical and statistical knowledge and a proven track record to drive business value using advanced data analysis and machine learning techniques. This position will collaborate with executives and other business departments to execute a variety of analytical projects in the drug safety (pharmacovigilance (PV)) department to improve processes and create efficiencies.\\n\\nPrincipal Duties and Responsibilities:\\n\\nAnalyze complex data sets to reduce costs and improve customer experience.\\n\\nCreate systems for gathering, extracting, preparing data from multiple sources.\\n\\nEvaluate safety operations and PV activities with measurable data points.\\n\\nUnderstand operational key performance indicators, report trends and patterns.\\n\\nEffectively communicate analysis with proper measurements and testing.\\n\\nFormulate enhancements through system optimizations and continual data analysis.\\n\\nChampion process improvements assessing pre-and post-process change.\\n\\nCollaborate within the drug safety function.\\n\\nQualifications:\\n\\nMaster's or PHD in Statistics, Mathematics, Computer Science or another quantitative field.\\n\\n4+ years of experience manipulating data sets and building statistical models.\\n\\nStrong written and verbal communication skillsProven experience solving complex business problems using Machine Learning techniques like Regression, Classification, Supervised or Unsupervised Recommenders, etc.\\n\\nExperience using statistical computer languages (R, Python,SQL,PHP, etc.) to manipulate data and draw insights from large data sets.\\n\\nMust meet requirements of company background check policy requirements.\\n\\nMeasure of Success:\\n\\nAbility to understand business and applications, capture, organize and present data for making better decisions and creating competitive advantages.\\n\\nAbility to enhance safety operations applications with streamlined data components to improve services through effective measurement and analysis.\\n\\nAbility to scope, manage and execute assigned projects with effective data visualization.\\n\\nHitting goals while promoting a positive attitude true to our mission and core values.\\n\\nExperience: relevant: 4 years (Required)\\n\\nEducation: Master's (Required)\",\n",
       "  \"Momentum Solar is a premier residential solar provider with offices throughout the U.S. Founded in 2009, Momentum has grown exponentially over the past decade. We implement the entire solar process to ensure a seamless transition to renewable energy.\\n\\nJob Overview:\\n\\nThe Data Scientist is responsible to improve and sustain an Advanced Analytics System within the Business Intelligence and Data Integration. This is a hands-on position which is primarily responsible for implementation and rollout of advanced analytics solutions with the goal of gaining an in-depth understanding of Client's customers and thereby improving a customer's experience, as well as automation of customer processes. The individual in this position is expected to work independently under minimum direction and supervision.\\n\\nRequirements:\\nB.S or higher preferably in Computer Science\\n0-4 years' experience in software development,\\nClear understanding of data structures, algorithms, software design and core programming concepts\\nIdeal candidates will have at least a year experience in professional or academic environment using Python, SQL, R and JavaScript specifically.\\nPreferred a strong understanding of SQL queries and development principles.\\nA strong working knowledge of CRM Systems, Excel, Access, Stata, HTML\\nExperience leveraging test driven development techniques\\nQualifications:\\nUpdate and maintain documentation for team processes.\\nMust be independent and comfortable in a fast-paced work environment\\nExceptional analytical skills, able to apply expertise to drive complex, Technical solutions\\nProven ability to initiate and drive projects from conception to completion\\nHands-on experience in Client model development.\\nMomentum Solar is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation / Age.\",\n",
       "  'WHO WE ARE:\\n\\n\\nThe Trade Desk is leading the way to the future of marketing by changing how advertising is bought and sold. Working with the largest brands and agencies around the world, our technology platform helps consumers discover products they want by enabling advertisers to target and reach them on the right channels at the right time.\\n\\nEmployees join The Trade Desk to discover opportunity, contribute to our customers\\' success, and be part of building the future of marketing. You\\'ll work with creative, compassionate, and collaborative colleagues that share a passion for making the internet better for all – a combination that simply can\\'t be beat.\\n\\nWhen you join The Trade Desk, you\\'re joining a family. We have open space work environments, adjustable sitting/standing desks, and a celebrated open-door policy (at all levels) that can inspire out-of-the-box solutions and camaraderie among your coworkers. The competitive compensation packages, full benefits, stock options, and additional discounted stock purchase opportunities, catered lunches, and offsite team building activities may cause slight to severe jealousy among your peers.\\n\\nWHAT YOU WILL BE DOING:\\n\\n\\nYou\\'ll be a member of our data science team with a focus on identity. Here at the Trade Desk, because all of our user information is anonymized and at a device level, we need the ability to link disparate anonymized identifiers back to an individual or household level for a myriad of reasons. In order to do that, we maintain a cross-device graph that allows us to create those links.\\n\\nIn this role, you will be a subject matter expert on every part of our identity offering, including our own first party cross-device solutions, how we match to third party vendors, and our unified graph, \"identity alliance\" which combines cross-device graphs from multiple partners. The identity graph is a foundational part of our platform, powering many of our measurement, targeting, and optimization capabilities. As a result, this person needs not just understand the graph itself, but, almost more importantly, all its downstream dependencies as well. Lastly you will be an expert in the processes and data that enable these solutions and provide input into the collection of new data sources. Your day-to-day will be spent interfacing between our Product Management, Data Partnerships, and Data Engineering teams to help lead the development and improvement of our identity solutions. In this role you may also serve as a technical lead, guiding projects and mentoring junior members of the team.\\n\\nIn addition to leading our identity efforts, you will also be a contributing member to the broader data science team, which includes the following responsibilities:\\nBe an intellectually generous and collaborative team member\\nDevelop end-to-end machine learning prototypes that could scale to run in a production environment\\nMaintain a focus on shipping product, understanding that oftentimes done is better than perfect\\nBecome an expert on TTD\\'s client-facing platform\\nWHAT YOU BRING TO THE TABLE:\\n\\n\\nNON-TECHNICAL QUALIFICATIONS:\\nYou are enthusiastic to learn (and teach) new technologies, machine learning techniques, and advancements in the adtech industry\\nYou are comfortable working on an agile, distributed team spanning multiple time zones and continents\\nYou have an abundance of intellectual curiosity\\nYou are able to deliver a convincing story derived from the data sets to both a technical and non-technical audiences\\nExperience in advertising or publishing related industry a plus\\nTECHNICAL QUALIFICATIONS:\\nPhD in a highly quantitative field (Computer Science, Applied Math, Statistics, Operations Research, etc.) with 5+ years of professional or academic experience in a data driven role working with massive data sets. MS considered with relevant background\\nStrong algorithm & data structure knowledge. A background in graph theory is a plus\\nYou have a solid understanding of statistical machine learning and optimization techniques as well as the ability to derive actionable insights from massive data sets\\nYou have experience with big data technologies such as Hadoop, Hive, Spark, AWS and are able to write efficient SQL\\nYou have experience in multiple programming languages (Python, R, Java, Scala, etc.)\\nThe Trade Desk does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of The Trade Desk. The Trade Desk is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.',\n",
       "  'Data Scientist Job Description\\n\\nThe Senior Data Scientist will be a part of the S&P Global Market Intelligence (SPGMI) Data Science team.\\n\\nThe Role:\\n\\n• Discover insights and identify opportunities through the use of statistics, algorithms, data mining and visualization techniques\\n• Build and evaluate models, make predictions, gather results, and communicate findings to stakeholders\\n• Evaluate the big picture and solve problems rather than looking at metrics alone\\n• Use advanced business knowledge and advanced machine learning techniques to acquire, combine & transform multiple dataset to solve a business use case\\n• Collaborate with engineering and product teams to create and build strategic models that drive product improvements while maintaining cost efficiency\\n• Build and maintain re-usable machine learning and model validation procedures for the rest of the team to use\\n\\nExperience and qualifications:\\n\\n• Bachelors Degree in Mathematics, Statistics, Computer Science, Engineering, Operations Research or related fields preferred (Masters degree an advantage)\\n\\n• 5+ years practical experience with statistical analysis and creating complex models, preferably in the financial services sector\\n\\n• Excellent analytical and problem solving skills\\n\\n• Advanced experience in at least one data analysis/data transformation package (R, Python, Alteryx)\\n\\n• Exposure to one or more data discovery, data visualization tools\\n\\n• 5+ years of hypothesis testing, web analytics and python scripting\\n\\n• Experience with Machine Learning and Natural Language Processing\\n\\n• Ability to remain focused and to think logically in a fast-paced environment\\n\\n(NICE TO HAVE)\\n\\n* experience with nlp\\n\\n* exposure to reinforcement learning\\n\\n* exposure to information retrieval (search)',\n",
       "  \"WHO WE ARE:\\n\\n\\nThe Trade Desk is leading the way to the future of marketing by changing how advertising is bought and sold. Working with the largest brands and agencies around the world, our technology platform helps consumers discover products they want by enabling advertisers to target and reach them on the right channels at the right time.\\n\\nEmployees join The Trade Desk to discover opportunity, contribute to our customers' success, and be part of building the future of marketing. You'll work with creative, compassionate, and collaborative colleagues that share a passion for making the internet better for all – a combination that simply can't be beat.\\n\\nWhen you join The Trade Desk, you're joining a family. We have open space work environments, adjustable sitting/standing desks, and a celebrated open-door policy (at all levels) that can inspire out-of-the-box solutions and camaraderie among your coworkers. The competitive compensation packages, full benefits, stock options, and additional discounted stock purchase opportunities, catered lunches, and offsite team building activities may cause slight to severe jealousy among your peers.\\n\\nDATA SCIENCE AT THE TRADE DESK:\\n\\n\\nAt The Trade Desk, the Data Science team is responsible for transforming our hundreds of terabytes of Real-Time Bidding (RTB) advertising auction data into actionable intelligence, using advanced modelling and optimization algorithms. Our mission is to develop new and powerful means of growing the value of our client's campaigns across millions of web-based publishers.\\n\\nOur team believes in innovation that contributes to our product. In this role, you will be a key member in the development of entirely new data products and the improvement of existing data, all while using world-class modelling & machine learning. You will interact primarily with our Product Management, Trading, Data Partnerships, and Data Engineering teams to oversee the implementation of your data products. Your ideas and research will contribute directly to our client facing products.\\n\\nWHAT YOU WILL BE DOING:\\n\\n\\nYou'll be a member of our data science team with a focus on ads measurement. In this role, you will be a subject matter expert on every part of our ads measurement offering – conversion lift and experiments, synthetic controls, reach and frequency, brand lift, etc. In addition, you will be an expert in the processes and data that enable these solutions – understanding the role of identity and advocating for changes if needed, building ways to account for missing transaction data, correcting non-response bias, panel bias, and so on. Your day-to-day will be spent interfacing between our Product Management, Data Partnerships, and Data Engineering teams to help lead the development and improvement of our measurement solutions. In this role you may also serve as a technical lead, guiding projects and mentoring junior members of the team\\n\\nIn addition to leading our measurement efforts, you will also be a contributing member to the broader data science team, which includes the following responsibilities\\nBe an advocate for good science\\nBe an intellectually generous and collaborative team member\\nMaintain a focus on shipping product, understanding that often done is better than perfect\\nProvide input into the collection of new data sources and the refinement of existing ones to improve analysis and model development\\nBecome an expert on TTD's client-facing platform\\nWHAT YOU BRING TO THE TABLE:\\n\\n\\nNON-TECHNICAL QUALIFICATIONS:\\nYou are enthusiastic to learn and teach statistical methods, machine learning techniques and advancements in the ad tech industry\\nYou are comfortable working on an agile, distributed team spanning multiple time zones and continents\\nYou have an abundance of intellectual curiosity\\nYou are able to explain insights and provide actionable recommendations to both technical and non-technical audiences\\nExperience in advertising or publishing related industry a plus\\nTECHNICAL QUALIFICATIONS:\\nA background in experimental design and causal inference\\nA MS or PhD in Statistics, Biostatistics, or related field\\n5+ years of professional or academic experience in a data driven role working with massive data sets\\nExperience with big data technologies such as Hadoop, Hive, Spark, AWS and the ability to write efficient SQL\\nExperience in multiple programming languages (Python, R, Java, Scala, etc.)\\nThe Trade Desk does not accept unsolicited resumes from search firm recruiters. Fees will not be paid in the event a candidate submitted by a recruiter without an agreement in place is hired; such resumes will be deemed the sole property of The Trade Desk. The Trade Desk is an equal opportunity employer. All aspects of employment will be based on merit, competence, performance, and business needs. We do not discriminate on the basis of race, color, religion, marital status, age, national origin, ancestry, physical or mental disability, medical condition, pregnancy, genetic information, gender, sexual orientation, gender identity or expression, veteran status, or any other status protected under federal, state, or local law.\",\n",
       "  \"Company Description: Quartet is a pioneering healthcare technology company striving to improve the lives of people with mental health conditions. We connect people to a personalized care team to get them the right care at the right time. Our collaborative technology platform and range of services brings together physicians, mental health providers, and insurance companies to effectively improve patient outcomes and drive down healthcare costs. Backed by $93MM in venture funding from top investors like Oak HC/FT, GV (formerly Google Ventures), F-Prime Capital Partners, and Polaris Partners, Quartet is headquartered in NYC and is currently operating in several markets across the United States — Pennsylvania, Washington, Northern California, and New Jersey.\\n\\nRole Description: As a Sr. Data Scientist at Quartet, you will work in collaboration with other data scientists, bioinformaticians and platform engineers. In this role, you will build machine learning models and recommendation services to enable our applications to suggest timely and appropriate behavioral health care interventions for patients. You'll work with datasets that include millions of detailed medical, pharmacy, lab claims, EHR, and application data. You will help with development and validation of new algorithms that enhance our system in terms of scalability, reliability and accuracy. The ideal candidate will be an entrepreneurial, motivated data scientist who is well-versed in data analysis and algorithm implementation and eager to learn new things and make an impact on the industry. Health data experience is a plus, but it's not necessary.\\n\\nResponsibilities:\\nWork with an interdisciplinary technical team to develop statistical models in Quartet’s platform.\\nApply data mining and machine learning techniques to develop better personalization and recommendation for patients’ and doctors’ needs.\\nDesign and develop effective models, features, and algorithms involving multiple datasets - user activity, EHR, ADT, medical claims, pharmacy claims, lab test claims etc.\\nDerive insights from descriptive analysis that drive a data-informed process for experimenting with new products to improve patient outcomes.\\nQualifications:\\nExperience building high quality data products.\\n7+ years experience as a data scientist, software engineer with predictive modeling, or similar experience of solving real problems with data mining and machine learning techniques.\\nPhD or Master’s Degree in computer science, machine learning, applied statistics, physics, or a related quantitative discipline.\\nProficiency in building Machine Learning (supervised and unsupervised) models and recommendation systems.\\nStrong knowledge of mathematical fundamentals: probability theory, linear algebra and statistics.\\nAbility to execute, starting from problem definition, to a working implementation.\\nAbility to clearly communicate across disciplines and work collaboratively.\\nProficiency in Python and code versioning systems like git.\\nExpertise with data science toolkits like scikit-learn and pandas.\\nKnowledge of software architectures and tools such as Scala, Hadoop.\\nFamiliarity working in a Linux server-based environment.\\nEmployee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, full medical, dental + vision coverage, generous parental leave, commuter benefits, 15 free therapy sessions + unlimited copay reimbursements for mental healthcare, 401K, ESPP, gym benefits.\\n\\nWant to know what Quartet life is like? Click here to meet our team. Quartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet. Please note: Quartet interview requests and job offers only originate from quartethealth.com email addresses (e.g. jsmith@quartethealth.com). Quartet will also never ask for bank information (e.g. account and routing number), social security numbers, passwords, or other sensitive information to be delivered via email. If you receive a scam email or wish to report a security issue involving Quartet, please notify us at: security@quartethealth.com.\\n\\nHave someone to refer? Email talent@quartethealth.com to submit their details to us.\",\n",
       "  'The Team: The Data science team is a newly formed applied research team within S&P Global Ratings that will be responsible for building and executing a bold vision around using Machine Learning, Natural Language Processing, Data Science, knowledge engineering, and human computer interfaces for augmenting various business processes.\\n\\nThe Impact: This role will have a significant impact on the success of our data science projects ranging from choosing which projects should be undertaken, to delivering highest quality solutions, ultimately enabling our business processes and products with AI and Data Science solutions.\\n\\nWhats in it for you:\\nThis is a high visibility team with an opportunity to make a very meaningful impact on the future direction of the company. You will work with senior leaders in the organization to help define, build, and transform our business.\\nYou will work closely with other senior scientists to create state of the art Augmented Intelligence, Data Science and Machine Learning solutions.\\nThe team actively participates in top-tier academic and industry conferences by publishing research and organizing workshops. Depending on your interest, you can be part of these efforts.\\nResponsibilities: As an NLP Data Scientist you will be responsible for building AI and Data Science models with a main focus on mining insights from text corpora. You will need to rapidly prototype various algorithmic implementations and test their efficacy using appropriate experimental design and hypothesis validation.\\n\\nBasic Qualifications: BS in Computer Science, Computational Linguistics, Artificial Intelligence with a heavy focus on NLP/text mining, or related field with 5+ years of relevant industry experience. There is some flexibility in adjusting the seniority to your level of education and experience.\\n\\nPreferred Qualifications:\\nMS in Computer Science, Computational Linguistics, Artificial Intelligence with a heavy focus on NLP/text mining with 3+ years of relevant industry experience.\\nExperience with Financial documents such as SEC filings, financial reports, credit agreements, business news, or S&Ps credit ratings process is a plus.\\nWhat we look for in your background:\\nCreativity, resourcefulness, and a collaborative spirit.\\nKnowledge and working experience in one or more of the following areas: Natural Language Processing, Clustering and Classification of Text, Question Answering, Text Mining, Information Retrieval, Distributional Semantics, Knowledge Engineering, Search Rank and Recommendation.\\nDeep experience with text-wrangling and pre-processing skills such as document parsing and cleanup, vectorization, tokenization, language modeling, phrase detection, etc.\\nProficient programming skills in a high-level language (e.g. Java, Scala, Python)\\nBeing comfortable with rapid prototyping practices.\\nBeing comfortable with developing clean, production-ready code.\\nBeing comfortable with pre-processing unstructured or semi-structured data.\\nExperience with statistical data analysis, experimental design, and hypotheses validation\\nProject-based experience with some of the following tools:\\nNatural Language Processing (e.g., Spacy, NLTK, ClearTK, ScalaNLP/Breeze, ClearNLP, OpenNLP, or similar)\\nApplied machine learning (e.g. libSVM, Shogun, Scikit-learn, SparkML, H2O, or similar)\\nInformation retrieval and search engines, e.g. ElasticSearch/ELK, Solr/Lucene\\nDistributed computing platforms, such as Spark, Hadoop (Hive, HBase, Pig), GraphLab\\nDatabases (traditional and noSQL)\\nProficiency in traditional Machine Learning models such as SVMs, LDA/topic modeling, HMMs, graphical models, etc.\\nOptional: familiarity with Deep Learning architectures and frameworks such as PyTorch, Tensorflow, Keras.\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.',\n",
       "  'Transfix, named one of Forbes\\' \"Next Billion-Dollar Startups\" in 2018, is the leading freight marketplace that\\'s transforming the $800 billion trucking industry, connecting shippers to a national network of reliable carriers. Fortune 500 companies such as Anheuser-Busch, Unilever, and Target rely on Transfix to handle their most important FTL freight needs. With instant pricing tools, guaranteed capacity, data-driven insights, and reliable service, Transfix is changing the world of transportation one load at a time. Join us, and leave a positive impact on the environment as we help reduce the carbon footprint caused by the 65 billion wasted miles on the road.\\n\\nBased in New York City, Transfix is backed by top VC firms including New Enterprise Associates (NEA), Canvas Ventures, and Lerer Hippeau.\\n\\nThe problems we solve everyday are real and require creativity, grit, and determination. We are building a culture that challenges norms while fostering experimentation and personal growth. We\\'re hiring team members who are passionate and are energized by the vision to simplify and transform one of the largest and most complex industries through technology, data and a strong commitment to customers.\\n\\nAs our Director, Data Science you will lead a team of talented Data Scientists and Analysts to build and evolve the core engine of Transfix’s marketplace. Using traditional and non-traditional techniques, you will lead the creation of pricing models, market forecasts, auction bidding strategies, and carrier matching algorithms. You will work with a varied set of teams across the organization to drive our company’s most important business goals.\\n\\nIf you’re excited about transforming an industry, being part of an innovative culture, and making a positive impact on the environment, send us your resume.\\nWhat you\\'ll do:\\nSet strategy and roadmaps for our Data Science and BI teams\\nDevelop models to predict freight costs and market forecasts using time series modeling and forecasting\\nCreate bidding and pricing strategies for automated daily and annual auctions\\nDeliver carrier-network matching algorithms and advanced network analyses to unearth optimization opportunities\\nCollaborate with our Sales, Account Management, and Operations teams to drive company KPIs.\\nDrive adoption of improved processes and innovative technologies and tools\\nPromote a culture of data-driven decision making by evangelizing and making data and insights easily accessible across the organization\\nPartner with data engineering for system design and infrastructure\\nAbout you:\\n3+ years experience leading and managing high performance Data Science teams\\n5+ years experience performing statistical modeling, machine learning, and advanced analytics with structured and unstructured data.\\nStrong problem solver who is comfortable devising creative solutions when presented with imperfect or sparse data sets\\nExperience with two-sided marketplaces and supply-demand dynamics\\nBiased for action and driving impact rather than “perfect” theoretical solutions\\nExperience in bidding and pricing (e.g., quant experience)\\nDeep experience with SQL and Python or R\\nProficiency in math, statistics, economics, engineering, or a related field.\\nTo all recruitment agencies: Transfix does not accept unsolicited agency resumes and will not be held responsible for any fees related to unsolicited resumes.',\n",
       "  \"(We are not sponsoring for this role now or in the future)\\n\\nPeople are at the core of Fareportal. We are one of the fastest growing travel technology companies in the world; our portfolio of travel brands, including flagship product CheapOair, receives over 100 million visitors annually.\\n\\nWe are seeking an Business Intelligence Analyst to extract and present meaningful data in order to understand our customers and drive opportunities to enhance customer experience and drive conversion. The ideal candidate will be responsible for performing data analysis and reporting to support strategic objectives. The right person will have a proven history of proactively addressing unasked questions through data and presenting solutions to top management in order to improve systems and processes.\\n\\nResponsibilities:\\nAnalyze customer data from a variety of sources using database querying tools\\nVisualize data and generate insights to help improve the business\\nSupport a wide variety of departments and product teams\\nCollaborate with the Product Owner to identify data sources of truth to support business decisions\\nInterface with entire IT Chapter (data science, BI, IT operations, IT engineering, IT Security)\\nEfficiently manage the backlog and delivery of analytical projects\\nEstablish processes and SLA for Development & IT Chapters\\nCreate and monitor real-time performance dashboards for key business metrics\\nImplement alerting systems to quickly identify issues, notify stakeholders, and coordinate to resolve the issues.\\nRequirement\\nBA degree, Computer Science, Information, engineering\\nMinimum 2-3 years' work or internship experience with business or data analysis\\nData modeling, Statistical Analysis, Sensitivity analysis (Mathematical modeling)\\nCritical thinking and problem solving skills\\nUnderstanding of database structure, design, query languages (e.g. SQL), Advance Excel Skills\\nRelational databases and querying such as Microsoft SQL Server, Oracle Database, or MySQL\\nExperience with visualization and dashboard tools such as Power BI or Tableau\\nA Plus\\nStrong PowerPoint presentation skills\\nExperience with BI Reporting tools such as Tableau, PowerBi, Business Objects, and/or Microstrategy\\nUnderstanding of Google Analytics\\nExperience with Python or R\",\n",
       "  'Company Overview\\n\\nH2O.ai is the open source leader in AI with a mission to democratize AI for everyone. H2O.ai is transforming the use of AI with software with its category-creating visionary open source machine learning platform, H2O. More than 18,000 companies use open-source H2O in mission-critical use cases for Finance, Insurance, Healthcare, Retail, Telco, Sales and Marketing. H2O Driverless AI uses \"AI to do AI\" in order to provide an easier, faster and cost-effective means of implementing data science. H2O.ai partners with leading technology companies such as NVIDIA, IBM, AWS, Intel, Microsoft Azure and Google Cloud Platform and is proud of its growing customer base which includes Capital One, Progressive Insurance, Comcast, Walgreens and MarketAxess. For more information and to learn more about how H2O.ai is driving an AI Transformation, visit www.h2o.ai.\\n\\nJob Summary:\\n\\nWe are looking for high-energy and driven account executives to help promote H2O.ai and close big deals. Successful candidates will have a proven ability to execute with seven or more years of enterprise software sales focused on enterprise big data, analytics, and/or data science software products. Ability to navigate and thrive in a fast-paced startup environment is critical. Specific vertical expertise in financial services, insurance, and/or healthcare is helpful, as is a knowledge of open source software business models.\\n\\nAs an Enterprise Account Executive at H2O.ai, you are skilled at navigating the customer buying process, conveying value and compressing the sales cycle. You bring a passion for innovation and translating complex technology into a high-ROI business outcome. You are skilled at qualifying opportunities, managing your time and that of your team wisely, and staying focused on the goal.\\n\\nQualifications and Skills:\\nEngage with CIOs, CDOs, CTOs, IT executives, Data Scientists and other key stakeholders\\nManage and grow a pipeline of both net new and existing accounts\\nCapitalize on our land and expand strategy for quick wins while cultivating complex 7-figure transactions\\nMaintain impeccable Salesforce hygiene on all active opportunities and customer activity\\nUse solution selling to create and demonstrate value for customers\\nStrong qualification and time management skills; excellent written and verbal communication\\nStrong listening skills and ability to think strategically about structuring and winning deals\\nRequirements:\\nMinimum of 5+ years field sales experience within big data, analytics, and/or data science tools and technologies in large enterprise accounts.\\nStrong existing network in large enterprise accounts in your territory.\\nProven track record of revenue overachievement, hunger to win.\\nTeam player that knows how and when to enlist internal resources to ensure the desired outcome.\\nBachelor\\'s Degree required.\\nBenefits and Perks:\\nFlexible work hours and time off.\\nCompetitive salary with uncapped commission and accelerators above quota\\nMedical, dental, vision\\n401k Retirement Plan\\nH2O.ai is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status.',\n",
       "  \"About the Role\\n\\nThe Business Intelligence Analyst will play an important role in leveraging data to accelerate our business. This role is an instrumental component of the overall operations function and will drive key projects related to data collection, transformation, analysis, and reporting, as well as enable key stakeholders to make better business decisions and improve core processes.\\n\\nResponsibilities\\nWork with cross-functional stakeholders to gather business requirements that matter for each functional team of the business including Sales, Customer Success, Product Marketing, Revenue Operations, and Finance\\nIdentify metrics to be used in reports and dashboards, and ad-hoc analysis needed by the business\\nMaintain existing dashboards and data visualizations\\nBuild dashboards and reports that are relevant and easily understood\\nContextualize your analysis for key stakeholders to enable decision makers to make better business decisions\\nPresenting insights to the executive team and other key stakeholders to enable data-driven decision-making and improve the business\\nDevelop systems and processes to measure the success of new systems, initiatives and processes\\nSkills and Experience\\nBachelor's degree in a quantitative field like business, management, accounting, economics, statistics, information science or similar required.\\n2-4 years of experience in a data analyst/business intelligence role, or in a role requiring the extraction and interpretation of data relevant to SaaS organizations\\nMastery of SQL and experience with tools like Salesforce, Excel or similar programs and languages\\nExperience with a BI tool (Looker, Domo, Tableau)\\nUnderstanding of SaaS technology stack tools (SalesForce, Marketo, Outreach, Clari)\\nPython and APEX/Java a plus\\nTraits\\nConsultative approach, proactive\\nDemonstrate a high degree of business acumen\\nIntellectual curiosity\\nSecurityScorecard embraces diversity. We believe that our team is strengthened through hiring and retaining employees with diverse backgrounds, skillsets, ideas, and perspectives. We make hiring decisions based upon merit and do not discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.\",\n",
       "  'The search science team @WalmartLabs in Hoboken, NJ is dedicated to the mission of helping millions of customers every day in finding the right products. We are at the forefront of attacking one of the most complex problems of e-commerce. Whenever a user types in a query or browses through product categories on the web site, phone or the iPad, our service goes to work. We mine billions of search queries and tens of millions of products to find the most relevant products for our customers. Team members take end-end responsibility in analyzing large amounts of data, creating complex models, improving their accuracy and deploying these models to serve customers. As part of this team, you\\'ll solve some of the most fascinating and impactful problems in machine learning, information retrieval, NLP or computer vision problems. The algorithms you develop will champion customers need to express their intent and our goal to understand their preferences, implicitly or explicitly across devices and modalities (voice, text, image etc.).\\nYou will develop and deploy machine learned models, to engage and converse with our customers and to understand their goals and expectations to enable them to make the right purchase decision. Your work will be visible to millions of customers and you will have a direct impact on the goals of the Fortune #1 enterprise. If you speak and think machine learning then we want to talk to you. Come join our team and be part of this exciting journey.\\n\\nResponsibilities:\\nApply machine learning and optimization algorithms to maximize the efficiency of our business and minimize risks.\\nBuild, validate, test and deploy predictive models using machine learning techniques to explain or predict behavior and solve a variety of business and engineering problems.\\nIdentify, collect, and explore the right data used for predictive modeling and algorithm development.\\n\\nRequirements\\n5+ years of work and research experience in the machine learning field.\\nA deep understanding of machine learning and interest in applying it at scale.\\nExperience with data cleaning, preparation, and feature building and selection techniques.\\nExperience working with large data sets to solve problems.\\nExperience with Spark and/or Hadoop would be very helpful.\\nExperience with deep learning library like TensorFlow and/or PyTorch is highly desirable.\\nExperience with GPU Computing is highly advantageous.\\nPrior hands-on experience with Python, Java or Scala and the ability to write reusable and\\n\\nefficient code to automate machine learning pipeline and data processes.\\nEffective communication, interpersonal and teamwork skills.\\nAbility to handle multiple concurrent projects while working independently and in teams.\\nAbility to work in a fast-paced and deadline driven environment.\\nPh.D. or MS in Computer Science, Statistics, Computational Linguistics, Artificial Intelligence,\\n\\nOperations Research, Mathematics or related fields.\\n\\nAbout Jet\\n\\nJet is reshaping ecommerce as we know it. Based in Hoboken NJ (just 10 min from Manhattan), we are a shopping site on the relentless pursuit to build the greatest shopping experience in the world. At Jet, we believe in bold. That means taking risks, asking \"why not\", looking where no ones looked before and bringing it! Our engineers are utilizing world class technologies, to optimize the supply chain, remove unnecessary costs, sprinkle in some surprise and delight all while saving customers every penny possible and we\\'re just getting started. At Jet, we have worked hard to build a culture that stresses the importance of learning and sharing knowledge. If you want to be part of the team that is changing the shopping norm and learning a lot along the way, we have a hunch you\\'d look good in purple.\\n\\nSee what we\\'ve been up to\\n\\n\"A New & Improved Saving Experience\" // by Jessica Anerella, Product Design at Jet.com',\n",
       "  \"Etsy is looking for a Data Scientist to join our Data Science team in Brooklyn, New York. We are committed to advancing E-commerce related fields by building technologies that help Etsy buyers and sellers discover and celebrate handmade goods from all over the world. We are seeking individuals passionate in areas such as Computer Vision and Signal & Image Processing in general, with experiences in Image Classification, Object Detection, Visual Question and Answering, and Deep Learning.\\n\\nOur data scientists have the opportunity to make core algorithmic advances and apply their ideas in the dynamic world of E-commerce in strengthening Etsy's global marketplace. Data scientists can publish their innovations at top tier conferences such as KDD, NIPS, ICML, ICLR, CVPR, ICCV, WWW, WSDM, SIGIR and etc. This position would be based in Brooklyn, New York.\\n\\nAbout the Job\\n\\nWhat we're working on:\\nComputer Vision\\nSignal and Image Processing\\nMachine Learning\\nDeep Learning\\nAbout You\\nYou share our values (below) and are looking for a company that has a solid mission.\\nYou have strong analytical and quantitative skills. You are familiar with techniques in Computer Vision and Signal & Image Processing in general, with experiences in Image Classification, Object Detection, Visual Question and Answering, and Deep Learning or related fields.\\nYou have a Ph.D. degree or Master degree in Computer Science, Machine Learning, or related fields.\\nYou have strong technical and programming skills. You are familiar with relevant technologies and languages (e.g. Python, Java, Scala, C++ and etc.). You have experience in or desire to learn Hadoop/Spark related Big Data technologies.\\nYou have demonstrated the capability to review and write technical papers.\\nYou can contribute to research that can be applied to Etsy products.\\nYou have the ability to quickly prototype ideas and solve complex problems by adapting creative approaches.\\nYou are a strong collaborator and communicator and you make the engineers around you grow and learn.\\nWhat's Next\\n\\nInterested in working with us? Send us a cover letter and your CV or resume explaining why you'd be great for the job. We value your unique talents and point of view, so feel free to tell us what you are all about. And if you write, draw, craft, or contribute to something you're proud of, we'd love to hear about it.\\n\\nAt Etsy, we believe that a diverse, equitable and inclusive workplace makes us a more relevant, more competitive, and more resilient company. We welcome people from all backgrounds, ethnicities, cultures, and experiences. Etsy is an equal opportunity employer. We do not discriminate on the basis of race, color, ancestry, religion, national origin, sexual orientation, age, citizenship, marital or family status, disability, gender identity or expression, veteran status, or any other legally protected status.\\n\\nSDL2017\\n\\n</br>\",\n",
       "  \"We are looking for a seasoned data analyst to become a key player in our Data Product team. Successful candidates will be responsible for empowering our team with meaningful data, validating new products and features, identifying optimization opportunities, and utilizing automation techniques to improve team efficiency. The candidate will be working on a myriad of projects and efforts, including many driven by their own insights and findings. There's never a dull moment; repetitive tasks are non-existent. There's an absolutely incredible amount you will learn here, and you'll be working with one of the largest user behavior datasets in existence.\\n\\nData is your passion, you're eager to learn, and you're ready to fully exercise your skill-set.\\n\\nResponsibilities\\nManage projects of the Analytics Team\\nDesign, Develop and Maintain new data sets\\nPerform rigorous audits of existing data for validity\\nReduce the current cost of data infrastructure with data minimization and query optimization\\nAutomate existing redundant tasks using data and tools like looker and airflow\\nWork with cross-departmental key stakeholders to prioritize projects and translate business needs to analytics needs\\nWork with members of the analytics team to hit KPI targets and ship new analytics products\\nRequirements\\n3+ years of experience\\nYou can perform advanced data transformations in SQL and Excel\\nYou have experience using Business Intelligence tools like Looker to make data available to a broader audience\\nYou can utilize scripting languages like python and R to complete advanced analyses of massive datasets\\nYou can design and create thoughtful queries that identify outliers and unexpected changes in business metrics\\nYou are comfortable reporting on the performance of digital marketing A/B tests\\nExperience working with abstract concepts and simplifying requirements for easier consumption\\nYou are comfortable using basic statistics concepts\\nStrong technical skills, business sense, and clear communicator\\nYou're forward-thinking, dynamic, and innovative\\nYou're eager to learn\\nBounceX is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.\\n\\nJOIN US ON OUR MISSION\\n\\nBounceX is a global marketing technology company that has been recognized as a best place to work by Glassdoor and Crain's, and one of America's fastest-growing SaaS companies.\\n\\nMore than 300 companies including JetBlue, Uniqlo, HelloFresh, and Comcast use BounceX to orchestrate real-time, multichannel marketing programs customized for every individual web visitor.\\n\\nWith offices in New York City and London, BounceX is built on the belief that the success of a company is rooted in the strength of its team, so we've created a collaborative, inclusive environment where people love coming to work.\\n\\nWe provide career coaching, growth and development opportunities, and benefits that are in the 95th percentile of all technology companies. Some highlights include excellent healthcare that starts day one, best-in-class fully paid family leave, 401(k) match, flexible work hours, and more.\\n\\nWhat bonds our community together is our commitment to 5 Core Values:\\nCome Hungry\\nCarry Each Other\\nDrive Undeniable Performance\\nRespect People, Privacy, Ideas\\nBounce Back\\nCome join us on our mission.\",\n",
       "  \"About Tachyon Technologies:\\n\\nTachyon TechnologiesÂis a Digital Transformation consulting firm that partners with businesses to implement customer-focused business transformation. Aligned with SAP's digital core, Tachyon Technologies collaborates with its clients to transform their business by leveraging existing IT investments and leading-edge digital solutions to positively impact their customers' experience. From initiation through realization, Tachyon Technologies understands what it takes for a consulting partner to be effective and strives to deliver a meaningful solution that exceeds its clients' expectations.\\n\\nÂ\\n\\nRole: Data Analyst\\n\\nLocation: NYC, NY\\n\\nDuration: Longterm\\n\\nDescription\\n\\nÂ\\nExpertise on SQL (Snowflake or any other MPP databases such as Redshift)<b> 2. Very good knowledge in data visualization tools such as Tableau and Looker( development and maintenance of LookerML).\\nGood experience on working with business stake holders such as Marking, Sales and Finance to understand the requirements and delivering the projects upon their sign-off\\nWorking experience on Media subscriptions domain is highly preferred.\\nBasic understanding of Python programming will be added advantage(To work with Data Eng Team on roll-up Tables)\\nÂ\\n\\nTachyon'sÂfull-time employees receive complete and competitive benefits. We offer a collaborative work environment, competitive compensation, generous work/life opportunities, and an outstanding benefits package that includes paid time off plus holidays. In addition, all colleagues are eligible for a number of rewards and recognition programs. Encouraging a healthy work/life balance and providing our colleagues great benefits are just part of what makesÂTachyonÂa great place to work.\\n\\nDisclaimer:ÂThe above statements are not intended to be a complete statement of job content, rather act as a guide to the essential functions performed by the employee assigned to this classification. Management retains the discretion to add or change the duties of the position at any time.\\n\\nÂ\\n\\nÂ\",\n",
       "  'Who You Are:\\nGetty Images is looking for individuals that enjoy leveraging new and traditional Machine Learning methods to help turn large-scale business data into actionable insights.\\nThe mission of the Data Science team at Getty Images Inc. is to leverage internal and third-party data to inform other groups on how to interact with its customer base. We achieve this goal by 1) building automated solutions that apply best-in-class Machine Learning and Engineering practices and 2) continuous interactions with stakeholders to identify critical needs that deliver results relevant to the business.\\nAs a Data Scientist - ML/AI at Getty Images, you will have end-to-end autonomy and ownership of your projects, and will work closely with other business units to build scalable and robust Machine Learning pipelines that will aim to improve workflows and improve business outcomes.\\n\\nYour Next Challenge:\\nYou will join a growing team of highly-collaborative and curious Data Scientists and Data Engineers. As a member of the team, you will have the chance to interact with key business stakeholders and leadership to define the biggest area of opportunities and accelerate the delivery of a robust portfolio of Data Science models.\\nYour primary goal will be to help extract insights and unlock value from customer data by transparently injecting Machine Learning methods to assist the business, and enabling marketing and sales professionals to define campaigns and actions based on your work. You enjoy writing rigorous code and will interact with the entirety of Getty Images technology stack to build and maintain a production-level data ecosystem that tackles challenging large-scale data problems.\\nWe value learning and development, and you will be given every opportunity to work on projects that excite you. You will have the opportunity to sit at the intersection of Engineering, Marketing, Product and Leadership to inform, influence, support, and execute on key decisions.\\n\\nWhat You’ll Need:\\nA Ph.D. or MS in Computer Science, Statistics, Economics/Econometrics, Sociology, Natural Sciences or any other equivalent quantitative project is preferred. If you are self-taught and believe you are a good fit for this role, or have significant work experience, we would love to hear from you as well.\\nProven experience of having worked hands-on as a Data Scientist, preferably in a product or customer-focused organization.\\nProficiency in Machine Learning, statistical modeling and/or data mining, and a good understanding of the real-world advantages and drawbacks of various Machine Learning techniques.\\nStrong working knowledge in Python (preferred) or R, along with experience working within the Hadoop ecosystem and standard tools such as Git, bash and SQL. Knowledge of Spark and AB testing methodology is also a big plus.\\nExcellent communication skills (both written and orally), and a proven ability to comfortably produce written reports and present findings to stakeholders.\\nAbility to independently execute on a project, from ideation to delivery to stakeholders, and can pro-actively interact with other engineers at Getty Images to access necessary resources or data.\\n\\n#LI-MM1\\nWho We Are:\\n\\nGetty Images is one of the most trusted and esteemed sources of visual content in the world, with over 300 million assets including photos, videos, and music, available through its industry-leading sites www.gettyimages.com and www.istock.com. The Getty Images website serves creative, business and media customers in nearly every country in the world and is the first-place people turn to discover, purchase and share powerful visual content from the world’s best photographers and videographers. Getty Images works with over 250,000 contributors and hundreds of image partners to provide comprehensive coverage of more than 160,000 news, sport and entertainment events each year, impactful creative imagery to communicate any commercial concept and the world’s deepest digital archive of historic photography.\\nGetty Images is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. Getty Images believes that diversity is critical to our success in moving the world with images and is committed to creating an inclusive, mutually respectful environment which celebrates diversity. We seek to hire on the basis of merit, competence, performance, and business needs.',\n",
       "  \"Job Description\\n\\n\\nThe leader in online lending is looking to bring on a Data Scientist to help grant personal loans. It's their mission to help make loans more accessible. This company has been around since 2014, and has granted over $5B in loans to over 300,000 people who utilize their loans to make large purchases or pay off debt. You will be a part of the Decision Science team and will be responsible for developing machine-learning models around credit, risk, and marketing data.\\n\\nRequired Skills & Experience\\nMaster’s or PhD in a technical field\\nKnowledgeable in stats\\nUnderstanding of how to leverage Big Data\\nProficient in Python\\nExperience in Machine Learning Libraries\\nWilling to travel to the East Coast on an as needed basis\\nDesired Skills & Experience\\nDesire to work in finance\\nStrong written and verbal skills\\nAbility to teach and learn from others\\nWhat You Will Be Doing\\n\\n\\nTech Breakdown\\nPython\\nSQL\\nKeras\\nTensorflow\\nScikit-learn\\nAWS\\nDaily Responsibilities\\n85% hands on\\n15% Traveling\\nThe Offer\\nCompetitive Salary, DOE, bonuses\\nYou will receive the following benefits:\\nMedical Insurance & Health Savings Account (HSA)\\n401(k)\\nPaid Sick Time Leave\\nPre-tax Commuter Benefit\",\n",
       "  \"Prognos is a NYC-based healthcare startup whose mission is to improve health by driving the best actions learned from the world's data. In order to achieve this goal we have curated the world's largest clinical lab dataset --covering almost 200M patients in the US-- and are currently deploying cutting-edge technology for predicting disease at the earliest possible time.\\n\\nThe Mission of the Data Science team at Prognos is to develop, deploy and maintain AI/ML pipelines within all of Prognos' products, addressing business-relevant problems in close collaboration with Engineering, Clinical and Product teams.\\n\\nWe are looking for a hands on (IC) Senior Data Scientist to join the team, and help us move this mission-critical task forward. This position will be focused on helping us learn about patient health from medical time series. Are you interested in applying cutting edge deep learning techniques to complex data? Do you want to work on interpretability and uncertainty quantification in deep learning? Then come work with us!\\n\\nCandidates must have a degree in Computer Science, Statistics, Mathematics, Electrical Engineering or a related quantitative field; Masters or higher is preferred. At least 3 years of hands on machine learning (ML) experience with real world data is required.\\n\\nRequired Skills and Experience\\nStrong statistical analysis background with a deep understanding of a variety of ML algorithms\\nThorough understanding of modern deep learning techniques.\\nAbility to code a proof of concept in a short period of time (in a programming language of their choice)\\nWillingness to stay abreast of and evaluate recent advances in machine learning\\nExperience with relevant software tools, such as Python, PyTorch, Pyro, TensorFlow, Edward, Chainer, scikit-learn, statsmodels, and/or others. You'll be free to use whatever tools you feel are appropriate\\nExperience with cloud-based services such as AWS, Databricks or GCP\\nComfortable extracting data using SQL, Apache Spark or other distributed compute engine\\nAccustomed to working with git and shared codebases\\nExtensive data cleaning and data manipulation experience\\nStrong communication skills. Experience presenting findings to people who are not Data Scientists\\nAbility to collaborate with the team and translate existing research into practical solutions and products\\nAbility to build and maintain relationships with various collaborators across the company, and take ownership of data science projects\\nPreferred Skills and Experience\\nExperience with Bayesian/probabilistic approaches to deep learning is a big plus\\nExperience with complex, high dimensional, sparse time series data\\nExperience with distributed deep learning\\nExperience with healthcare data and/or insurance data a plus\\nPrognos Values & Culture\\nBe Collaborative: Always do what is best for the client. Check your ego at the door. We deliver premium value at a premium price. Practice blameless problem solving. Create win/win solutions.\\nBe Courageous: Do the right thing, always. Look at the facts and don't assume.\\nBe Curious: Think big and start small. Be relentless about improvement. Be predictive. Be curious and always ask why\\nBe Enthusiastic: Celebrate success. Be enthusiastic and positive. Let's have fun. Have purpose and believe in the greater good.\\nBe Driven: Act with a sense of urgency. Take ownership and honor commitments. Either find a way or make one. Deliver results.\\nBe a Superstar: Make quality personal. Deliver remarkable client service. Be better than your previous self. Go above and beyond.\\nOur culture guide: http://bit.ly/2ISMzjl\\nAbout Prognos\\n\\nPrognos is a healthcare AI company focused on predicting disease to drive decisions earlier in healthcare in collaboration with payers, Life Sciences and diagnostics companies. The Prognos Registry is the largest source of clinical diagnostics information in 40+ disease areas, with over 25B medical records for 200M patients. Prognos has 1000 extensive proprietary and learning clinical algorithms to enable earlier patient identification for enhanced treatment decision-making, risk management and quality improvement. The company is supported by a $42M investment from Safeguard Scientifics, Inc. (NYSE: SFE), Merck Global Health Innovation Fund (GHIF), Cigna (CI), GIS Strategic Ventures, Hikma Ventures, Hermed Capital, and Maywic Select Investments. For more information, visit www.prognos.ai.\\n\\nOur Mission\\n\\nTo improve health by driving the best actions learned from the world's data\\n\\nOur Vision\\n\\nTo prevail over disease and empower people everywhere to live life to the fullest\\n\\nSelected Perks\\nFree FreshDirect food, snacks, drinks, etc. delivered to the NYC office weekly\\nFlexible work arrangements and unlimited PTO\\nMonthly Happy Hours with Leadership\\nSome of our benefits include: Health Insurance, Life Insurance, Short Term and Long Term Disability, Dental, Vision, 401k, HSA, FSA, Dependent care flexible spending, commuter benefits, free access to One Medical Group, Gym discounts, flexible work hours and locations, access to the WeWork network, a Health Advocate, Employee Stock Option Plan, and more\",\n",
       "  'The One Amex team at American Express is a core foundational platform that helps drive decisions that impact our customer experience at American Express.\\n\\nThe platform is leveraged by various teams in the company to drive decisions, and customer experience.\\n\\nAs part of the Data Science team, you will be responsible for:\\nDriving requirements for structured data formats from a wide variety of data sources\\nBuild data science products across a wide range of digital data streams (Anomaly Detection, Deep Learning, Information Retrieval)\\nWork with engineering teams to deploy robust, highly available decisioning and alerting pipelines based on your models.\\nPlease note, Salary increases in case of a lateral move are provided only on an exception basis and in line with compensation guidelines.\\nRegularly attend key initiative stand-ups, proactively advising on opportunities to apply the best approach to apply underlying empirically-developed algorithms.\\nQualifications\\n\\n\\nQualifications should include:\\nDeep technical skills in data engineering, statistics, machine learning, or deep learning and a passion for making these methods more rigorous, robust and scalable\\nStrong programming skills in Python or Java\\nPractical experience working with and conducting experiments on large datasets then turning prototypes into production models in one or more domains\\nExperience applying analytical techniques to provide solutions to real business and engineering problems\\nAbility to explain and present analyses and machine learning concepts to a technical audience\\nEmployment eligibility to work with American Express in the U.S. is required as the company will not pursue visa sponsorship for these positions.\\n\\nWhy American Express?\\n\\nTheres a difference between having a job and making a difference.\\n\\nAmerican Express has been making a difference in peoples lives for over 160 years,\\n\\nbacking them in moments big and small, granting access, tools, and resources to take\\n\\non their biggest challenges and reap the greatest rewards.\\n\\nWeve also made a difference in the lives of our people, providing a culture of learning\\n\\nand collaboration, and helping them with what they need to succeed and thrive. We\\n\\nhave their backs as they grow their skills, conquer new challenges, or even take time to\\n\\nspend with their family or community. And when theyre ready to take on a new career\\n\\npath, were right there with them, giving them the guidance and momentum into the\\n\\nbest future they envision.\\n\\nBecause we believe that the best way to back our customers is to back our people.\\n\\nThe powerful backing of American Express.\\n\\nDont make a difference without it.\\n\\nDont live life without it.\\n\\nReqID: 19009478\\nSchedule (Full-Time/Part-Time): Full-time\\nDate Posted: May 22, 2019, 11:07:03 AM',\n",
       "  'Tracking Code\\n\\n1062432_RR00035566\\n\\nJob Description\\n\\nNYU School of Medicine is one of the nation\\'s top-ranked medical schools. For 175 years, NYU School of Medicine has trained thousands of physicians and scientists who have helped to shape the course of medical history and enrich the lives of countless people. An integral part of NYU Langone Health, the School of Medicine at its core is committed to improving the human condition through medical education, scientific research, and direct patient care. For more information, go to med.nyu.edu, and interact with us on Facebook, Twitter and Instagram.\\n\\nPosition Summary:\\n\\nWe have an exciting opportunity to join our team as a Data Analyst.\\n\\nThe Data Analyst will support a Robin Hood funded project, Babies In Shelter in the Department of Population Health. The purpose of the Robin Hood grant is to adapt an evidence-based postpartum depression prevention intervention for use among women in NYC homeless shelters. The data analyst will help support the teams ongoing effort to assess maternal outcomes in the third phase of the project (January-June 2020). During the 6 month outcomes phase, the study team will be collecting baseline and post-intervention surveys from 6 intervention sites and 3 control sites. The data collected in this phase of the project will help inform study teams recommendations for scaling of the project beyond the 18 month pilot phase.\\nThe data analyst will support the evaluation and data management needs of the project. The Data Analyst will be responsible for reviewing data entered by project coordinators, and analyzing quantitative and possibly qualitative data as well. We are seeking dynamic, dedicated and team-oriented candidates with exemplary organizational skills and ability to work independently and flexibly. Candidates will have strong programming and data management skills, including but not limited to proficiency in SAS, and experience in retrieving and analyzing data.\\n\\nJob Responsibilities:\\n1. Data Management: Develop, update and maintain existing data collection and analysis systems in support of the project.\\n2. Train and consult with Project team about data management. Attend weekly data meetings to insure data quality and provide training, guidance and consultation on data management.\\n3. Develop Data dictionary. Develop a data dictionary to inform data management, including all syntax and key variables for analysis.\\n4. Design data analysis plan. Consult and meet with the investigators to design and revise a data analysis plan that fits with the aims of the study.\\n5. Conduct data analysis. Conduct the data analyses and create tables and graphs to present to the team for review and discussion.\\n6. Develop reports. Create descriptive data summaries including visualizations and written descriptions of data findings for a wide range of audiences\\n7. Provide technical assistance to staff and analysts in accessing and analyzing datasets.\\n8. Publications. Support the development of publications and conference presentations, including drafting sections of publications and abstracts and creating tables and other data visualizations.\\nMinimum Qualifications:\\nTo qualify you must have a Bachelor\\'s Degree in pertinent field (public health, biostatistics, epidemiology, psychology) plus two years related experience or equivalent combination of education and experience.\\nCandidates with a Masters degree in pertinent fields are also encouraged to apply.\\nMinimum of four years of related experience with proven experience to design data management and analysis.\\nProficiency in using various data analysis programs, such as SAS, and Microsoft Office applications such as World, Excel, Access, Power Point and Outlook.\\nPreferred Qualifications:\\nExcellent oral and written communication, and interpersonal skills.\\nAbility to work within a team environment as well as independently.\\nTime management skills and ability to work well under pressure.\\nQualified candidates must be able to effectively communicate with all levels of the organization.\\n\\nNYU School of Medicine provides its staff with far more than just a place to work. Rather, we are an institution you can be proud of, an institution where you\\'ll feel good about devoting your time and your talents.\\n\\nNYU School of Medicine is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sex, sexual orientation, transgender status, gender dysphoria, national origin, age, religion, disability, military and veteran status, marital or parental status, citizenship status, genetic information or any other factor which cannot lawfully be used as a basis for an employment decision. We require applications to be completed online.\\n\\nIf you wish to view NYU School of Medicine\\'s EEO policies, please click here. Please click here to view the Federal \"EEO is the law\" poster or visit https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm for more information. To view the Pay Transparency Notice, please click here.\\n\\nCompany Location\\n\\nNYU School of Medicine\\n\\nDepartment\\n\\nPopulation Health-CEHD\\n(S1641)\\n\\nPosition Type\\n\\nPart-Time',\n",
       "  \"Description: </br>Participates in the development, validation and delivery of algorithms, statistical models and reporting tools. Solves moderately complex analytical problems.\\n\\n64411\\n\\n</br></br> Fundamental Components: </br>Develops, validates and executes algorithms and predictive models to investigate problems, detect patterns and recommend solutions. Explores, examines and interprets large volumes of data in various forms. Performs analyses of structured and unstructured data to solve moderately complex business problems. utilizing advanced statistical techniques and mathematical analyses. Develops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs. Uses data visualization techniques to effectively communicate analytical results and support business decisions. Creates and evaluates the data needs of assigned projects and assures the integrity of the data. Explores existing data and recommends additional sources of data for improvements. Documents projects including business objectives, data gathering and processing, detailed set of results and analytical metrics.</br></br> Background Experience: </br>Demonstrates good written and verbal communication skills. Able to present information to various audiences.Effectively resolves problems and roadblocks as they occur.Demonstrates proficiency in several areas of data modeling, machine learning algorithms, statistical analysis, data engineering and data visualization.Ability to work with large data sets from multiple data sources.3 years of relevant programming or analytic experience. Masters degree preferred.Bachelor's degree or equivalent work experience in Mathematics, Statistics, Computer Science, Business Analytics, Economics, Physics, Engineering, or related discipline.</br></br> Potential Telework Position: </br>No</br></br> Percent of Travel Required: </br>0 - 10%</br></br> EEO Statement: </br>Aetna is an Equal Opportunity, Affirmative Action Employer</br></br> Benefit Eligibility: </br>Benefit eligibility may vary by position. Click here to review the benefits associated with this position.</br></br> Candidate Privacy Information: </br>Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.\\n\\n#LI-DT1</br></br>\",\n",
       "  \"Senior Data Scientist\\n\\nWe are looking for an experienced data scientist to help drive our modeling and data transformation efforts across all sparks & honey products.\\n\\nAs a Senior Data Scientist at sparks & honey, you'll be balancing being your day-to-day work, collaborating with the larger dev team on product builds, and mentoring the team around you as you build the data and intelligence tools that help companies shape culture. You will work with data analysts, architects, engineers and developers to create the data handling rules and models for our growing portfolio of products and clients.\\n\\nAbout sparks & honey:\\n\\nIn 2012, we created a new agency based on the belief that cultural relevance is an important business driver. Six years later, we've expanded our NYC team to more than sixty individuals, and are rapidly growing our office in Los Angeles as well. We count some of the most important brands in diverse industries as our clients. Our leadership headlines industry events and contributes to academic think tanks, and our quarterly trend reports have been downloaded hundreds of thousands of times.\\n\\nWho We Are:\\n\\nWe're a blend of types A and B, but all of us are type C—for Culture. We make change visible and visceral—melding the best of man and machine. Our mission is to open minds and create possibilities. We place a premium on curiosity, exploration, edge-dwelling, and encouraging one another to challenge conventions and produce new norms. We value collaboration, co-creation, and openness.\\n\\nCultural Requirements:\\nInnate curiosity and keen interest in the intersection of data, research and product\\nA balance of collaborative, creative exploration; and attention to detail\\nYou enjoy working in an intense, fast-paced, agile, anti-disciplinary environment\\nWhat You'll Do:\\nGarnish and transform unstructured data to produce a new understanding of culture and business performance\\nEstablish new nomenclature and categorization schemes for ergodic data\\nDefine criteria, specifications and standards for the creation, storage and usage of operational and analytical data models\\nCreate measurement and evaluation frameworks, learning agendas, and test/experiment designs\\nBuild models; selecting appropriate variables, adjusting variable weights, testing, and readying your performant codebase for production implementation\\nEvolve existing data models, including the functions and entities within those functions to support emerging business needs\\nEstablish and enforce the means to monitor and communicate the meaning of data on ongoing basis for internal team and clients\\nSupport the overall master data governance process\\nEnsure strong collaboration with business teams on all projects from requirements gathering to client work executions\\nWho You Are/Experience Requirements:\\nA data scientist with 4-6+ years of experience in data science roles\\nThe possessor of a background in social or behavioral science is always a plus\\nSomeone with a strong background in statistics, including:\\nExperiment design, especially with non-linear and confounding variables\\nPrevious implementation of floating point macros a plus\\nA person with excellent forecasting and modeling chops (required), especially:\\nMarketing mix model variants\\nBayesian and stochastic forecasting models\\nPropensity modeling & propensity scoring\\nSomeone with 3+ years of significant experience working with complex unstructured and semi-structured data, and an excellent understanding of relational database concepts and performant system design, as well as experience extracting insights from large data sets using advanced Python\\nA producer of production ready code\\nA human used to working with BI tools for data visualization like Tableau, D3, Kibana\\nHappy to explore public speaking, presenting data and leading training sessions\\nExperienced in managing your own projects, vendor and client relationships, and supervising individuals\\nWork samples or case studies will be required\\n\\nWe are proud to be an Equal Opportunity/Affirmative Action Employer and committed to leveraging the diverse backgrounds, perspectives and experience of our workforce to create opportunities for our colleagues and our business. We do not discriminate in employment decisions on the basis of any protected category.\",\n",
       "  \"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.Facebook is seeking an experienced data and research professional to provide data driven insights around how News media partners can leverage Facebook's products to perform successfully. The position will drive key strategic data initiatives and market analysis, as well as providing actionable recommendations around optimizing partners performance in the News ecosystem. The role will be part of our News Partner Solutions team and is based at the Facebook New York.\\n\\nResponsibilities:\\n\\nDrive research and measurement strategy and manage prioritization of initiatives across internal and external stakeholders\\nBe able to interpret data and structure analyses to identify trends and actionable insights to drive strategic partnerships and industry communications\\nBuild/maintain reports, dashboards and metrics to monitor the team's key performance indicators\\nWork with global team to scale programs\\nDeliver insights and make recommendations to partners\\nWork closely with Partner Solutions Managers and Strategic Partner Managers to educate and act as a consultant for internal stakeholders, scale data analysis in the region\\nAbility to analyze data to tell a story and make recommendations\\nQuery internal data and provide ongoing reporting/analysis about News partners and ecosystem performance for new product launches or company priorities\\nBuild dashboards to support News Media Partnerships, both within the US and globally\\nAnalyze and segment partners to better understand our ecosystem and allocate resources\\nCreate and manage data pipelines to aggregate data for News Media Partnerships team\\nManage requests and priorities while creating presentations for internal use with cross-functional teams\\nWork with other analysts to understand how major media publishers, journalists, and the News industry are using Facebook and Instagram to connect and share with people around the world\\nMininum Qualifications:\\n\\nBA/BS degree or higher\\n5+ years experience, including leading research/measurement projects\\nExperience working in Hadoop/Hive, SQL and Excel to drive analysis and reporting of trends\\nExperience working independently and a track record of taking initiative\\nCommunication and presentation experience\\nExperience thinking strategically about issues, leading to recommendations and action plans\\nExperience working cross-functionally with all levels of management, both internally and externally\\nPreferred Qualifications:\\n\\nUnderstanding of Tableau or similar software and experience with a statistical programming language (R, python, etc.)\\nUnderstanding of Comscore and other digital analytics tools\\nExperience in a consumer technology and/or media company\",\n",
       "  \"Job Description\\nSUMMARY OF POSITION\\n\\nThis full-time position involves working with the application development and database teams to provide support and light development work. Specifically, this will include end user support; needs analysis; software administration, maintenance, testing, and report development.\\n\\nESSENTIAL RESPONSIBILITIES\\n\\n§ Participate and contribute in all phases of software development and/or deployment Ensure systems design/configuration adhere to user specifications and agency security standards\\n\\n§ Administration, maintenance and testing of various Enterprise software systems\\n\\n§ End user systems support\\n\\n§ User/business needs analysis and requirements documentation\\n\\n§ Custom reports development and deployment using various tools such as SSRS, Tableau etc.\\n\\n§ Participate in all other IT related initiatives as required\\n\\nQUALIFICATIONS\\n\\n§ Bachelor’s degree in Computer Science or Information Technology\\n\\n§ At least two years of experience in systems development, deployment, end user support or database engineering/administration\\n\\n§ Must be a self-started and have the ability to work independently\\n\\n§ Must have excellent knowledge of SQL Server (stored procedures, functions, triggers, views, linked servers)\\n\\n§ Must have SSRS experience\\n\\n§ Experienced in diverse data extraction, integration, scrubbing, and transformation\\n\\n§ Strong written and oral communication skills\\n\\n§ Ability to manage all tasks and assignments to given timeframes\\n\\n§ Some Java experience preferred\\nCompany Description\\nForrest Solutions is the nation's first and leading onsite outsourcing and staffing firm providing enterprise-wide people solutions on an onsite outsourced, direct hire, temporary, temp-to-hire, or consulting basis.\\n\\nWe are here to provide strategic counsel and unparalleled service personnel.\\n\\nForrest Solutions' leadership team are industry experts who have introduced transformative solutions to thousands of the world's best known brands, and created tens of thousands of career opportunities. Our goal is to continually provide clients with the industry's best people, solutions and technology.\\n\\nNo other company has the breadth of experience, cutting edge technology, or deeper bench of talent. We hire and train people who are passionate about their purpose, excited about what they do, and are committed to your success.\",\n",
       "  \"The Opportunity\\n\\nWe are a small, early stage, high growth company focused on using data science to transform the way people are hired. We seek an inventive, collaborative, methodologically-grounded, experienced, socially-minded data scientist to join our team. Reporting to the Vice President of Data Science, this individual will lead the exploration of applicant and employee data streams that Arena collects and pioneer new development of machine learning and AI methods suited for that data. He/she will also work closely with Arena's Engineering and Product teams to ensure the successful implementation and refinement of such methods.\\n\\nMission and culture\\n\\nAt Arena our mission is simple: use data to transform the workforce such that people and organizations thrive. If you ask anyone at Arena about our company culture the first word you'll probably hear is collaborative. While it's true the people here are all intelligent, hardworking, and capable, we are also people who are open to criticism and check our egos at the door. We are charting new territory, and empathy and humility are critically important to our journey.\\n\\nThis is just the beginning of a revolution in how employers use data to make better HR decisions and how individuals are able to find fulfilling work beyond the bounds of bias.\\n\\nWe enjoy substantial funding and are building out our team as we gain market share.\\n\\nWhat We Believe In:\\nEmpathy with one another and our users. Inclusiveness, communication and collaboration are core tenets of how we work.\\nContribution to the open source community. We are built on open source technologies and we give back.\\nSmall feedback loops are critical to developing product and technology.\\nOwnership gives individuals the agency to grow and aligns outcomes.\\nResponsibilities\\nDevelop innovative solutions to business, product or technical requests using appropriate Data Science techniques\\nPrototype solutions using appropriate machine learning or statistical models and techniques; assist the engineering team in implementing such solutions\\nReview existing processes, approaches, and models related to data science; propose and initiate solutions and/or improvements\\nDisseminate knowledge and techniques; solidify best practices by contributing to internal tools or libraries; mentor junior data scientists\\nContribute code and documentation to the company code/knowledge base\\n\\nRequirements\\n3+ years practical experience in industry product as a Data Scientist in a production environment\\nExcellent Python (numpy, pandas, scikit-learn, etc.) or R programming skills, familiar with open source libraries and tools for data science\\nTrack record of speedily and rigorously developing and deploying machine learning models to resolve industry problems\\nStrong SQL skills\\nSuperior communication and data visualization skills\\nExperience working in an agile development environment\\n\\nBonus Skills\\nMaster's or PhD in a quantitative field (such as computer science, statistics, a quantitative social, biological, or physical science) is preferred but not required\\nExperience with Natural Language Processing, deep learning, experimental design and A/B testing, recommender systems or other deployed data products\\nExperience working in software development\\nExperience with sophisticated statistical or econometrical models\\nPerks\\nThe mission, the people and the work.\\nThe mission: work on a project that is making the world a better place.\\nThe people: work for a high growth company, and learn in an environment where people care about growth and work/life balance.\\nThe work: pragmatically use amazing technology.\\nCompetitive salary, benefits and equity.\\nPlease visit us at: https://www.arena.io/about/careers.html\",\n",
       "  'About Outcome Health\\n\\nWe Are:\\nA healthcare innovation company\\n\\nOur Strategy:\\nReinventing in the point of care\\n\\nOur Purpose:\\nFacilitating a better personal Outcome\\n\\nWhether it’s supporting the diagnosis of a family member or as patients ourselves, we all have connections to healthcare. That’s why all of us at Outcome Health are committed to transforming the point of care experience, both for our loved ones and for ourselves.\\n\\nWe are looking for people to bring their diverse talent, perspectives and career experiences to help us create the future in point of care.\\n\\nJoin our expanding team and be part of creating #ABetterOutcome.\\n\\nResponsibilities:\\nReasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\\nCreate reports to track the performance of live advertising campaigns and publish them to both internal stakeholders (sales, client success, executive team) and clients\\nEnsure live campaigns are deployed accurately to their target audiences, and consistently verify that targeting requirements match contractual obligations\\nImprove inventory yield by surfacing underutilized pockets of inventory and helping the sales team target accordingly\\nCreate and apply sophisticated optimization algorithms to effectively allocate available inventory against sales planning demands\\nSupport third-party audit firms with access to production datasets, business logic, and any additional materials needed for them to verify ad delivery\\nSupport third-party industry measurement partners in obtaining datasets on program exposure and campaign impressions\\nServe as subject matter expert on key internal sales tools, such as Proposal Generation Tool, and provide training/coaching to new sales/analyst hires\\nWrite complex queries to analyze data housed in various formats (e..g, raw files, data lake, data warehouse)\\nCollaborate with business stakeholders to elicit, understand, and articulate their business needs\\nIteratively build, prototype, and automate core datasets, reports, and dashboards to provide insights at scale, solving for the team’s analytical needs\\nMaintain comprehensive end-user documentation on release features and bug fixes\\nTriage end-user support tickets and escalate any bugs as necessary\\n\\nQualifications\\n2+ years experience analyzing large datasets and writing SQL queries in a data warehouse environment\\n2+ years experience using Python or R for statistical data analysis\\n2+ years experience with Tableau or another data visualization/dashboarding tool\\nPrior experience using a CRM tool (such as Salesforce) is preferred\\nStrong team collaborator; always solutions driven, and able to identify signals within the data\\nAbility to work independently; self-discipline to effectively juggle multiple campaign deliverables, proactive attitude to quickly identify issues, and a drive to exceed client expectations\\n\\nRequired Experience\\n2+ years prior experience in a data analyst role\\n\\nPreferred Experience\\n5+ years prior experience in a data analyst role\\nPrevious exposure to healthcare and digital advertising industries\\n\\nTravel\\nTravel may occasionally be required but will make up <10>\\n\\nOther Duties\\nPlease note this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities and activities may change at any time with or without notice.\\n\\nEEO Statement\\nOutcome Health does not discriminate in employment opportunities or practices on the basis of age, race, gender, gender identity (including gender nonconformity and status as a transgender or transsexual individual), gender expression, color, religion, creed, national origin, ancestry, sex (including pregnancy), medical condition, physical or mental disability, marital status, covered veteran status, sexual orientation, status with respect to public assistance, citizenship, genetic information, past, current or prospective service in the uniformed services, and other characteristics protected under applicable state, federal, or local law.',\n",
       "  'Position Description Overview\\n\\nInterested in stopping fraud before it occurs? Join Fiserv\\'s FirstSense fraud intelligence team to work in a highly collaborative setting to deliver a cutting-edge service that uses advanced analytics, dark web monitoring and financial transaction research to mitigate fraud. Our team needs a strong data analyst/data scientist to contribute to the reporting we produce for our customers.\\n\\nJob Responsibilities\\nProcess, clean, and verify the integrity of data used for analysis\\nWork on end-to-end analytical projects, from initial identification of business needs, through to completion\\nCoordinate with different functional teams to integrate data sets and implement models\\nAnalyze large sets of data to develop new insights into potential fraudulent behavior and create models to identify these patterns in broader datasets utilizing the multiple Analytical Stacks like R, Python, Spark, SAS, SQL MongoDB, etc.\\nCommunicate complex concepts to business stakeholders in a clear and concise manner\\nBecome familiar with payment card fraud the fraud life-cycle and become well-versed in the dark web threat landscape\\nJob Requirements\\nMinimum of 3 years of relevant work experience (5 years preferred)\\nStrong project management and attention to detail to bring projects to fruition on time\\nExcellent written and verbal communication skills for coordinating across teams\\nA drive to learn and master new technologies and techniques\\nStrong problem solving skills\\nKnowledge of advanced statistical techniques and concepts\\nExperience working with and creating data architectures\\nExperience using statistical computer languages (R, Python, SLQ, etc.) to manipulate data and draw insights from large data sets\\nCoding knowledge and experience with several languages: C, C , Java, JavaScript, etc\\nExperience with distributed data/computing tools: Map/Reduce, Hadoop, Hive, Spark, Gurobi, MySQL, etc\\nKnowledge of a variety of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and associated advantages and drawbacks\\nStrong problem solving skills\\nDegrees\\nBachelor of Science\\nMaster of Science\\nLearn more about Fiserv:\\n\\nLife moves fast. And as it does, we know most people aren\\'t thinking about \"financial services\" But we are.\\n\\nWe help people and businesses move money and information every minute of every day. Our solutions connect financial institutions, corporations, merchants and consumers to one another, millions of times a day, behind the scenes, reliably and securely.\\n\\nWe\\'re Fiserv, a global leader in Fintech and payments enabling innovative financial services experiences that are in step with the way people live and work today. The company\\'s approximately 44,000 associates proudly serve clients in more than 100 countries, so their customers, members and consumers can move money when and where they need it, at the point of thought.\\n\\nOur Aspiration is to move money and information in a way that moves the world. As a FORTUNE 500 company and one of FORTUNE Magazine World\\'s Most Admired Companies for the sixth consecutive year, we are committed to excellence and purposeful innovation.\\n\\nWe welcome and encourage diversity in our workforce. Fiserv is an equal opportunity employer/disability/vet\\n\\nExplore the possibilities of a career with Fiserv and Find Your Forward with us.\\n\\nSDL2017\\n\\n</br>',\n",
       "  'Sr Data Engineer - Product\\n\\n\\nApply\\n\\nRef#: 34702\\n\\nCBS Business Unit: Showtime\\n\\nJob Type: Full-Time Staff\\n\\nJob Schedule: Full-Time\\n\\nJob Location: New York, NY, US\\n\\nDescription:\\nThe Showtime Product team is looking for an experienced, curious and creative Sr. Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.\\n\\nIn this role, you will be an integral part of our data engineering team and collaborate with our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team, and our in-house backend and front-end engineering groups. You will work with the rest of the data team to architect solutions and develop technologies, systems and workflows that enable our analysts and data scientists to focus on algorithms and analyses rather than on the associated engineering.\\n\\nIdeal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.\\n\\nKey Technologies: Java, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA, etc\\n\\nResponsibilities:\\n\\n● Develop understanding of key business, product and user questions.\\n\\n● Collaborate with other Product Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.\\n\\n● Evolve data-driven feature prototypes into production features that scale\\n\\n● Streamline feature engineering, so that the underlying data is efficiently extracted.\\n\\n● Build flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.\\n\\n● Develop and enhance our data warehouse in AWS S3.\\n\\nQualifications:\\nYou have at least 3 years of relevant experience in a comparable data engineering role\\nYou have expert-level knowledge of SQL/Spark SQL\\nYou have experience in pursuing and launching data-backed decisions, such as recommendations, to make end-user-facing products better\\nYou like to dive-deep on data questions to come up with effective solutions\\nYou believe in writing code that is easy to understand, test and maintain\\nYou thrive in a workplace that values autonomy, applauds ideas and a enjoys a sense of humor\\n\\nAbout Us:\\nSHOWTIME and its critically-acclaimed, award-winning original series continue to make their mark on the cultural landscape, with one of the most successful programming slates in all of television. With an impressive line-up of new and returning original series, the SHOWTIME hit dramas and comedies include HOMELAND, SHAMELESS, BILLIONS, RAY DONOVAN, THE AFFAIR, SMILF, THE CHI, KIDDING, ESCAPE AT DANNEMORA and BLACK MONDAY. Original series play a key part in the SHOWTIME programming mix, along with box office hits, comedy and music specials, provocative documentaries, and hard-hitting sports programming, including the flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning veteran series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue, and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at www.showtime.com and www.showtimeanytime.com.\\n\\nEEO Statement:\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled\\n\\nApply',\n",
       "  'The Ads Localization and International Expansion team owns and builds services and applications across Amazon WW Advertising focused on removing language barriers that prevent Advertisers from advertising globally across. Our mission is to enable millions of advertisers around the world to reach shoppers in their language of preference and accelerate their path to becoming global advertisers and brands. In addition, we are responsible for orchestrating the rollout of mission-critical software to enable the Advertising organization to expand its footprint internationally.\\nIn this role, you will build solutions around our multi-channel and multi-lingual advertising offerings for our clients that connects technology, data, machine learning, human translators and marketing assets to deliver the best in class experience to our advertisers. You will use ideas from every facet of Data Science including Natural Language Processing, Speech to Text analysis, Machine Learning and artificial intelligence.\\n\\nThe charter of this team is to eliminate all barriers that prevent Amazon customers from advertising world wide. We are innovating in this space by building and rolling out new capabilities to allow advertisers to seamlessly launch campaigns across the globe in multiple markets. One of the key challenges that we are currently working on is to enable human independent translation workflow\\'s for advertiser content, which is extremely difficult to automate through machine translations. The challenge is to rollout solutions which can be scaled without sacrificing the translation quality (critical to Advertiser content). We are also rolling out solutions for our clients to cover video translations and the ability to scale seamlessly between machine and human translations based on detailed analysis of Advertiser content.\\n\\n\\nYou will LOVE this opportunity if you are:\\n· Highly analytical: You solve problems backed with verifiable data. You focus on driving processes, tools, and statistical methods that support rational decision-making.\\n· Technically fearless: You arent satisfied by performing as expected and push the limits past conventional boundaries.\\n· Team obsessed: You help grow your team members to achieve outstanding results. You foster a creative atmosphere to let scientists and engineers innovate, while holding them accountable for making smart decisions and delivering results.\\n· Ambitious: Youre ambitious, yet humble. You recognize that theres always opportunity for improvement - using introspection, feedback from teammates and peers to \"raise the bar\" for your team.\\n· Engaged by ambiguity: Youre able to explore new problem spaces with unique constraints and non-obvious solutions.\\n\\n\\n\\nBasic Qualifications\\n\\n· M.S. or Ph.D. in Computer Science, Machine Learning, Statistics, Applied Mathematics, Information Retrieval, or related discipline\\n· Breadth and depth knowledge in machine learning algorithms and best practices.\\n· At least 5 years of hands-on experience in building Machine Learning solutions to solve real-world problems.\\n· At least 3 years of experience with computer science fundamentals in object-oriented design, data structures, algorithm design, problem solving, and complexity analysis.\\n· At least 3 years of experience with, at least, one modern programming language such as Java, Python, Scala, C++\\n\\nPreferred Qualifications\\n\\n· Ph.D. Degree in quantitative field with a strong Machine Learning background\\n· Experience in building large-scale machine-learning models for online recommendation, ads ranking, personalization, or search, etc.\\n· Experience with Big Data technologies such as AWS, Hadoop, Spark, Pig, Hive, Lucene/SOLR or Storm/Samza\\n· Strong proficiency with Java, Python, Scala or C++\\n· Experience in computational advertising technology is a big plus\\n· Published research work in academic conferences or industry circles.\\n· Excellent oral and written communication skills, with the ability to communicate complex technical concepts and solutions to all levels of the organization\\n\\n\\n\\nAmazon is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation',\n",
       "  'Excited by using massive amounts of data to develop Machine Learning (ML) and Deep Learning (DL) models? Want to help the largest global enterprises derive business value through the adoption of Artificial Intelligence (AI)? Eager to learn from many different enterprises use cases of AWS ML and DL? Zealous to be key part of Amazon, who has been investing in Machine Learning for decades, pioneering and shaping the worlds AI technology?\\n\\nAt Amazon Web Services (AWS), we are helping large enterprises build ML and DL models on the AWS Cloud. We are applying predictive technology to large volumes of data and against a wide spectrum of problems. Our Professional Services organization works together with our AWS customers to address their business needs using AI.\\n\\nAWS Professional Services is a unique consulting team. We pride ourselves on being customer obsessed and highly focused on the AI enablement of our customers. If you have experience with AI, including building ML or DL models, wed like to have you on our team. You will get to work with an innovative company, with great teammates, and have a lot of fun helping our customers.\\n\\nThis role will focus specifically on AWS most complex and largest customers in the world to help solve a wide range of business problems. Consultants will provide deep and broad insight to customers and partners to help remove constraints that prevent them from leveraging AWS services to create strategic value.\\n\\nA successful candidate will be a person who enjoys diving deep into data, doing analysis, discovering root causes, and designing long-term solutions. It will be a person who likes to have fun, loves to learn, and wants to innovate in the world of AI.\\n\\nMajor responsibilities include:\\n· Understand the customers business need and guide them to a solution using our AWS AI Services, AWS AI Platforms, AWS AI Frameworks, and AWS AI EC2 Instances .\\n·\\n· Assist customers by being able to deliver a ML / DL project from beginning to end, including understanding the business need, aggregating data, exploring data, building & validating predictive models, and deploying completed models to deliver business impact to the organization.\\n·\\n· Use Deep Learning frameworks like MXNet, Caffe 2, Tensorflow, Theano, CNTK, and Keras to help our customers build DL models.\\n·\\n· Use SparkML and Amazon Machine Learning (AML) to help our customers build ML models.\\n·\\n· Work with our Professional Services Big Data consultants to analyze, extract, normalize, and label relevant data.\\n·\\n· Work with our Professional Services DevOps consultants to help our customers operationalize models after they are built.\\n·\\n· Assist customers with identifying model drift and retraining models.\\n·\\n· Research and implement novel ML and DL approaches, including using FPGA.\\n·\\nThis is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.\\n\\nNote: If you do not live in a market where we have an open Data Scientist position, please feel free to apply. Our Data Scientists can live in any location that has an AWS office.\\n\\nBasic Qualifications\\n\\n· A Bachelor or Masters Degree in a highly quantitative field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.) or equivalent experience\\n· 6+ years of industry experience in predictive modeling, data science and analysis\\n· Previous experience in a ML or data scientist role and a track record of building ML or DL models\\n· Experience using Python and/or R\\n· Experience using ML libraries, such as scikit-learn, caret, mlr, mllib\\n· Experience in writing and tuning SQL\\n· Experience with SparkML\\n· Experience working with GPUs to develop model\\n· Experience handling terabyte size dataset\\n· Experience using data visualization tools\\n\\nPreferred Qualifications\\n\\n· PhD in a highly quantitative field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.)\\n· 8+ years of industry experience in predictive modeling and analysis\\n· 3+ years of experience delivering technical solutions and cloud services to global Media & Entertainment (industry/vertical) customers\\n· Skills with programming languages, such as Java or C/C++\\n· Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations\\n· Consulting experience and track record of helping customers with their AI needs\\n· Publications or presentation in recognized Machine Learning, Deep Learning and Data Mining journals/conferences\\n· Experience with AWS technologies like Redshift, S3, EC2, Data Pipeline, & EMR\\n· Combination of deep technical skills and business savvy enough to interface with all levels and disciplines within our customers organization\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment\\n· Able to write production level code, which is well-written and explainable\\n· Experience diving into data to discover hidden patterns\\n· Past and current experience writing and speaking about complex technical concepts to broad audiences in a simplified format\\n· Experience giving data presentations\\nAmazon is an Equal Opportunity Employer Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.',\n",
       "  \"Hiring in Los Angeles, CA | New York, NY\\n\\nEmail careers@edo.com for more information about EDO and the application process.\\n\\nWho We Are\\n\\nEDO is a data science software firm that develops analytics tools to make data accessible and actionable for the media and entertainment industry. Currently focused on film and TV, we work with many major movie studios and TV networks to help them forecast, market and distribute their content more effectively. Building from this strong base, we are growing into adjacent verticals.\\n\\nWe are a team of world-class engineers and data scientists backed by top leaders in entertainment and technology. Our co-founders and executive leadership have a strong track record with other successful ventures.\\n\\nWhat You Will Do\\n\\nIn addition to standard client engagement responsibilities, you'll get to\\nPerform advanced statistical analyses for our clients, who will use the insights and opinions you generate to make key decisions\\nUnderstand our clients’ needs and figure out how to present our data in compelling ways that match those needs\\nCreate client deliverables that showcase our products and present our data\\nWork closely with our engineers and data scientists to plan and prioritize new features\\nUnderstand industry dynamics and the competitive landscape\\nHelp our executive leadership with strategic planning, investor pitches, strategic partnership development, and corporate finance activities\\n\\nWhat We Are Looking For\\n\\nAt least 1-2 years of professional experience in a role involving on quantitative analysis\\nComfort with advanced statistical analysis and ability to translate into layman’s terms\\nExperience in SQL and with a programming language commonly used for statistical analysis such as R or Python\\nAbility to write well and create slide decks and reports that present our information in a succinct and efficient manner\\nStrong presence in meetings with senior executives\\nPassion for movies, TV and advertising\\nThe ability and willingness to wear multiple hats and switch gears frequently\\n\\nBenefits\\n\\nEarly-stage equity and competitive salary\\nMedical, dental, & vision insurance\\nMeals and snacks during work\\nMovie tickets + concessions, fitness discounts, and Apple hardware\",\n",
       "  'We are looking for a seasoned Data Science Practice Lead, who will be responsible for driving excellence in Analytics and Data Science across the whole company, with wide-ranging influence over how Data Science is practiced in our diverse community over 300+ people. They will impact our choice of tools, our documentation and training, career progression, and the development of our data infrastructure. Above all, they will have a chance to define the “Spotify Way” for Data Science, in partnership with other leaders in the Insights organization.\\n\\nWhat you’ll do\\nThis person will lead a team dedicated to the Data Science practice, within our Platform Mission, working alongside the teams that build our data tools and infrastructure.\\nWorking across our embedded community of data scientists and engineers — and partnering closely with central tooling and infrastructure teams — you’ll lead Spotify’s effort to minimize duplication and hours spent on discovering, generating and preparing data to a state where it’s optimised for insights.\\nBy engaging across disciplines you’ll develop a practice that’s easy for others to adopt and is equally easy for others to contribute to. You’ll build and maintain an evolving set of shared principles, shared tools, shared knowledge and most importantly shared behaviours for how we produce and manage data for insights across the company.\\nTo tackle the data for insights challenges at Spotify, we’ll need to work across the entire data preparation workflow, including generating data through instrumentation, normalising data into easy to query tables and producing flat tables that are ready to analyze.\\nYou’ll work with data scientists to cultivate a shared practice for defining data collection requirements across all of our experiences. You’ll partner with central tooling and infrastructure teams to make sure it’s trivially easy for these requirements to be implemented by product teams.\\nWork with dedicated data production teams to identify opportunity to produce datasets centrally. You’ll be the lead designer for Spotify’s data, modeling data across the company and helping teams to identify and design data interoperably with their bounded context.\\nYou’ll work closely with tooling and infrastructure teams to ensure we have the tools we need to make it easy to produce and manage data for insights. When input is not enough, you’ll embed with our partners to help design, build or activate shared data, tooling and infrastructure.\\nWho you are\\nYou’re passionate about the role of modelled, analyst friendly data in maximizing a company’s potential as a learning organisation. Your passion for good practice is infectious. As you continuously improve your own practice, Spotify’s practice will improve alongside you.\\nYou’re collaborative. You work hands on with a wide variety of teams to ensure that both the most recent needs of the community are reflected in our shared practice and that behaviours of the community are changing to adopt it. When there are competing shared practices you facilitate discussions, providing your experienced perspective where needed, in order to establish a single view, e.g. the guiding principles or when to use which practice. When infrastructure is missing or incomplete you surface the needs and work with infra teams to ensure that they are understood.\\nYou’re innovative. You’re a builder. You show people the value of your ideas by bringing them to life in tangible, innovative ways. You work closely with central engineering and infrastructure teams to tackle the most complex challenges at Spotify are addressed in a sustainable way.\\nYou’re playful. You’re always looking for new ways to engage across disciplines to build engagement and cultivate a shared sense of ownership of the practice.\\nYou’re sincere. As community lead, you take an active interest in your own development and the development of other data scientists across the organisation.\\nYou have 5+ years as a practicing data scientist, analyst or data/analytics engineer.\\nYou have a high level of ability in data science languages such as SQL and Python and/or R.\\nBreadth of experience more important than expertise in one specific area.\\nYou have previous experience leading data scientists in developing and adopting shared practice is essential (e.g. building an internal community of practice, contributing to open source)\\nYou have extensive experience working with engineers, product managers and designers in a product environment to decide what data to collect and how to collect it, to enable the data science downstream.\\nYou have (ideally) 4+ years experience with data modelling in a variety of contexts (e.g. web sites, applications, financial) and ideally in variety of use cases (e.g. preparing data for your own ad hoc data science projects, enterprise data warehousing and/or modelling data for software applications).\\nYou have (ideally) a strong command of Domain-Driven Design principles, with experience applying it to software or data design at an enterprise scale.\\nWe are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music.',\n",
       "  'Excited by using massive amounts of data to develop Machine Learning (ML) and Deep Learning (DL) models? Want to help the largest global enterprises derive business value through the adoption of Artificial Intelligence (AI)? Eager to learn from many different enterprises use cases of AWS ML and DL? Thrilled to be key part of Amazon, who has been investing in Machine Learning for decades, pioneering and shaping the worlds AI technology?\\n\\nAt Amazon Web Services (AWS), we are helping large enterprises build ML and DL models on the AWS Cloud. We are applying predictive technology to large volumes of data and against a wide spectrum of problems. Our Professional Services organization works together with our AWS customers to address their business needs using AI.\\n\\nAWS Professional Services is a unique consulting team. We pride ourselves on being customer obsessed and highly focused on the AI enablement of our customers. If you have experience with AI, including building ML or DL models, wed like to have you join our team. You will get to work with an innovative company, with great teammates, and have a lot of fun helping our customers.\\n\\nIn our Global Specialist Practice, you will also have the opportunity to create white papers, write blog posts, build demos and other reusable collateral that can be used by our customers, and you will work closely with our Solution Architects and Service Engineering teams.\\n\\nIf you do not live in a market where we have an open Data Scientist position, please feel free to apply. Our Data Scientists can live in any location where we have a Professional Service office.\\n\\nA successful candidate will be a person who enjoys diving deep into data, doing analysis, discovering root causes, and designing long-term solutions. It will be a person who likes to have fun, loves to learn, and wants to innovate in the world of AI. Major responsibilities include:\\n\\n· Understand the customers business need and guide them to a solution using our AWS AI Services, AWS AI Platforms, AWS AI Frameworks, and AWS AI EC2 Instances .\\n· Assist customers by being able to deliver a ML / DL project from beginning to end, including understanding the business need, aggregating data, exploring data, building & validating predictive models, and deploying completed models to deliver business impact to the organization.\\n· Work with our Professional Services Big Data/Analytics consultants to analyze, extract, normalize, and label relevant data.\\n· Work with our Professional Services consultants and customer teams to help our customers operationalize models after they are built.\\n· Assist customers with identifying model drift and retraining models.\\n· Research and implement novel ML and DL approaches, including using FPGA.\\n\\n\\nThis is a customer facing role. You will be required to travel to client locations and deliver professional services when needed.\\n\\n\\nBasic Qualifications\\n\\n· B.S. degree in mathematics, statistics, computer science or a similar quantitative field\\n· 5+ years work experience in relevant field\\n· Experience in using SQL to analyze data in a database or data warehouse and be able to use a major programming (e.g. Java/C) and/or a scripting language (Perl, Unix shell, Python) to process data for modeling\\n· Experience working with a wide range of predictive and decision models and data mining techniques, as well as tools for developing such models\\n\\nPreferred Qualifications\\n\\n· Experience in data modeling, ETL development, and Data warehousing.\\n· Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets\\n· Data Warehousing Experience with Oracle, Redshift, etc\\n· Experience with software coding practices is a strong plus.\\n· Experience using Linux/UNIX to process large data sets\\n· Meets/exceeds Amazons functional/technical depth and complexity for this role\\n· Experience with AWS technologies like Redshift, S3, EC2, Data Pipeline, & EMR\\n· Combination of deep technical skills and business savvy enough to interface with all levels and disciplines within our customers organization\\n· Demonstrable track record of dealing well with ambiguity, prioritizing needs, and delivering results in a dynamic environment\\n\\nAmazon is an Equal Opportunity Employer Minority / Women / Disability / Veteran / Gender Identity / Sexual Orientation / Age.',\n",
       "  \"Nomad Health is the first digital marketplace for healthcare jobs, efficiently connecting quality clinicians with rewarding career opportunities. Our technology takes the busywork out of finding clinical work. The U.S. healthcare system is experiencing a staffing crisis. Employers spend $20 billion per year recruiting clinicians to care for the rapidly aging U.S. population. Nomad replaces antiquated staffing agencies with modern technology to efficiently source, qualify, and hire medical talent on demand. Clinicians find better jobs with higher pay. Employers fill roles faster with higher quality care.\\n\\nNomad is a fast growing team of technologists, creators, and industry experts passionate about modernizing healthcare staffing so doctors and nurses can get back to the work they do best: caring for others.\\n\\nWho are you + what will you do at Nomad?\\n\\nNomad is growing fast and accumulating a wealth of data on which to capitalize. You will be at the center of efforts to analyze, understand, and visualize data to drive growth and new product initiatives. You love everything about data: cleaning it, transforming it, analyzing it, visualizing it, and using it to tell stories. You are intellectually curious and unafraid to ask questions. You are driven and eager to answer tough questions to help others. You are great at communicating insights to promote actionable change, and love to automate manual processes with data. You work autonomously, while also collaborating across the organization. You’re comfortable speaking technically with developers and engineers about data needs, but just as comfortable digging into business questions and presenting to senior leaders. You want to develop your passion for data into a career at a rapidly growing company with ambitions to be the source of all healthcare clinician data.\\n\\nThis position reports to the VP of Product, but you will interact and work with every team and leader at Nomad, including the COO and VP of Business Development.\\n\\nYour Responsibilities\\n● Implement, administer and utilize our analytical tools to provide insights that drive Nomad’s rapid growth - We use Looker and Amplitude.\\n● Own ad hoc analyses on operational efficiency, business trends & performance, user behavior, and more.\\n● Develop compelling, logically structured presentations (including story-telling of research/analytics findings) to share insights at all levels of the company on a regular basis.\\n● Represent Nomad's data needs and collaborate with product management and engineering on new initiatives.\\n● Partner with Product and Design to analyze user behavior, set metrics and measure outcomes of product releases and inform Nomad’s Product strategy and roadmap.\\n● Partner with the Operations and Growth teams to improve workflows and processes, and ensure data collection is consistent and accessible.\\n● Define, design, and build dashboards to track objectives and key results.\\n● Champion data throughout Nomad; consistently work to train others and share knowledge to support a self-sufficient analysis culture.\\n● Clean, document, and transform raw data into accessible data formats for our analytics tool stack.\\n● Contribute to the overall data strategy to simplify access to data and improve the overall data foundations for reporting and analytics.\\n● Respond to (or triage) data requests with minimal guidance or oversight.\\n● Work independently to prioritize your time across multiple projects.\\nWhat we’re looking for:\\n\\n● 3+ years of experience in an analytical/data-centric role, preferably in a fast-paced, startup environment.\\n● Bachelor's Degree in Computer Science, Mathematics, Statistics preferred, or a related field and experience with exceptional analytical skills and attention to detail.\\n● Experience with programming languages such as R, Matlab, Python, SQL, etc.\\n● An Excel pro with business intelligence tools experience (Looker, Tableau, Qlik, Amplitude, etc.). Experience developing in Looker's LookML is a plus.\\n● Understanding of basic Statistics.\\n● Working knowledge of relational (SQL) and/or non-relational (Mongo DB) databases a plus.\\n● Familiar and comfortable with data technology (Hadoop, Redshift, Big Query).\\n● Strong communication skills.\\n\\nNomad offers a fast-paced, supportive, diverse culture. Benefits include comprehensive health, dental, and vision plans, free snacks and drinks, generous parental leave, gym discounts, regular team outings, and a whole lot more.\\n\\nExciting challenges lie ahead. Join us! Let's get to work.\",\n",
       "  'Title: Research Scientist - Machine Learning\\n\\nCompany: Samsung Research America (SRA)\\n\\nLab: Artificial Intelligence Center\\n\\nLocation: New York, NY\\n\\nLab Summary:\\n\\nSamsung is building a new, forward-looking AI Research lab located in New York, NY, that will conduct fundamental research at the intersection of AI, robotics, and neuroscience. Our team is composed of world experts who collaborate with leading academic groups to create both innovative theories and state-of-the-art technological demonstrations that engender pioneering products and applications for Samsung Electronics over both long and short time scales. We are currently seeking individuals who share our passion and motivation to advance fundamental science and create revolutionary technological prototypes. Team members will have the intellectual freedom to cultivate their research roots by partnering with universities and publishing in leading scientific conferences and journals.\\n\\nPosition Summary:\\n\\nResearch Scientists in Machine Learning tackle unsolved problems in neural network architectures, learning algorithms, reinforcement learning, computer vision, natural language, and multimodal sensory processing. They work closely with other Research Scientists and Engineers in collaborative, interdisciplinary teams to discover, invent, and build things that have meaningful real-world impact. The Samsung AI Center in New York will lead in turning fundamental scientific discoveries into revolutionary products that reach hundreds of millions of users across the world.\\n\\nExperience Requirements:\\nPhD in Computer Science or related disciplines is required.\\nExpertise in designing and validating machine learning algorithms.\\nProgramming experience in one or more of the following: C, C++, Python, and/or functional programming languages.\\nStrong record of publishing papers at top quality conferences and journals.\\nSuperior communication skills, both verbally and in writing.\\nAbility to excel in international and collaborative teams.\\nMotivation to initiate and accomplish ambitious research agenda.\\nSamsung is an EEO/Veterans/Disabled/LGBT employer. We welcome and encourage diversity as we strive to create an inclusive workplace.',\n",
       "  'Position Overview\\n\\nMunich Re sees the use of data as instrumental in making it easier for people to buy life insurance and to expand the number of people insured. IA leads this effort in the US and Canada for the North American Life business. The role of the Data Engineer is to develop and deploy data intensive applications in support of underwriting, audit, fraud detection and portfolio monitoring. Job Responsibilities\\n\\nResponsibilities may include, but will not be limited to the following:\\nAssist with the development of Python based applications and predictive models deployed as RESTful microservices;\\nHelp establish CI/CD workflows and cloud deployment using Docker and various Azure resources;\\nParticipate in various projects in the fields of statistics, machine learning, and deep learning collaborating with our greater team of scientists and engineers;\\nSupport in adapting and customizing machine learning and natural language processing to execute core data ingestion and transformation tasks (translations, validations, exception detection);\\nHelp employ image recognition and natural language processing to capture data from unstructured data including image, text, and handwriting;\\nBuild data pipelines based on modern IT architectures (batch & streaming) configure extract/transform/load software and rules engines.\\n\\nSDL2017\\n\\n</br>',\n",
       "  'New York Life Investments (NYLIM), an indirect, wholly owned subsidiary of New York Life Insurance Company, is a top 25 global asset management firm. With more than $500 billion in assets under management, NYLIM is a premier investment management firm serving a variety of client segments including retail, institutional, insurance and defined contribution and benefit on a global basis. New York Life Investments offers a diverse set of investment capabilities ranging from traditional equity and fixed income to alternative investment strategies and multi-asset solutions. Renowned for its premier investment acumen and client focus, NYLIM’s vision is to be one of the most trusted providers of investment management expertise and long-term financial security.\\n\\nIndexIQ, a New York Life Investments Company, is a trusted provider of innovative financial solutions. IndexIQ ETFs are built and delivered in a way that provides exposures that investors can rely on. A subsidiary of one of the oldest and largest insurance companies in the world, we have a solid foundation and the resources to continue our culture of innovation. IndexIQ has established a long track record of demonstrating the value of innovation. From offering the first liquid alternative ETF, to leveraging our multi-boutique platform, each ETF is thoughtfully constructed. Our specialized investment solutions help our clients meet the increasingly complex challenges they face in their portfolios today. With coverage across all broad asset classes, IndexIQ offers smart solutions for building better portfolios within:\\nFixed Income: Core, municipal, investment grade, high yield\\nEquities: Domestic large and small cap, global/international\\nAlternatives: Hedge fund replication\\nSpecialty: Merger arbitrage, commodity, real estate\\n\\n\\nPosition Overview\\n\\nThe Quantitative Research Analyst will report to the Director of Research and support IndexIQ team’s efforts in developing quantitative, rules-based investment strategies for new ETF products, vet 3rd party quantitative investment strategies and produce high quality research content supporting marketing and distribution.\\n\\nResponsibilities\\nDesign and back test quantitative investment strategies and conduct detailed performance analysis to evaluate strategy performance.\\nStay tuned with latest academic research in quant finance field in search for investment ideas.\\nDevelop and maintain the existing code library and database to support the investment research process.\\nEvaluate 3rd party quantitative strategies for potential adoption\\nCollaborate with the index management team to conduct ongoing research and analysis supporting existing products\\n\\n\\nQualifications\\nGraduate degree in a quantitative field with solid training in statistics and optimization\\n3+ years of experience working in quantitative finance or related projects\\nGood programming skills, Matlab preferred\\nProficient in excel and VBA\\nSolid knowledge in SQL database, experience with Factset and Bloomberg is a plus\\nWorking knowledge in financial products: equity, fixed income, options and futures.\\nKnowledge in risk premia products is a plus\\nStrong communications and time management skills\\n#LI-MD1\\n\\nPlease note: This role requires FINRA licensed and/or FINRA Associated Person pre-hire fingerprinting.\\n\\nEOE M/F/D/V\\n\\nIf you have difficulty using or interacting with any portions of this Web site due to incompatibility with an Assistive Technology, if you need the information in an alternative format, or if you have suggestions on how we can make this site more accessible, please contact us at: (212) 576-5811.',\n",
       "  'OVERVIEW:\\nWe are seeking for a data scientist to join its Human Capital Analytics and Reporting team (HCAR). Members of HCAR are curators of employee data and leverage it to help answer strategic and operational talent questions for leaders across the business.\\n\\nWHAT YOULL DO DAY-TO-DAY:\\nThe data scientist will be responsible for envisioning, implementing, and maintaining the technical infrastructure that will drive HCARs people analytics priorities forward. The output of this effort will be automated reporting products and data visualizations that provide efficient access to talent insights for senior business leaders throughout the firm. This position provides ample opportunities to collaborate with technical professionals across the firm and deliver best-in-class people analytics infrastructure, including the design and implementation of a central data warehouse.\\n\\nWHO WERE LOOKING FOR:\\nIdeal applicants will have a bachelors degree or higher and three to five years of relevant work experience developing automated analytical reporting products in a business setting. Experience building ETLs, manipulating data at scale, and overseeing the technical operations of data infrastructure is essential.\\nKnowledge of scripting languages for data retrieval and transformation (e.g., R, Python, SQL) is critical. The ability to collaborate with a product manager and software development team in the architecture of a central data warehouse that will facilitate efficient reporting automation is also vital.\\nFamiliarity with business intelligence programs (e.g., Tableau, Qlik, Pentaho, Shiny) and leading practices in data visualization is desired.\\nIndividuals who possess a dedicated customer service mentality, excellent interpersonal communication skills, an ability to see how small details impact the bigger picture, and a knack for rapid problem solving are highly sought after.\\nWillingness to dive into details, collaborate effectively with teammates, and drive projects to completion is imperative. The ability to exercise extreme discretion with confidential information is also essential.\\nPeople analytics experience is preferred but not required.',\n",
       "  'Job Description\\nKey Responsibilities:\\nEstablish a Data Governance framework and standards in alignment with the Bank’s BSA/AML and Sanctions Policy, Risk Appetite, BSA/AML and Sanctions risk assessments;\\nWork with members of the three lines of defense, including Head Office and NY Branch businesses, Compliance, IT and Audit, to establish requirements and business alignment;\\nEstablish FCC’s Data Quality standards including developing business requirements as well as metrics that define acceptable levels of data quality. This is a key aspect of the ongoing AML Model Validation, as defined by NYSDFS Rule Part 504, which requires validation of transaction monitoring and OFAC filtering data feeds for covered banking institutions;\\nRepresent and report on FCC’s Data Governance interests at working groups and committees, including issue management and resolution, relevant AML and Sanctions committees;\\nEstablishment core FCC technical infrastructure, including the implementation of an operational data repository for AML (“AML Data Mart”) and supporting implementation of the NICE Actimize transaction monitoring platform;\\nBudget, implement and manage the team to carry out the mandate of FCC Data & Analytics group;\\nProvide training and communication of senior management and regulatory agencies, as needed;\\nSuccessfully complete the Branch’s mandatory annual BSA/AML and OFAC training.\\nKnowledge & Experience Requirements:\\nBachelor’s degree in a related field, Master’s preferred;\\n10 years experience in compliance-related positions in the financial industry (international banking experience preferred);\\nThorough knowledge of BSA/AML and OFAC regulations;\\nSolid background in relevant technical subjects such as Python, SQL and data management tools.\\n\\nCompany Description\\nGlobal strives to be the best at providing our customers and associates with the finest personal service of a local business, while offering a wide range of global resources. Those resources include human resource, staffing, consulting and executive search to inspire and reward our associates and provide superior results to our customers.',\n",
       "  \"Description: </br>The Clinical analytics group uses an industry leading, scalable data platform and tools to glean insights from Aetnas large array of data sources, including medical claims, prescriptions, demographics, call data, and surveys. From these insights and understanding of the business needs, you would participate in the expansion of reporting and evaluation of innovative clinical initiatives and programs executed via a new clinical advocate model. You would help to develop appropriate reporting metrics and visualizations to manage and evaluate the program. The work environment is fast-paced, collaborative, and iterative, requiring quick learning, adaptability, and creativity. You would be part of a new and expanding team, with plenty of opportunities to interact with project stakeholders at all levels of the organization.\\n\\n63289\\n\\n</br></br> Fundamental Components: </br>\\nDevelops, validates and executes algorithms and predictive models to investigate problems, detect patterns and recommend solutions\\nExplores, examines and interprets large volumes of data in various forms\\nPerforms analyses of structured and unstructured data to solve moderately complex business problems utilizing advanced statistical techniques and mathematical analyses\\nDevelops data structures and pipelines to organize, collect and standardize data that helps generate insights and addresses reporting needs\\nUses data visualization techniques to effectively communicate analytical results and support business decisions\\nCreates and evaluates the data needs of assigned projects and assures the integrity of the data\\nExplores existing data and recommends additional sources of data for improvements\\nDocuments projects including business objectives, data gathering and processing, detailed set of results and analytical metrics.\\n</br></br> Background Experience: </br>\\n3 years of relevant programming or analytic experience.\\nAbility to work with large data sets from multiple data sources.\\nDemonstrates proficiency in several of the following areas: analysis methods, statistical analyses, stratification, data visualization, data modeling, study design, randomization\\nProficient in SQL, Python, R, or HQL.\\nProficient in Tableau or similar BI design tool.\\n</br></br> Potential Telework Position: </br>No</br></br> Percent of Travel Required: </br>0 - 10%</br></br> EEO Statement: </br>Aetna is an Equal Opportunity, Affirmative Action Employer</br></br> Benefit Eligibility: </br>Benefit eligibility may vary by position. Click here to review the benefits associated with this position.</br></br> Candidate Privacy Information: </br>Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.\\n\\n#LI-DT1</br></br>\",\n",
       "  \"Enigma is seeking an experienced Data Science professional who's excited to take on some of the world's most difficult data challenges. A Lead Data Scientist is a proven leader and mentor who loves getting their hands on data and solving challenging problems with real-world impact. You will be in a position to shape how data science is done on a wide variety of problems at Enigma and how we grow our data science function. Additionally, you will be our in-house subject matter expert and represent Enigma in the overall analytics community.\\n\\nWhat we are looking for:\\nYou will have at least 5 years of experience in applied research and statistical modeling with a degree or higher (MS/Ph.D.) in statistics, mathematics, physics, or similar field\\nA track record of creatively tackling challenging data problems\\nFull understanding of common machine learning techniques and familiarity with ongoing research\\nSuccess mentoring and growing exceptional data scientists\\nFluency with analytical programming, including libraries for cleaning, reshaping, exploring and visualizing data\\nThe ability to clearly convey complex concepts with plain language\\nDemonstrated ability to produce high-quality, product-focused, scalable analysis\\nProblem-solving with determination, perseverance, and grit\\nYou are motivated by working on hard problems with smart people\\nAbility to prioritize\\nYou will join Enigma at an exciting growth stage where your contributions will impact the entire company.\\n\\nOur Story:\\n\\nEnigma is a rapidly growing enterprise technology company based in the Flatiron neighborhood of New York City. We are Series C funded, partnering with some of the best investors in the world: New Enterprise Associates, Two Sigma Ventures, Comcast Ventures, Crosslink Capital, American Express Ventures, and others.\",\n",
       "  \"News and social media move financial markets. Bloomberg is one of the largest producers of news in the world and we ingest millions of news stories every day from over 70,000 external news feeds and social media such as Twitter. This data keeps our clients informed, and our team's insights help make sense of it for our customers.\\n\\nWho are we? Bloomberg's Artificial Intelligence (AI) group: researchers and engineers who have a passion for solving complex problems. Our charter: to extract and identify relevant, meaningful, tradable, and actionable information (such as pricings, earnings, recommendations and major events) from data (including news, web, social media, and structured data) in real-time. Since our customers rely on this information to make swift financial decisions, we guarantee precision, accuracy, and latency numbers beyond most academic and industry standards.\\n\\nWe aren't just building customer-facing products, as the infrastructure and algorithms we develop are themselves used across the company. We also publish papers, attend conferences, organize workshops, and contribute back to the larger data science community whenever we can (seehttps://www.techatbloomberg.com/nlp/ andhttps://bloomberg.com/company/d4gx/).\\n\\nWho are you? A research scientist and engineer who wants to work in the areas of machine learning, natural language processing, information extraction, reinforcement learning, graphical models, recommender systems, and/or knowledge graphs. You want to join a close-knit group and make a big impact.\\n\\nWe'll trust you to:\\nWork with others in the AI group and the company on production systems and applications\\nPublish research findings in leading academic venues and represent Bloomberg at industry conferences\\nWrite, test and maintain production-quality code\\nDesign, experiment, and evaluate algorithms, and models\\nYou'll need to have:\\n\\n\\n5+ years of experience in AI, NLP, ML, Optimization, or related fields\\n5+ years of experience programming in C++, Python or Java\\nA master's degree (PhD preferred) with industrial experience\\nWe'd love to see:\\n\\n\\nA quantitative background (Probability, Statistics, Linear Algebra, etc.)\\nExperience with distributed computational frameworks (YARN, Spark, Hadoop, Kubernetes, Docker)\\n3+ publications in top-tier conferences or journals (such as ACL, AAAI, SIGIR, KDD, EMNLP, ICML, NeurIPS or equivalent)\\nIf this sounds like you, apply!\\n\\n\\nIn addition, do check out our blog, TechAtBloomberg.com/NLP, to learn more about our publications and projects in data science.\",\n",
       "  \"Medidata: Conquering Diseases Together\\n\\nMedidata is leading the digital transformation of life sciences with the world's most-used platform for clinical development, commercial and real-world data. Powered by artificial intelligence and delivered by #1 ranked industry experts, the Intelligent Platform for Life Sciences helps pharmaceutical, biotech, medical device companies and academic researchers accelerate value, minimize risk and optimize outcomes. Medidata serves more than 1,000 customers and partners worldwide and empowers more than 100,000 certified users every day to create hope for millions of patients. Discover the future of life sciences: www.mdsol.com\\n\\nWe know that diverse teams win and are fully committed to selecting leaders and employees that represent the markets in which we operate. We are still led by our Co-founders, Tarek Sherif and Glen de Vries, and have global operations in US, Europe and Asia with over 2000 employees.\\n\\nAbout AcornAI - a Medidata company\\n\\nMedidata’s journey started in 1999 when a scientist working on his first clinical trial waded through inefficiencies and delays – and knew that technology could improve the process. Almost 20 years later, we've grown to be the leader in clinical technology. In 2019, we launched AcornAI to meet changing biopharma need as companies rapidly progress to the age of precision medicine.\\n\\nBuilt on our platform with the industry’s largest structured, standardized and growing clinical trial data repository consisting of 17,000+ trials and 4.5M patients, AcornAI is one of the largest AI companies exclusively dedicated to life sciences. Our team is composed of over 40 PhD/Masters statisticians, data scientists, analytical product leads, former FDA biostatisticians and computational genomicists.\\n\\nAcornAI’s delivery team is looking for individuals who will help us tackle some of the most complex questions facing the industry today using our proprietary AcornAI platform and advanced analytics. In this role, you will engage with clients throughout projects to ensure the best analytical approach, successful delivery and client satisfaction. At AcornAI, we never work alone. This role will partner heavily with all of the key stakeholder functions including Product, Data Science, Engineering, partnerships and Biostatistics. Successful candidates will be skilled in analytical/quantitative thinking, structured communication and excited about building the next horizon of Medidata’s journey of powering smarter treatments and healthier people.\\n\\nYour Mission:\\nWork cross-functionally to gather and analyze information, formulate and test hypotheses, and communicate recommendations on some of the most complex issues in the industry today\\nDevelop external communication materials, translating and summarizing technical information for business audiences and developing key messaging\\nPerform and share analyses to guide internal and external decision making\\nDrive problem-solving with data scientists and other stakeholders to identify the analytics needed to answer client questions and generate impact in line with our mission\\nTrack and communicate progress and decisions clearly, triage issues to the appropriate internal stakeholder and ensure timely effective issue resolution\\nCreate processes so that each project is easier and faster than the last one\\nYour Education & Experience:\\n\\nMust-have qualifications and expertise:\\nBachelors degree or equivalent experience\\n2+ years of work experience in one of the following types of highly structured or scientific environments: management/healthcare consulting, life sciences, data and analytics role\\nOutstanding structured and critical thinking\\nA passion for understanding complex issues with a data driven approach\\nA technical bent and the ability to use or to learn to use tools to answer your own questions (e.g. Excel/SQL/Python)\\nSuperb project management and a relentless drive to get things done\\nWilling to master the detail when you need to but also zoom out and have a view on how to shape the bigger picture\\nThe following will be helpful, but are not essential:\\nAn entrepreneurial drive and a passion to advance the development of new treatments\\nEnjoy experimenting and iterating on different ways to solve a problem\\nA strong analytical background and the ability to quickly digest and evaluate complex analyses with a keen eye for detail\\nExperience working in Life sciences or across the healthcare value chain\\nCan drive forward even amidst ambiguity (people, process, data) and complexity\\nMedidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation. Nine of the top 10 best-selling drugs in 2017 were developed on the Medidata platform.\\n\\nMedidata’s solutions have powered over 14,000 clinical trials giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner.\\n\\nMedidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.\\n\\n#LI-AS2\",\n",
       "  \"Job Description We are looking for a quantitative analyst to join our modelling team, which is responsible for all the models and scorecards used in the rating process. This is a good opportunity to join a growing team where you will be expected to participate in the development of credit rating models and scorecards and assist with documentation, oversight and maintenance of some of the rating agency models and scorecards.\\n\\nKey Responsibilities:\\nDevelop in-depth knowledge of our Rating Agency models and scorecard\\nAssisting with ad-hoc quantitative projects to update and improve the rating process (structured finance simulations, macroeconomic projects, etc.).\\nWork with managers to create documentation around credit rating models.\\nParticipate in professional training for the analytical staff\\nBuild relationships with analytical staff and management; Act as a key point of contact to provide guidance to rating teams to ensure consistency with rating models and scorecards when appropriate\\nResearch mathematical and Technical techniques to evaluate performance of credit rating models and scorecards\\nQualifications\\nStrong academic background, MSc or PhD in relevant areas like Mathematics, Computer Science, Physics, Financial Engineering, Mathematical Finance, Engineering.\\nUnderstanding of model development including model design and implementation.\\nGood development skills in two of the following technologies: C# or other .NET languages, C++, VBA, and Matlab\\nAdvanced Excel knowledge\\nExperience in financial modeling - experience in credit risk, credit derivatives or structured finance a strong plus.\\nUnderstanding of fixed income; understating of structured finance is a plus.\\nGood verbal and written communication skills, able to communicate clearly and succinctly\\nHighly organized and efficient, with ability to multi-task and manage multiple projects\\n\\n#LI-RD1\\nMoody's is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, gender, age, religion, national origin, citizen status, marital status, physical or mental disability, military or veteran status, sexual orientation, gender identity, gender expression, genetic information, or any other characteristic protected by law. Moody's also provides reasonable accommodation to qualified individuals with disabilities in accordance with applicable laws. If you need to inquire about a reasonable accommodation, or need assistance with completing the application process, please email accommodations@moodys.com.. This contact information is for accommodation requests only, and cannot be used to inquire about the status of applications.\\n\\nFor San Francisco positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the San Francisco Fair Chance Ordinance. For New York City positions, qualified applicants with criminal histories will be considered for employment consistent with the requirements of the New York City Fair Chance Act. For all other applicants, qualified applicants with criminal histories will be considered for employment consistent with the requirements of applicable law.\\n\\nClick here to view our full EEO policy statement. Click here for more information on your EEO rights under the law.\\n\\nCandidates for Moody's Corporation may be asked to disclose securities holdings pursuant to Moody's Policy for Securities Trading and the requirements of the position. Employment is contingent upon compliance with the Policy, including remediation of positions in those holdings as necessary.\",\n",
       "  'This Jobot Job is hosted by: Gene Choi\\nAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.\\nSalary: $100,000 - $150,000\\nA bit about us:\\nBased in New York, NY we use big data and predictive analytics to help transform health care organizations. We collect data about applicants through our cloud-based platform, including responses, text, interaction, public, and other data. Then we assemble the most relevant applicant data to generate a prediction for each candidate in each organization, location, department, and role. The predictions are optimized for specific outcomes such as employee retention, patient satisfaction, employee engagement, quality of care, readmissions, medical incidents, and candidates who would be good fits for positions they did not apply for.\\n\\nIf you are a Data Scientist, then please read on.\\n\\nWhat can we do for you?\\nCompetitive Base Salary!\\n401K, PTO, and Excellent Benefits!\\nAccelerated Career Growth!\\nIs your background a fit? We are looking for\\n3+ years practical experience in industry product as a Data Scientist in a production environment\\nExcellent Python (numpy, pandas, scikit-learn, etc.) or R programming skills, familiar with open source libraries and tools for data science\\nStrong SQL skills\\nMasters or PhD in a quantitative field (such as computer science, statistics, a quantitative social, biological, or physical science) is preferred but not required\\nExperience with Natural Language Processing, deep learning, experimental design and A/B testing, recommender systems or other deployed data products\\nExperience with sophisticated statistical or econometrical models\\nWhy join us?\\nWe can offer you the opportunity to work on State-of-the-Art technology and make a meaningful and important impact on todays society and the next generation! If you\\'re interested, please send an email to gene.choi@jobot.com!\\n\\nInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.',\n",
       "  'Job Description\\nData Scientist - NLP\\nNew York, New York\\n$160,000 - $180,000 base salary + full benefits\\n\\nHarnham are partnered exclusively with one of the fastest growing technology startups here in New York. They are at an exciting point in their growth as they are looking to rapidly scale up, having gained VC backing for one of the most successful holdings business. This position reports directly into the VP of Data Science having a high-level of visibility and calls for an experienced candidate with a strong technical background within NLP.\\n\\nTHE ROLE\\n\\nAs the Data Scientist, you will be required to:\\nUse natural language processing expertise to analyze various datasets, building new products and evolving existing models\\nWork closely across different Data Science and mentor junior team members\\nOwn new projects supporting marketing and sales functions and product development\\nExpand upon existing APIs to bring in alternative datasets.\\nProvide deep knowledge and leadership in NLP/NLU/NLG statistical analysis as a thought leader\\nWrite production code and communicate with the entire technical team to impact algorithm production\\n\\n\\nYOUR SKILLS AND EXPERIENCE\\n\\nThe successful Data Scientist - NLP will likely have the following skills and experience:\\nMS or PhD in Computational Linguistics or quantitative field with thesis using NLP\\nStrong experience developing and implementing NLP algorithms in a commercial setting\\nExperienced in statistical modeling, Bayesian inference or other machine learning and statistical methodologies\\nExpert in NLP methodologies: Word2Vec, sentiment analysis, entity extraction\\nExperience with NLTK, Gensim, SpaCy, CoreNLP\\nProgramming experience in Python preferred, R and Java a plus\\nMachine Translation and BERT experience a plus\\nComfortable using neural networks: RNN/CNN\\nPrevious experience leading projects preferred\\nKnowledge of Big Data packages a plus, Hadoop, Spark, Hive or others\\nProven communicator with technical team, clients and senior stakeholders\\nEntrepreneurial spirit!\\n\\n\\nTHE BENEFITS\\n\\nA competitive base salary of $160,000 - $180,000 + bonus + benefits.\\n\\nHOW TO APPLY\\n\\nPlease register your interest by sending your résumé to Tim Jonas via the Apply link on this page.\\n\\nKEYWORDS\\n\\nNLP | Statistics | Natural Language Processing | Machine Learning | Deep Learning | Data Science | Analytics | Big Data | Technology\\nCompany Description\\nData and Analytics recruitment is our core business and we’re proud to say, our customers believe we’re good at it. In our most recent customer satisfaction survey, 95% of respondents said that they would recommend Harnham.\\n\\nHarnham has actively chosen to focus on Data and Analytics, we’ve immersed ourselves in this market and are now an integral part of this business community.\\n\\nOur capability has grown to provide recruitment services and advice across the Marketing Analytics, Credit Risk, Data Science, Data and Technology and Digital sectors.',\n",
       "  \"What you’ll be doing...\\n\\nThis is a senior level technical role in the company. This position will help lead innovations through building and applying cutting edge machine learning techniques and AI technology to drive business growth. It will lead strategic initiatives and research in developing AI technology and machine learning algorithms and techniques. You will provide guidance to teams of talented data scientists and machine learning engineers in developing and delivering the next generation AI/ML solutions to the business.\\n\\nYou’ll be a thought leader who challenges the status quo and brings new, innovative concepts and technology into Verizon. You’ll be an educator and evangelist who is influential and persuasive in promoting AI/ML across the company. You’ll represent Verizon on AI/ML and interact with leading researchers and industry leaders. You will also be part of the driving force for attracting top talent.\\nLead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.\\nLead the design, development, and deployment of AI solutions that drive business growth and better customer experience.\\nContribute to the development of AI/ML strategy and roadmap for the company.\\nContribute to research and development of AI/ML techniques and technology that fuels the business innovation and growth of Verizon.\\nRepresent Verizon in AI/ML research and the industry through publications, conference speeches, collaboration with leading researchers and universities.\\nBuild strong influence both among AI/ML community and with senior business leaders and actively promote the effective applications of AI/ML technology.\\nLead engagement with key business stakeholders in discussion on business strategies and opportunities.\\nBuild strong working relationship and develop deep partnership with the business.\\nWork closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.\\nBe a subject matter expert on machine learning and predictive modeling and a mentor to junior data scientists.\\nWhat we’re looking for...\\n\\nYou are an accomplished data scientist, a recognized machine learning expert, and a thought leader in AI technology and business innovation. You are a master at analyzing big data. You thrive in an environment where enormous volumes of data are generated at rapid speed. You’re a creative thinker who likes to explore, and uncover the issues. You are decisive. You are great at influencing up, down, and across groups, and you take satisfaction in mentoring others. Communicating what you’ve uncovered in a way that can be easily understood by others is one of your strengths.\\n\\nYou’ll need to have:\\nBachelor’s degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven better if you have:\\nMaster’s degree in a quantitative field or in a related filed.\\nPh.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.\\nEight or more years of experience in practicing machine learning and data science in business.\\nAccomplished researcher and expert in machine learning, neural networks, reinforcement learning, chatbot technology, and NLP\\nStrong communication and interpersonal influencing skills.\\nExcellent problem solving and critical thinking capabilities.\\nExperience in leading large scale data science projects and delivering from end to end.\\nStrong computing/programming skills; Proficient in Python, Spark, SQL, Linux shell script.\\nStrong experiencein Big Data and Cloud technology.\\nWillingness to travel.\\nVZAnalytcis\\n\\nWhen you join Verizon...\\n\\nYou’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.\\n\\nEqual Employment Opportunity\\n\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.\\n\\n]]>\",\n",
       "  'Job Title: Data Scientist with SQL, Python, R, Tableau.\\n\\nLocation: Manhattan, NY\\n\\nDuration: 18 Months Plus\\n\\nInterview Mode: F2F Mandatory. Only 1 round of interview and it will be direct F2F.\\n\\nExperience Requirements\\n5+ years as a data scientist leading efforts to identify relevant questions, collect data from a multitude of different data sources, organize the information, translate results into solutions, and communicate findings in a way that positively affects decisions\\n5+ years working with SQL, Python, R, Tableau and other data science programing languages and tools\\nStrong quantitative and problem-solving skills\\nExperience and passion for data wrangling, data cleaning, and ETL\\nExperience working with administrative data sets\\nExperience with statistical modelling and machine leaning analysis\\nExperience with Bayesian analysis\\nProficiency in GIS concepts and software (ArcGIS, Google Maps, QGIS, Carto)\\nAttention to detail for documenting work processes and writing clear instructions for technical tasks\\nAbility to distill complex material into actionable recommendations\\nExcellent written and oral communication skills\\nJob Duties\\nDevelop SQL and Python queries to analyze the completeness and quality of key data elements in StreetSmart, including demographics, caseload history, mental illness diagnoses, and substance abuse details\\nDevelop and monitor a data cleaning prioritization plan, working with a data analyst dedicated to data cleaning\\nManipulate and analyze administrative data in order to predict outcomes and make data-driven recommendations\\nApply statistical and data mining techniques to conduct performance audits, trend analysis, and predictive analytics using StreetSmart data\\nCollaborate with team members to develop novel strategies for technical analysis\\nEvaluate ethical implications of design choices for predictive analytics models and automated decision support systems\\nCreate and present compelling reports to stakeholders based upon project findings and methods\\nWe re looking for a candidate with data science experience to develop predictive analytic models to improve service delivery for client s work with street homeless individuals to bring them on the path to stable housing.\\n\\nWe re looking for someone who is quantitative and civic-minded, with outstanding communication and organizational skills. Familiarity with social service programs is a plus.\\n\\nIf interested share profile with details to shravya(at)techprojects(dot)us and samia(at)techprojects(dot)us\\n\\n- provided by Dice',\n",
       "  'Position:\\n\\nWe are looking for an accomplished big data developer\\nwith strong experience in the cloud AWS data implementation to help us build\\nand integrate data-driven intelligent cloud solutions for EDL. This role will\\ninvolve a close collaboration with our team of passionate and innovative big data\\nspecialists, application developers and product managers.\\n\\nThis is a unique opportunity\\nto be a member of our corporate CRM and Analytics Team, tackling our toughest\\nand most exiting data lake challenges across multiple divisions in Jefferies.\\n\\nCRM & Analytics Team\\nOverview:\\n\\nThe CRM & Analytics team is a highly\\nstrategic and cross-functional team responsible for leading the firm’s global\\ndigitalization effort. This initiative, spanning all client-facing business\\nunits and corporate functions, will drive innovation and strategic change\\nthrough technology, data science, and deep analytics. The team partners\\nwith key business leaders and industry experts to build transformational\\ntechnology to drive revenue, maximize efficiency, and optimize the allocation\\nof resources. The CRM & Analytics team is at the forefront of\\nJefferies’ cloud initiative, leveraging best-in-class cloud-based technologies\\nto replace legacy on-premises solutions to provide intelligent trend insights,\\nactionable opportunities, decision support, and transparency into all client and\\nbusiness-related activities. This team is also responsible for Enterprise\\nData Lake.\\n\\nPosition Overview:\\n\\nWe are looking for an accomplished big data\\narchitect with strong experience in the cloud AWS data architecture and\\nimplementation.\\n\\nThis role will involve a close collaboration\\nwith our team of passionate and innovative big data specialists, application\\ndevelopers and product managers.\\n\\nThis is a unique opportunity\\nto be a member of our corporate CRM and Analytics Team, tackling our toughest\\nand most exiting data lake challenges across multiple divisions in Jefferies.\\n\\nEnterprise Data Lake\\n(EDL) Team Responsibilities:\\n\\nThe EDL team will oversee\\nand support architecture and implementation of EDL for all Jefferies big data\\ninitiatives. It will drive the data governance and facilitate data onboarding.\\nIt will approve the design of data and software architecture, perform\\narchitecture review to pass EDL tollgates, evaluate and select cloud/AWS/Big\\nData tools for acceptance, and serve as a vendor liaison with data lake tool\\nvendors and out internal infrastructure teams. Also, it will certify data for\\nconsumption, EDL patterns and processes, manage and govern data access controls,\\nand will manage data lake and data governance training initiatives across\\nenterprise.\\nThe EDL team will\\nbecome the center of excellence for the following EDL components and associated\\ntools:\\n\\nq EDL architecture and patterns\\n\\nq\\nEDL Data Stores\\n\\nq\\nEDL Governance\\n\\nq\\nEDL Data Discovery\\n\\nq\\nEDL Data Preparation\\n\\nq\\nEDL Reporting\\n\\nq\\nEDL Ingestion tools & other technologies\\nand tools\\n\\nq Educate teams to migrate and develop new cloud\\napplications\\n\\nBasic\\nRequirements:\\n\\n· A minimum of 5\\nyears of hands-on technical experience with:\\n\\n·\\nbig data implementation\\nand technology offerings\\n\\n·\\n\\nAWS/cloud big data\\nmodeling & data management\\n\\n·\\nanalytics and ingestion\\narchitecture of big data\\n\\n·\\n\\ndata lake\\nmanagement and data architecture\\n\\n·\\ndata lake design\\npatterns & cloud best enterprise practices\\n\\n·\\n\\nIoT and streaming,\\nreal time processing\\n\\n·\\nBig data related\\nAWS technologies\\nExperience in AWS technologies such as Kinesis,\\nLambda, EC2, Redshift, RDS, Cloud formation, EMR, AWS S3, AWS Analytics, Spark,\\nDatabricks\\nExperience with at least one of the following\\nlanguages Scala, Python, R and or Java\\nExperience with designing, developing, and\\nimplementing complex integration for end-to-end solutions at a middleware and\\napp level with focus on performance optimization\\nStrong implementation skill in area of cloud\\ndevelopment in AWS\\nDemonstrated ability in implementing cloud scalable,\\nreal time and high-performance data lake solutions (AWS)\\nAbility to quickly perform proof-of-concepts\\nfor validating new technology or approach\\nAbility to exercise independent judgment and\\ncreative problem-solving techniques in a highly complex environment using\\nleading-edge technology and/or integrating with diverse application systems\\nAbility to lead and drive technology change in\\na fast-paced, dynamic environment and all phases of the entire software life\\ncycle\\nStrong experience with data catalog, data\\ngovernance, Collibra, MDM and/or Data Quality (IDQ) toolset\\nStrong experience with integration of diverse\\ndata sources (batch and real time) in the cloud\\nLead the design and sustainment of data\\npipelines and data storage\\nExpertise in Structured, unstructured, SQL and\\nNo-SQL technologies\\nExpertise with identifying and understanding source\\ndata systems and mapping source system attributes to the target\\nExperience with design and automation of ETL\\\\ELT\\nprocesses\\nAWS and cloud performance tuning and\\noptimization experience\\nExperience with effort estimation for new\\nprojects/proposals on an ongoing basis.\\nExcellent communication skills across all\\nlevels; ability to communicate with ease the complex and technical concepts.\\nAbility to work effectively in a fast-paced\\nenvironment',\n",
       "  'JobDescription :\\nThe Team: The Data science team is a newly formed applied research team within S&P Global Ratings that will be responsible for building and executing a bold vision around using Machine Learning, Natural Language Processing, Data Science, knowledge engineering, and human computer interfaces for augmenting various business processes.\\n\\nThe Impact: This role will have a significant impact on the success of our data science projects ranging from choosing which projects should be undertaken, to delivering highest quality solutions, ultimately enabling our business processes and products with AI and Data Science solutions.\\n\\nWhat’s in it for you:\\nThis is a high visibility team with an opportunity to make a very meaningful impact on the future direction of the company. You will work with senior leaders in the organization to help define, build, and transform our business.\\nYou will work closely with other senior scientists to create state of the art Augmented Intelligence, Data Science and Machine Learning solutions.\\nThe team actively participates in top-tier academic and industry conferences by publishing research and organizing workshops. Depending on your interest, you can be part of these efforts.\\nResponsibilities: As an NLP Data Scientist you will be responsible for building AI and Data Science models with a main focus on mining insights from text corpora. You will need to rapidly prototype various algorithmic implementations and test their efficacy using appropriate experimental design and hypothesis validation.\\n\\nBasic Qualifications: BS in Computer Science, Computational Linguistics, Artificial Intelligence with a heavy focus on NLP/text mining, or related field with 5+ years of relevant industry experience. There is some flexibility in adjusting the seniority to your level of education and experience.\\n\\nPreferred Qualifications:\\nMS in Computer Science, Computational Linguistics, Artificial Intelligence with a heavy focus on NLP/text mining with 3+ years of relevant industry experience.\\nExperience with Financial documents such as SEC filings, financial reports, credit agreements, business news, or S&P’s credit ratings process is a plus.\\nWhat we look for in your background:\\nCreativity, resourcefulness, and a collaborative spirit.\\nKnowledge and working experience in one or more of the following areas: Natural Language Processing, Clustering and Classification of Text, Question Answering, Text Mining, Information Retrieval, Distributional Semantics, Knowledge Engineering, Search Rank and Recommendation.\\nDeep experience with text-wrangling and pre-processing skills such as document parsing and cleanup, vectorization, tokenization, language modeling, phrase detection, etc.\\nProficient programming skills in a high-level language (e.g. Java, Scala, Python)\\nBeing comfortable with rapid prototyping practices.\\nBeing comfortable with developing clean, production-ready code.\\nBeing comfortable with pre-processing unstructured or semi-structured data.\\nExperience with statistical data analysis, experimental design, and hypotheses validation\\nProject-based experience with some of the following tools:\\nNatural Language Processing (e.g., Spacy, NLTK, ClearTK, ScalaNLP/Breeze, ClearNLP, OpenNLP, or similar)\\nApplied machine learning (e.g. libSVM, Shogun, Scikit-learn, SparkML, H2O, or similar)\\nInformation retrieval and search engines, e.g. ElasticSearch/ELK, Solr/Lucene\\nDistributed computing platforms, such as Spark, Hadoop (Hive, HBase, Pig), GraphLab\\nDatabases (traditional and noSQL)\\nProficiency in traditional Machine Learning models such as SVMs, LDA/topic modeling, HMMs, graphical models, etc.\\nOptional: familiarity with Deep Learning architectures and frameworks such as PyTorch, Tensorflow, Keras.\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.\\n\\nJob ID :\\n249493\\n\\nPosted On :\\n9-23-2019\\n\\nLocation :\\nNew York, NY US',\n",
       "  'Overview\\n\\nThis role will be a key member of the ION Analytics team. You will work with senior/executive management, inspire new thoughts, and develop algorithms to solve interesting problems that have a significant influence on the organization. You will create actionable analytics and solutions that will solve challenging business problems in various departments. You will contribute to the long term analytics strategies and competencies needed to support ION across the various business units. Your experience, knowledge, translation of data and use of analytics will shape strategic plan and business growth.\\n\\nAs a member of the Analytics team at ION - which drives analytics, big data and decision science for all business functions across ION - you and your team will also promote synergies across the solutions and models we develop for other parts of ION. This role offers excellent opportunity for an innovative and driven individual who is enthusiastic about using data and analytics to drive significant results in an environment that rewards performance.\\n\\nResponsibilities\\nDesign and implement quantitative analyses and predictive modeling that result in actionable solutions.\\nCreate tools to empower operational and exploratory analysis and automate reporting and processes.\\nMaintain and extend an internal data warehouse with large datasets from various sources.\\nApply your proficiency in quantitative analysis, data mining, and visualization to see beyond the data in identifying and quantifying high-impact opportunities.\\nEffective communication with non-technical team members, from creating ideas and scoping requirements to explaining methodology and results.\\nDevelop a complete understanding of IONs business, processes, data & systems.\\nSupport a data-driven culture, educating the organization on everyday use of data as a resource.\\nQualifications\\nAdvanced degree (MS or PhD) in Computer Science, Operations Research, Engineering, Statistics, Econometics or a related quantitative field.\\nStrong programming skills with Python, R and familiar with Java (or a similar language).\\nMinimum of 5 years professional experience in solving challenging business problems using computational techniques.\\nKnowledge of relational database (Microsoft SQL is preferred) and Extract, Transform, Load (ETL) processes.\\nKnowledge of common software development and ability to develop small app independently.\\n\\nSDL2017\\n\\n</br>',\n",
       "  \"Entera, where residential real estate investing is made simple\\n\\nAt Entera, we are on a mission to transform the way investors find and buy properties. Powered by machine-learning, Entera's end-to-end residential real estate platform modernizes the real estate buying process. Entera's property source aggregation platform, discovery algorithms, intelligent tools and expert real estate service team help our clients access and evaluate more properties, make data-driven investment decisions, and win more - 100% online.\\n\\nEntera is based in San Francisco, New York & Houston, with satellite service offices in 12 additional markets across the US. We're always looking for talented, creative and passionate people to join our team. If you're interested in opportunities at Entera, we'd love to hear from you!\\n\\nJob Description\\n\\nAs a Senior Data Scientist at Entera, you'll develop both batch and real-time processing capabilities and improvements in our machine learning efforts. Some of the projects we work on include image recognition across a massive dataset, geospatial analysis using tools in PostgreSQL, and probability/outcome detection as a service to better help match our customers to product. You'll work in Python, developing and measuring the accuracy of Tensorflow models that use datasets in stored in Spark. You'll work with cross-functional teams to do data-mining in an iteratively improving process that mirrors that of our agile engineering organization. You'll work with some of the brightest engineers you'll meet across the full stack to help further integrate our AI systems to drive a better transaction quality and experience on our platform.\\n\\nSuccessful candidates will thrive in Entera's unique operating environment and culture: high-growth, innovative, lean, and values-driven. As such, successful candidates must be highly capable in each of the following dimensions (among others): adaptability, curiosity, resourcefulness, analytical thinking/problem solving, proactivity, collaboration, technological savvy, and operating in a dynamic environment.\\n\\nJob Responsibilities\\nDevelop production-grade Tensorflow models and work towards improvements in our existing customer-facing machine learning systems\\nCreate NLP-based systems to structure new sources of data using SpaCy and surface that data via APIs\\nWork with Data Engineering to ensure you have access to the source data needed for modeling\\nDevelop novel ways of analyzing and working with our datasets\\nUse Spark as a feature-store and help further improve our labeling and annotation process\\nWork within a team structure to deliver on machine-learning projects and data efforts\\nIterate in an agile development workflow and use version control systems (Git and the TFX suite of tooling) to manage model validation and data accuracy\\nPreferred Qualifications:\\nMS or PhD in Computer Science, Mathematics, Statistics, Physics, Economics, or similar hard-science.\\n5+ years hands-on experience in Data Science roles at fast-paced product-driven tech companies\\nExtensive experience creating measured and validated models in Tensorflow\\nDeep and broad interest in all aspects of machine-learning\\nStrong experience with Spark, SQL, R, and Python\\nWorking knowledge of cloud-services\\nExperience using and working with accompanying data pipelines\\nWorking knowledge of a Python web framework like Flask\",\n",
       "  'The Applications Development Group Manager is a senior management level position responsible for accomplishing results through the management of a team or department in an effort to establish and implement new or revised application systems and programs in coordination with the Technology Team. The overall objective of this role is to drive applications systems analysis and programming activities.\\n\\nResponsibilities:\\nDesign and implement a data strategy for a cross functional organization that processes over 40 billion transactions per annum.\\nIntegrate in-depth knowledge of applications development and cutting edge technologies to establish a client-centric, scalable, best-in-class reconciliation architecture to achieve established goals\\nManage multiple teams of professionals to accomplish established goals and conduct personnel duties for team (e.g. performance evaluations, hiring and disciplinary actions)\\nProvide strategic influence and exercise control over resources, budget management and planning while monitoring end results\\nUtilize in-depth knowledge of concepts and procedures within own area and basic knowledge of other areas to resolve issues\\nEnsure essential procedures are followed and contribute to defining standards\\nProvide evaluative judgement based on analysis of facts in complicated, unique, and dynamic situations including drawing from internal and external sources\\nInfluence and negotiate with senior leaders across functions, as well as communicate with external parties as necessary\\nAppropriately assess risk when business decisions are made, demonstrating particular consideration for the firm\\'s reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency, as well as effectively supervise the activity of others and create accountability with those who fail to maintain these standards.\\nQualifications:\\n10+ years of relevant experience\\nExperience in applications development\\nExperience in management\\nExperience managing global technology teams\\nWorking knowledge of industry practices and standards\\nConsistently demonstrates clear and concise written and verbal communication\\nEducation:\\nBachelors degree/University degree or equivalent experience\\nMasters degree preferred\\n-------------------------------------------------\\n\\nGrade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US\\n\\n------------------------------------------------------\\n\\nTime Type :\\n\\n------------------------------------------------------\\n\\nCiti is an equal opportunity and affirmative action employer.\\nMinority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.\\n\\nCitigroup Inc. and its subsidiaries (\"Citi) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.\\n\\nTo view the \"EEO is the Law\" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.\\nTo view the EEO Policy Statement CLICK HERE.\\nTo view the Pay Transparency Posting CLICK HERE.',\n",
       "  'Salary: $23.00 to $27.00 hourly\\n\\nA midtown software company is looking for a talented Data Analyst to join their team. Must haves include:\\nWorking with large data sets\\nAbility to organize and present data\\nHeavy use of excel\\nFor immediate consideration please email your resume to: Kevin.Scagnelli@accountemps.com\\nData Analysis\\nExcel',\n",
       "  \"Description: </br>Use statistical principals to evaluate scenarios and make predictions on future outcomes in order to solve highly complex business problems and support decision making. Analyze large data sets in real time databases. Develop and implement mathematical approaches and solve complex business problems. Participate in the development, validation and delivery of algorithms, statistical models and reporting tools. Convey the solution of complex numerical problem to technical and non-technical business partners. Develop and / or use algorithms and statistical predictive models and determine analytical approaches and modeling techniques to evaluate scenarios and potential future outcomes. Perform analyses of structured and unstructured data to solve multiple and / or complex business problems utilizing advanced statistical techniques and mathematical analyses and broad knowledge of the organization and / or industry. Collaborate with business partners to understand the nature of their business and recommend an appropriate statistical approach to deal with business problems and measurements. Develop and participate in presentation and consultations to existing and prospective constituents on analysis results and solutions. Interact with internal and external peers and managers to exchange complex information related to areas of specialization. Use strong knowledge in algorithms and predictive models to investigate problems, detect patterns and recommend proactive solutions. Use strong programming skills to explore, examine and interpret large volume of data in various forms.\\n\\n64920\\n\\n</br></br> Background Experience: </br>Masters degree in Statistics, Mathematics or related\\n\\nMinimum 1 year of experience with data analytics/engineering in the healthcare industry, either as a Data Engineer or Data Analyst. Must have experience using advanced analytics tools and languages to extract, transform and analyze large data sets from multiple data sources; extracting/manipulating large datasets; and experience handling large datasets for data processing, analytics and reporting purposes. Must also have experience with the following tools/technologies: Hadoop; Hive; Spark; Python; and SQL.\\n</br></br> Potential Telework Position: </br>No</br></br> Percent of Travel Required: </br>N/A</br></br> EEO Statement: </br>Aetna is an Equal Opportunity, Affirmative Action Employer</br></br> Benefit Eligibility: </br>Benefit eligibility may vary by position. Click here to review the benefits associated with this position.</br></br> Candidate Privacy Information: </br>Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately.</br></br>\",\n",
       "  'Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of what’s possible—together.\\n\\nFounded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 6,000 employees. We were named one of Fortune’s 100 Best Companies to Work For in 2018 and are regularly recognized by our employees as a best place to work. You can find us in 28 cities across the U.S., U.K., and Canada.\\n\\nJob Title:\\n\\nData Engineer\\n\\nData is served to hundreds of our clients and their customers. There is a big demand for analytics and data-freshness, so if you are not afraid of complex business problems, and you think traditional tools are slow, and you won’t stop looking for new approaches and technologies, then we have a position for you!\\n\\nAs a Data Engineer, you should have the expertise in the design, creation, management, and business use of large data sets to drive practical insights. You know and love working with analytics tools and can use your technical skills and creative approaches to help clients solve their most critical business challenges. This individual will be an integral part of New York’s Data and Analytics Practice and, in addition to working with clients, collaborate closely with members of the Slalom team who are focused on data visualization, advanced analytics, data science, and data strategy.\\n\\nResponsibilities:\\nCollaborate with engineers and business customers to understand data needs, capture requirements and deliver complete BI solutions\\nConduct and support white-boarding sessions, workshops, design sessions, and project meetings as needed, playing a key role in client relations\\nDesign and build data extraction, transformation, and loading processes by writing custom data pipelines using either cloud native services (AWS/GCP/Azure) or using open source tools (like Airflow and Python)\\nDesign, implement, and support an Enterprise Data platform (Data Lake, Data Warehouse, etc.) that can provide ad-hoc access to large scale structured, semi-structured, and/or unstructured datasets\\nExperience with both traditional (i.e. SSIS, Informatica, Talend) and modern (i.e. Dell Boomi) data integration iPaaS technologies\\nHighly self-motivated to deliver both independently and with strong team collaboration\\nAbility to creatively take on new challenges and work outside comfort zone\\nStrong written and oral communications along with presentation and interpersonal skills\\nDeliver transformative solutions to clients that are aligned to industry best practices and provide thought leadership in data architecture and engineering space\\nQualifications:\\n5+ years of experience in using SQL and databases in a business environment\\n5+ years of experience in custom ETL/ELT design, implementation, and maintenance\\n3+ years of experience with schema design and data modeling\\n3+ years of experience applying data architecture or engineering to solve real-world business problems\\n2+ years of experience with building integration and ingestion frameworks leveraging API based tools and platforms (i.e. Dell Boomi)\\nManipulating/mining data from database tables (i.e. SQL Server, Redshift, Oracle)\\nSQL, ETL/ELT optimization, and analytics tools experience (i.e. R, HiveQL)\\nPrior implementation experience in building both batch and real-time/near real-time data ingestion frameworks using technologies like Kafka, AWS Kinesis etc.\\nNice to have:\\nImplementation experience with various technologies under Hadoop eco-system (HDFS, Hive, HBase, Sqoop, Pig, Presto etc.) and Spark (PySpark preferred)\\nExperience with designing digital data platforms leveraging clickstream data from Adobe Analytics (Omniture/SiteCatalyst) or Google Analytics\\nExperience in languages such as Python and Spark\\nExperience working in Agile Scrum teams\\nLinux and Windows proficiency\\nManagement consulting experience\\nProject management experience\\nExperience working with various verticals (i.e. insurance, retail, healthcare, financial services, technology)\\nSlalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law.',\n",
       "  \"Fora Financial, located in NYC, is a leading financial services company providing working capital to small and mid-size businesses nationwide.\\n\\nEstablished in 2008, we support small businesses in need of financing to sustain or grow their enterprise by providing innovative, customized, and flexible working capital solutions. We operate through a consultative approach, listening to customers and providing what best meets their needs in an ever-changing financial market. To date, Fora Financial has funded over 40,000 individual transactions to more than 25,000 unique customers. The company has supplied over $1.7 billion in working capital solutions for small businesses since its inception.\\n\\nA Day in the Life of a Data Analyst:\\nThis position will require strong SQL programming skills to write SQL queries, gather data, prepare reports and provide analytical support to senior management within the Credit Team\\nDevelop an understanding of all the data contained in multiple databases\\nHelp to build an infrastructure to ensure data integrity across the organization\\nWork with Credit Team leaders to define metrics, capture data and build informative dashboards and reporting\\nPartner with IT team to facilitate and support Enterprise-wide data quality improvement initiatives\\nWhat You Have:\\nBachelor's degree in Computer Science, Engineering, Statistics, Mathematics or related field\\nAt least 3 years of professional work experience as a Data Analyst or in an analytical role\\nProficiency in writing and understanding SQL queries\\nAbility to develop SQL code as needed to extract and manipulate data\\nAdvanced level of MS Excel skills, including advanced formulas, VBA, Macros and data tables\\nIn depth understanding of data collection, data flow management, data manipulation, data archiving and data standards\\nStrong communication skills with respect to explaining technical database details to non-technical audience and explaining insights and results of analysis\\nThe Benefits of Working for Fora Financial:\\nCompetitive Base Salary\\nComprehensive Product and Industry Training\\nMedical, Dental, Vision Coverage\\n401K Match\\nLife Insurance at No Cost to Employees\\nGenerous Time Off Plan Including Rollover Vacation Days\\nCommuter Benefits\\nHealth Care and Dependent Care Flexible Spending\",\n",
       "  '1) 8+ years of experience in Capital Markets\\n2) Strong Data Analysis skill\\n3) Good in Trade /Position and Reference Data\\n3) Strong SQL knowledge\\n4) Should have experience in writing BRD & FRD',\n",
       "  \"About Neo4j\\n\\nNeo4j, Inc. is the graph company behind the #1 platform for connected data. The Neo4j Graph Platform helps organizations make sense of their data by revealing how people, processes and digital systems are interrelated. This connections-first approach powers intelligent applications tackling challenges such as artificial intelligence, fraud detection, real-time recommendations, and master data.\\n\\nThe company boasts the world's largest dedicated investment in native graph technology, has amassed more than ten million downloads and has a huge developer community deploying graph applications around the globe. More than 250 commercial customers, including global enterprises like Walmart, Comcast, Cisco, eBay, and UBS use Neo4j to create a competitive advantage from connections in their data.\\n\\nRole Description\\nThe Neo4j Solutions organization is seeking an experienced data scientist who is passionate about graphs and machine learning to join a growing team. As a solutions data scientist, you will be responsible for scoping, prototyping and productionizing solutions to technical problems that make use of traditional data science methodologies as well as graph algorithms. Examples include building tools for entity resolution (disambiguation) across large heterogeneous databases, recommendation systems that make use of both collaborative filtering and machine learning, or fraud detection models that use graph based features to better identify fraudulent accounts or transactions. This role will work with the rest of the solutions team to develop frameworks and examples that can be deployed to customers or customized for specific use cases. You will also be working directly with Neo4j’s customers and our professional services team to help customers realize their AI/ML solutions. This role is 50% billable on customer projects throughout the year.\\n\\nYou will:\\nPartner with external customers to identify, scope, and implement solutions to graph driven data science problems,\\nWork independently to prototype and productionize deployable machine learning models that can support real world use cases like entity resolution, recommendation engines, fraud detection, or relational inference\\nWork with Solutions Engineering to develop customizable user interfaces and solution frameworks\\nSupport sales and professional services teams in their conversations and interactions with customer data science teams, including helping scope and assess user requirements\\nCollaborate with Product Management and Engineering to leverage new product features, and to drive product prioritization.\\nWrite code to ingest and process large datasets and develop appropriate data models (database architectures) to meet user requirements to support machine learning applications\\nBecome an expert in the application of graph algorithms, and graph native machine learning approaches, to real world machine learning problems.\\nBe willing to travel up to 50%\\n\\nIdeally, you should have:\\nMasters or PhD in Computer Science, Mathematics, Statistics or other quantitative fields, or equivalent experience\\n2+ years of experience in building and productionizing machine learning models using both supervised and unsupervised approaches.\\nClient facing consultative experience of at least 5 years\\nProven experience working with data science teams to implement AI/ML solutions\\nExperience in mentoring or leading small teams\\nFamiliarity with NLP approaches, including entity recognition, ontology management, and topic modeling\\nProficiency in Python, familiarity with Java, Javascript or Scala\\nExperience working with modern machine learning libraries, such as SKLearn, Pandas, Numpy, PyTorch, Keras, TensorFlow, nltk, PySpark, etc.\\nExperience working with big data frameworks such as Hadoop or Spark\\nExcellent client facing skills, including working with both technical and non-technical customers to define requirements, approaches, and acceptance criteria\\n\\nNice to have:\\nExperience working with Neo4j and Cypher\\nDomain expertise in at least one of - Financial Services, Manufacturing/Logistics/Supply Chain, Healthcare or Retail\\nFamiliarity with R\\nExpertise in graph driven data science applications, such as graph based feature engineering, graph embeddings, or graph neural networks\\n\\n\\nWhy join Neo4j?\\nWe are one of the fastest growing startups in Enterprise Software in Silicon Valley and this is an opportunity to join our fast-growing team.\\nWe offer a competitive salary and employer-paid benefits for employees and their dependents.\\nWe pride ourselves in being an American company with a Swedish soul. We hire smart, funny, ambitious, humble people. You will love your co-workers.\\n\\nNeo4j is an equal opportunity/affirmative action employer supporting workforce diversity.\\n\\nNeo4j is a privately held company funded by One Peak Partners, Morgan Stanley, Fidelity Growth Partners Europe, Sunstone Capital and Conor Venture Partners, Creandum, and is headquartered in San Mateo, CA, with offices in Sweden, UK, and Germany. For more information, please visit www.neo4j.com.\",\n",
       "  \"JPMorgan Chase & Co . (NYSE: JPM) isa leading global financial services firm with assets of $2.6 trillion andoperations worldwide. The firm is a leader in investment banking, financialservices for consumers and small business, commercial banking, financialtransaction processing, and asset management. A component of the Dow JonesIndustrial Average, JPMorgan Chase & Co. serves millions of consumers inthe United States and many of the world's most prominent corporate,institutional and government clients under its J.P. Morgan and Chase brands.Information about JPMorgan Chase & Co. is available at www.jpmorganchase.com\\n\\nChase Consumer & Community Banking (CCB) serves nearly 66 million consumers and 4 million small businesses with a broad range of financial services through our 137,000 employees. The Consumer & Community Banking Business Modeling Center of Excellence is a newly formed unit in support of modeling needs for marketing, finance and operations. Our modelers work directly with each line of business, Consumer Banking, Business Banking, Auto Finance, Credit Card and Commerce Services, Chase Wealth Management and Home Lending, to build state-of-the-art models and tools to support business growth.\\n\\nWe are an intellectually diverse team of economists, statisticians, engineers, and other analytics professionals, focused on quantitative modeling within CCB at JPMorgan Chase & Co. The team answers complex and unique questions, utilizing cutting edge quantitative and computational techniques and leveraging one of the world's largest repositories of consumer data. We work closely with our partners throughout JPMorgan Chase to assess and execute critical business decisions.\\nThe position is located at either New York, NY or Columbus, OH\\n\\nResponsibilities:\\nApply critical thinking skills and perform advanced analytics with the goal of solving complex and multi-faceted business problems.\\nGenerate deep insights through the analysis of data and understanding of business processes and turn them into actionable recommendations.\\nPerform advanced quantitative and statistical analysis of large datasets to identify trends, patterns, and correlations that can be used to improve business performance.\\nKnow what type of algorithm to use and how to implement them\\nBuild and deploy prototype solutions to demonstrate ideas and prove concepts.\\nDevelop presentations to summarize and communicate key messages to senior management sponsors and colleagues.\\nBecome a subject matter expert and trusted advisor in the analytics discipline.\\nCollaborate with others in the organization to develop new ideas and brainstorm potential solutions.\\nActively contribute to the continuous learning mindset of the organization by bringing in new ideas and perspectives that stretch the thinking of the group\\nQualifications:\\nDeep quantitative/programming background with a graduate degree (M.S., Ph.D.) in Statistics, Engineering, Computer Science, Mathematics, Operations Research, or Economics,\\n3 years of related experience preferred\\nHands-on experience with Machine Learning and Artificial Intelligence\\nAbility to write code and develop production-ready analytical applications\\nSignificant experience working with very large scale (structured and unstructured) data\\nExpertise in at least one of the following: Python, R\\nExcellent written and oral communication skills to clearly present analytical findings and business recommendations. Highly motivated, productive, and teamwork oriented.\\nGood project management skills (clear goal setting, well-organized, detailed planning, and ability for tight-timeline deliverables).\\nDesired characteristics\\nAble to translate ambiguous business problems into a conceptual mathematical framework\\nPassionate about continuous learning and professional development\\nDeeply curious; creative and imaginative\\nAbility to influence and become a trusted advisor\\nAbility to convey complex concepts to non-technical audiences\\nEffective communication and presentation skills\\nCan work both independently and collaboratively\",\n",
       "  \"Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities - we're just getting started.At Facebook, we pride ourselves on making data-informed decisions. This includes not only decisions we make about our platform, which serves over 1 billion users, but also how we learn more about our most valuable asset our people. For instance, we use data to understand how to hire top talent, to ensure our people are engaged, and to support diversity.\\n\\nWere looking for a People Research Scientist to join our people analytics function. Our team uncovers data-driven insights to better attract, develop, motivate, and retain Facebooks most important asset our people. This role will serve as an active partner with HR and functional leaders to perform research and analyses on a broad spectrum of people issues: to optimize employee attraction and selection, performance, growth, engagement, diversity, compensation, and retention. Youll also help develop and refine people processes to make an exceptional workplace even better. The ideal candidate will have strong quantitative skills and experience working across a variety of people areas.\\nThis is a 2 year position based in Menlo Park, CA or New York, NY (preferred).\\n\\nResponsibilities:\\n\\nApply your expertise in people research, quantitative analysis, data science and data visualization to help Facebook recruit, select, grow, and retain talent\\nSee beyond the data to identify solutions that will raise the bar for all things people-related\\nCollaborate with data engineering and visualization engineers to access and manipulate data, explain data gathering requirements, and display results\\nPartner with the HR, recruiting and diversity teams on people projects to identify and clarify critical people issues, deeply understand our people processes, and analyze data to drive improvements\\nBring together Facebook-specific data and outside research to help leaders understand people issues\\nCommunicate statistical analyses and results, along with implications, to technical and non-technical audiences\\nCollaborate effectively with the Employment Law team on a variety of sensitive projects\\nCollaborate effectively with and mentor other team members\\nDemonstrate exceptional judgment and discretion when dealing with highly sensitive people data\\nMininum Qualifications:\\n\\nPhD, or MS with 2+ years of experience in a field emphasizing people research in organizations (e.g., Industrial/Organizational Psychology, Organizational Behavior, Economics, Management, Policy Analysis, etc. or related degree)\\n3+ years of experience applying multivariate statistical methods such as GLM, analysis of quasi-experimental research designs with non-equivalent groups (e.g., regression adjustment, matching, propensity score stratification), longitudinal analysis, classification, dimension reduction, clustering, hierarchical linear (random effects) modeling, etc.\\nExperience managing and analyzing structured and unstructured data using tools such as R or Python (or using syntax in SPSS or STATA)\\nExperience communicating technical results to technical and non-technical audiences\\nPreferred Qualifications:\\n\\n2+ years of experience with machine learning, text mining or modeling high-dimensional data\\n1+ years experience working internally (as opposed to consulting) in an organization with 500+ professional employees (e.g., high-tech, financial services, healthcare, biotech, etc.)\\n1+ years of experience conducting quantitative people research in an organization or consulting environment, including working with stakeholders to understand and clarify their people research needs, and communicating analyses to technical and non-technical audiences\\nExperience developing tools, conducting empirical research, practical implementation, knowledge of best practices in at least three of the following areas: diversity, employee engagement, performance management, recruiting, or retention\",\n",
       "  'Greetings of the day, I am Shravya working as US IT Recruiter in Tech Projects Company,i have a requirement with one of our client,if you are interested for below position you can reach me atÂshravya@techprojects.us/732-258-0370/315-928-2169.\\n\\nJob Title: Data Scientist with SQL, Python, R, Tableau.\\n\\nLocation: Manhattan, NY\\n\\nDuration: 12 Months Plus\\n\\nInterview Mode: F2F Mandatory.\\n\\nÂ\\n\\nDescription:\\n\\nExperience requirements\\n5+ years as a data scientist leading efforts to identify relevant questions, collect data from a multitude of different data sources, organize the information, translate results into solutions, and communicate findings in a way that positively affects decisions\\n5+ years working with SQL, Python, R, Tableau and other data science programing languages and tools\\nStrong quantitative and problem-solving skills\\nExperience and passion for data wrangling, data cleaning, and ETL\\nExperience working with administrative data sets\\nExperience with statistical modelling and machine leaning analysis\\nExperience with Bayesian analysis\\nProficiency in GIS concepts and software (ArcGIS, Google Maps, QGIS, Carto)\\nAttention to detail for documenting work processes and writing clear instructions for technical tasks\\nAbility to distill complex material into actionable recommendations\\nExcellent written and oral communication skills\\nJOB Duties\\nDevelop SQL and Python queries to analyze the completeness and quality of key data elements in StreetSmart, including demographics, caseload history, mental illness diagnoses, and substance abuse details\\nDevelop and monitor a data cleaning prioritization plan, working with a data analyst dedicated to data cleaning\\nManipulate and analyze administrative data in order to predict outcomes and make data-driven recommendations\\nApply statistical and data mining techniques to conduct performance audits, trend analysis, and predictive analytics using StreetSmart data\\nCollaborate with team members to develop novel strategies for technical analysis\\nEvaluate ethical implications of design choices for predictive analytics models and automated decision support systems\\nCreate and present compelling reports to stakeholders based upon project findings and methods\\nIf you are interested you can reach me at shravya@techprojects.us/732-258-0370.',\n",
       "  \"Senior Data Scientist\\nSartorius\\n\\nSartorius As one of the world‘s leading laboratory and pharmaceutical equipment providers, Sartorius is actively shaping dynamic, innovative and high-growth markets.\\nFounded in 1870, the company earned sales revenue of more than 1.3 billion euros in 2016. More than 6,900 people work at the Group's approximately 50 manufacturing and sales sites, serving customers around the globe.\\n\\nWhat you can look forward to accomplishing Sartorius Stedim Data Analytics, a market leader in advanced analytics and provider of Umetrics Suite of Data Analytics Solutions, is on a strong growth path driving new innovative solutions in the field. It currently has an outstanding opportunity for a Senior Data Scientist to drive customer applications, support the regional sales teams and feedback customer needs into the development of new solutions. The incumbent for this key role will have a strong customer focus to help secure innovation and growth from both new and existing customers.\\n\\nWhat you can look forward to accomplishing:\\nAssisting customers in solving business related problems using advanced data analysis and data driven technologies\\nSupporting implementation of advanced analytics in industry\\nAnalyzing large amounts of data and building data driven models\\nInitiating, leading and driving customer projects\\nPerforming regular follow up with customers to track satisfaction, secure project momentum and discover additional opportunities\\nTraining customers on the use of our software\\nSupporting sales, marketing and development\\nCreating technical documents\\nAttending and presenting at conferences, seminars and forums\\nDeveloping & cultivating strong relationships with external and internal stakeholders\\nBeing part of a team of data scientists\\n</p>\\n\\nSkills and experience that you need to bring to our company\\nMinimum of a master’s degree in a relevant field (engineering, informatics, biology, automation/control, etc.)\\nMinimum of five years’ work experience within Biopharma, Pharma, Chemical or Food/Agriculture\\nMinimum of five years’ working knowledge of applied statistics including a combination of; MVDA, DOE, MSPC, PAT, QbD, Model Predictive Control (MPC), MMPC, AI/Machine learning\\nFamiliarity with Umetrics Suite of Data Analytics Solutions is a plus (MODDE, SIMCA, SIMCA-online, Control Advisor, Active Dashboard)\\nWorking knowledge of prescriptive analytics is a plus\\nWorking knowledge of Python, MATLAB, data historians, automation & control\\nis a plus\\nTeaching, consulting and/or project management experience is desirable\\nExcellent communications skills, strong presentation skills\\nComfortable working independently\\nComfortable working from home office, willingness to travel (approx. 20%)\\nStrong identification with Sartorius core values: sustainability, openness, enjoyment\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex or national origin. We are also an equal opportunity employer of individuals with disabilities and protected veterans.\\nPlease view equal employment opportunity posters provided by OFCCP here\\n\\n</p>\\n\\nInterested? Driving our future growth requires talented people. Sartorius is a dynamic organisation suited to people who want to showcase skills, be recognised for expertise and thrive in a vibrant and innovative environment.\\n\\nTo find out more about Sartorius as an employer visit us on https://www.sartorius.com/sartorius/en/EUR/company/careers?setCountry=WW-en\\n\\nBecome part of our global team. Apply directly via our online job portal.\",\n",
       "  'WHO WE ARE\\n\\nBraze is a customer engagement platform that forms strong bonds between people and the brands they love. We empower brands to humanize their connections with customers through technology, resulting in better experiences and increased retention, lifetime value, and ROI. Teams use Braze to deliver highly personalized messaging experiences that span across channels, platforms, and devices. This is made possible by our stack, which is located at the core of a modern marketing technology ecosystem.\\n\\nBraze is a venture-backed company with hundreds of employees. Our offices are located in New York City, San Francisco, and London. Most recently, we\\'ve been named a Leader in the Forrester Wave™: Mobile Engagement Automation, Q3 2017 evaluation. We\\'ve received the Digiday Signal Award for Best CRM, in addition to being named VentureBeat Omnichannel MMA\\'s \"Best Bet,\" a Cloud100 Rising Star in Forbes, and #21 in the Deloitte Technology Fast 500 List. Learn more at Braze.com.\\n\\nWHAT WE\\'RE LOOKING FOR\\n\\n\\nYou love everything about data. You have no problem diving into data; cleaning it, transforming it, analyzing it, visualizing it, and using it to tell stories. You want to develop your passion into a career at a rapidly growing company with tons of data about mobile and digital interactions. You want to contribute to the business as well as grow the business.\\n\\nWHAT YOU\\'LL DO\\nIdentify reports, dashboards, and analysis needed by the business\\nDesign and implement said reports, dashboards, and analysis\\nAnalyze billions mobile and digital messaging data points\\nBe in the weeds pulling, cleaning, analyzing, and transforming raw data into insightful findings\\nWHO YOU ARE\\n4-year college degree AND 2 - 6 years of experience professional experience, preferably working with data\\nWorking knowledge of relational (SQL) and/or non-relational (Mongo) databases and syntax\\nStrong math, statistics, and reasoning skills\\nFast learner, self-starter, go-getter, and intellectually curious\\nExperience partnering with Marketing teams\\nIDEALLY, WHAT YOU\\'LL HAVE\\nFamiliarity with Python, Ruby, and/or Javascript\\nFamiliarity with Looker and/or Snowflake\\nExperience with Marketo\\nWHAT WE OFFER\\n\\nComplete tech startup vibe including free daily lunches & snacks, group events and top of the line computer setup! A general feeling throughout the office that what you do matters everyday.\\nExcellent medical insurance and life assurance coverage for you and your dependents\\nMatching 401K\\nDaily catered lunches, snacks and beverages\\nCollaborative, transparent, collegial and fun loving office culture\\nFlexible time off policy to balance your work and life in the way that suits you best\\nIn addition, this position is exempt under the provisions of the Fair Labor Standards Act.',\n",
       "  \"The Role\\n\\nTripleLift is seeking an experienced data scientist to join our team full time. We are a fast growing startup in the advertising technology sector, trying to tackle some of the most challenging problems facing the industry. As a senior data scientist, you will responsible for exploring and leveraging our data in order to turn into actionable insights that help push the business forward for TripleLift and our customers. This includes optimizing our bidding algorithm, improving our computer vision logic, and planning and executing experiments to understand our real time bidding marketplace.\\n\\nAbout TripleLift\\n\\nTripleLift is building ads that people enjoy.\\n\\nAdvertising is an essential part of the internet. Yet most ads are either never noticed, or are interruptive. We make advertising that's helpful and effective.\\n\\nWe believe that by recreating the advertising experience to be seamless and focused on content, we can usher in a new era of content monetization. All day we are bombarded by ads online, many of which were created over 20 years ago. TripleLift is building the ad experience of the future.\\n\\nTripleLift is focused on delivering consumer-focused, digital advertising technology that seamlessly integrate native advertising into every channel of content consumption, including web, app, branded content, and connected TV. By utilizing our proprietary computer vision technology, TripleLift is able to dynamically assemble native ads that match publisher content that are accessible through the industry's first real-time, native programmatic exchange.\\n\\nTripleLift is one of the fastest growing companies in America, and has achieved a diversity of product lines, a leadership position in its industry and is regularly recognized as an innovator in its space - all while being consistently profitable.\\n\\nCore Technologies\\n\\nThe Data Science team employs a wide variety of technologies to accomplish our goals. From our early days, we've always believed in using the right tools for the right job, and continue to explore new technology options as we grow. The Data Science team uses the following technologies at TripleLift:\\nLanguages: Python, R, Java 8\\nFrameworks: Spark, Airflow, DataBricks\\nDatabases: MySQL, Redshift, MongoDB, S3/Parquet\\nAmazon Web Services to keep everything running\\nResponsibilities\\nOptimize our estimation models in order to improve the performance of our bidding algorithm and maximize performance for our customers;\\nPlan, execute, and analyze experiments to understand the dynamics of our real time bidding marketplace;\\nEagerly explore the dark corners of our data in order to push our business forward.\\nDesired Skills and Attributes\\nStrong Python skills, Spark is a plus;\\nProven experience building out predictive and forecasting models;\\nComfortable taking ownership of projects and showcasing key accomplishments;\\nStrives for continued learning opportunities to build upon craft;\\nExcellent organizational skills and attention to detail;\\nAbility to work quickly and independently with minimal oversight;\\nAbility to work under pressure and multitask in a fast-paced start-up environment;\\nDesire to accept feedback and constructive criticism;\\nExtremely strong and demonstrable work ethic;\\nProven academic and/or professional achievement.\\nEducation Requirement\\n\\nA Bachelor's degree in a technical subject is preferred, although candidates with relevant experience who hold other degrees will be considered.\\n\\nExperience Requirement\\n\\nAt least three years of working experience in a professional, collaborative environment.\\n\\nLocation\\n\\nNew York, NY\\n\\nBenefits and Company Perks\\nAmazing company culture\\nComprehensive Medical, Dental and Vision insurance\\nEquity options\\n401(k) program\\nSnacks on snacks on snacks\\nYoga, massages, and meals\\nOngoing professional development\\nTripleLift Awards\\nInc. Magazine's list of fastest-growing companies - 2017, 2018, 2019\\nCrain's Best Places to Work - 2015, 2016, 2017, 2018\\nForbes Next Billion Dollar Companies - 2018 Inc.\\nCrain's Fast 50, Fastest Growing Companies in New York - 2017, 2018\\nDeloitte Technology Fast 500 - 2017, 2018\\nCEO & Co Founder Eric Berry won the Ernst & Young Entrepreneur Of The Year 2019 New York Marketing and Advertising Award\\nNote: The Fair Labor Standards Act (FLSA) is a federal labor law of general and nationwide application, including Overtime, Minimum Wages, Child Labor Protections, and the Equal Pay Act. This role is a FLSA exempt role.\",\n",
       "  'FTSE Russell / Yield Book prides itself in setting a market standard for RMBS and CMBS prepayment models that are the cornerstone of Fixed Income Analytics products and services provided by the firm.\\n\\nReporting to the Director – Head of Mortgage Research, the RMBS Data Scientist is a junior to mid-level role responsible for analysis of mortgage collateral data and contributing to development of the suite of firms’ Agency and Non-Agency Prepayment models. The individual will be involved in building ETL processes, and will also participate in building model construction and fitting tools. A successful candidate is a self-starter and highly motivated individual striving to deliver high quality product. This person is expected to grow over time to participate in estimation of the models.\\n\\nKey Responsibilities\\nDevelop and maintain ETL processes of large MBS collateral data sets, under guidance of senior modellers and developers.\\nBuild model construction and fitting tools, as well as model performance monitoring tools.\\nEnforce data quality checks, collaborate with other members of the team to build scalable QA frameworks and deploy them to production.\\nParticipate in development of data-driven and behavioural models utilized in MBS space.\\nBe proactive in learning about the product, stay on top of new technologies.\\nContribute to model validation process, as required by internal and external requests.\\nMaintain model documentation at the appropriate level of clarity and detail, as required for data processes.\\nKey Behaviors\\n\\nIntegrity\\nHas the sustained drive and energy to deliver on time and with high quality.\\nPartnership\\nValues practical aspect of data and code development, emphasis on quality of code, scalability.\\nDeals with conflict successfully.\\nInnovation\\nOpen to and willingly adopts new processes / approaches / ways of working.\\nSeeks information/inputs from colleagues.\\nExcellence\\nExcellent communication skills are required to interact with other modelers and developers.\\nWillingly puts in the effort to ensure activities completed on time and to the quality required.\\nPro-active and demonstrates initiative.\\nPrioritises activities according to business and operational need.\\nAnalysis and problem solving.\\nAnalyses issues to identify the most appropriate solutions.\\nUtilises all available resources and toolsets to investigate and resolve problems.\\nCandidate Profile / Key Skills\\nAdvanced academic degree (Masters or PhD equivalent) in a quantitative field, such as Science, Engineering, Statistics, Data Science, or Mathematics or equivalent.\\nExcellent coding experience with Python. Coding experience in C/C++ is a plus.\\nKnowledge of Cloud computing is a plus.\\nKnowledge of Machine Learning frameworks is a plus.\\nDemonstrable years of Industry experience within Fixed Income Analytics field is a plus.\\nKnowledge of Securitized Products sector and MBS models is a plus.\\nGood understanding of Fixed Income models and analytics, including interest-rate, and credit is a plus.\\nTeam player, ability to learn new models quickly and efficiently.\\nHigh motivation.\\nPeople are at the heart of what we do and drive the success of our business. Our colleagues thrive personally and professionally through our shared values of Integrity, Partnership, Innovation and Excellence are at the core of our culture. We embrace diversity and actively seek to attract people with unique backgrounds and perspectives. We are always looking at ways to become more agile so we meet the needs of our teams and customers. We believe that an inclusive collaborative workplace is pivotal to our success and supports the potential and growth of all colleagues at LSEG.\\n\\nA career with London Stock Exchange Group offers you the opportunity to be at the centre of the financial community. As well as competitive salaries and a range of attractive benefits, we maximise each employee’s potential through personal development plans, training, coaching and mentoring.\\n\\nPlease take a moment to read this privacy notice carefully, as it describes what personal information the London Stock Exchange Group (“We”) may hold about you, what it’s used for, and how it’s obtained. If you would like this information to be removed from the London Stock Exchange Group HR database, please contact workday@lseg.com. If you choose to have your information removed, you will be removed as a candidate and we will not be able to progress your application for opportunities at the London Stock Exchange Group.',\n",
       "  'Why join LYNK?\\nLYNK is a VC-backed, product-driven startup working with leading institutional clients, top level experts and thought leaders globally\\nWe operate in a high-octane environment where our people think about the big picture and always strive to “make it happen”\\nOur team, spread across four countries today (and growing!), is multinational, multilingual and multicultural. Our clients have likened us to a mini United Nations.\\nYou will be constantly challenged with new problems to solve every day.\\nWe are here to realize big dreams and have a firm belief in our core mission – to democratize access to knowledge.\\nThis position is based in NYC.\\n\\nWhat You’ll Do:\\n\\nJoin us, and tackle some of the most challenging problems in natural language processing and large scale applied machine learning. You will build cutting edge natural language understanding technologies and deploy them on a global scale.\\nDeliver data-driven products, insights and inferences employing methodologies in the Data Science and primarily Machine Learning domains\\nMaintain knowledge of and concurrency with cutting-edge but commercially-viable (resilient and scalable) methodologies in the Data Science and primarily Machine Learning domains\\nTranslate complex and embryonic ideas into tangible data-driven deliverables; iterate; test; and sensibly promote or demise solutions to our customers\\nDesign and implement Semantic-enabled systems, enterprise data lakes and semantic search applications in the intersection of Semantic web and ontology, knowledge graph and domain model, NoSQL and graph database, NLP, Solr/Lucene, text mining and machine learning\\nBuild semantic master/ systems to automate data analytics and services using AWS EC2, S3 (distributed data lake), Elastic Map Reduce (computation cluster), Data Pipeline (job management in Spark (Scala) on Hadoop, using Avro and Parquet) and Redshift (columnar data warehouse), and Google Big Query and CloudStorage.\\nDesign domain-driven event models and event sourcing paradigm for a real-time Web-scale \"event\" processing platforms\\nRequirements\\n\\nWhat Expertise You’ll Add To The Team:\\nTop-notch expertise in Knowledge Representation and Discovery, Semantic Web, Ontology and Knowledge Graph, NLP and text mining\\nMS or PhD in computer science, statistics, or a quantitative domain\\n3+ years professional experience (post-graduation) delivering, scaling, and leading highly successful and innovative machine learning products\\nAdvanced expertise core ML concepts (i.e. feature discovery and engineering, model validation, retraining strategies)\\nThe ability to train elementary machine learning models such as logistic regression and random forests, to select the right tool or the right task and to judge when a model is good enough for a particular purpose\\nSubstantial depth in at least a few key areas of NLP such as :\\nclassic NLP approaches (i.e. LDA, TF-IDF),\\nembeddings based techniques (i.e. word2vec, doc2vec, GloVe),\\nneural architectures (i.e. CNN, RNN, attention),\\nvariational inference techniques (LDA)\\nProven track record of implementing a real-world Recommender or Ranking system\\nExperienced in solving real problems using machine learning techniques and with statistical rigor\\nExcellent communication skills; the ability to convey complex analysis results clearly and with conviction to all stakeholder levels\\nBonus Attributes:\\nStrong passion for creative content curation and building brand awareness\\nPassion for business and enthusiastic about taking part in shaping LYNK’s growth\\nFunction well in a very fast-paced startup environment\\nTrack record of excelling in small teams\\nTeam players who thrive in uncertainty and like to “make things happen”!\\nBenefits\\n\\nWhat We Commit To You\\nCompetitive remuneration package in a rapidly-expanding startup\\nWork in a collaborative, co-creation hub in the heart of the city - with amazing facilities\\nComprehensive medical insurance coverage, including dental\\nGenerous leave policy, including a ‘work remote policy’\\nThe opportunity to travel and work around the globe with our international clients and growing number of offices (Hong Kong, Singapore, Mumbai, New York City)\\nThe opportunity to be a part of something impactful\\nVisit http://lynk.global for more info.',\n",
       "  'Who we want:\\nAre you committed to using your talents in research and advanced analytics to give global organizations critical advice about their growing needs?\\nAre you a leader who applies your instinct and expertise to discover breakthroughs that are key to clients’ growth?\\nAre you a driven professional who can manage multiple projects, set a standard of excellence and follow through on commitments for exceptional results?\\nDo you instinctively connect with others, understand individuals’ needs and share your passion for analytics to achieve shared goals?\\nDo you excel at building predictive models using various data sources and techniques to inform practical business decisions?\\nWhat you will do:\\nAs a Gallup senior data scientist, you will help clients effectively use data to make better decisions. You will apply your knowledge of various statistical and machine-learning techniques to lead a wide variety of challenging projects — ranging from designing custom builds for client needs to automating solutions to the challenging problems clients face every day. You will partner with client teams to increase Gallup’s global impact by helping explain and predict large-scale social behavior (e.g., consumer spending, lifestyle trends, political stability, election outcomes, and employee performance and retention) using data from Gallup, the web and third parties (e.g., governments, IGOs and NGOs). You will mentor and develop other data scientists, helping them learn and grow their technical and consulting skill sets. In short, you will be a senior leader who will help further develop data science at Gallup.\\n\\nWe have data that no one else has. This gives you an unparalleled opportunity to use your creativity to explore new avenues of social research as part of the legacy founded by George Gallup in 1936 that set the gold standard in survey research methodology.\\n\\nWhat you need:\\nA master’s degree or Ph.D. from a statistics, engineering, mathematics, computer science, computational social science, physics or operations research program is preferred.\\nMinimum of 12 years of work experience (advanced degrees count toward years of experience).\\nExpert-level production coding in Python is required.\\nMastery in conducting analysis in Python and/or R is required; additional analytic software experience is a plus.\\nMinimum of four years of experience building production-level machine learning and predictive analytics systems along with data pipelines.\\nA deep understanding of the mathematical fundamentals of machine learning and statistics, with an emphasis on non-parametric, non-linear methods (e.g., random forests, support vector machines, neural networks) and natural language processing.\\nMinimum of one year of experience working within distributed systems and managing workflows in a cloud infrastructure.\\nCandidates must be authorized to work in the United States.\\n\\nGallup is an equal opportunity/affirmative action employer that celebrates, supports and promotes diversity and inclusion. We will consider all qualified applicants without regard to race, color, religion, sex, national origin, disability, protected veteran status, sexual orientation or gender identity, or any other legally protected basis, in accordance with applicable law.',\n",
       "  'ADARA serves its customers by delivering critical intelligence that drives personalization and relevance throughout the customer’s journey resulting in more enduring and profitable relationships. Fueling these insights is our digital identity network which connects over 1 billion digital identities to create a comprehensive view of their behaviors across brands, channels, and devices. ADARA transforms how our B2C clients leverage consumer insight at every stage—learn, act, measure and modify—to unleash the revenue potential of each individual.\\n\\nADARA is seeking a results-oriented Field Data Scientist to work in a fast-paced client-facing role with competing priorities. Working in the Customer Success team you will be responsible for delivering key analytics and models to Adara customers to help drive the performance of their digital and non-digital media campaigns. The Field Data Scientists should be business savvy and able to leverage their technical skills (strong SQL, Python or other languages) to create models and actionable insights to drive improved client business outcomes. The Field Data Scientist will have previous experience building various algorithms: e.g., logistic regressions, decision trees, K-means and forest models.\\nResponsibilities\\nPartner with ADARA’s Customer Success Managers to ensure optimal performance and delivery of its media and data portfolio\\nUnderstand our client’s business requirements and provide consultation on how to best deploy ADARA’s data and media offerings.\\nDeliver to our clients best-in-class descriptive, predictive & prescriptive analytics\\nAct as a subject matter expert on the analysis of campaign performance trends, making recommendations\\nDiligently monitor, optimize and report on data and media performance for key accounts against KPIs, with proactive cross-functional and internal communication on customer deployments.\\nRegular consultation with cross-functional teams on account deployments to ensure optimal performance and delivery of media and data portfolio\\nManage competing priorities across the company\\nCreatively explore data to find interesting stories to tell\\nRequirements\\n4+ years’ experience advanced analytics or data scientist role\\n2+ years’ client-facing experience Agency or consulting experience is strongly preferred\\nPast digital marketing experience\\nExperience in performing data analytics using SQL, Python, and R\\nDepth and breadth in quantitative knowledge.\\nSolid quantitative modeling, statistical analysis skills and problem-solving skills.\\nExperience in performing Data Visualization (Tableau, Oracle BI, or Power BI)\\nExperience building predictive models to improve marketing campaign targeting\\nExcellent written and oral communication skills\\nBS in Computer Science or Computer Engineering, other quantitative degree or relevant experience',\n",
       "  \"DIRECT CLIENT REQUIREMENT\\nJob Title: Data Analyst\\nLocation: Piscataway, NJ\\nDuration: 6+ Months\\n\\nJob description:\\nPosition Description:\\nThe Senior Consultant, Client's Analytics will be part of the Platform Management group within the Client's Marketing, Sales, & Design organizational unit (OU). This non-technical position is responsible for leading, managing, and delivering actionable data, analyses, and reporting for effective decision-making on Client's information products and services. The scope of this role includes Client's branded web, mobile, and API platforms. This position works closely with business stakeholders, technical teams, and other Client's OUs to drive analytics data requirements and capabilities and deliver timely analyses and reporting. This role requires extensive and proven data analysis experience along with strong knowledge of web analytics tools and solutions.\\n\\nKey Responsibilities:\\n\\nPlan, organize, and drive measurement, analysis, and business reporting for a vast range of user and other activity data associated with Client's digital platforms.\\nCompile, analyze, interpret, prepare, and present recurring and ad-hoc analytics reports to various Client's stakeholder and management audiences.\\nDefine and drive business requirements for ongoing and timely improvements in analytics technology and systems capabilities with a focus on automation, efficiency, and speed.\\nDefine and maintain a multi-year roadmap for new analytics capabilities and enhancements.\\nBusiness ownership and management of the internal Client's Analytics Portal.\\nPartner with business and technical stakeholders across Client's to enable data-driven decision-making.\\nQualifications:\\nEducation and Experience:\\nBachelor or Master's in Business, Computer Science, Engineering, Data Analytics, Statistics, or other technical discipline and/or equivalent data analytics business experience.\\nAt least 7 years of hands-on experience in business intelligence, data analytics, or web analytics.\\nPrevious experience in STM publishing and knowledge of STM delivery platforms a plus.\\nSkills & Other Requirements:\\nSuperb analytical skills; must be able to look at and beyond vast amounts of numerical data and discern patterns, correlations, anomalies, trends, etc. and arrive at logical conclusions.\\nHighly advanced skills in Microsoft Excel and data reporting and visualization tools (e.g. Tableau).\\nStrong knowledge of tag management systems (e.g. Tealium), enterprise web analytics solutions (e.g. Adobe Analytics) and other business intelligence tools.\\nExpert at anticipating and identifying data collection needs, requirements, and strategies.\\nDetail-oriented with a passion for number-crunching and data forensics.\\nStrong communication skills and ability to explain analytical conclusions to diverse audiences.\\nExperience working with technical roles such as developers, programmers, etc.\\nWorking familiarity with data storage, analysis, and reporting technologies.\\nMust be able to function independently in a fast-paced, deadline-driven environment.\",\n",
       "  'Senior Data Scientist\\n\\nMunich Re, New York, United States\\n\\nEntry level\\n\\nProfessional\\n\\nType of job\\n\\nFull-time\\n\\nArea of Expertise\\n\\nData Analytics/Engineering\\n\\nYour job\\n\\n\\nApply advanced statistical and machine learning techniques to build models for underwriting, pricing, and claims management\\nHelp us to drive innovation, enabling new underwriting paradigms, distribution models, and data management\\nBuild and implemen...\\nApply now\\nOpen job offer\\n\\nPublished on 08/14/2019',\n",
       "  \"Amazon AI is looking for world class scientists and engineers to join its AI Labs. This group is entrusted with developing core data mining, natural language processing, and machine learning algorithms for AWS services. At Amazon AI you will invent, implement, and deploy state of the art machine learning algorithms and systems. You will build prototypes and explore conceptually new solutions. You will interact closely with our customers and with the academic community. You will be at the heart of a growing and exciting focus area for AWS and work with other acclaimed engineers and world famous scientists.\\n\\nBasic Qualifications\\n\\n· 3+ years of non-internship professional software development experience\\n· Programming experience with at least one modern language such as Java, C++, or C# including object-oriented design\\n· 1+ years of experience contributing to the architecture and design (architecture, design patterns, reliability and scaling) of new and current systems.\\n· PhD degree in computer science, operations research, statistics, engineering, or mathematics\\n· 3+ years of experience in the field with a proven track record\\n· Experience with fast prototyping\\n· Experience with object oriented languages and efficient low level coding\\n· Experience with data mining or machine learning applications\\n· Experience working effectively with software engineering teams\\n· Excellent written and verbal communication skills\\n· Strong publication record at top conferences and journals\\n\\nPreferred Qualifications\\n\\nAs an Applied Scientist you are expected be an expert in an area relevant for large scale machine learning and its applications.\\nYour position will require you to:\\n· Be an active member of a software development team.\\n· Improve and accelerate our technology with science, statistical modeling, algorithm design, and prototyping.\\n· Maintain an understanding of industry and technology trends in said area of research.\\n· Contribute to Amazon's Intellectual Property through patents and/or external publications.\\n· Understand business context to decisions made within and across groups.\\n\\n\\nAmazon.com is an Equal Opportunity-Affirmative Action Employer Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation.\",\n",
       "  \"What you’ll be doing...\\n\\nThis position will drive profitable growth and business innovation by applying cutting edge machine learning techniques and AI technology. It will lead data science projects that drive product personalization, marketing effectiveness, channel optimization, better customer experience, and operational efficiency. You will work closely with a team of talented data scientists, Big Data engineers, and software developers and play a key role in developing and delivering the next generation AI/ML solutions to the business.\\n\\nYou’ll create analytical models to predict customer behavior and business events. You’ll then present the results to various business partners such as marketing, finance, and our customer relationship management group.\\n\\nWith an eye towards improving performance and predictability, you like the science of analytics. Developing resolutions to complex problems, using your sharp judgment to develop methods, techniques, and evaluation criteria enables you to deliver solutions that make a big impact. You’re able to communicate technical information to non-technical audiences, and you take pride in your ability to share your considerable knowledge with your peers.\\nLead engagement with key business stakeholders in discussion on business strategies and opportunities.\\nBuild strong working relationship and develop deep partnership with the business.\\nLead large portfolios of data science projects and deliver innovative technical solutions to solve problems and improve business outcomes.\\nLead the design and development of machine learning/statistical models and ensure best performance.\\nWork closely with engineers to deploy models in production both in real time and in batch process and systematically track model performance.\\nBe a subject matter expert on machine learning and predictive modeling and a mentor to junior data scientists.\\nDrive technical innovation through active research and applications of new theories, techniques, and technologies.\\nWhat we’re looking for...\\n\\nYou are a master at analyzing big data. You thrive in an environment where enormous volumes of data are generated at rapid speed. You’re a creative thinker who likes to explore, and uncover the issues. You are decisive. You are great at influencing up, down, and across groups, and you take satisfaction in mentoring others. Communicating what you’ve uncovered in a way that can be easily understood by others is one of your strengths.\\n\\nYou’ll need to have:\\nBachelor’s degree or four or more years of work experience.\\nSix or more years of relevant work experience.\\nEven better if you have:\\nMaster’s degree in a quantitative field or equivalent.\\nA Ph.D. in Statistics, Math, Economics, Engineering, Computer Science, Business Analytics, Data Science.\\nEight or more years of experience in practicing machine learning and data science in business.\\nStrong foundational quantitative knowledge and skills; extensive training in math; statistics, physical science, engineering, or other related fields.\\nExperience in leading large scale data science projects and delivering from end to end.\\nStrong technical experience in machine learning and statistical modeling.\\nStrong on computing/programming skills; proficiency in Python, R, and Linux shell script.\\nExperience in data management and data analysis in relational database and in Hadoop.\\nStrong communication and interpersonal influencing skills.\\nExcellent problem solving and critical thinking capabilities.\\nExperience with NLP and chatbot technology.\\nExperience with Hadoop, Spark, C++, scala, or Java.\\nStrong experience in SQL and database management.\\nVZAnalytics\\n\\nWhen you join Verizon...\\n\\nYou’ll be doing work that matters alongside other talented people, transforming the way people, businesses and things connect with each other. Beyond powering America’s fastest and most reliable network, we’re leading the way in broadband, cloud and security solutions, Internet of Things and innovating in areas such as, video entertainment. Of course, we will offer you great pay and benefits, but we’re about more than that. Verizon is a place where you can craft your own path to greatness. Whether you think in code, words, pictures or numbers, find your future at Verizon.\\n\\nEqual Employment Opportunity\\n\\nWe're proud to be an equal opportunity employer- and celebrate our employees' differences,including race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, and Veteran status. Different makes us better.\\n\\n]]>\",\n",
       "  \"Job Description: Data Analyst - Analytics/SQL\\nLocation: New York, NY\\nData Operations and Analytics team at Morgan Stanley Wealth Management(MSWM) is responsible for managing batch and data operations, enterprise reporting to its IT and business partners. The team is looking primarily for a resource with relational database (e.g. Oracle, SQL Server, Teradata, LUW DB2) and SQL skills to help advance its goals to build a world class data-driven enterprise.\\nRoles and Responsibilities:\\nThe person will be primarily responsible for:\\n* Interact with business & IT users to understand and document the analysis requirements\\n* Write SQL statements to conduct data analysis\\n* Profile and analyze data stored in various data sources\\n* Document results and present to business and IT stakeholder\\nRequired Skills:\\n* Experience with Relational databases (e.g. Oracle, SQL Server, Teradata, LUW DB2)\\n* Strong knowledge and experience with SQL to conduct data analysis\\n* Prior knowledge and experience with Data Analytics is a plus\\n* Knowledge and experience with Financial Services (Wealth Management) data domain is a big plus\\nJob Type: Contract\\nSalary: $65.00 to $70.00 /hour\\nExperience:\\nrelevant: 9 years (Required)\\nLicense:\\nDriving License (Preferred)\\nWork authorization:\\nUnited States (Required)\\nAdditional Compensation:\\nTips\\nWork Location:\\nOne location\\nBenefits:\\nHealth insurance\\nCompany's Facebook page:\\nhttps://www.facebook.com/Ricefw-Technologies-Inc-109236477147047/?modal=admintodotour\\nSchedule::\\nMonday to Friday\",\n",
       "  'Global Compliance plays a critical role in the successful execution of the firms compliance mission. The Compliance function ensures the development and maintenance of a strong compliance culture by developing and maintaining program infrastructure that identifies measures and monitors adherence with applicable laws, regulations and rules that govern our business globally. Compliance teams work closely with business, legal, and audit functions to provide expertise on regulatory compliance matters, assess and measure compliance and related risk and, monitor and test the adequacy of the firms compliance control environment.\\n\\nWe are seeking highly qualified data science professionals in the execution of strategic projects designed to strengthen and support Citi Compliance\\'s move to using predictive analytics for monitoring and surveillance functions.\\n\\nThe candidate will be a part of a team of data scientists that will be tasked with developing predictive / scoring models using supervised and unsupervised techniques. The team will leverage Citi\\'s Enterprise Analytics platform (EAP) for this purpose.\\n\\nJOB RESPONSIBILITIES:\\n\\n• Ability to perform data assessment and understand data imputations and impact\\n\\n• Be able to define features in consultation with business / data SMEs\\n\\n• Apply statistical aggregations to data to come with features that are not hard bound to limits\\n\\n• Apply industry standard techniques for feature selection like PCA, MCMC, Clustering etc.\\n\\n• Ability to develop and train models through use of various algorithms like random forest, GBT, Logistic regression, SVM etc.\\n\\n• Able to articulate thought process and approach and develop documentation related to defensibility of the solution with auditors (external and internal)\\n\\nKNOWLEDGE & EXPERIENCE:\\nMust be able to work with team members in different geographic locations.\\nMust have prior proven experience in building predictive analytics solutions from ground up.\\nMust have at 3+ years of relevant experience\\nKnowledge of Hadoop, Hive, Spark, Python is must\\nExperience with other technologies such as machine learning, Kafka and Oozie is a plus.\\nExperience with NLP (Natural language Processing) systems is preferred.\\nPrior banking and financial sector experience is a bonus\\nCANDIDATE QUALIFICATIONS:\\nMinimum Years of Experience: 3 Years\\nBachelor\\'s Degree or equivalent work experience. Master\\'s Degree preferred.\\n\\nThis job description provides a high-level review of the types of work performed. Other job-related duties may be assigned as required.\\n\\n-------------------------------------------------\\n\\nGrade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US\\n\\n------------------------------------------------------\\n\\nTime Type :Full time\\n\\n------------------------------------------------------\\n\\nCiti is an equal opportunity and affirmative action employer.\\nMinority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.\\n\\nCitigroup Inc. and its subsidiaries (\"Citi) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.\\n\\nTo view the \"EEO is the Law\" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.\\nTo view the EEO Policy Statement CLICK HERE.\\nTo view the Pay Transparency Posting CLICK HERE.',\n",
       "  'About Conduent\\n\\n\\nConduent is the world\\'s largest provider of diversified business process services with leading capabilities in transaction processing, automation, analytics and constituent experience. We work with both government and commercial customers in assisting them to deliver quality services to the people they serve. We manage interactions with patients and the insured for a significant portion of the U.S. healthcare industry. We are the customer interface for large segments of the technology industry and the operational and processing partner of choice for public transportation systems around the world. Whether it\\'s digital payments, claims processing, benefit administration, automated tolling, customer care or distributed learning - Conduent manages and modernizes these interactions to create value for both our clients and their constituents. Learn more at www.conduent.com.\\n\\nJob Description\\n\\n\\nResponsibilities\\nIdentify, analyze and interpret transaction trends, patterns and anomalies\\nBuild and maintain, daily, weekly dashboards in Tableau for the management team\\nDevelop ad-hoc SQL queries\\nConducts needs analysis, determines project specifications, and identifies solutions to a customer\\'s business challenges.\\nCompiles business data using industry specific knowledge to gain insight into a business organization and operating structure.\\nDefine process improvement opportunities based on data trends\\nRequirements\\nMinimum of 2 years’ of experience in Oracle, SQL, Tableau\\nAdept at Query writing, presenting findings\\nFamiliarity with Python, Java, ETL frameworks is an asset\\nBachelors in Computer Science, Engineering, Information Management, Mathematics or relevant field is required.\\nExcellent verbal and written communication skills\\nProblem solving aptitude\\nClosing\\n\\n\\nConduent is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, creed, religion, ancestry, national origin, age, gender identity, gender expression, sex/gender, marital status, sexual orientation, physical or mental disability, medical condition, use of a guide dog or service animal, military/veteran status, citizenship status, basis of genetic information, or any other group protected by law. People with disabilities who need a reasonable accommodation to apply for or compete for employment with Conduent may request such accommodation(s) by clicking on the following link, completing the accommodation request form, and submitting the request by using the \"Submit\" button at the bottom of the form. For those using Google Chrome or Mozilla Firefox please download the form first: click here to access or download the form.',\n",
       "  'Position Objective:\\n\\nConducts Research and engages in New Product Development(NPD) of advanced filtration independent of direct supervision. Provides R&D leadership within the group and cross-functionally as needed to drive innovation. Chips in to NPD roadmap using application knowledge from Semiconductor OEMs. Role is a part of our growing R&D team at Filter Design Center of Excellence for Microelectronics to develop the state-of-the-art filtration, purification and separation products.\\nEssential Job Functions:\\n\\nConducts research in fluid flow and filtration to enable New Product roadmap\\nDesigns, Analyzes and Engineers new products with the use of standard tools, engineering software, design calculations, and best-known methods for product designs\\nLeads innovation and develops new technologies, builds new ideas that enable New Product Development\\nUtilizes application knowledge from Semiconductor industry in filter product development\\nMaintains collaborations with other research and academic entities that enable breakthrough innovation\\nUtilizes sound fundamentals like Design of Experiments (DOE); leads project teams and task effort\\nCommunicates clearly to leadership, team members, and other key partners.\\nComprehends Voice of the Customer and engages technical expertise to meet voice of customer in new product design and development activities\\nCalculates and predicts product performance including fluid flow, stack-up tolerances, structural integrity etc.\\nWorks with Test Laboratories to identify test protocols and quality performance criteria\\nWorks with global teams by overcoming cultural and language barriers\\nStudies and learns the fundamental technology of a wide variety of filtration products, processes and equipment\\nUses global R&D and engineering resources in other time zones\\nParticipates in teleconferences during off hours\\nRequires 10> travel both nationally and internationally\\nActs as a domain guide in resolving customer problems\\nDevelops engineering design specifications for materials, processes and equipment with critical to quality measures.\\nPosition Competencies:\\n\\nLeadership: ability to influence decision-making with or without authority, facilitate groups with different perspectives, bring teams to agreement. High tolerance for ambiguity\\nCommunication: Interpersonal skills that build productive relationships and influence others, both within the business and across organizations. Ability to interact with all levels of the business (senior leaders to working teams), extraordinarily good listening skills, able to present complex or new ideas with clarity and simplicity\\nTeamwork: collaborate effectively with others in the spirit of common objectives\\nAnalytical Problem Solving: Able to identify problems; apply structured and focused methodology to identify root cause using data. Innovative and effective solution development, risk mitigation and execution.\\nDrives Innovation and Growth\\nLeads Through DBS\\nBuilds People, Teams, and Organizations\\nActs with Integrity\\nUnderstanding of the hazardous nature of materials used in processing of Microelectronic products.\\nUnderstand need and concern of maintaining Form, Fit and Function of product to prevent failure and liability during product use in the field.\\nBasic Qualifications:\\n\\nBachelor’s degree in Mechanical, Chemical or other Engineering discipline, Chemistry or Polymer Science\\nMinimum of 10 years of experience – out of that 5 years MUST be in R&D/product development and/or design engineering\\nPreferred Qualifications:\\n\\nExperience in Filter Design, Fluid dynamics, Chemical and Hydromechanics, advanced membrane technology\\nApplication knowledge of Semiconductor OEMs or Fab Equipment Engineering experience\\nTechnical background related to Plastic Molding, Pressure vessel design and injection molding\\nMS/PhD degree in Science or Engineering\\nExperience speaking Asian language and adaptable to other cultures\\nPall is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, national origin, religion, gender, age, marital status, disability, veteran status, sexual orientation, gender identity, or any other characteristic protected by law.\\n\\nDiversity & Inclusion\\nAt Danaher, we are dedicated to building and sustaining a truly diverse and inclusive culture. These are not just words on a page—Diversity and Inclusion is a top priority for the company, and it ties deeply to each of our core values. Danaher Corporation and all Danaher Companies are equal opportunity employers that evaluate applicants without regard to race, color, national origin, religion, sex, age, marital status, disability, veteran status, sexual orientation, gender identity, or other characteristics protected by law.',\n",
       "  \"Who we are\\nFor over 230 years, Bank of New York Mellon (BNY Mellon) has been at the center of the global financial markets, providing the worlds leading institutions the tools, capabilities, and services to be distinctive investors. BNY Mellon has approximately $15 billion in revenues and market capitalization of approximately $50 billion.\\nBNY Mellon is a leader in the world of investment services and investment management, and our businesses support the full range of stakeholders of the financial system including:\\nManaging the custody of over $31 trillion financial assets of the worlds leading institutional investors, hedge funds, sovereign wealth funds, and corporates\\nInvesting over $1.8 trillion as one of the largest global asset managers across a wide range of asset classes\\nProviding collateral, liquidity, and funding for the worlds largest banks through our markets franchise\\nServing family offices and high net worth individuals in our top 10 wealth manager ($250 billion assets)\\nProviding a full suite of solutions to advisors, broker-dealers, family offices, hedge and '40 Act fund managers, registered investment advisor firms and wealth managers (over $1.8 trillion in client assets)\\nAdvising large global corporations on a range of trust and other solutions\\nProviding integrated managed data services to asset managers (over $4.7 trillion)\\nIn addition, BNY Mellon is a strategic partner to a variety of financial technology companies and a convener of influential industry and market structure forums.\\nWhat we are doing\\nBNY Mellon recently announced the launch of a newly formed global digital team, led by Roman Regelman, a former BCG partner who drove a number of large scale digital transformations for global financial services clients. The Digital team drives BNY Mellons core digitization strategy and is responsible for mapping our digital future, investing in digital client and internal capabilities, including data management, analytics, artificial intelligence, machine learning and robotics.\\n\\nThe Digital team focuses on maximizing investments in our digital capabilities to drive significant improvements in business performance and the client and employee experience. The team is accelerating the banks innovation strategy, developing more cross-functional integration and mining best practices to integrate across businesses, regions, front line relationship managers, innovation centers. We are currently looking to fill positions at our global headquarters in Tribeca, New York.\\nRole responsibilities\\nAs a member of the Advanced Digital Solutions team, the primary focus and responsibilities will be to deliver tangible value and advance the digital agenda across the enterprise by accelerating effective use of AI. Main responsibilities will include:\\nServing as AI solutions expert to address critical business needs in partnership with business teams, digital team members and technology. This includes designing and delivering proofs of concept as required.\\nProviding guidance on AI capabilities available internally or in the market place to determine best path forward factoring in feasibility, maturity of solutions, cost, time to market etc.\\nDriving the use of shared library / repository and model management of AI solutions across the enterprise\\nDriving / promoting best practices on effective use of AI solutions\\nSupporting lines of business and functions in developing AI capabilities to drive scale\\nQualifications\\nBachelors Degree in business or a related discipline, or equivalent work experience required. Advanced degree in Math, Science / Engineering or a related discipline is preferred.\\n\\nPreferred Qualifications:\\n\\nAn AI practitioner with hands on experience in designing and deploying machine learning solutions\\nExcellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forecasts, etc.\\nExperience with common data science toolkits, such as R, NumPy, MatLab, etc.\\nExperience with data visualization tools\\nProficiency in using query languages and databases\\nApplied statistics skills, such as distributions, statistical testing, regression, etc.\\nEntrepreneurial and results oriented\\nExcellent analytical and problem solving skills\\nAble to deal well with uncertainty and unstructured problems to be solved\\nAble to work across businesses and functions at all levels\\nAble t lead ad-hoc and structured teams for achieving goals within established time parameters\\nExcellent interpersonal skills necessary to accomplish goals through others, including employees, peers, and other function/business areas of the company\\n7-10 years of related experience preferred\\nExperience in the securities, financial services, or asset management industry a plus\\nGraduate degree from top universities a plus\\nBNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.\\nMinorities/Females/Individuals With Disabilities/Protected Veterans.\\n\\nPrimary Location: United States-New York-New York\\nInternal Jobcode: 70195\\nJob: Digital\\nOrganization: Digital Global-HR17565\\nRequisition Number: 1912216\",\n",
       "  'The Applications Development Group Manager is a senior management level position responsible for accomplishing results through the management of a team or department in an effort to establish and implement new or revised application systems and programs in coordination with the Technology Team. The overall objective of this role is to drive applications systems analysis and programming activities.\\n\\nResponsibilities:\\nDesign and implement a data strategy for a cross functional organization that processes over 40 billion transactions per annum.\\nIntegrate in-depth knowledge of applications development and cutting edge technologies to establish a client-centric, scalable, best-in-class reconciliation architecture to achieve established goals\\nManage multiple teams of professionals to accomplish established goals and conduct personnel duties for team (e.g. performance evaluations, hiring and disciplinary actions)\\nProvide strategic influence and exercise control over resources, budget management and planning while monitoring end results\\nUtilize in-depth knowledge of concepts and procedures within own area and basic knowledge of other areas to resolve issues\\nEnsure essential procedures are followed and contribute to defining standards\\nProvide evaluative judgement based on analysis of facts in complicated, unique, and dynamic situations including drawing from internal and external sources\\nInfluence and negotiate with senior leaders across functions, as well as communicate with external parties as necessary\\nAppropriately assess risk when business decisions are made, demonstrating particular consideration for the firm\\'s reputation and safeguarding Citigroup, its clients and assets, by driving compliance with applicable laws, rules and regulations, adhering to Policy, applying sound ethical judgment regarding personal behavior, conduct and business practices, and escalating, managing and reporting control issues with transparency, as well as effectively supervise the activity of others and create accountability with those who fail to maintain these standards.\\nQualifications:\\n10+ years of relevant experience\\nExperience in applications development\\nExperience in management\\nExperience managing global technology teams\\nWorking knowledge of industry practices and standards\\nConsistently demonstrates clear and concise written and verbal communication\\nEducation:\\nBachelors degree/University degree or equivalent experience\\nMasters degree preferred\\n-------------------------------------------------\\n\\nGrade :All Job Level - All Job FunctionsAll Job Level - All Job Functions - US\\n\\n------------------------------------------------------\\n\\nTime Type :\\n\\n------------------------------------------------------\\n\\nCiti is an equal opportunity and affirmative action employer.\\nMinority/Female/Veteran/Individuals with Disabilities/Sexual Orientation/Gender Identity.\\n\\nCitigroup Inc. and its subsidiaries (\"Citi) invite all qualified interested applicants to apply for career opportunities. If you are a person with a disability and need a reasonable accommodation to use our search tools and/or apply for a career opportunity CLICK HERE.\\n\\nTo view the \"EEO is the Law\" poster CLICK HERE. To view the EEO is the Law Supplement CLICK HERE.\\nTo view the EEO Policy Statement CLICK HERE.\\nTo view the Pay Transparency Posting CLICK HERE.',\n",
       "  'Advantage #556878\\n3 Months, with a strong possibility to extend up to 1 year\\n\\nJOB DESCRIPTION\\n\\nThe successful candidate will be responsible for FI Analytics, which will include data sourcing, data transformation, predictive modelling and creating Tableau dashboards.\\n\\nKEY RESPONSIBILITIES\\n\\nCreate strong blending data capabilities within team so that we can come up with useful client information from data very quickly\\n\\nEngage with business stakeholders cross countries and gather requirement and liaise with ETL team to get the required data preparation.\\n\\nPromote analytics driven decision making processes across multiple segments/product portfolios\\n\\nEngage and execute the dashboards on Tableau or similar visualization tools\\n\\nExplore statistical and automation tools like R, SAS, Python for predictive and prescriptive analytics\\n\\nEngage relevant technology teams to embed advanced statistical tools\\n\\nPrepare and/or contribute to regular updates for various committees and governance forums\\n\\nProvide insights to senior management and the network based on MIS and analytics\\n\\nHighlight any perceived risks and early alerts to the management based on data analytics\\n\\nPerform data validation and dashboard performance optimization\\n\\nREQUIRED KNOWLEDGE, EXPERIENCE AND COMPETENCIES\\n\\nA proven track record with 4 years or more of relevant analytics work experience and demonstrated career progression/increase in responsibilities\\n\\nStrong academic record: Degree in Finance, Statistics, Computer Science or related field\\n\\nStrong Knowledge of Tableau\\n\\nKnowledge of either SAS, R or Python\\n\\nStrong understanding of Hadoop Database\\n\\nProficient in excel and SQL\\n\\nKnowledge of statistical modelling and machine learning will be an added plus\\n\\nUnderstanding of the ETL process\\n\\nHighly effective verbal and written English communication skills\\n\\nGood presentation, time management, negotiation and influencing skills\\n\\nAbility to influence without authority\\n\\nStrong analytical and problem solving skills\\n\\nExperience in project management from business requirement definition, solution validation, user testing, technical documentation and production roll out\\n\\nAbility to manage multiple work streams with strong problem solving and analytical skills\\n\\nStrong communication and presentation skills to varying levels of management\\n\\nAdvantage xPO is a premier global workforce solutions provider and part of Recruit Holdings Co. Ltd., one of the largest human capital management companies in the world. Widely recognized by industry experts and HRO Today magazine, Advantage xPO specializes in providing temporary to contract-to-hire and direct/permanent placements with proven expertise in Engineering/Technical, Information Technology, Scientific, Finance/Accounting, Human Resources, Clerical and Light Industrial. Advantage xPO connects you with the right opportunity at the right company.\\n\\nAdvantage Resourcing Americas, Inc. is an Equal Opportunity Employer offering employment without regard for race, color, religious creed, national origin, ancestry, gender, marital status, age, sexual orientation, sex, gender identity, disability, veteran status, or other legally protected categories. Advantage is a VEVRAA Federal Contractor.\\n\\n- provided by Dice',\n",
       "  \"Innovate. Collaborate. Shine. Lighthouse KPMG's Center of Excellence for Advanced Analytics has both applied data science, AI, and big data architecture capabilities. Here, you'll work with a diverse team of sophisticated data and analytics professionals to explore the solutions for clients in a platform-diverse environment. This means your ability to find answers is limited only by your creativity in leveraging a vast array of techniques and tools. Be a part of a high-energy, diverse, fast-paced, and innovative culture that delivers with the agility of a tech startup and the backing of a leading global consulting firm. For you, that translates into the chance to work on a wide range of projects covering technologies and solutions from AI to optimization and the power to have a real impact in the business world. So, bring your creativity and pioneering spirit to KPMG Lighthouse.\\n\\nKPMG is currently seeking a Sr. Associate to join our KPMG Lighthouse - Center of Excellence for Advanced Analytics.\\n\\nResponsibilities:\\nWork in multi-disciplinary and cross-functional teams to translate business requirements into artificial intelligence goals and modeling approaches\\nRapidly iterate models and results to refine and validate approach working across different areas (risk management, financial services, mergers and acquisitions, and public policy)\\nWork in a fast-paced and dynamic environment with both virtual and face-to-face interactions utilizing structured approaches to solving problems, managing risks, documenting assumptions, communicating results, and educating others through insightful visualizations, reports and presentations\\nBuild ingestion processes to, prepare, extract, and annotate a rich data variety of unstructured data sources (social media, news, internal/external documents, images, video, voice, emails, financial data, and operational data)\\nLeverage a variety of tools and approaches to solve complex business objectives, from Statistical Natural Language Processing, Information Retrieval/Extraction, Machine Learning/Deep Learning, Image Processing, Rules Engines, Knowledge Graphs, and Semantic Search\\nRefactor deploy and validate models; work with clients iteratively to validate performance metrics, and sample output to drive towards a business-first solution; utilize APIs, platforms, containers, multi-threading, and distributed processing to achieve throughput goals\\nQualifications:\\nMinimum of two years of experience leading work streams with at least two data scientists, engineers, and other data & analytics professionals, including innovation, quality management, utilizing analytics and software development processes for natural language processing, machine learning on unstructured data, and/or information retrieval; Multidisciplinary backgrounds\\nMasters degree from an accredited college/university in Computer Science, Engineering, or related fields. PhD from an accredited college/university is preferred\\nAbility to apply artificial intelligence techniques to achieve concrete business goals while understanding available resources and constraints around data (sources, integrity, and definitions), processing platforms, and security; Ability to discuss mathematical formulations, alternatives, and impact on modeling approach and understanding of development practices (testing, code design, complexity, and code optimization)\\nSolid experience performing data science from data discovery, cleaning, model selection, validation, and deployment; experience coding artificial intelligence methods using object-oriented programming in a software development process, and ability to restructure, refactor and optimize code for efficiency\\nFluency in Python; Proficiency in AI related frameworks (NLTK, Spacy, Scikit-Learn, and Tensorflow); Experience with platforms (Google Cloud, Azure, and AWS); Ability to pick up new languages and technologies quickly and to work efficiently under Unix/Linux environment with experience with source code management systems like GIT; Ability to work with a variety of databases (SQL, ElasticSearch, Solr, Neo4j)\\nAbility to travel up to eighty percent of the time; Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future\\nKPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an affirmative action-equal opportunity employer. KPMG complies with all applicable federal, state and local laws regarding recruitment and hiring. All qualified applicants are considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. The attached link contains further information regarding the firm's compliance with federal, state and local recruitment and hiring laws. No phone calls or agencies please.\\n\\nSDL2017\\n\\n</br>\",\n",
       "  \"As a Data Science Principal, you'll utilize advanced quantitative and statistical analysis techniques to drive business model innovation for Via, work closely with our senior management to help drive decisions, and lead, mentor, and train data scientists and analysts in the art of leveraging data.\\n\\nResponsibilities\\nAdeptly interpret and utilize mass quantities of data to generate innovative hypotheses & insights\\nQuantitatively test hypotheses about rider and driver behavior using large sets of proprietary data; leverage results to improve business metrics at every touch point\\nDesign and implement novel experiments to better understand and improve current operation as well as expansion to new markets\\nTrain and mentor data analysis and scientist, assist in their professional growth and develop the analytic ecosystem in teams across the company\\nQualifications\\n4+ years of industry experience with predictive modeling and statistical analysis techniques in a business environment; preferably with team leadership experience\\nObsessed with data; analytical and rigorous, with a thorough understanding of statistics and machine learning at both a practical and theoretical level\\nExtraordinary communicator with demonstrated writing, editing, presentation and visualization skills. You understand the importance of graphic techniques in communicating a quantitative idea effectively\\nA Masters Degree or PhD (statistics, machine learning, physics, math, systems biology, or highly quantitative fields in social sciences), including 2+ years of graduate-level research experience (or the equivalent)\\nMastery in some or all of the following: SQL, Python, R.\\nAttention to detail is critical. Please mention that soup of the day is mushroom barley in the cover letter\\nExperience in experiment design and interpretation is a plus. Experience with different optimization methods is a plus\\nAt Via, we're on the cutting edge of mobility. We're building revolutionary technology that's changing the way people get around. It's on-demand transit on a mass scale, a smarter transportation that's friendly to our planet. From on-demand autonomous shuttles in Australia to dynamically routed bus fleets in Singapore, our sophisticated operating system is powering transportation in the world's biggest cities and is sought after by prominent transportation players globally. We've provided more than 50 million shared rides already, and we're growing at an astonishing rate. We have offices in more than 15 countries and deployments in more than 50 markets, with a goal of hundreds of deployments within the next two years. If you're someone who relishes wearing multiple hats, never backs down from a challenge, and loves getting things done, we'd love to hear from you!\\n\\nVia offers above market compensation packages and benefits, including equity, health insurance, and relocation assistance.\\n\\nVia is an equal opportunity employer.\",\n",
       "  'Title: Research Scientist Robotics\\n\\nCompany: Samsung Research America (SRA)\\n\\nLab: Artificial Intelligence Center\\n\\nLocation: New York, NY\\n\\nLab Summary:\\n\\nSamsung is building a new, forward-looking AI Research lab located in New York, NY, that will conduct fundamental research at the intersection of AI, robotics, and neuroscience. Our team is composed of world experts who collaborate with leading academic groups to create both innovative theories and state-of-the-art technological demonstrations that engender pioneering products and applications for Samsung Electronics over both long and short time scales. We are currently seeking individuals who share our passion and motivation to advance fundamental science and create revolutionary technological prototypes. Team members will have the intellectual freedom to cultivate their research roots by partnering with universities and publishing in leading scientific conferences and journals.\\n\\nPosition Summary:\\n\\nResearch Scientists in Robotics solve difficult challenges for autonomous robotic systems, addressing problems in real-time perception, decision-making, high-dimensional planning, and robust motor control. They work closely with other Research Scientists and Engineers in collaborative, interdisciplinary teams to discover, invent, and build things that have meaningful real-world impact. The Samsung AI Center in New York will lead in turning fundamental scientific discoveries into revolutionary products that reach hundreds of millions of users across the world.\\n\\nResponsibilities Include:\\nPhD in Computer Science, Electrical or Mechanical Engineering or related disciplines is required\\nExpertise with robot systems and autonomy\\nProgramming experience in one or more of the following:\\nC, C++, Python, and/or functional programming languages\\nStrong record of publishing papers at top quality conferences and journals\\nSuperior communication skills, both verbally and in writing\\nAbility to excel in international and collaborative teams\\nMotivation to initiate and accomplish ambitious research agenda\\nSamsung is an EEO/Veterans/Disabled/LGBT employer. We welcome and encourage diversity as we strive to create an inclusive workplace.',\n",
       "  'JPMorgan Chase & Co. (NYSE: JPM) is\\na leading global financial services firm with assets of $2.6 trillion and\\noperations worldwide. The firm is a leader in investment banking, financial\\nservices for consumers and small business, commercial banking, financial\\ntransaction processing, and asset management. A component of the Dow Jones\\nIndustrial Average, JPMorgan Chase & Co. serves millions of consumers in\\nthe United States and many of the world’s most prominent corporate,\\ninstitutional and government clients under its J.P. Morgan and Chase brands.\\nInformation about JPMorgan Chase & Co. is available at www.jpmorganchase.com\\n\\nChase Consumer & Community Banking (CCB)\\nserves nearly 66 million consumers and 4 million small businesses with a\\nbroad range of financial services through our 137,000 employees. The Consumer & Community Banking Business Modeling Center of Excellence\\nis a newly formed unit in support of modeling needs for marketing,\\nfinance and operations. Our modelers work directly with each line of\\nbusiness, Consumer Banking, Business Banking, Auto Finance, Credit Card\\nand Commerce Services, Chase Wealth Management and Home Lending, to\\nbuild state-of-the-art models and tools to support business growth.\\n\\nWe\\nare an intellectually diverse team of economists, statisticians,\\nengineers, and other analytics professionals, focused on quantitative\\nmodeling within CCB at JPMorgan Chase & Co. The team answers complex\\nand unique questions, utilizing cutting edge quantitative and\\ncomputational techniques and leveraging one of the world’s largest\\nrepositories of consumer data. We work closely with our partners\\nthroughout JPMorgan Chase to assess and execute critical business\\ndecisions.\\n\\nThe position is located at either New York, NY or Columbus, OH\\n\\nResponsibilities:\\nApply\\ncritical thinking skills and perform advanced analytics with the\\ngoal of solving complex and multi-faceted business problems.\\nGenerate\\ndeep insights through the analysis of data and understanding of\\nbusiness processes and turn them into actionable recommendations.\\nPerform\\nadvanced quantitative and statistical analysis of large datasets\\nto identify trends, patterns, and correlations that can be used to\\nimprove business performance.\\nKnow what type of algorithm to use and how to implement them\\nBuild and deploy prototype solutions to demonstrate ideas and prove concepts.\\nDevelop presentations to summarize and communicate key messages to senior management sponsors and colleagues.\\nBecome a subject matter expert and trusted advisor in the analytics discipline.\\nCollaborate with others in the organization to develop new ideas and brainstorm potential solutions.\\nActively\\ncontribute to the continuous learning mindset of the organization\\nby bringing in new ideas and perspectives that stretch the thinking\\nof the group\\nQualifications:\\nDeep\\nquantitative/programming background with a graduate degree (M.S.,\\nPh.D.) in Statistics, Engineering, Computer Science, Mathematics,\\nOperations Research, or Economics,\\n3 years of related experience preferred\\nHands-on experience with Machine Learning and Artificial Intelligence\\nAbility to write code and develop production-ready analytical applications\\nSignificant experience working with very large scale (structured and unstructured) data\\nExpertise in at least one of the following: Python, R\\nExcellent\\nwritten and oral communication skills to clearly present\\nanalytical findings and business recommendations. Highly motivated,\\nproductive, and teamwork oriented.\\nGood\\nproject management skills (clear goal setting, well-organized,\\ndetailed planning, and ability for tight-timeline deliverables).\\nDesired characteristics\\nAble to translate ambiguous business problems into a conceptual mathematical framework\\nPassionate about continuous learning and professional development\\nDeeply curious; creative and imaginative\\nAbility to influence and become a trusted advisor\\nAbility to convey complex concepts to non-technical audiences\\nEffective communication and presentation skills\\nCan work both independently and collaboratively',\n",
       "  \"Socure is headquartered in NYC and is the leader in machine learning driven digital identity verification. The company’s predictive analytics platform applies artificial intelligence and machine learning to trusted online/offline sources to verify identities in real-time across the web with the singular mission to become the trusted source of identities on the internet.\\n\\nSocure was included in Forbes AI 50 (America’s Most Promising Artificial Intelligence Companies), was named Forbes Cloud 100 Rising Star, Red Herring Top 100 Private Tech Company and currently services 2 of the top 5 banks, 5 of the top 10 issuers, multiple shared marketplaces, the leading insurance underwriters, top 3 payroll service providers, top 3 telcos and the majority of consumer-facing fintechs.\\n\\nFollowing our recent Series C funding that will be propelling our growth, we are excited to welcome the best talent to help us stay ahead of the curve! We are currently looking for a Senior Data Scientist in NYC. We are also open to remote employees.\\nRESPONSIBILITIES:\\nDevelop machine learning, data mining, statistical, and graph-based algorithms designed to analyze massive data sets; partner with Cloud technologists to ensure proper implementation and usage of said algorithms.\\nAnalyze large data sets to develop multiple, custom models and algorithms to drive innovative correlations for fraud and acceptance data as well as social data.\\nProvide analytic support to the data science team.\\nDevelop machine learning models to apply test data algorithms to future data.\\nDevelop models of current state in order to determine improvements needed.\\nImplement new data modeling solutions and improvements to existing solutions.\\nReport out on product/project status to senior management and clients.\\nWork well in a fast-paced cross-functional environment.\\nQUALIFICATIONS:\\nBachelor's Degree in CS, Mathematics, Statistics or similar or equivalent work experience\\nA minimum of 3+ years of industry experience working in a similar role.\\nExperience in developing data-driven algorithms in information retrieval, relevance, or machine learning and working with distributed systems.\\nKnowledge of UNIX systems management, Java, Scala, Python, C/C++, SQL, HQL, SPARQL for RDF.\\nUnderstanding of: Spark, Hadoop/Hive, StarDog, Amazon Redshift, MongoDB.\\nExperience with R, data mining, machine learning, statistical modeling tools and underlying algorithms.\\nNICE TO HAVE:\\nProgramming experience in (1 or more): Python/Shell/C/Java/Scala.\\nWorking knowledge of Relational Data Base Systems.\\nLOVE WHERE YOU WORK:\\nCUTTING EDGE TECH: Socure provides the most accurate identity verification solution on the market by applying Machine Learning techniques with biometrics and data intelligence in order to capitalize on alternative data that has proven to provide better fraud prediction capabilities. We’re not the only ones excited by it! We are trusted by the top banks, credit card issuers, mortgage lenders, payment processors...and the list goes on.\\nAMAZING CULTURE: Our Data Science team is at the core of Socure’s success and we are dedicated to hiring people with the mentality of purposeful innovation. We hire people who are committed to each other and the entire team and this translates into a core value of transparency that is reflected in the wider business at Socure!\\nAUTONOMY, TRUST & FLEXIBILITY: We are a community of professional adults and we treat each other like it! We encourage everyone to contribute their feedback and ideas into our operations, product and development. We also offer a flexible working schedule to make sure you have the optimal work-life balance.\\nPERKS & BENEFITS:\\nCompetitive base salary\\nEquity - every employee is a stakeholder in our enormous upside\\nA tech-first company culture driven by entrepreneurial thinking and talent\\nA group of highly intelligent peers that are all working in unison towards the same mission\\nTransparency is what our product is built on—and so is our culture\\nGenerous medical, dental and vision benefits for employees and their dependents\\nFlexible PTO\\n401K with company match\\n...and so much more\\n\\nWe are an equal opportunity employer and value diversity of all kinds at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.\",\n",
       "  'Req ID: 173251\\n\\nOVERALL SUMMARY\\n\\nWe are looking for a Machine Learning (ML) Engineer to help us create artificial intelligence products. ML Engineer responsibilities include creating machine learning models and retraining systems. Your ultimate goal will be to shape and build efficient self-learning applications.\\n\\nYou are:\\n\\nYou can design and build web scale distributed systems. You are passionate about data science and machine learning. You are excited about recommender systems and comfortable reading research papers, interacting with data scientists and implementing ml algorithms from proof of concept to production. Additionally, you understand the constraints of working with a growing team and thrive in an environment that is fast-paced and sometimes scrappy. You understand that serving a user-facing model comes with a set of restrictions and you know how to be creative to solve them. Finally, you are business focused and proactively thinking about new projects that could have a high impact result for the company.\\n\\nWe value:\\nTeam player\\nProblem solver\\nRadical candor\\nTesting rigor\\nHumility\\nPRIMARY RESPONSIBILITIES\\nStudy and transform data science prototypes\\nDesign and develop machine learning systems for data scientist to create, train and deploy ML models\\nResearch and implement appropriate ML algorithms and tools\\nDevelop machine learning applications according to requirements\\nRun machine learning tests and experiments\\nPerform statistical analysis and fine-tuning using test results\\nTrain and retrain systems when necessary\\nExtend existing ML libraries and frameworks\\nKeep abreast of developments in the field\\nREQUIREMENTS\\nProven experience as a ML Engineer or similar role\\nUnderstanding of data structures, data modeling and software architecture\\nFamiliarity with machine learning frameworks (like Keras or PyTorch) and libraries (like scikit-learn)\\nExperience in applying machine learning, predictive analytics and classification techniques towards real product and problems\\nAbility to write robust code in Python or Java or equivalent modern programming language\\nProficiency in SQL, big data technologies and working with large data sets\\nBS in Computer Science, Mathematics or similar field; Masters degree is a plus\\nWorks independently, able to manage multiple projects simultaneously\\nDeep curiosity and a demonstrated ability to craft original solutions\\nHighly communicative with collaborators and manager\\nProactive in seeking opportunities for innovation\\nPluses:\\nDesigned and built web scale distributed systems\\nDeep knowledge of math, probability, statistics and algorithms\\nExperience in the media industry\\n[#video#https://youtu.be/L-oQ0Akgx5k{#400,300#}#/video#]\\n\\n173251',\n",
       "  \"# Data Scientist - Data Engineer\\n*REF#:** 32836\\n*CBS BUSINESS UNIT:** Simon & Schuster\\n*JOB TYPE:** Temporary / Per Diem / Freelance\\n*JOB SCHEDULE:** Full-Time\\n*JOB LOCATION:** New York, NY\\n*ABOUT US:**\\nSimon & Schuster, a part of CBS Corporation, is a global leader in the field of general interest publishing, dedicated to providing the best in fiction and nonfiction for consumers of all ages, across all printed, electronic, and audio formats. Its divisions include Simon & Schuster Adult Publishing, Simon & Schuster Children's Publishing, Simon & Schuster Audio, Simon & Schuster Digital, and international companies in Australia, Canada, India and the United Kingdom.\\n*DESCRIPTION:**\\nSimon & Schuster has an exciting role for a data engineer to join a fast-paced, leading-edge team working to help advance it's publishing business. In this Freelance or Independent Contractor role, you will be working with a small team of data scientists and analysts to rapidly prototype data applications and analytics tools to serve our fast-growing imprint.\\n\\nThe full-time, contract position will be responsible for developing, testing and maintaining data flows and information architecture. This includes aggregating data from multiple databases in our data warehouse as well as external sources, establishing data pipelines, designing data models, deploying machine learning models to production and building robust data visualizations and reporting tools that align with the business needs of our team.\\n\\nThis is an exciting opportunity for the right candidate to build a robust data science and analytics environment from the ground up. You'll be working with a small team of professionals based remotely. Hours are flexible. Two scheduled check-ins/code reviews are required weekly.\\n\\nResponsibilities\\nCollaborate with team to align architecture with business requirements\\nStand up, test and maintain databases and information architecture\\nBuild tools to automate data acquisition and aggregation\\nDevelop data models and table schemas\\nIdentify ways to improve data reliability, efficiency and quality\\nDeploy sophisticated analytics programs, machine learning and statistical methods\\nPrepare data for predictive and prescriptive modeling\\nUse data to discover tasks that can be automated\\nDevelop data visualizations, dashboards, reports and notification tools that respond to business needs\\nDesign analytical models that support analysis and inference.\\n*QUALIFICATIONS:**\\nSkills and experience:\\nComputer science degree or equivalent with 2+ years data engineering and/or full stack developer experience\\nBackground in data science or equivalent continuing education\\nStrong knowledge of Python 3\\nExperience in developing and maintaining SQL databases and writing SQL queries; NoSQL (MongoDB) experience preferred but not required\\nExperience establishing and maintaining AWS, Google Cloud, Heroku, Tableau or similar cloud-based environments\\nKnowledge of HTML, CSS and JS and lightweight web frameworks such as Django, Flask\\nFamiliarity with database administration tools and best practices\\nBackground in extracting and storing data from APIs preferred but not required\\nBackground in deploying machine learning models to production preferred but not required\\nKnowledge of Git/GitHub and software development workflows\\nStrong written and verbal communication skills.\\nAbility to summarize key findings into clear and actionable recommendations, and to interact with individuals at all levels.\\nStrong project management skills and ability to plan and prioritize work in a fast-paced environment.Please send a link to your work samples on Github\\n*EEO STATEMENT:**\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled\",\n",
       "  'JobDescription :\\nAs a Senior Application Developer for the Market Intelligence Data Science Team, you will take part in building out new functionalities and enhance the same using the big data, spark and microservices stack on Machine Learning Algorithms and building out ML Services Stack using Docker and Kubernetes. In addition, you will be supporting the existing applications from your team. You will also work very closely with cross-functional teams in the Market Intelligence space to build platform agnostic Machine Learning Services. Our team currently uses these skills/tools: Java, SQL, NoSQL, Distributed Computing Platform (HDFS, Apache Spark), REST services, Python, Jupyter, AWS SageMaker, Docker, Kubernetes, Git, Shell Scripting, and Python.\\n\\nBasic Qualifications:\\n\\nBachelor Degree in Computer Science or equivalent. MS in Computer Science is preferred. Candidates without Computer Science or Information Technology degree should have significant work experience\\nAt least 5+ years of hands on development in building enterprise applications to cloud [AWS / Azure / GCP];\\nMust have Hands-on experience with Machine Learning and Deep Learning Algorithms [ like RNN, CNN, LSTM, Logistic Regression, XGBoost etc] , Core Java 8(Collections, Concurrency, Multi-Threading, Locks), Java 8 Streams, Spring Boot, Docker, Kubernetes, SQL and NoSQL databases, Git, Shell Scripting\\n\\nPreferred Qualifications:\\n\\nFamiliarity in building microservices using Zuul, Ribbon, Hystrix is desirable.\\nExperience in working with REST APIs and Cassandra is highly desirable\\n2-3 years of experience with Docker and Kubernetes is a must\\n2-3 years of building and implementing ML Algorithms [Logistic Regression, Neural Networks, Random Forest and Gradient Boosting Algorithms]\\nAny prior experience in financial industry is a big plus\\nExposure to TDD and Agile Methodologies and working in a Scrum Team\\nKnowledge and experience in Distributed Computing Platforms like HDFS & Apache Spark are a must\\n\\nTo all recruitment agencies: S&P Global does not accept unsolicited agency resumes. Please do not forward such resumes to any S&P Global employee, office location or website. S&P Global will not be responsible for any fees related to such resumes.\\n\\nS&P Global is an equal opportunity employer committed to making all employment decisions without regard to race/ethnicity, gender, pregnancy, gender identity or expression, color, creed, religion, national origin, age, disability, marital status (including domestic partnerships and civil unions), sexual orientation, military veteran status, unemployment status, or any other basis prohibited by federal, state or local law. Only electronic job submissions will be considered for employment.\\n\\nIf you need an accommodation during the application process due to a disability, please send an email to: EEO.Compliance@spglobal.com and your request will be forwarded to the appropriate person.\\n\\nThe EEO is the Law Poster http://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdfdescribes discrimination protections under federal law.\\n\\nJob ID :\\n250414\\n\\nPosted On :\\n10-14-2019\\n\\nLocation :\\nNew York, NY US',\n",
       "  'At HelloFresh, we want to revolutionize the way we eat by making it more convenient and exciting to cook meals from scratch. We have offices all over the world and we deliver delicious meals to millions of people.\\n\\nWe are the industry leader in meal-kit subscription services and we’re growing all the time. We have distinct meal-kit services that cater to everyone with the most menu variety in the market, which allows us to reach an incredibly wide population of people.\\n\\nJob Description:\\n\\nWe’re hiring a CRM Analytics Manager to support our Customer Marketing (CRM) team and own reporting, in-depth analytical insights and experimentation to improve and optimize user engagement. As the CRM Analytics Manager you will champion a data-centric perspective and approach to our operations while working closely with business intelligence, product, and growth teams. Put simply, you lead analytics and reporting for CRM.\\n\\nTo succeed in this role, you must demonstrate a desire to improve on existing techniques and processes, have a passion for reporting on analytical insights and campaign performance, drive opportunities on experimentation and predictive modeling, and recommend improvements on strategies used by our marketing team across their campaigns.\\n\\nYou will champion solving the challenges of marketing optimization, ROI, user behavior pre- and post-purchase, as well as marketing channel effectiveness. You will define the KPIs for marketing performance as well as the questions that should be solved as a result of CRM’s marketing strategy. You will find the opportunities for CRM to scale its conversion and retention functions.\\n\\nYou will….\\nAnalyze complex business problems using data from internal and external sources to provide insight to decision-makers.\\nServe as the key analytics contact for the customer marketing (CRM) team. Collaborate with marketing managers and data science members on project planning/prioritization, segmentation strategies, testing, pre- and post-campaign performance analysis, and reporting efforts.\\nConstruct forecasts, recommendations and strategic/tactical plans based on business data and market knowledge.\\nDevelop clear KPIs and measures of success for use by CRM to track existing and future marketing campaign and experiments.\\nCreate clear specifications and development plans for reports and analysis based on business needs.\\nWork with local and global counterparts to develop dashboards that provide insights and visualization into customer and channel performance based on existing and new KPIs, channel projections, and historical performance.\\nProduce in depth insights for decision making using various reporting and analysis tools; including SQL, Tableau, Python, Google Sheets, MS Excel\\nWork hand in hand with data scientists, engineers and other data professionals to show the impact of data products.\\nAt minimum, you have…\\nA BS/MS in computer science, mathematics, economics, statistics, engineering or similar program.\\nMinimum 3-5 years of professional analytical experience.\\nMinimum 1-2 years of experience managing a team.\\nCRM analytics and marketing analytics experience required.\\nAble to balance attention to metric details and efficient and rapid execution.\\nProficient coding skills in SQL and Python\\nA solid understanding of testing / experimentation, math and statistics\\nRequired experience in data visualization and creating dashboards. Experience querying and using data to create and publish dashboards in Tableau is a plus.\\nSense of ownership: you take responsibility for your projects and pride in your work.\\nYou’ll get…\\nCompetitive Salary & 401k company match that vests immediately upon participation\\nGenerous parental leave of 16 weeks & PTO policy\\n$0 monthly premium and other flexible health plans\\n75% discount on your subscription to HelloFresh (as well as other product initiatives)\\nSnacks, cold brew on tap & monthly catered lunches\\nCompany sponsored outings & Employee Resource Groups\\nCollaborative, dynamic work environment within a fast-paced, mission-driven company\\nIt is the policy of HelloFresh not to discriminate against any employee or applicant for employment because of race, color, religion, sex, sexual orientation, gender identity, national origin, age, marital status, genetic information, disability or because he or she is a protected veteran.',\n",
       "  'PwC Labs is focused on standardizing, automating, delivering tools and processes and exploring emerging technologies that drive efficiency and enable our people to reimagine the possible. Process improvement, transformation, effective use of innovative technology and data & analytics, and leveraging alternative delivery solutions are key areas of focus to drive additional value for our firm.\\n\\nThe AI Lab focuses on implementing solutions that impact efficiency and effectiveness of our technology functions. Process improvement, transformation, effective use of technology and data & analytics, and leveraging alternative delivery are key areas to drive value and continue to be recognized as the leading professional services firm. AI Lab is focused on identifying and prioritizing emerging technologies to get the most out of our investments.\\n\\nTo really stand out and make us ?t for the future in a constantly changing world, each and every one of us at PwC needs to be an authentic and inclusive leader, at all grades/levels and in all lines of service. To help us achieve this we have the PwC Professional; our global leadership development framework. It gives us a single set of expectations across our lines, geographies and career paths, and provides transparency on the skills we need as individuals to be successful and progress in our careers, now and in the future.\\n\\nAs an Associate, youll work as part of a team of problem solvers, helping to solve complex business issues from strategy to execution. PwC Professional skills and responsibilities for this management level include but are not limited to:\\nInvite and provide evidence-based feedback in a timely and constructive manner.\\nShare and collaborate effectively with others.\\nWork with existing processes/systems whilst making constructive suggestions for improvements.\\nValidate data and analysis for accuracy and relevance.\\nFollow risk management and compliance procedures.\\nKeep up-to-date with technical developments for business area.\\nCommunicate confidently in a clear, concise and articulate manner - verbally and in written form.\\nSeek opportunities to learn about other cultures and other parts of the business across the Network of PwC firms.\\nUphold the firms code of ethics and business conduct.\\n\\nOur team is capability centric, focusing on AI and machine learning techniques that are broadly applicable across all industries. We work with a variety of data mediums including text, audio, imagery, sensory, and structured data. Our work involves the use of supervised/unsupervised machine learning algorithms, traditional statistical models, deep neural networks, terabyte scale data, and simulation modelling. Our work is having a tremendous impact on how PwC & our clients do business.\\n\\nJob Requirements and Preferences:\\nBasic Qualifications:\\n\\nMinimum Degree Required:\\nBachelor Degree\\n\\nMinimum Years of Experience:\\n1 year(s)\\n\\nPreferred Qualifications:\\n\\nPreferred Fields of Study:\\nComputer and Information Science, Computer Engineering, Computer and Information Science & Accounting, Economics, Economics and Finance, Economics and Finance & Technology, Engineering, Mathematics, Mathematical Statistics, Statistics\\n\\nPreferred Knowledge/Skills:\\n\\nDemonstrates some knowledge and/or a proven record of success in the following areas:\\nExploring new analytical technologies and evaluating their technical and commercial viability quickly;\\nWorking in 4-week sprint cycles to develop proof-of-concepts and prototype models that can be demoed and explained to data scientists, internal stakeholders, and clients;\\nTesting and rejecting hypotheses around data processing and machine learning model building;\\nDemonstrating ability to experiment, fail quickly, and recognize when you need assistance vs. when you conclude that a technology is not suitable for the task;\\nBuilding machine learning pipelines that ingest, clean data, and make predictions;\\nStaying abreast of new AI research from leading labs by reading papers and experimenting with code;\\nDeveloping innovative solutions and perspectives on AI that can be published in academic journals/arXiv and shared with clients;\\nDemonstrating ability to continuously learn new technologies and quickly evaluate their technical and commercial viability;\\nApplying machine learning techniques for addressing a variety of problems (e.g. consumer segmentation, revenue forecasting, image classification, etc.);\\nUnderstanding of machine learning algorithms (e.g. k-nearest neighbors, random forests, ensemble methods, deep neural networks, etc.) and when it is appropriate to use each technique;\\nBuilding machine learning models and systems, interpreting their output, and communicating the results;\\nMoving models from development to production is a plus; and,\\nConducting research in a lab and publishing work is a plus.\\nDemonstrates some abilities and/or a proven record of success learning and applying new skills quickly, including the following areas and technologies:\\nProgramming: Python, R, Java, JavaScript, C++, Unix;\\nHardware: sensors, robotics, GPU enabled machine learning, FPGAs, Raspberry Pis, etc.;\\nData Storage Technologies: SQL, NoSQL, Hadoop, cloud-based databases such as GCP BigQuery, and different storage formats (e.g. Parquet, etc.);\\nData Processing Tools: Python (Numpy, Pandas, etc.), Spark, cloud-based solutions such as GCP DataFlow;\\nMachine Learning Libraries: Python (scikit-learn, genism, etc.), TensorFlow, Keras, PyTorch, Spark MLlib;\\nVisualization: Python (Matplotlib, Seaborn, bokeh, etc.), JavaScript (d3); and,\\nProductionization and containerization technologies: GitHub, Flask, Docker, Kubernetes.\\nAll qualified applicants will receive consideration for employment at PwC without regard to race; creed; color; religion; national origin; sex; age; disability; sexual orientation; gender identity or expression; genetic predisposition or carrier status; veteran, marital, or citizenship status; or any other status protected by law. PwC is proud to be an affirmative action and equal opportunity employer.',\n",
       "  'Job Description:\\n\\nYou will work with a cutting-edge technology stack that includes:\\n\\n• Software development based environment for agile development\\n• Globally distributed object-oriented petabyte-class databases\\n• Linux compute farms on-tap\\n• A core framework of reusable business objects\\n• A multi-million line code base\\n• Automated tools for testing, integration and global application deployment\\nYou will:\\n• Design, develop, and maintain high-performance systems • Build core technology components\\n• Enhance functionality to be used and shared by a large community of developers\\n• Research and analyze data processing functions, methods and procedures\\n• Monitor program execution for expected performance\\nYou will have:\\n• Programming experience\\n• The desire to work in a test driven software development environment that relies on iterative feedback\\n• An interest in financial services (capital markets, market indices and various securities)\\n• An overwhelming urge to solve complex problems without being told to do so\\n• Exceptional knowledge of computer science data structures\\n\\nQualifications\\n\\nBS Degree in Computer Science or Related\\n\\nShift:\\n\\n1st shift (United States of America)\\n\\nHours Per Week:\\n\\n40',\n",
       "  'Senior Data Scientist\\n\\nOverview\\n\\nOUR STORY\\n\\nWe are a company with integrated luxury and lifestyle offerings centered on Movement, Nutrition and Regeneration. In addition to Equinox, our other brands, Blink, Pure Yoga, SoulCycle, Furthermore, and Equinox Hotels are all recognized for inspiring and motivating members and employees to maximize life. Within our portfolio of brands, we have more than 200 locations within every major city across the United States in addition to London, Toronto, & Vancouver.\\n\\nOUR CODE\\n\\nWe are passionate about high performance living and we practice what we preach – investing time in our own health and fitness. We believe that everyone has untapped potential within them and it takes a disruptive approach to unleash it. We dream big and don’t settle for the status quo. We sweat the details. We never accept less than 110% to help each other deliver the Equinox experience and enable our members to get great results. We are obsessed with what’s new, what’s now, what’s next. Never following, always leading, living ahead of the moment in fashion, culture and consumer behavior. We aren’t just a company; we’re a community vested in each other’s success. We value humility and a team approach at every level of the company.\\n\\nIf you are a high performing individual who is passionate about winning and inspiring others, then we are excited to discuss career opportunities with you.\\n\\nJob Description\\n\\nEquinox is seeking a Data Scientist to join our Data Insights team. This role is a hands-on engineering and researching position responsible for using machine learning and statistical tools to leverage management and member experience. This role will also be involved in Equinox Innovation team which aims at building up next generation digital platform to bring our members completely new experience.\\n\\nRESPONSIBILITIES\\nGathering and analyzing data, performing POC analysis from a statistical perspective, identifying key factors, building prototypes and reporting findings to product owners;\\nFormulating machine learning/statistical/optimization approaches according to business metrics, designing features from rich data sources, training, evaluating and deploying models;\\nImplementing statistical testing to measure the success of projects and business impact;\\nResearching forefront techniques, proposing and initiating innovative data science efforts to drive management efficiency and member experience;\\nDesigning and building up scalable data science pipelines, creating high-performance APIs within engineering according to business requirements, maintaining data science pipelines;\\nConstructing APIs for data gathering, exploring and performing QA for third party data;\\nLearning and sharing knowledge about data science, proposing the capability of data science to business team.\\nQualifications\\nBachelor degree in a computational science with 2+ years of post graduate work experience in Machine Learning/Statistics/Data Science, Master degree is preferred;\\nExperience with traditional as well as modern machine learning/statistical techniques, including Regression, Classification, Regularization, Ensemble Methods, Deep Neural Network, Causal Inference, and Hypothesis Testing;\\n3+ years experience with programming languages, such as Python, R, Java, C/C++; familiarity with Linux/Unix/Shell environments, Python preferred.\\n3+ years experience in working with large-scale database, experience with AWS products is preferred;\\n2+ years hands-on skills in sourcing, cleaning, manipulating and analyzing large volumes of data;\\nStrong written and oral communication skills.\\n2+ years experience in building and maintaining large scale data/machine learning pipelines in online advertising, recommender system, ecommerce or relevant areas;\\nExperience leading machine learning/data science projects;\\nExperience working with product, sales and other key stakeholders to drive business success.\\nAdditional Information\\n\\nAS A MEMBER OF THE EQUINOX TEAM YOU WILL RECEIVE:\\nWe offer competitive salary, benefits and industry leading commission opportunities for club employees\\nComplimentary Club membership\\n30- 50% discounts on all Equinox products and services including Personal Training, Private Pilates, Spa and Café’ services and Shop items\\nThis job description is intended to describe the general requirements for the position. It is not a complete statement of duties, responsibilities or requirements. Other duties not listed here may be assigned as necessary to ensure the proper operations of the department.\\n\\nEquinox is an equal opportunity employer. For more information regarding our career opportunities, please visit one of our clubs or our website at https://careers.equinox.com/\\n\\nAll your information will be kept confidential according to EEO guidelines. Must have a legal right to work in the United States.',\n",
       "  'About Us\\nVenmo was founded on the principles of breaking down the intimidating barriers around financial transactions to make them intuitive, friendly, and even fun. And it worked: people love sending money with Venmo, and were growing by leaps and bounds!\\nBut were only just getting started. We want to take that magic of sending money with Venmo and cascade it into every place where people use money. That means connecting people to their money in the most intuitive and fun way possible, then connecting people with each other. Users already love Venmo, but we know there are lots of things we havent thought of to make the experience of using Venmo even more delightful and valuable. All thats going to take a lot of figuring out. Lets figure it out together!\\n\\nRisk Decision Scientist\\nWere looking for a Risk Decision Scientist to join our Risk team.\\nDecision Scientists are on the front line of identifying and targeting fraud to keep our millions of users secure and engaged. Risk Decision Scientists apply their analytical and technical skills into researching data, monitoring and creating logics that stop fraudulent activity on Venmos platform. They analyze existing loss trends and define logics, solutions and operational processes that prevent them - while delivering best-in-class customer experience for Venmo users.\\nRisk Decision Scientists work closely with one another, with Venmo Risk Operations and with Venmos Customer Support, in order to keep learning about current fraud. They also partner closely with the Venmo Product teams to define features and integrate risk prevention tools to support business goals. As Venmo is a subsidiary of PayPal, this role also involves working closely with PayPal Risk teams to align on fraud-prevention methodologies and tools.\\nOur team members are excited about solving analytical problems. They are bright, they are responsible, and they know how to follow through. If youre also like that, wed love to hear from you.\\n\\nSpecific responsibilities include\\nDeveloping expertise in specific areas by leading analytical projects independently, while setting goals, providing benefit estimations, defining workflows and coordinating timelines in advance.\\nFight fraudulent activity in ones domain of ownership by analyzing existing loss trends through review of specific cases, research on our payments database and incorporation of feedback from operations teams.\\nProduce accurate, automated risk solutions and manual handling policies to block fraudulent activities by declining payments, adding accounts to the fraud review queue or restricting suspicious users.\\nWork closely with product teams on development of new products to identify potential gaps that might create loss, and define requirements to mitigate those risks.\\nHave a good understanding of general business trends and directions to be able to put own work in the broad business context.\\nWork with various partners within Venmo and PayPal to coordinate the execution of fraud mitigation solutions.\\nDemonstrate flexibility that is needed to succeed within a matrix organization while being proactive and comfortable with working alongside stakeholders from different domains.\\nDevelop controls and monitoring dashboards to ensure performance against business goal, regulatory requirements and business priorities.\\nRemain up-to-date on industry information, federal regulations and new products, while continuously learning from peers across the organization about fraud and fraud-prevention tools..\\nBe data-driven and outcome-focused\\n\\nQualifications\\nBS/BA degree in related field required or equivalent professional work related experience.\\n3+ years of experience in solving problems using data-driven approach or Masters degree and 2+ year.\\nStrong analytical and reporting skills (SQL experience is a requirement)\\nHigh proficiency in fundamental SQL. Other technical skills are a plus (Java/C/C++; Perl/Python; strong UNIX background; Hadoop; R )\\nStrong organizational skills and excellent follow-through\\nOutstanding written, verbal and interpersonal communication skills\\nHigh emotional intelligence, a can-do mentality and a creative approach to problem solving\\nExperience with working in a technology environment\\nExperience with program management and process mapping is a plus\\nWillingness to travel up to a week every 3 months',\n",
       "  \"DESCRIPTION\\n\\n\\nHaven Life is an insuretech innovator at MassMutual that offers a new way to get life insurance online that's actually simple.\\n\\nWe combine the culture of a startup with the stability and backing of a Fortune 500 company to create an environment that's truly unique.\\n\\nOur diverse team is comprised of smart, collaborative people who think big, execute quickly and don't take themselves too seriously. We're located in New York's Flatiron District and in case you're wondering, yes, we provide free snacks. Cold brew too.\\n\\nIf you're creative, professional and kind, we'd love to hear from you.\\n\\nABOUT THIS ROLE:\\n\\nWe are looking for an individual with an enduring curiosity about data and with an interest in evolving best practices and tools. In collaboration with other team members, you will help shape the vision, planning, and execution of Haven technology and product innovation. You will:\\nPartner closely with product managers, engineers, designers, marketing, insurance experts, and other team members to improve customer experience and enhance our business.\\nInterpret data, analyze results using statistical techniques and provide ongoing analysis to various business areas within the Haven Life team.\\nIdentify, analyze, and interpret trends or patterns in complex data sets\\nInform, influence, and support product in making data driven decisions.\\nIdentify data needs and gaps, and work with developers to devise solutions in data gathering, ingestion, and modeling.\\nAcquire data from primary or secondary data sources\\nWork closely with management to prioritize business and information needs\\nAd hoc support for key business needs or requests.\\nREQUIREMENTS\\nBS/BA Required\\nWork Full-Time from our Manhattan office\\nMinimum 1-3 years' of experience in a Data Analyst role\\nComfortable using BI tools to analyze and manipulate data, such as SQL, Tableau, and Qlik\\nStrong skills in data extraction, cleaning, analysis and visualization (e.g. SQL, Python, R)\\nExcellent written and verbal communication skills; Ability to present and communicate findings in a clear manner to a variety of audiences.\\nStrong work ethic, intellectual curiosity and attention to detail.\\nAuthorized to work in the US for any employer without sponsorship.\\nPREFERRED REQUIREMENTS\\n3-5 years of experience in a Data Analytics role\\nBENEFITS\\n\\n\\nWe pay competitive base salaries and we reward performance. Our salary structure is commensurate with experience. In addition, youll be eligible to participate in our comprehensive benefits program including medical insurance and 401(K).\",\n",
       "  'Company Description\\n\\nDigitas is The Connected Marketing Agency, committed to helping brands better connect with people through Truth. Connection. Wonder. With diverse expertise in data, strategy, creative, media, and tech, we work across capabilities and continents to make better connections and achieve ambitious outcomes through ideas that excite, provoke and inspire. Curious and fully transparent, we are always examining real human behavior to create authentic connections—between brands and consumers, clients and partners, and ideas and outcomes. Digitas has 3,300 employees across 12 countries and 24 offices, with an extended network via Publicis Media of over 23,500 employees present in more than 100 countries worldwide. To connect with Digitas or learn more, visit www.digitas.com.\\n\\nJob Description\\n\\nData & Analysis -- Senior Data Scientist\\n\\nDigitas is a highly-caffeinated playground where brilliant minds come together to bring bold, award-winning ideas to life.\\n\\nThe Data & Analysis team uses data-driven insights to fuel strategic growth for clients. We believe that data should never exist in a vacuum; instead, it should be put to work to bring the best ideas and stories to our clients.\\n\\nDigitas’ Data Science practice sits within our DNA (Data & Analysis) capability, and develops industry-leading analytical solutions for clients across industries, channels and business functions. We apply a bespoke and cutting-edge arsenal of statistical, analytical and computing techniques to complex data problems at scale, with an emphasis on game-changing—and measureable—business impact. We work in close collaboration with colleagues across all agency disciplines to develop truly innovative, highly effective, data-powered solutions for our clients.\\n\\nTo help with this, we’re looking for an outstanding Senior Data Scientist– an intellectually curious and creative problem solver who is willing to tackle the familiar and unfamiliar of all things data, including methods, technologies and applications. Sound like you? Read on.\\n\\nWhat you’ll do:\\n\\nOur Data Scientists deliver analytic solutions across a wide variety of client applications. We build inferential and predictive models, including machine learning algorithms and AI; we process, integrate and manipulate big data with distributed systems and customer data pipelines; we synthesize results and translate findings into compelling stories that resonate with clients.\\n\\nAs a Senior Data Scientist, you’ll solve complex marketing and business challenges—from cross-channel media and customer experience optimization to segmentation, targeting and business strategy—by accessing, integrating, manipulating, mining and modeling a wide array of data sources.\\n\\nDay-to-day, your role includes:\\n\\n· Translating and reframing marketing and business questions into analytical plans.\\n\\n· Using distributed computing systems to ingest, access and integrate disparate big data sources.\\n\\n· Conducting extensive exploratory analysis to identify relevant insights, useful transformations and analytical applications.\\n\\n· Applying quantitative techniques, including statistical and machine learning, to uncover latent patterns in the data.\\n\\n· Building and testing scalable data pipelines or models for real-time applications.\\n\\n· Summarizing, visualizing, communicating and documenting analytic concepts, processes and results for technical and non-technical audiences.\\n\\n· Collaborating with internal and external stakeholders to establish clear analytical objectives, approaches and timelines.\\n\\n· Sharing knowledge, debating techniques, and conducting research to advance the collective knowledge and skills of our Data Science practice.\\n\\nWe’re looking for rigorous analytic training and 3+ years professional experience in a data science or analytics role, which typically includes:\\n\\n· A Bachelor’s or Master’s degree in a quantitative field such as statistics, mathematics, econometrics, operations research,\\n\\ndata science, computer science, engineering, marketing or social science methods.\\n\\n· Hands-on experience mining data for decision-focused insights.\\n\\n· Hands-on experience running common statistical or machine learning procedures, such as descriptive statistics, hypothesis testing, dimension reduction, feature transformation, supervised or unsupervised learning.\\n\\n· Hands-on experience using Python or R, SQL, and distributed computing systems such as Hadoop or AWS. Familiarity with Linux and/or Spark preferred.\\n\\n· Demonstrated interest in marketing analytical applications.\\n\\n· Demonstrated self-starter who thrives in a fast-paced environment with flat structure.\\n\\nGot what it takes? We’d love to hear from you.\\n\\nDigitas is an equal opportunity employer.',\n",
       "  'The Team\\n\\nAt Fidelity Enterprise Risk, we are looking for a skilled data scientist who has the passion and capability to respond to business questions bringing to bear a wide range of enterprise data sources, advanced analytics and machine learning techniques and tools. You will play a meaningful role on the Fidelity Risk Group Data Science & Analytics team to help craft the data science and execute on key foundational projects. Your work will have a significant impact across the entire Fidelity enterprise.\\n\\nThis position will be located full time in Jersey City, NJ.\\nThe Expertise You Have\\n3+ years of experience crafting, developing, validating, and deploying predictive models that directly support business decision-making\\nAdvanced degree in Engineering, Computer Science, Computational Statistics, Operations Research or equivalent experience\\nProven experience with advanced Machine Learning techniques including neural networks, deep learning, reinforcement learning, support vector machines, principal component analysis, regression, time series analysis and clustering\\nBackground in projects involving large scale-multi dimensional databases, complex business infrastructure, and multi-functional teams\\nThe Skills You Bring\\nProven experience using mainstream languages, such as Python, R, or equivalent to source, cleanse, and model large data sets.\\nSkilled in visualizing and communicating data using packages such as d3.js, Plotly, and Seaborn, and tools such as Tableau and Qlik\\nHands-on experience applying machine learning techniques using the standard data science tool kits and libraries\\nIntermediate level experience with sourcing and wrangling data from warehouses, Big Data (e.g. Hadoop, Spark) and other sources using SQL and scripting\\nComfortable with working across technologies and business functions, as well as the ability and willingness to develop solutions at all levels of the data science stack, from data engineering and sourcing to productionalization.\\nSelf-starter mindset the ability to manage ambiguity, and identify what needs to be done and execute rather than waiting for explicit direction\\nThe Value You Deliver\\nDiscovering and verifying new opportunities to identify risk, grow business, scale and optimize operations\\nHelping Senior Leaders to make decisions utilizing the data and analytical skills\\nHelping to structure work, planning new analyses, translating business questions into analytical projects\\nIdentifying and ingesting new data sources and performing feature engineering for integration into models\\nCompany Overview\\n\\nAt Fidelity, we are focused on makingour financial expertise broadly accessible and effective in helping people livethe lives they want. We are a privately held company that places a high degreeof value in creating and nurturing a work environment that attracts the besttalent and reflects our commitment to our associates. We are proud of our diverse and inclusive workplace wherewe respect and value our associatefor their unique perspectives and experiences. For information about working atFidelity, visit FidelityCareers.com.\\n\\nFidelityInvestments is an equal opportunity employer.',\n",
       "  \"The Opportunity\\n\\n\\nWe're looking for a Medical Data Scientist, working in the Pharmaceuticals & Medical Products industry in Woodcliff Lake, New Jersey.\\nPerforms reconciliation of safety report and clinical patient profile.\\nPosts queries for relevant information from safety report to be added in the clinical database.\\nPerforms narrative writing for clinical data review profile.\\nPerforms a comprehensive review for the all SAE data but not limited to reported and coded terms.\\nOur Client\\n\\n\\nOur client is an award-winning clinical development company. Improving lives globally with 20+ years experience in clinical research and strategic resourcing. Elevating results with proven strategies, comprehensive solutions, and customized delivery models.\\n\\nServing pharmaceutical, biopharmaceutical, biotechnology, and medical device organizations.\\n\\nStrong relationships provide you connections and access to great opportunities. Industry expertise sets you up for success with helpful insights, career coaching, and professional training. Grow and learn while you put your skills to work.\\n\\nExperience Required for Your Success\\nGraduate of adequate health care related course\\nFamiliarity with medical terminologies\\n1 year experience in AE/SAE data review in Clinical Research\\nAt least 2 years of clinical data review experience in Clinical Research, review protocol and CRF designs\\nAdequate knowledge of ICH guidelines\\nStrong knowledge of ICH guidelines\\nKnowledge of INFORM, J-review and/or other clinical trial systems is a plus\\nProficiency on all related regulations, GCP and Good Clinical DM Practice\\nComputer proficiency\\nExpertise on use of Microsoft applications, such as MS Word, PowerPoint and Excel\\nStrong oral and written communication skills\\nDetails\\n\\n\\nThe pay range we're offering is 50 to 65 per hour. This position may present an opportunity to go permanent.\\n\\nWhat Do You Think?\\n\\n\\nDoes your experience reflect what it takes to be successful in this role? Do the work and challenges get you excited about what's possible? Apply here.\\n\\nNot exactly? Join Our Talent Community, and we'll let you know of additional opportunities.\",\n",
       "  \"Data Scientist (Advanced Analytics) - RET000816\\nData Scientist (Advanced analytics), Pharma\\nFunction : Operations\\n\\nRTP, NC, USA\\n\\nWith a startup spirit and 80,000+ curious and courageous minds, we have the expertise to go deep with the world’s biggest brands—and we have fun doing it. Now, we’re calling all you rule-breakers and risk-takers who see the world differently, and are bold enough to reinvent it. Come, transform with us.\\n\\nInviting applications for the role of Data Analyst, Pharma\\nIn this role, you will be responsible for analytics services and thought leadership role for Advanced analytics and Marketing Mix function. You will function as a lead for delivering analytics and insights to address a wide range of business needs.\\n\\nThe role involves working with various stakeholders to understand the business problem and consult with them to design execute solutions. This would require sound knowledge and hands on experience of advanced statistical modeling techniques, Market Mix models, Test control studies etc.\\n\\nThis would require sound knowledge of Pharmaceutical data sources (e.g., Sales, Rx, HCP/Patient/Payer-level data, Formulary data, quantitative research outputs, etc.).\\n\\nYou will work with client’s leadership team to understand the business needs, help them think through the risks and opportunities that impacts their business performance.\\n\\nYour role will require you to leverage appropriate advanced and sophisticated methods & approaches to synthesize, clean, investigate data as appropriate and deliver as per business needs.\\n\\nYou will be expected to be lead and deliver the proposed analytical projects.\\n\\nResponsibilities\\nEstablishing and maintain process that support efficient and effective data collection, reporting and analysis. In addition, collaborate Business, Technology Partners and External vendors to ensure the accuracy and completeness of data.\\nCo-ordinate with business and technology partners to participate in the Business Insights development effort to analyze, map, and define requirements.\\nUnderstanding business rules and their application, and documenting the same to ensure accuracy in alignment with the business requirement.\\nProvide consultation and support in interpretation and management of complex data sources to support operations and data quality.\\nTriaging and addressing quality issues related to data.\\nWork with business to identify new data requirements to support business needs.\\nMonitor incoming requests, research data issues / discrepancies, implement changes in internal systems, communicate decisions back to impacted teams and closeout requests.\\nProvides consultation and support in interpretation and management of complex data sources to support operations and data quality.\\nCreating reports and dashboards using all relevant data to inform decisions.\\nDemonstrated knowledge of data manipulation and transformation methods, quality check approaches to develop a deep understanding of the pharmaceutical data assets available for usage.\\nQualifications\\nMinimum qualifications\\n• Master’s/Bachelor’s\\n\\n• Robust analytics experience (including Statistics and SAS/R)\\n\\n• Experience with analytics tools (Tableau, etc.)\\n\\nPreferred qualifications\\n• Expertise in SQL, Teradata, Azure and Unix Shell Scripting\\n\\nGenpact is an Equal Opportunity Employer and considers applicants for all positions without regard to race, color, religion or belief, sex, age, national origin, citizenship status, marital status, military/veteran status, genetic information, sexual orientation, gender identity, physical or mental disability or any other characteristic protected by applicable laws. Genpact is committed to creating a dynamic work environment that values diversity and inclusion, respect and integrity, customer focus, and innovation. For more information, visit www.genpact.com . Follow us on Twitter , Facebook , LinkedIn , and YouTube .\\n\\nJob Manager\\nPrimary Location United States\\nEducation Level Master's / Equivalent\\nJob Posting Oct 16, 2019, 7:26:19 PM\\nUnposting Date Dec 16, 2019, 6:29:00 PM\\nMaster Skills List Operations\\nJob Category Full Time\",\n",
       "  'ABOUT THE TEAM:\\n\\nAs part of the Data Sciences team, the Manager helps measure the performance of campaigns, illustrates clients’ return on investment, and provides insights into ad performance. You will utilize various analysis and measurement tools to provide timely, in-depth reporting solutions. The Manager is expected to be an active participant on the business, consistently engaged and contributing ideas and insights that will benefit our clients.\\n\\nDATA ANALYSIS AND EXECUTION:\\nResponsible for pulling data, analyzing data, and quality control of reporting/analysis for multiple brands\\nUtilizes tools and databases to report and analyze clients’ post analysis data\\nIdentify effective and innovative solutions to be used for digital, SEM, Social and e-retail measurement and optimization.\\nEnsures that quality and timeliness of deliverables meet client expectations\\nOffer attention to detail and ability to identify trends in data across sources\\nSTRATEGIC THINKING:\\nAssess problems and concerns in a systematic, logical and rational manner\\nProactively resolve issues\\nManage multiple deliverables across client and offer an ability to seamlessly transition among them\\nRELATIONSHIP MANAGEMENT:\\nBuilds positive relationships with internal teams and clients\\nManages day-to-day client deliverables\\nCOMMUNICATION:\\nExpresses oneself clearly and concisely in communications\\nOrganizes ideas and information logically and sequentially\\nDelivers client reports that are clear and compelling\\nEnsure key information is provided as needed\\nYOUR QUALIFICATIONS:\\nBachelor’s Degree\\nMust be able to demonstrate data interpretation capabilities\\nExcellent presentation and report writing skills\\nStrong analytical skills and ability to relate results to client business objectives\\nStrong computer skills (Excel, Access PowerPoint)\\nKnowledge of working with large relational datasets helpful\\nEntrepreneurial attitude\\nThrives in a fast paced, constantly changing environment; can adapt plans due to changes in client objectives, priorities or budgets while minimizing impact on project momentum\\nABOUT WAVEMAKER:\\nHi Future Makers. We are Wavemaker. We are a next-generation agency delivering world-class solutions through media, content and technology for some of the world’s biggest brands. We recognize that our people create our competitive advantage and we will only achieve our fullest potential by leveraging each individuals’ unique skills, expertise and perspectives. As a result, we’ve designed an inclusive culture that encourages and embraces diversity in all its dimensions. As Future Makers, we are driven by our deep audience and purchase journey understanding, coupled with our unrivalled scale and access through GroupM, to ensure the delivery of leading-edge solutions at the intersection of media, content & technology. Our mission is ‘Let’s make the future’ – through a dazzling blend of magic and logic -- we never stop Caring, Creating and Growing to help our clients and people succeed.\\n\\nTake a virtual tour of the office: https://roundme.com/tour/326585\\n\\nGroupM and all of its affiliates embrace and celebrate diversity, inclusivity, and equal opportunity.We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. We are a worldwide media agency network that represents global clients. The more inclusive we are, the more great work we can create together.',\n",
       "  \"The customer journey starts with a question. And consumers expect answers. Yext puts businesses in control of their facts online with brand-verified answers in search. By serving accurate, consistent, brand-verified answers to consumer questions, Yext delivers authoritative information straight from the source — the business itself — no matter where or how customers are searching. Taco Bell, Marriott, Jaguar Land Rover, and businesses around the globe use the Yext platform to capture consumer intent and drive digital discovery, engagement, and revenue — all from a single source of truth. Yext's mission is to provide perfect answers everywhere.\\n\\nWe are looking to hire a Senior Data Analyst to join our Business Intelligence team at Yext. You will take responsibility for managing our master data set, developing reports, troubleshooting data issues, and planning and enabling complex new reporting in a SQL-heavy environment. To do well in this role you need a very fine eye for detail, experience as a Data Analyst and deep understanding of the popular data analysis tools such as Looker and databases such as MySQL and Postgresql (Redshift).\\n\\nResponsibilities\\nManaging master data, including creation, updates, and deletion.\\nProvide quality assurance of imported data\\nCommissioning and decommissioning of data sets.\\nEffectively communicate complex analysis and processes to Data Engineers.\\nProcessing confidential data and information according to guidelines.\\nHelping develop reports and analysis.\\nSupport and direct cross functional efforts across business stakeholders to plan and enable new reporting\\nManaging and designing the reporting environment, including data sources, security, and metadata.\\nSupporting the data warehouse in identifying and revising reporting requirements.\\nSupporting initiatives for data integrity and normalization.\\nAssessing tests and implementing new or upgraded software and assisting with strategic decisions on new systems.\\nGenerating reports from single or multiple systems.\\nTroubleshooting the reporting database environment and reports.\\nEvaluating changes and updates to source production systems.\\nTraining end users on new reports and dashboards.\\nProviding technical expertise on data storage structures, data mining, and data cleansing.\\nMinimum Requirements\\nBS or similar college level education in Mathematics, Economics, Computer Science, Information Management, Statistics or relevant work experience in an analytical field.\\nAt least 4+ years of experience in a Data Analyst or analytical role.\\nExperience with Data Visualization tools, Looker preferred.\\nStrong SQL skills, preferably in MySQL or Postgresql; ability to write complex SQL queries from scratch is a must.\\nExperience enabling lead-to-cash reporting (preferred).\\nExperience with a scripting language(s) preferably Python.\\nAbility to work with stakeholders to assess potential risks.\\nExperience engaging with stakeholders on complex business requirements.\\nAbility to analyze existing tools and databases and provide software solution recommendations.\\nAbility to translate business requirements into non-technical, lay terms.\\nHigh-level experience in methodologies and processes for managing large scale databases.\\nDemonstrated experience in handling large data sets and relational databases.\\nHigh-level written and verbal communication skills.\\nAbility to work in a fast-paced environment with evolving goals.\\nCompensation, Benefits & Perks\\n\\nYext offers the following exceptional benefits: competitive compensation, 401k, unlimited snacks, daily meal allowance, flexible hours/paid time off, and excellent health/dental/vision insurance. We treat our employees well and offer tremendous growth opportunities. Challenging work pushes our people to be creative in a casual environment that is caring, fun, and collaborative. We believe that when you have smart, happy people working together you can produce something special.\\n\\nAbout\\n\\nYext has been named a Best Place to Work by Fortune and Great Place to Work®, as well as a Best Workplace for Women. Yext is headquartered in New York City with offices in Amsterdam, Berlin, Chicago, Dallas, Geneva, London, Miami, Milan, Paris, San Francisco, Shanghai, Tokyo and the Washington, D.C. area.\\n\\nYext is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ethnicity, religion, creed, national origin, ancestry, genetics, sex, pregnancy or childbirth, sexual orientation, gender (including gender identity or nonbinary or nonconformity and/or status as a trans individual), age, physical or mental disability, citizenship, marital, parental and/or familial status, past, current or prospective service in the uniformed services, or any characteristic protected under applicable law. We also consider qualified applicants regardless of criminal histories, consistent with legal requirements. If you have a disability or special need that requires accommodation, please let us know.\\n\\n#LI-AS2\",\n",
       "  'JobDescription :\\nData Scientist Job Description\\n\\nThe Senior Data Scientist will be a part of the S&P Global Market Intelligence (SPGMI) Data Science team.\\n\\nThe Role:\\n\\n• Discover insights and identify opportunities through the use of statistics, algorithms, data mining and visualization techniques\\n• Build and evaluate models, make predictions, gather results, and communicate findings to stakeholders\\n• Evaluate the big picture and solve problems rather than looking at metrics alone\\n• Use advanced business knowledge and advanced machine learning techniques to acquire, combine & transform multiple dataset to solve a business use case\\n• Collaborate with engineering and product teams to create and build strategic models that drive product improvements while maintaining cost efficiency\\n• Build and maintain re-usable machine learning and model validation procedures for the rest of the team to use\\n\\nExperience and qualifications:\\n\\n• Bachelor’s Degree in Mathematics, Statistics, Computer Science, Engineering, Operations Research or related fields preferred (Master’s degree an advantage)\\n\\n• 5+ years practical experience with statistical analysis and creating complex models, preferably in the financial services sector\\n\\n• Excellent analytical and problem solving skills\\n\\n• Advanced experience in at least one data analysis/data transformation package (R, Python, Alteryx)\\n\\n• Exposure to one or more data discovery, data visualization tools\\n\\n• 5+ years of hypothesis testing, web analytics and python scripting\\n\\n• Experience with Machine Learning and Natural Language Processing\\n\\n• Ability to remain focused and to think logically in a fast-paced environment\\n\\n(NICE TO HAVE)\\n\\n* experience with nlp\\n\\n* exposure to reinforcement learning\\n\\n* exposure to information retrieval (search)\\n\\nJob ID :\\n249692\\n\\nPosted On :\\n7-22-2019\\n\\nLocation :\\nNew York, NY US',\n",
       "  \"The Opportunity:\\n\\nAs a Business Intelligence Analyst, you will play a key role in designing and architecting business intelligence/data management solutions, and analytic strategies across our enterprise. In this role you will be bringing your excellent analytical and organizational skills to enable our talented team of BI and data engineers to design and build analytics solutions. You will partner with all business stakeholders at Shutterstock to identify solutions that could be automated to solve critical business needs. You will become an expert on the existing BI and analytics ecosystem and solutions available in the company to consult other teams on existing availability of the solution.\\n\\nResponsibilities:\\nAssist in development and execution of strategic, operational, and analytical initiatives\\nParticipate in all phases of the project lifecycle, and work with project management to remove impediments and provide visibility to stakeholders\\nRequirements management from planning to execution, developing, and tracking of project requirements\\nHelp design and implement data catalog, data classification and data lineage solutions\\nWork on ad-hoc data requests, identify and design automated solutions where possible\\nEvaluate relevant new and mature technologies as needs, gaps, and opportunities arise\\nWork closely and collaboratively in an Agile environment with our analysts, engineers and product teams to analyze issues and find new insights covering our business and operations\\nTeach, mentor, and provide value on a consistent basis to stakeholders.\\nRequirements:\\n3+ years of relevant experience\\nStrong analytical orientation\\nIn-depth knowledge of SQL\\nKnowledge of data modeling and dimensional modeling concepts\\nExcellent verbal and written communication skills\\nExcellent attention to detail and highly organized with demonstrated ability to work on multiple projects\\nAbility to identify problem patterns and apply solutions that enable our team to accelerate delivery\\nAbility to work in a dynamic and fast-paced environment with changing priorities\\nSystems Analyst skills and experience (requirements gathering, driving discussions with the business, building relationships)\\nBS or MS degree in a related field\\nBonus Skills:\\nFamiliarity with MS Sql Server, SSIS, SSAS\\nFamiliar with data governance\\nExperience with modern data pipeline, data warehouse, ETL, data modeling, and BI tools\\nAbout Shutterstock:\\n\\nShutterstock, Inc. (NYSE: SSTK), directly and through its group subsidiaries, is a leading global provider of high-quality licensed photographs, vectors, illustrations, videos and music to businesses, marketing agencies and media organizations around the world. Working with its growing community of contributors, Shutterstock adds hundreds of thousands of images each week and has millions of images and video clips available.\\n\\nHeadquartered in New York City, Shutterstock has offices around the world and customers in more than 150 countries. The company also owns Bigstock, a value-oriented stock media agency; Shutterstock Custom, a custom content creation platform, Offset, a high-end image collection; PremiumBeat a curated royalty-free music library; Rex Features, a premier source of editorial images for the world's media.\\n\\nFor more information, please visit www.shutterstock.com and follow Shutterstock on Twitter, Facebook and Instagram.\",\n",
       "  'Job Summary: The Senior Scientist will conduct research in the areas of Flavor and Natural Product Chemistry. Will be responsible for conducting investigative studies applicable to the Citrus Flavor Industry.\\n\\nEssential Job Functions:\\nThe candidate is expected to perform natural botanical isolations utilizing techniques in the area of distillation and extraction. In addition, developing analytical techniques and methods to evaluate volatile and non-volatile components.\\nMaintain and update flavor raw material files, essential oil analysis files and Mass Spec libraries as needs arise and new or existing chemicals are introduced.\\nPlanning and conducting scientific experiments.\\nCollecting and analyzing data collected from scientific experiments.\\nOrganizing, summarizing, and presenting data and conclusions of scientific experiments.\\nPreparation of written research reports.\\nMaintaining accurate official records of research (laboratory notebooks, etc.)\\nTraining of other R&D (and other Takasago groups) staff members.\\nConducting scientific literature searches related to research projects.\\nAttending outside instrumentation, technique, and scientific knowledge training courses.\\nAttending external scientific and other industry specific meetings.\\nWhen applicable, directing the research of other R&D members.\\nPresenting summaries of research to clients.\\nConducting scientific literature summary analyses.\\nWhen applicable, preparation of patent applications and peer reviewed research publications.\\nPreparation of proposals for new areas of research.\\nEducational and Expereince:\\n\\nB.S. Chemistry (with 10+ years); M.S. Chemistry (with 5+ year); or Ph.D. Chemistry (0-5 years). Citrus Industry Experience a plus.\\n\\nCompetencies: Experience developing analytical methods for GC, GC/MS, HPLC, and LC/MS instrumentation analysis. Conducting routine maintenance and troubleshooting of analytical equipment. Experienced in the analysis of non-volatile food and flavor components is a plus as well as separation science, physical chemistry, Design of Experiment (DOE), and pilot plant experience.',\n",
       "  'Engineering\\n\\nMachine Learning Engineer\\n\\nNew York City\\n\\nCompass is building the first modern end-to-end real estate platform by integrating agents, buyers and sellers through technology. Until Compass, no one has achieved the blend of the Natural Intelligence that hundreds of thousands of enterprising real estate agents bring to this market, with the “Artificial Intelligence” that cloud, mobile and AI technologies enable. We are building AI to empower AI - Artificial Intelligence to empower Agent Intelligence.\\n\\nAs one of the fastest growing technology companies of our generation, in an industry larger than any other, we have an opportunity and obligation to build a world-class engineering team and the operating platform that will transform real estate. In 2019 we will triple the size of our engineering team and are searching for creative and inspiring colleagues at all levels of the engineering organization.\\n\\nAt Compass, our mission is to help everyone find their place in the world. This means we continually celebrate the diverse community different individuals cultivate. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone’s place. Our entrepreneurship principles bind us together and inform how we tackle the tremendous challenges ahead.\\n\\nAbout the Role\\n\\nAs an AI / Machine Learning Engineer at Compass, you will use your experience with AI tools and technology to improve every step of the real estate agent, buyer and seller experience at Compass. Leveraging the data-rich features of the real estate industry, you will build technology that analyzes listings and other relevant geospatial features, observes user behavior, ingests financial transaction data and builds models that predict the needs of Compass stakeholders at every step of their real estate journey.\\n\\nAt Compass You Will:\\nBuild, develop and scale our machine learning infrastructure that powers data science and AI features\\nBe a domain expert on real estate technology and products and an empathetic partner to our customers\\nOperate in a scalable engineering culture that leverages modern principles of decoupled systems and automated CI/CD/testing/monitoring to drive efficiencies\\nWhat We Look For:\\nBA/BS in Computer Science, related technical field or equivalent practical experience.\\nStrong programming skills in one or more of the following: Python, Go, C++, and/or Java\\nExperience with modern web frameworks (e.g. Go/React), distributed computing (e.g. Spark), public cloud platforms (e.g. AWS) and data pipelines\\nFamiliarity with containerization, microservices architecture, continuous integration and delivery (CI/CD)\\nExperience with products throughout their life cycles, from idea conception to product release and maintenance\\nExperience in platform and infrastructure development in supporting machine learning applications in one or more of the following areas: Search, RecSys, Natural Language Processing (NLP), Computer Vision (CV), Optimization, Time Series Forecasting and Data Mining.\\n10+ years of relevant experience\\nAt Compass, our mission is to help everyone find their place in the world. This means we continually celebrate the diverse community different individuals cultivate. As an equal opportunity employer, we stay true to our mission by ensuring that our place can be anyone’s place.\\n\\nCheck out our Engineering blog!\\n\\nOffice\\n\\nGreater New York City\\n\\nTeam\\n\\nEngineering',\n",
       "  'Overview\\n\\n\\nData Engineer\\n\\nAt Hospital for Special Surgery our clinicians and scientists collaborate to deliver the most innovative care. Our specialized focus on orthopedics and rheumatology enables us to help patients get back to what they need and love to do reliably and efficiently. Our patients are overwhelmingly satisfied with the care they receive at our facilities. When you join us, you will become part of this legacy of commitment to the most cutting-edge research and coordinated care.\\n\\nThe ideal candidate must be passionate about the field of analytics and well versed in all aspects of an enterprise-wide analytics architecture. We are looking for Data Engineers who have a strong understanding of data pipelines, ready to work hard but most importantly curious. They will work collaboratively across the organization with clinical and operational leaders on a diverse range of projects to find insights that support our mission of providing world class musculoskeletal health.\\n\\nAs a Data Engineer You Will\\nDesign, implement, and automate data flows\\nAssist in the implementation and administration of the analytics data lake\\nWork with partners and vendors on data integration projects\\nDesign and create data models for new or existing data sources\\nDesign and implement ETLs\\nQualifications\\n\\n\\nYour Technical Skills and Experience\\n3+ years’ experience with ETL tool SQL Server Integration Services (SSIS) or similar ETL Tool\\n3+ years’ experience with SQL Server, strong expertise in T-SQL required\\n3+ years’ experience with Web Services: RESTful APIs, .NET WCF and SOAP\\n1+ years’ experience with .NET C# or other OO language required\\nExperience with Windows Server Operating Systems required\\nExperience with SSAS and building cubes required\\nExperience with statistics and machine learning tools preferred: (Python, R, Matlab or Spark)\\nExperience with public cloud preferred: AWS or Azure\\nExperience with Big Data or NoSQL technology preferred: Hadoop, Spark, AWS EMR, Hive, etc.\\nExperience in healthcare a plus\\nAbout You\\nYou are passionate about data and exploring new technologies\\nYou are a critical thinker who loves solving difficult problems\\nYou are flexible and a natural team player\\nEducation and Certifications\\nMS/BS in Computer Science or related field',\n",
       "  \"HireArt is looking to hire a Marketing Data Analyst to work on-site at Netamorphosis' New York City office on a contract basis. In this role, youll ensure that technology and omnichannel platforms can be quantified, calculated and reported against, directing the companys collective path forward with data-based insights.\\n\\nYoull work to ultimately build credibility, resourcing, and security into the relationships cultivated with clients and strategic and investment partners, working hand-in-hand with developers to make things happen. This requires an entrepreneurial spirit, the capability to move forward with challenges, and an ability to set aside ego in order to build something amazing.\\n\\nWere seeking candidates with experience in data or marketing analytics who are driven, effective and detailed communicators, and problem solvers. The ideal candidate is also autonomous in ensuring the work gets done, meets collective standards, and paves the way forward on a growing team.\\n\\nResponsibilities:\\nLead UAT system and data integrations and evaluate the performance of business solutions.\\nIdentify improvements and refine targets for future releases.\\nLearn and apply best practice financial modeling.\\nUse data to develop and present strategic insights for clients.\\nStay up-to-date on emerging technology trends and practices, adapting processes as needed.\\nBuild knowledge across a dynamic service offering in order to best grow amazing clients and to navigate your own growth path.\\nBe the communication bridge between client services and technology teams on web platform and application development throughout a project life cycle.\\nRequirements:\\n1+ years experience in compiling data analytics for a digital, eCommerce, or omnichannel attribution\\nTechnical competence to perform data analytics and utilize web analytics to manage web development life cycle\\nExposure to profit and loss and pro-forma financial statements\\nStrong verbal and written communication skills, and able to communicate technical terms to non-technical audiences\\nTime management skills and the ability to meet strict deadlines\\nAnalytically minded, highly organized, and able to prioritize and multi-task\\nStrong work ethic and high level of integrity\\nMac proficient with solid experience in all Microsoft Office programs, especially Microsoft Excel,\\nWord and PowerPoint/Keynote\\neCommerce, qualitative research practices, and strategy development a plus\\nCoding skills a plus\\nCommitment: This is a full-time position (40+ hours per week). It is a 6-month contract position based in New York, NY with potential for FT conversion.\\n\\nApply Now!\",\n",
       "  \"Job Description\\nMarketing Data Analyst- CONTRACT ROLE\\n\\n$110,000- $40-60hr\\n\\nNew York, New York\\n\\nIf you are interested in joining a digital platform for one of the top video-streaming companies in the world, we have an opportunity to join their data-driven marketing campaign on a contract role.\\n\\nTHE COMPANY:\\n\\nThis company is consistently in the top of the market for online video streaming and has over 100 million subscribers using their services. They are looking to continue expanding and hope to do so by bringing on an experienced data analyst with a strong programming background.\\n\\nTHE ROLE:\\n\\nIn this role you will be responsible for the constant flow of data into the marketing technology systems. You will be responsible for compiling marketing requirements, translating and implementing technical requirements and testing the final product.\\n\\nAs a Marketing Data Analyst, you can expect to be involved in the following:\\nData collection and testing by representing marketing data collection points\\nPartner with data engineering teams to help with the operational side and maintain the marketing analytics environment\\nEnsure data readiness for the customer interaction channels; including in-apps, on-site, paid media, email, mobile messaging, and more.\\nYOUR SKILLS AND EXPERIENCE:\\n\\nThe successful marketing data analyst will have the following skills and experience:\\nSQL expert\\nFamiliarity with data warehouses, like AWS and Snowflake\\nBachelor's degree, Masters level degree preferred\\nProject management and organizational skills\\nTHE BENEFITS:\\n\\n$110,000\\n\\nOverall this company provides a very collaborative environment where the candidate will be able to grow and develop as a data analyst in an industry leading organization.\\n\\nHOW TO APPLY:\\n\\nPlease register your interest by sending your CV to JP Villamar via the Apply link on this page.\\nCompany Description\\nData and Analytics recruitment is our core business and we’re proud to say, our customers believe we’re good at it. In our most recent customer satisfaction survey, 95% of respondents said that they would recommend Harnham.\\n\\nHarnham has actively chosen to focus on Data and Analytics, we’ve immersed ourselves in this market and are now an integral part of this business community.\\n\\nOur capability has grown to provide recruitment services and advice across the Marketing Analytics, Credit Risk, Data Science, Data and Technology and Digital sectors.\",\n",
       "  \"At Uber, we ignite opportunity by setting the world in motion. We take on big problems to help drivers, riders, delivery partners, and eaters get moving in more than 600 cities around the world.\\n\\nWe welcome people from all backgrounds who seek the opportunity to help build a future where everyone and everything can move independently. If you have the curiosity, passion, and collaborative spirit, work with us, and let’s move the world forward, together.\\n\\nAbout the Role\\n\\nUber Everything Data Scientists help solve the most challenging problems related to Uber's ambitious and rapidly expanding on-demand delivery businesses such as Uber Eats. Come tackle fascinating and difficult problems associated with Uber's three-sided delivery marketplace, including personalized search and recommendation for restaurants and dishes, travel and food preparation time prediction, text mining and natural language processing, demand and supply forecasting, growth and spend optimization, dynamic pricing, dispatch and routing optimization, and many more. To solve these problems, data scientists leverage unique data sources diverse in both geographical and temporal dimensions and in both structured (data from app sessions, trips, etc.) and unstructured (menu descriptions, food photos, support contacts, etc.) forms.\\n\\nWhat You’ll Need\\nGraduate degree required with PhD preferred (with anticipated graduation in fall 2019 - spring 2020) in statistics, math, machine learning, operations research, economics, EECS, etc.\\nPrior research, data science modeling, or engineering experience in the aforementioned domains\\nFamiliarity with technical tools for analysis - Python (with Pandas, etc.), R, SQL, etc.; previous software engineering background a plus\\nResearch mindset with bias towards action - able to structure a project from idea to experimentation to prototype to implementation\\nPassionate and attentive self-starters, great communicators, amazing follow-through - you have a great work ethic and love the responsibility of being held accountable for the results\\nBonus Points For\\nExperience with A/B testing\\nAbout the Team\\n\\nUber Eats Data Scientists help solve the most challenging problems related to Uber's ambitious and rapidly expanding on-demand food delivery businesses, which currently operates in more than 45 countries globally and is the largest outside of China. These fascinating and difficult problems include personalized search and recommendation for restaurants and dishes, travel and food preparation time prediction, text mining and natural language processing, demand and supply forecasting, growth and spend optimization, dynamic pricing, dispatch and routing optimization, and many more.\\n\\nBelow is a list of sub domains in our NYC location:\\nRestaurant (NYC) | The restaurant team aims to increase restaurant selection on Uber Eats while setting up restaurant partners for success. We leverage statistics and machine learning techniques to optimize the experience of restaurants at different life cycles. From onboarding, to menu creation, marketing, and order experience, we empower them with tools to increase demand and maintain a frictionless interaction with the platform.\\nMarketplace - Logistics and Marketplace Intelligence (NYC) | From dispatch and routing optimization to predicting food delivery time, the Logistics and & MI team provides DS solutions to create the best customer experience on all sides of the Eats marketplace in the most efficient way. The team also provides marketplace management solutions using state of the art machine learning techniques and our dispatch simulator.\",\n",
       "  \"Company Description\\n\\nQuartet is a pioneering healthcare technology company striving to improve the lives of people with mental health conditions. We connect people to a personalized care team to get them the right care at the right time. Our collaborative technology platform and range of services brings together physicians, mental health providers, and insurance companies to effectively improve patient outcomes and drive down healthcare costs. Backed by $153MM in venture funding from top investors like Oak HC/FT, GV (formerly Google Ventures), F-Prime Capital Partners, Polaris Partners and Centene Corporation, Quartet is headquartered in NYC and is currently operating in several markets across the United States Pennsylvania, Washington, Northern California, New Jersey, North Carolina, Louisiana, and Illinois.\\n\\nAbout the team & Opportunity:\\n\\nQuartet's Data Platform team uses cutting-edge data engineering frameworks to help solve one of the most urgent problems in our society.\\n\\nData Platform Engineer is a cross-cutting role that involves handling both complex business problems and complex technology: our incoming data sources are crucial to guiding correct clinical decisions and creating a stellar product. You will build and scale systems that ingest, standardize, enrich, and load external healthcare data sources. You will also build data exploration and munging infrastructure to support our growing data integration team.\\n\\nWorking with analysts, data scientists, clinicians, and product managers, you will collaborate across the company on our shared mission.\\n\\nAccountabilities:\\nWork as a member of an agile scrum team\\nDevelop robust and production level code to implement new product features in collaboration with other engineers and subject matter experts\\nTroubleshoot problems and improve product quality\\nTake pride and ownership of the code and build high quality software\\nMinimum Qualifications:\\nStrong proficiency with Python (ideally PySpark)\\nExperience with AWS S3, EC2, EMR, Redshift or an equivalent cloud-hosted infrastructure\\nInterest in building distributed computing and orchestration frameworks; we use Apache Spark and Airflow\\nExperience writing and productionizing complex data transformations in SQL and related frameworks\\nStrong oral and written communication, including experience writing detailed design proposals\\nBachelor's degree in Computer Science, Software Engineering or equivalent experience\\nPreferred Qualifications:\\nKnowledge of health systems and technologies, especially involving healthcare claims data\\nStrong background in statistics\\nEmployee Benefits for Quartet include: Unlimited vacation, volunteer opportunities, catered lunches, snacks, team events and outings, mental healthcare coverage of 15 free therapy sessions + unlimited copay reimbursements, medical, dental + vision coverage, generous parental leave, commuter benefits, 401K, stock option grants, gym benefits.\\n\\nWant to know what Quartet life is like? Click here to meet our team.\\n\\nQuartet is committed to building a diverse team and fostering an inclusive culture, and is proud to be an equal opportunity employer. We embrace and encourage our employees' differences in race, religion, color, national origin, gender, family status, sexual orientation, gender identity, gender expression, age, veteran status, disability, pregnancy, medical conditions, and other characteristics. Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Quartet does not accept unsolicited headhunter and agency resumes. Quartet will not pay fees to any third-party agency or company that does not have a signed agreement with Quartet.\\n\\nPlease note: Quartet interview requests and job offers only originate from quartethealth.com email addresses (e.g. jsmith@quartethealth.com). Quartet will also never ask for bank information (e.g. account and routing number), social security numbers, passwords, or other sensitive information to be delivered via email. If you receive a scam email or wish to report a security issue involving Quartet, please notify us at: security@quartethealth.com.\\n\\nHave someone to refer? Email talent@quartethealth.com to submit their details to us.\",\n",
       "  'BNY Mellon’s Data and Analytics Solutions further extend Asset Servicing capabilities in securities and cash into the world’s most important ‘asset class,’ data. As a software and content business, inclusive of Eagle Investment Systems’ data management, accounting, and performance platform and Intermediary Analytics’ sales and distribution data, the offering also includes a suite of new cloud-based products. An ecosystem of proprietary and third-party business applications are available to help firms manage their core investment process and beyond.\\n\\nWe are seeking strong individuals to assist with decision and data science. The successful candidate will support the Product Management team in deriving client insights, analytics and interpretation of large amounts of data and ways to differentiate in the market. The Product Management team will work with senior leaders internally and externally to help shape initial Minimal Viable Solutions (MVPs) to fully scaled products.\\n\\nWhat You Will Do\\nWork closely with cross-functional team of data scientists, software engineers, product managers, and product development\\nProvide expertise in big data analytics from a data science and data engineering perspective (expertise in data manipulation, visualization, building and optimizing classifiers using machine learning and deep-learning based techniques)\\nProvide excellent analytical skills to mine data, develop algorithms and then analyze results to determine decisions or actions relevant to a clients’ vision and analytics (e.g., trade performance and risk, investor analytics, custom indices & benchmarks)\\nYou will be Successful If You\\nHave a Quantitative background – Understanding of statistics, optimization and machine learning methodologies. Ideally experienced with large scale graph inference. Mastery of software design principles and development skills\\nAre hybrid at your core - Part statistician, part computer scientist, part researcher, looking for order and patterns in data. This is an incredible opportunity for hybrid thinkers that love to learn and synthesize in a high performing culture.\\nAre an analytical thought leader - You can define the analytical agenda for projects, frame ambiguous business questions into analytical plans (e.g., assess data needs, source files, prepare data, create new features, evaluate quality), and execute leveraging technical competence in advanced analytics techniques (e.g., linear regression, logistic regression, discriminant analysis, neural networks)\\nHave cognitive and communications Skills - Highly articulate, built upon an underlying fundamental clarity of thought. General ability to root out fundamental issues, bring order from chaos, synthesize elegant insights, and drive to clear decisions.\\nAre a leader - Primary focus, building something of significance. Willingness to roll up sleeves and do whatever it takes. Self-confidence, poise, and personal presence that inspires confidence in others. Charisma, gravitas, intellect, flexibility, and integrity that motivates others to trust, collaborate, and follow.\\nThe Experience You Will Bring…\\nYou know how to scope and solve loosely defined problems and come up with actionable insights to guide product development decisions\\nExperience applying statistical methods (distribution analysis, classification, clustering, etc.)\\nYou have a deep understanding of statistics and experience working with real-world data, as well as a strong business sense\\nYou have working experience with:\\nC, C , R, Python, R, Java, SQL including Analytical SQL, SOQL, PL/SQL; preferably Python and R\\nData stores like MSSQL, Oracle, Sybase, DB2, Hadoop, MPP Databases like HP Vertica and Redshift, Pig, Hive, Spark, Azure; preferably Azure\\nStatistical methods like linear regression, logistic regression, discriminant analysis, neural networks, ARIMA, CHAID, Cluster Analysis, Market Basket Analysis, Classification\\nData Visualization and Business Intelligence tools like R Shiny, Power BI, Quicksight, BigQuery BI, Tableau, Kibana, Microstrategy; preferably Power BI\\nYou have 5 years of relevant experience, with a degree in statistics, mathematics, computer science, engineering or related quantitative discipline',\n",
       "  \"Instagram is a global community of more than 1 billion, which means jobs here offer countless ways to make an impact in a fast growing organization. Instagram was built to connect people to the people and interests they love. Our app has played a critical part in forming meaningful communities where people can connect with each other and share what matters most to them.The Instagram Shopping organization is working to build an e-commerce business natively on Instagram. Our vision is to make Instagram everyone's personalized mall, leveraging its unique strengths its large number of retail business and community of creators.\\n\\nWere looking for a Data Science Manager for the Supply pillar of our Instagram Shopping organization, to work on core products and help drive informed product decisions.\\n\\nYou will enjoy working with one of the richest data sets in the world, cutting edge technology, and the ability to see your insights turned into real products on a regular basis. The perfect candidate will have experience building and managing analytics teams, will have worked with large data stores, and will have experience building software. You are scrappy, focused on results, a self-starter, and have demonstrated success in using analytics to drive the understanding, progression, and user engagement of a product. This position is based in New York City.\\n\\nThe Data Science Manager is responsible for building a world-class data organization to support the Instagram Shopping team. This role is directly responsible for managing the data science team to ensure proper goal-ing, testing, and forecasting and to develop insights that identify the the biggest opportunities for the IG community and drive product strategy.\\n\\nResponsibilities:\\n\\nTechnical Execution:\\nThis role has less direct technical involvement, but needs to manage a team with a high-degree of technical competency\\nEnsure our data and analysis are reliable and rigorous across all of our products\\nPartner with Data Engineering to ensure the right metrics are created and validate accuracy\\nOversee the building/maintaining of reports, dashboards, and metrics to monitor the performance of our products\\nEnsure we have the appropriate logging, tables and dashboards to measure product success\\nDevelop new metrics with accuracy and stability\\nValidate metric accuracy for internal and external reporting\\nStrategy:\\nAnalyze Data Trends to Identify Opportunities and Drive Product Strategy\\nSize opportunities to ensure product teams are prioritizing the most impactful work\\nEnsure product strategy has actionable measurement plans by developing goal metrics and evaluating product team performance\\nDetermine ways to use data as a strategic asset in product development\\nOperations:\\nDeveloping Goal Metrics and Evaluate Product Team Performance\\nInvestigating Ad Hoc Issues and Debugging Regressions\\nDevelop Testing Plans and Validate Statistically Accurate Results\\nReporting to Instagram and FB Inc. Stakeholders and Coordination Across the Company\\nMininum Qualifications:\\n\\n3+ years of experience in managing other team members in a formal or informal capacity\\nExperience initiating and driving projects to completion with minimal guidance\\nExperience crisply and clearly communicating the results of analysis to an executive audience\\nPreferred Qualifications:\\n\\nBachelor Degree in Computer Science, Math, Physics, Engineering, or related quantitative field\\nUnderstanding of statistical analysis, experience with packages, such as R, MATLAB, SPSS, SAS, Stata, etc.\\nExperience with large data sets and distributed computing (Hive/Hadoop)\\n5+ years of experience doing quantitative analysis at a technology company, consulting, investment banking, or product management, including experience with SQL or other programming languages\",\n",
       "  \"(We are not sponsoring for this role or in the future)\\n\\nFare Buzz is one of the leading online travel providers specializing in worldwide travel and innovative technology; due to our buying power we leverage a unique ability to negotiate aggressive deals on air, hotels, car and vacation rentals to pass the savings on to business professionals who travel frequently and the everyday consumer.\\n\\nWe are seeking a Business Data Analyst to help lead analysis on pricing opportunities, customer behavior, and competitive positioning to build a long-term pricing strategy for the firm. This person will be part of the Pricing team, giving them unique access to make impactful decisions on Fareportal's revenue. The right candidate should be high-performing, driven, willing to learn and excited about travel.\\n\\nResponsibilities:\\nWork collaboratively with departmental managers to identify areas of potential revenue growth (e.g. brand teams, marketing teams)\\nPerform analysis of highest growth opportunity areas to identify and recommend new ways to improve revenue (e.g. analyses around pricing opportunities, competitive positioning, and customer behavior)\\nUse statistical methods to analyze data and generate useful business reports outlining pricing recommendations\\nBuild insight from the data collected to improve customer retention and marketing\\nDesign a strategy to test pricing recommendations including a reasonable hypothesis, implementation, and key metrics for determining success\\nTrack test performance against key performance indicators (KPIs) and launch successive tests\\nClearly communicate results visually and verbally to stakeholders and senior management\\nSupport executive leaders in mobilizing and execution of long-term plans (e.g., program management, setting/tracking metrics, developing roadmaps, creating process maps)\\nRequirements:\\nBachelor's Degree in Statistics, Mathematics, Economics or similar quantitative discipline\\nMaster's Degree preferred\\n2+ years of experience working in a data analyst role\\nResults-oriented, self-driven individual with the ability to manage complex projects\\nSQL experience and advanced Excel skills required; Python, R, PowerBI preferred\\nMust have experience using google analytics\\nExcellent verbal and written communication skills; ability to create an effective presentation to drive action\\nAbility to collaborate effectively and work as part of a team\\nDetail-oriented individual\\nBasic understanding of Agile methodology\",\n",
       "  'Data Scientist (NLP)\\nEXL (NASDAQ:EXLS) is a leading operations management and analytics company that helps businesses enhance growth and profitability in the face of relentless competition and continuous disruption. Using our proprietary, award-winning Business EXLerator Framework™, which integrates analytics, automation, benchmarking, BPO, consulting, industry best practices and technology platforms, we look deeper to help companies improve global operations, enhance data-driven insights, increase customer satisfaction, and manage risk and compliance. EXL serves the insurance, healthcare, banking and financial services, utilities, travel, transportation and logistics industries. Headquartered in New York, New York, EXL has more than 24,000 professionals in locations throughout the United States, Europe, Asia (primarily India and Philippines), Latin America, Australia and South Africa.\\nEXL Analytics provides data-driven, action-oriented solutions to business problems through statistical data mining, cutting edge analytics techniques and a consultative approach. Leveraging proprietary methodology and best-of-breed technology, EXL Analytics takes an industry-specific approach to transform our clients’ decision making and embed analytics more deeply into their business processes. Our global footprint of nearly 2,000 data scientists and analysts assist client organizations with complex risk minimization methods, advanced marketing, pricing and CRM strategies, internal cost analysis, and cost and resource optimization within the organization. EXL Analytics serves the insurance, healthcare, banking, capital markets, utilities, retail and e-commerce, travel, transportation and logistics industries.\\nPlease visit www.exlservice.com for more information about EXL Analytics.\\nPosition Overview\\nEXL Analytics is developing advanced analytics solutions for some of the most challenging business problems in the industry by leveraging AI, NLP, Computer Vision and Speech. If you are interested in working on cutting-edge technologies and solutions, we have an excellent opportunity for you.\\nWe are looking for Data Scientists in the field of NLP who can develop state of the art, scalable, and self-learning systems for enterprises. Primary responsibility will be to develop advanced analytics based NLP solutions for real world business problems using machine learning, deep learning as well as other relevant NLP capabilities.\\n\\nDesign, develop and implement solutions for a wide range of NLP use cases across different domains involving classification, extraction and search on unstructured text data\\nCreate and maintain state of the art scalable NLP solutions for multiple business problems. This involves :\\nChoosing the most appropriate NLP technique based on business needs and available data\\nPerforming data exploration and preprocessing\\nTraining and tuning a variety of NLP models/ solutions which include regular expressions, traditional NLP and/or deep learning models\\nAnalyzing and reporting solution output, performance and accuracy\\nDeploying the solution on premise or on cloud\\nMonitoring the solution outcome\\nCollaborate with ML engineering team to deploy NLP solutions in production- both on premise as well as cloud deployment\\nInteract with clients and internal business teams to perform solution feasibility as well as design and develop solutions\\nPresent solutions to clients and internal stake holders\\n\\n3+ years of hands on experience in developing and deploying NLP solution leveraging Regular Expressions and/or Machine Learning\\nEducation: Bachelor\\'s/ Master’s degree in quantitative fields (Statistics, Operations Research, Mathematics, Econometrics, Computer Science, Engineering etc.). An advanced degree is preferred.\\nProficiency in one or more of programming languages like Python/ Java/ Scala\\nProficiency in NLP packages that perform regular expressions based search and extraction, named entity recognition and extraction\\nExperience with document data bases like MongoDB, Elastic Search etc.\\nExperience with one or more of deep learning frameworks like TensorFlow, Torch, Keras, Caffe, MXNet, Theano is a plus\\nAbility to learn emerging NLP, Machine Learning and Deep Learning techniques and apply them to solve business problems\\nExperience with model optimization on GPUs is preferred\\nExperience with big data analytics technologies like Apache Spark, Kafka, Flume, ElasticSearch is a plus\\nOrganized, self-motivated, disciplined and detail oriented\\nSuperior verbal and written skills\\nWhat we offer:\\nEXL Analytics offers an exciting, fast paced and innovative environment, which brings together a group of sharp and entrepreneurial professionals who are eager to influence business decisions. From your very first day, you get an opportunity to work closely with highly experienced, world class analytics consultants\\nYou can expect to learn many aspects of businesses that our clients engage in. You will also learn effective teamwork and time-management skills - key aspects for personal and professional growth\\nAnalytics requires different skill sets at different levels within the organization. At EXL Analytics, we invest heavily in training you in all aspects of analytics as well as in leading analytical tools and techniques.\\nWe provide guidance/ coaching to every employee through our mentoring program wherein every junior level employee is assigned a senior level professional as advisors.\\nSky is the limit for our team members. The unique experiences gathered at EXL Analytics sets the stage for further growth and development in our company and beyond.\\n\"EOE/Minorities/Females/Vets/Disabilities\"\\n\\nEEO/Minorities/Females/Vets/Disabilities',\n",
       "  \"Here’s the gist:\\n\\nVettery is fundamentally changing the way people hire and get hired. Leveraging machine learning models that track real-time data, monitor trends and predict hiring behavior, we’re able to help companies grow their teams with accuracy, speed, and compatibility. We’re currently working with over 20,000 companies of all sizes, ranging from Fortune 500 giants to startups based out of co-working spaces.\\n\\nThe focus of the data science team is digitizing the recruiting process - from cutting edge recommendations algorithms, to analytics for performance predictions, to NLP projects for candidate sourcing and profile creation. Vettery data scientists work on decision support, product optimization, and operations scaling projects across these areas. We’re growing quickly and this role has the potential for huge impact on our trajectory. You’ll be reporting directly to our VP/Head of Data Science and working closely with the product and engineering teams. The ideal candidate is a self-starter, a critical thinker and a good communicator who excels in a fast-paced environment. Join our team and help us build the game-changing data science products that will transform recruiting!\\n\\nWho you are:\\nPh.D. in Statistics, Operations Research, Mathematics, Computer Science, or other quantitative field.\\n3+ years of industry experience leading the delivery of successful and innovative machine learning products, working independently and with key stakeholders.\\nExtensive track record of success working both independently and with key stakeholders to identify and solve data science problems.\\nStrong understanding of core ML/Statistics/CS principles.\\nStrong skills in statistical languages (e.g. Python/R) and querying languages (e.g. SQL).\\nStrong communication skills to own and share findings and key results across the company and up to the executive level.\\nExperience with NLP a plus! (but not required)\\nWhat you’ll do:\\nWork with a highly motivated, fun and productive team.\\nMove quickly and deliver amazing data science products, developing creative solutions to our biggest data science and engineering challenges.\\nBuild machine learning infrastructure and models (e.g. recommendation engines, statistical models, NLP engines) that drive activity on our platform, scale our business, and enhance the user experience.\\nCollaborate with engineering and product leaders, as well as the co-founders, to frame and tackle a problem, both mathematically and within the business context.\\nCommunicate rationale and findings from analyses to facilitate operational decisions.\\nDesign and track experiments for data science products as well as analyses throughout the organization.\\nIn short, own all phases of the data science product lifecycle (exploratory data analysis, model development, model productionizing, rollout, and evaluation)\\nBenefits:\\nCompetitive salary\\nOpen vacation & sick time\\nMedical, vision, and dental insurance\\nApple laptop computer\\nFrequent team outings, lunches, and team building events\\nA beautiful office located in a sunny corner in the Flatiron district, recently named one of Fortune's 60 Best Companies to Work for in New York City\\nLots of free food: stocked kitchen + beverages\\nVettery values the multitude of talents and perspectives that a diverse workforce brings. All qualified applicants will receive consideration for employment without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or protected veteran status.\",\n",
       "  '114 5th Ave (22114), United States of America, New York, New York\\n\\nAt Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.\\n\\nGuided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.\\n\\nPrincipal Associate, Data Scientist\\nData is at the center of everything we do. As a startup, we disrupted the credit card industry by individually personalizing every credit card offer using statistical modeling and the relational database, cutting edge technology in 1988! Fast-forward a few years, and this little innovation and our passion for data has skyrocketed us to a Fortune 200 company and a leader in the world of data-driven decision-making.\\nAs a Data Scientist at Capital One, youll be part of a team thats leading the next wave of disruption at a whole new scale, using the latest in computing and machine learning technologies and operating across billions of customer records to unlock the big opportunities that help everyday people save money, time and agony in their financial lives.\\n\\nTeam Description:\\n\\nThe Card DS+ team is working continuously to innovate the way we handle data to transform every corner of our business through analytics, infrastructure, valuations, and strategy. We build human-centric experiences for moments that matter throughout the customer journey, and we do this by harnessing data, technology, and talent to propel our business forward. To accomplish this goal, we design, build, and maintain the appropriate solutions we use across Card to make smart decisions. We use the latest techniques in machine learning to build predictive models and utilize Big Data tools for cloud-based analysis. Through these techniques, we extract relevant insights from data to tackle a huge variety of business problems.\\n\\ncreditflow is a Python package which serves as the analytical foundation of the US credit card business. Users leverage our elegant python interface to score Capital Ones models which predict potential value. With this information, our users build the business strategy which determines which customers get approved and what card terms they receive.\\n\\nThe creditflow team is a fun, diverse, cross-functional team of engineers, data scientists, and product managers. Were passionate about creating a positive, collaborative environment where we each grow our respective skill sets. This role involves joining our developer team which follows agile, software engineering, and DevOps best practices. Our work is a good mix of smaller tasks and larger, more involved projects. Our tech stack includes Python, Conda, AWS, Jenkins, and our Python packages include numba, dask, pytest, flake, scikit-learn, xgboost, and gurobi. Youll work both independently and as a pair on a wide variety of technical tasks including:\\n\\n- Building new features, addressing bugs, and tackling tech debt\\n\\n- Wrangling data\\n\\n- Implementing machine learning models\\n\\n- Supporting users and our platform\\n\\n- Larger strategic initiatives like architecture changes\\n\\nThe Ideal Candidate is:\\n\\n- Experienced with Python and motivated to learn more\\n\\n- Familiar with machine learning, can intelligently build and discuss models with a passion for engineering too\\n\\n- A successful individual contributor, but also knows teamwork is the secret to modern software teams\\n\\n- Interested in growing into a tech lead role in the future or continuing to develop as an individual contributor - we welcome both\\n\\nBasic Qualifications:\\n\\n-Bachelors Degree plus 2 years of experience in data analytics, or Masters Degree plus 1 year of experience in data analytics, or PhD\\n\\n-At least 1 year of experience in open source programming languages for large scale data analysis\\n\\n-At least 1 year of experience with machine learning\\n\\n-At least 1 year of experience with relational databases\\n\\nPreferred Qualifications:\\n\\n-Masters Degree in STEM field (Science, Technology, Engineering, or Mathematics) plus 1 year of experience in data analytics, or PhD in STEM field (Science, Technology, Engineering, or Mathematics)\\n\\n-At least 1 year of experience working with AWS\\n\\n-At least 3 years experience in Python\\n\\n-At least 3 years experience with machine learning\\n\\n-At least 3 years experience with SQL\\n\\nCapital One will consider sponsoring a new qualified applicant for employment authorization for this position.',\n",
       "  'Blink Health is a well-funded healthcare technology company on a mission to make prescription drugs more accessible and affordable for everyone. We\\'re scaling up in a highly complex vertical to change the way Americans access the prescription drugs they need.\\n\\nOur proprietary platform and supply chain allows us to offer everyone whether they have insurance or not amazingly inexpensive prices on over 15,000 medications. With the addition of telemedicine and home delivery for prescriptions, Blink is providing a life-changing experience for people all over the country and fixing how opaque, unfair and overpriced healthcare has become. We are a highly collaborative team of builders and operators who invent new ways of working in an industry that historically has resisted innovation. Join us!\\n\\nSuccess:\\nLead our Data Science and Monetization Team. This is a small and growing team of versatile analysts positioned at the center of everything at Blink Health.\\nHire and develop a team of talented analysts.\\nProvide both strategic context and technical mentorship to their team.\\nAct as a player/coach, jumping in to support strategic projects requiring senior level involvement.\\nA strong foundation of technical and quantitative skills and be focused on how insights can impact business results.\\nHow to achieve success/acumen:\\n\\nAll Blinkers are expected to operate with our value of \"Good Giving\" in mind. Our culture is infused with the dedication and enthusiasm of employees who continuously strive to make a difference. Here\\'s how you will do that in this role.\\n\\nGood Execution - Do your best work\\nProvide technical leadership to the team. You do not need to be an expert in everything, but you do need to be someone the team can learn from.\\nJump in when needed to deliver insights to Blink\\'s Senior Leadership Team.\\nGood Owner - Be the CEO of your role\\nHire and develop a team of high performing Data Scientists and Product Analysts.\\nEfficiently manage the team\\'s resources to ensure it remains focused on the highest impact projects and its priorities are aligned with Product and Engineering.\\nGood Learning - Learn something new every day\\nWork cross-functionally to understand how teams can better utilize insights either from the Data Science team or by helping teams become more self-sufficient.\\nGood Feedback - Consider the perspective of others\\nListen actively and respond effectively through a variety of channels\\nGive and receive candid and constructive feedback\\nPromote trust and encourage teamwork to allow the data science team to do their best work\\nDesired experience:\\n4+ years\\' experience managing quantitative teams and 3+ years of prior experience working as a data scientist or analyst.\\n4+ years of experience directly managing high performing teams.\\nDeep quantitative skills and experience with applied analytics. Strong understanding of statistical analysis, including experimental design, and experience with packages, such as R, MATLAB, SPSS, SAS, Stata, etc.\\nStrong technical skills and experience working with large and complex data sets.\\nAbility to translate business needs to analytic requirements and take the results of complex analyses and communicate them to a broad audience.\\nOperate independently in a fast-paced environment\\nExperience in either the consumer internet space or healthcare industry.\\nBachelors Degree in Computer Science, Math, Physics, Engineering, or related quantitative field. Advanced degree preferred.\\nWhy Join Us:\\n\\n\\nAt Blink, we put humans first. We want everyone at Blink to be able to do the best work of their lives. We are a relentlessly learning, constantly curious and aggressively collaborative cross-functional team dedicated to inventing new ways to improve the lives of our customers.\\n\\nLearn more:\\nBlink Website\\nBlink Pharmacy App for Android\\nBlink Pharmacy App for iOS\\n\\nWe are an equal opportunity employer and value diversity of all kinds. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.',\n",
       "  'About Us\\n\\nJust as the field of Data Science is growing and ever-changing, so is our team: we’re on the hunt for talented instructors who are passionate about what they do and want to make a difference in the education of the world’s up-and-coming data scientists — our students.\\n\\nThe NYC Data Science Academy prides itself on housing the most comprehensive 12-week intensive boot camp in data science methodologies, providing theoretical, practical, and hands-on knowledge to our scholars. We nurture top talents in the industrial & academic world into industry-ready data scientists, by equipping them with knowledge, technical skills, and insights aiming at maximal impacts to the business world. We adapt faster than the quickest machine learning algorithms out there — with content that reflects research and application in the growing industry and teaching expertise which goes beyond extraordinary.\\n\\nThat’s where you come in.\\n\\nFrom our part-time weekend/evening classes to our part-time online and full-time in-person boot camps, our courses are both designed in-house and taught by our robust team of data scientists and engineers. Instructors have the opportunity of taking part in corporate training and consulting client projects, building both data science and big data solutions. We encourage collaboration and positive change in not only our students and clients but also in our team. Nerding out is also highly encouraged.\\n\\nResponsibilities\\nTrain in-person or online boot camp students, the next generation of data scientists, on a wide range of topics ranging from statistics, probability, SQL, R/python coding, data analysis, machine learning, deep learning, and big data, etc. Topics in Big Data to include:\\nHadoop and MapReduce, HDFS, Hive\\nApache Hive\\nSpark\\nSpark SQL\\nSpark Machine Learning\\nDocker\\nDevelop, evaluate, and maintain cutting-edge technical content for training experiences relevant to audiences with a diverse set of educational backgrounds and industry experience.\\nTailoring teaching content for both corporate and client training.\\nGuide and mentor our students inside and outside of the classroom for their industry-ready projects. Help to orient our students’ learning toward the target industries.\\nLead by example as a data scientist, to be an excellent role model for our industry-ready students.\\nCooperate with marketing/admission teams by participating in the student enrollment/admission procedure and by developing relevant content for them.\\nParticipate in periodic training workshops to sharpen the relevant skill-sets.\\nAbout You\\nMinimum of a Master’s Degree in a Quantitative Science, Technology, Engineering, or Mathematics-related field; Computer Science and Statistics preferred.\\nProficiency in statistical computing/machine learning and/or programming in R and/or Python.\\nExperience/extensive knowledge in R or Python data analysis.\\nTeaching experience in undergraduate or graduate level coursework in STEM required.\\nDemonstrated experience/knowledge of Statistics and Machine Learning from both theoretical and applied perspectives.\\nExperience in developing a curriculum or providing training in a client-facing endeavor.\\nA good sense of the relevance of academic data science to industrial applications.\\nForward thinking with a strong growth mentality, i.e. constantly strengthening yourself on all fronts relevant to the growth of our school.\\nPassionate about teaching and helping your students succeed in their careers.\\nA team player who can multitask if necessary and can step up to new challenges in a growing company.\\nPerks\\nCompetitive salary, adjustable hours, and flexible vacation policy.\\nBenefits include 401k retirement plan and medical, dental, vision insurances.\\nOpportunity to train, research, and learn in the field of data science on-the-job; a chance to interact with active data science communities through conferences, Meetup events, etc.\\nCompletely stocked snack pantry.\\nHigh-quality computational equipment.',\n",
       "  'Lead Data Scientist – Two Sigma Private Investments\\n\\nSightway Capital is a Two Sigma company focused on private equity investments. We employ a principal mindset and flexible capital approach to building successful business platforms with experienced operators and strategic partners. The team at Sightway Capital thinks long-term, targeting business opportunities that we believe afford both asymmetric risk rewards and enterprise value creation over time. Sightway’s unique platform building approach affords team members the opportunity to participate in each investment’s growth and success from an early stage. Data science is key to Sightway’s strategy, and we aim to leverage Two Sigma’s expertise, data sources and analytical models in order to make informed investment decisions, and create enterprise value and positive outcomes for our portfolio companies.\\n\\nSightway Capital is building a world class data science team in order to execute on this vision. The team has a multifaceted data driven private investment strategy that encompasses private equity deal sourcing, due diligence, and portfolio company value creation. This role includes working with business analysts to gather business needs and requirements from our portfolio companies; forming creative data driven hypotheses to solve these needs; using a range of statistical and machine learning methods using a range of different data sources in order to build models that prove or disprove these hypotheses; communicate findings to management and work with software engineers to craft solutions based on the models you build.\\n\\nYou will take on the following responsibilities:\\n\\nDesign and implement models that explore, predict, and optimize a range of key business drivers such as lead generation, customer risk, revenue, pricing\\nLead a team of data scientists - mentor team members, set technical standards and processes, review work\\nBuild productive working relationships with other senior data scientists within the Two Sigma organization\\nWork with business analysts to extract business needs from portfolio company management teams\\nPerform data exploration and visualization in order to uncover insights from data\\nBrainstorm hypotheses to solve business needs\\nWork with Data Strategy team to source and on-board new data sets and portfolio company technical teams to on-board company data\\nBuild models using a range of analytical techniques in order to prove or disprove hypotheses\\nPrepare and present findings to TSPI management, portfolio executive team\\nWork with software engineers to engineer tools that use production models to deliver impact for the portfolio company\\nYou should possess the following qualifications:\\n8+ years of experience in applied data analysis & prediction, preferably in an industry setting\\nDemonstrated experience leading technical teams\\nExperience solving business problems using data science by directly interfacing with a client’s management team would be ideal\\nDegree in a technical or quantitative disciplines, like statistics, mathematics, physics, electrical engineering, or computer science\\nDemonstrably strong data science modeling intuition and feature engineering creativity\\nIntimate familiarity with the potential flaws & fallacies in the applications of specific statistical methods\\nExperience specifying & managing requirements for datasets leveraged in your analyses\\nWorking knowledge of SQL and common data science toolkits : Python, R, Spark, Matlab\\nStrong written & verbal communication and presentation skills, with experience crafting a compelling narrative supported by data\\nA portfolio of open-data analyses or data-driven research publications would be ideal\\nYou will enjoy the following benefits:\\nCore Benefits: Fully paid medical and dental insurance premiums for employees and dependents, competitive 401k match, employer-paid life & disability insurance\\nPerks: Onsite gyms with laundry service, wellness activities, casual dress, snacks, game rooms\\nLearning: Tuition reimbursement, conference and training sponsorship\\nTime Off: Generous vacation and unlimited sick days, competitive paid caregiver leaves\\nWe are proud to be an equal opportunity workplace. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity/expression, age, status as a protected veteran, status as an individual with a disability, or any other applicable legally protected characteristics.',\n",
       "  \"One of the core functions of the Bloomberg software product is the manufacturing of high quality, structured data for financial professionals from unstructured, noisy sources with the aid of deep domain specialists. Since beginning thirty years ago, Bloomberg's Global Data Department has built one of the largest and most sophisticated systems to provide this crucial data. We're looking for an experienced researcher to join the CTO office and advise on the technical strategy and roadmap for the growing machine learning and data science efforts. You will join a talented group of engineers and data scientists to make deep changes to one of the largest data manufacturing initiatives in the world.\\n\\nWith your help we'll:\\nProvide support and leadership to the data analysts and scientists working in the Global Data Department and influence best practices across a large global team\\nGuide and advise the team on statistical methods and create new tools to help them work with domain specialists effectively\\nCarry out research to explore new innovative ways to improve the quality, efficiency, and coverage of our data ingestion and extraction operations\\nLook at new areas where machine learning and natural language processing can be used to streamline and extend our processes to identify the most important, relevant data and to derive new premium content\\nWe'll trust you to:\\nBe the technical architect and point person for the next-generation data manufacturing process\\nCollaborate with application engineering teams and providing support for domain specialists applications in a very agile environment\\nWork with others in the CTO office, Engineering, and the Global Data Department on a suite of company-wide initiatives with visibility up to Bloomberg's top executives\\nYou'll need to have:\\nPh.D. or equivalent experience in machine learning, statistics or a related field\\n5+ years of experience implementing machine learning in a business context\\nProduct management and proven ability to work across organizational boundaries in a multi-stakeholder context\\nExperience performing high quality research and applying statistical approaches in novel contexts\\nIf this sounds like you, apply! In addition, do check out our blog, TechAtBloomberg.com, to learn more about our publications and projects in data science.\",\n",
       "  'Job Title: Data Modeler/Analyst\\n\\nDepartment: Data\\n\\nJob Type: Contract\\n\\nLocation: Jersey City , NJ\\n\\nJob Overview:\\n\\nFidelity Brokerage Technology has an opportunity for a Senior Data Analyst/Modeler. The Senior Data Analyst/Modeler will provide data analysis leadership on complex data analysis projects, often across systems and companies. They will be responsible for gathering data requirements and translating those requirements into business solutions which ensure integrated, accurate data within the organization.\\n\\nResponsibilities:\\nGather requirements from business and technical staff to analyze data requirements and recommend appropriate solutions.\\nDocument database solutions and present solutions to project teams, Data Engineering team and architecture review boards.\\nIntegrate data into existing enterprise logical model and physical data stores to avoid data redundancy.\\nEnsure all technical database decisions result in coherent system designs which use the most effective methods and tools.\\nUse modeling tool to capture clear definitions of data elements and produce code to effect database changes.\\nImplement data solutions to satisfy business requirements.\\nPerform research into emerging technologies, define data solution standards and guidelines, and drive technical approaches at a tactical level, serving as the data engineering member on FI and cross-Fidelity systems projects.\\nAct as mentor and provide guidance and leadership to junior members of team.\\nProvide data warehousing / dimensional modeling and OLTP modeling expertise.\\nWrite and maintain the business rules, logic and SQL queries\\nSKILLS AND EXPERICENCE REQUIRED:\\n10 years experience in a technical field (analysis, development, database administration, report development).\\n6 years experience in relational data modeling preferred & Data Analysis.\\nKnowledge of financial services business in depth knowledge of institutional brokerage business preferred.\\nExcellent requirements gathering and written and verbal communication skills.\\nAbility to manage workload across multiple projects and balance deadlines and deliverables.\\nExcellent presentation and negotiation skills.\\nExpert knowledge of systems development methodologies, specifically Agile.\\nDexterity in using data modeling tools such as Oracle Designer, PowerDesigner, Erwin. Prefer experience with PowerDesigner.\\nKnowledge understanding of different database platforms, such as, Oracle, Postgres, Hadoop, Snowflake, Netezza, DB2.\\nAdvanced ability to use SQL to query data on any of the above DBMS s.\\nKnowledge and in-depth exposure to modeling strategies, such as relational (logical, physical), star, snowflake, unstructured, to support OLTP, Warehousing and Data Marts, Data Lakes, MDM).\\nEducational Qualifications:\\nRequired - Bachelor s degree in Computer Science, Information Technology, Computer Engineering or closely related or equivalent\\nPreferred - Master s degree in Management Information Systems (MIS), Computer Science, Big Data or Analytics or equivalent\\nTravel:\\nOpen to travel based up on the nature of the engagement\\nEqual Employment Opportunity\\n\\nReliable Software employment does not discriminate on the basis of race, religion, gender, sexual orientation, age or any other basis as covered by federal, state, or local law.\\n\\nEmployment decisions are based solely on qualifications, merit and business needs.\\n\\nShameer K\\n\\nIT Recruiter\\n\\n(O) 248-814-2363\\n\\n2260 Haggerty Road, Suite#285, Northville, MI 48167.\\n\\n- provided by Dice',\n",
       "  'Komodo Health is addressing the global burden of disease through the development of the world’s most actionable map of healthcare data. Our solutions drive a more transparent, efficient and productive healthcare ecosystem through the creation of quantitative solutions to qualitative problems.\\n\\nAs a fast growing startup that has already partnered with multiple Fortune 500 companies, we have very ambitious goals that have been designed with career development in mind. As a company, we value our culture of encouraging growth, collaboration, and constructive debate as well as delivering innovative solutions that “wow” our customers.\\n\\nWe are looking for a Senior Data Scientist to play a key role in the success of our Data Science team. This is a role for someone who wants to use one of the biggest, most complete healthcare datasets to solve real world problems that deliver value to our clients while being part of the collaborative, fun, nurturing Komodo community!y!\\n\\nRESPONSIBILITIES\\nAnalyze and understand the disparate sources of industry data\\nApply data-mining and machine learning techniques to large structured and unstructured datasets\\nIterate to push the boundaries on what problems can be solved through data analytics\\nScaling from ad-hoc advanced analytics prototypes to large-scale learning solutions in live systems\\nClearly communicate analysis methods and results to Komodo and client team\\nCollaborate closely with the product team and other engineers to translate business objectives into new features and products\\nREQUIREMENTS\\nBS, MS, or PhD in Computer Science, Engineering or related field\\n4+ years experience with Python, Scala, or Java and distributed frameworks (Hadoop, Spark, etc.)\\n4+ years of experience with…\\nRapid prototyping and advanced analytics (e.g. R, MatLab, Pandas, SciPy)\\nSQL and Relational Databases\\nExperience with applied machine learning or algorithm development\\nDemonstrable successes in delivering analytical projects, including structuring and conducting analyses to generate business insights and recommendations\\nSolution/Delivery-oriented team player willing to go above and beyond\\nAbility to work beyond familiar algorithms - to get out of one’s comfort zone and earnestly seek to integrate the best solutions to business challenges\\nHigh motivation, work ethic, and self-discipline to organize and complete tasks\\nSincere interest in working at a startup and scaling with the company as we grow\\nNICE TO HAVE\\nExperience or interest in the life sciences industry\\nTrack record of delivering analytical solutions to poorly scoped problems\\nPrior client-side analytical experience serving Marketing, Sales, Medical Affairs, or Clinical Development, or, vendor-side analytical experience working with Life Sciences clients\\nExperience working with proprietary secondary data including medical / pharmacy claims, prescription data, sales data, and longitudinal patient-level data\\nBENEFITS\\nCompetitive salary and equity compensation\\nFull Medical, Dental, and Vision benefits\\n401k plan with employer match\\nFlexible hours and a “use as you need” vacation policy\\nOpportunities to attend industry conferences and events\\nGreat office location(s) in SF/SOMA and NY/FLATIRON',\n",
       "  \"Sr. Product Manager, Product Data and Analytics\\n\\n\\nApply\\n\\nRef#: 34677\\n\\nCBS Business Unit: Showtime\\n\\nJob Type: Full-Time Staff\\n\\nJob Schedule: Full-Time\\n\\nJob Location: New York, NY, US\\n\\nDescription:\\nWe're Showtime (part of the CBS family) and have some of the most groundbreaking original programming in the industry, including Emmy and Golden Globe winners like Homeland, Ray Donovan, Billions, Shameless and Dexter. This position is within the Product Analytics team, which is embedded within the Product team that manages SHOWTIME, our stand-alone OTT streaming service, and Showtime Anytime, our TV Everywhere offering.\\n\\nReporting to the head of the Product Analytics team, this role will be a focal point for features, information and ideas involving our various data products and pipelines, will collaborate with a team of data engineers, data scientists and analysts, and will work closely with customers of our data products from across the Showtime organization.\\n\\nIdeal candidates will be technically fluent, , naturally curious and adept at solving challenges, enjoy team camaraderie and possess a ready sense of humor.\\n\\nResponsibilities:\\nManage new product and feature development: Define objectives, business cases, requirements and success metrics\\nGroom a product roadmap for our data products via collaboration with product team members, management and cross-departmentally across the Showtime organization.\\nWork closely with users of our data products to understand pain points and use cases\\nManage trade-offs and guide decisions about product priorities given business goals and resource limitations\\nEngage directly with data engineering via an agile development process and build scalable, products that can be adjusted quickly based on emerging opportunities or business needs.\\nResearch market trends and innovations in data technologies, as well as measurement and monitoring tools.\\nDevelop internal documentation, support material, and other collateral to help customers of our data products effectively use them\\nProvide product education to all users\\n\\nQualifications:\\nYou have 3+ years of experience in product management\\nYou’ve got keen analytical skills and experience with data-driven product design and decision-making\\nYou must enjoy working with data and exhibit proficiency with SQL\\nYou should have a solid understanding of big data storage within cloud-based environments\\nYou have a deep understanding of details, but with the ability to zoom out and see bigger picture.\\nYou’re scrappy. You’ve got no fear of getting your hands dirty, and you are often the first to raise your hand to tackle the latest challenge.\\nYou have exceptional written and oral communication skills\\nFamiliarity with stream-processing services such as Kafka, Spark Streaming, etc is a plus\\nCS / engineering degree, or other relevant technical training is a plus\\nExperience with data visualization (i.e. Tableau, Looker) is a plus\\n\\nAbout Us:\\nSHOWTIME and its critically-acclaimed, award-winning original series continue to make their mark on the cultural landscape, with one of the most successful programming slates in all of television. With an impressive line-up of new and returning original series, the SHOWTIME hit dramas and comedies include HOMELAND, SHAMELESS, BILLIONS, RAY DONOVAN, THE AFFAIR, SMILF, THE CHI, KIDDING, ESCAPE AT DANNEMORA and BLACK MONDAY. Original series play a key part in the SHOWTIME programming mix, along with box office hits, comedy and music specials, provocative documentaries, and hard-hitting sports programming, including the flagship franchise SHOWTIME CHAMPIONSHIP BOXING® and the Emmy Award-winning veteran series INSIDE THE NFL. SHOWTIME is currently available to subscribers via cable, DBS and telco providers, and as a stand-alone streaming service through Amazon, Apple®, Google, LG Smart TVs, Oculus Go, Roku®, Samsung and Xbox One. Consumers can also subscribe to SHOWTIME via Amazon’s Prime Video Channels, DirecTV Now, FuboTV, Hulu, Sling TV, Sony PlayStation™ Vue, and YouTube TV. The network’s authentication service, SHOWTIME ANYTIME, is available at no additional cost to SHOWTIME customers who subscribe to the network through participating providers. Subscribers can also watch on their computers at www.showtime.com and www.showtimeanytime.com.\\n\\nEEO Statement:\\nEqual Opportunity Employer Minorities/Women/Veterans/Disabled\\n\\nApply\",\n",
       "  'Description\\nData analyst responsibilities include conducting full lifecycle analysis to include requirements, activities and design. Data analysts will develop and process techniques to extract/manipulate data from multiple data sources including ERP, PLM, homegrown MS Access databases etc. They will also monitor performance and quality control plans to identify improvements.\\nThe successful candidate will be a team player and is expected to assume a role within a collaborative design team including, but not limited to, a technical specialist/layout drafter, detail drafter, packaging engineer, core technology specialist, and service center/factory assembly team, as required.\\nThis position offers the applicant the ability to interact with the global *** organization including Engineering, Operations and Marketing and Sales. There is potential to expand this role.\\nLimited domestic/international travel is required (0-10%).\\nResponsibilities\\n\" Work safely following all corporate / local procedures and timely completion of all required safety training\\n\" Maintain accurate and organized records\\n\" Follow all processes and obtain necessary approvals where required\\n\" Researches project and part history in support of standard business.\\n\" Interpreting data, analyzing results using statistical techniques\\n\" Developing and implementing data analyses, data collection systems and other strategies that optimize statistical efficiency and quality\\n\" Acquiring data from primary or secondary data sources and maintaining databases\\n\" Identify, analyze, and interpret trends or patterns in complex data sets\\n\" Filter and clean data by reviewing computer reports, printouts, and performance indicators to locate and correct code problems\\n\" Work with management to prioritize business and information needs\\n\" Locate and define new process improvement opportunities\\n\" Writing reports to give to management describing the business.\\n\" Other miscellaneous duties to support the product lines success\\n\\nRequired Knowledge/Skills, Education, and Experience:\\n\" Bachelor s Degree in Mathematics, Economics, Computer Science, Information Management or Statistics or equivalent.\\n\" Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.\\n\" High attention to detail.\\n\" Ability to work collaboratively and individually with the same enthusiasm and intensity.\\n\" Excellent communication, interpersonal, and writing skills\\n\" Technical expertise regarding data models, database design development, data mining and segmentation techniques\\n\" Strong knowledge of and experience with reporting packages\\n\" Strong knowledge of and experience with Oracle\\n\" Adept at queries, report writing and presenting findings\\n\" Possess and demonstrate a high degree of tact and professionalism; and be capable of interacting with Management, Peers, Shop Personnel, Inside Sales and SCM.\\n\" Good organizational and time management skills\\n\" Willing and able to travel domestically and internationally with short notice (estimated 0-5% travel)\\n\\nPreferred:\\n\" 2 4 years Oracle experience\\n\" 2 4 years SAP (Atlas) experience\\n\" Knowledge and understanding of *** / Dresser-Rand s products and services\\n\" Experience with Alteryx data manipulation software and Tableau data visualization software.',\n",
       "  'Job Description\\nA New York City prop shop is on a lookout for a Senior Data Scientist to join their fixed income desk. This prop shop specializes in distressed residential mortgages.\\n\\nAs a senior data scientist you will be working along the software engineer team and the quant trading desk to find new opportunities in the market and improve the alpha of the company. Your goal will be to improve database structures, performance measures, and predictive analytics using your machine learning skillset.\\n\\nRequirement:\\nMachine Learning and Statistical Analysis expertise\\nExtensive knowledge of Python/Java/R\\nAdvanced training in Mathematics, Statistics, Physics, Computer Science or another highly quantitative field ( Master’s, PhD degree)\\nAt least 3 years of experience in fixed income and mortgages.\\nCompensation:\\n\\nBase salary: $200,000 (+ competitive bonuses)\\n\\nIf you would be interested in hearing more about the Senior Data Scientist position then please do feel free to get in touch and I can give further details.\\n\\nThe Head of the Data scientist is starting interviews on April 4th.',\n",
       "  \"Introduction\\nAs an IBM consultant, you directly help clients transform their business and solve complex problems. You will analyze, define, configure, implement, test and support projects that deliver customized solutions using your knowledge of Microsoft. You are a valued team member, serving as a liaison between IBM and our clients to deliver outstanding results.\\n\\nYour Role and Responsibilities\\n\\nIBM GBS Enterprise Applications is seeking keen-minded Microsoft Analytics Consulting Specialists. In this job role, you'll use your expertise to provide best-in-class analytics based business solutions to help clients achieve their business objectives. Roles and responsibilities include participating in all aspects of the implementation. This role will have emphasis on client facing skills including defining and analyzing business requirements and working with customers to educate on analytics in business. Successful candidates will have experience with data mining, data modeling, process modeling, and a keen insight for translating requirements into a robust design enabling the configuring and customizing of Microsoft based Analytics and Cognitive solutions.\\n\\nSuccessful candidates will demonstrate the following capabilities:\\n\\n•Lead and deliver complex high quality solutions to clients in response to varying business requirements\\n•Lead and educate a project team on an Analytics Software Implementation/Development Lifecycle\\n•Evangelize PowerBI, Cortana Intelligence Suite, Analytics Platform System (APS)\\n•Familiarity with moving clients to Office 365 and Azure Active Directory Integration\\n•Educate clients on SQL Server 2014+ Analytics features and capabilities\\n•Be a trusted advisor to client personnel and organizations, including executive management\\n•Show excellent diplomacy skills with executive management\\n•Capability and responsibility to define and select tools, processes, priorities and resources necessary to manage, analyze, design and implement Microsoft practice directives\\n•Utilize high level analytical ability and creativity to analyze complex/new situations, anticipate potential problems and future trends, assess opportunities, impacts and risks while developing and implementing plans and producing solutions to client problems\\n•Deliver and develop PowerBI(Cloud and Desktop), SQL Server Analytics Platform Solutions and other Microsoft BI products (SSIS and SSRS)\\n•Previous depth with implementing custom applications in .NET / SQL Server environment pot .NET 3.5\\n•Utilize and lead workshops to assist clients in implementing Analytics solutions\\n•Lead teams using an agile methodology and deliver Analytics Implementations\\n\\nRWMS\\n\\nRequired Technical and Professional Expertise\\n\\n• At least 5 years experience in configuration and development of Sql Server Analysis Services, SSRS, SSI\\n\\n• At least 5 years experience in SQL Server infrastructure optimization\\n\\n• At least 3 years experience in PowerBI or Analytics Platform System (APS)\\n\\n• At least 5 years experience in extending Analytics solutions with other Microsoft technologies and tools such as custom development SharePoint solutions, ASP.net, VB, .Net, C#, javascript\\n\\n• At least 4 years experience in application development of .NET / SQL Server business solutions\\n\\n• At least 3 years experience in Azure applications\\n\\n• At least 2 years experience in Azure Active Directory integration\\n\\n• At least 3 years experience in Office 365 integration\\n\\n• Any Microsoft SQL Server Certification\\n\\nPreferred Technical and Professional Expertise\\n\\n• At least 8 years experience in configuration and development of PowerBI or Analytics Platform System (APS)\\n\\n• At least 8 years experience in SQL Server Integration Services (SSIS) or SQL Server Reporting Services (SSRS)\\n\\n• At least 8 years experience in extending Analytics solutions with other Microsoft technologies\\n\\n• At least 1 year experience in PowerBI, APS, SSIS or SSRS\\n\\n• At least 1 year experience in Azure Machine learning\\n\\n• At least 1 year experience with regression analysis and statistical modelling in Microsoft\\n\\n• At least 1 year experience with R based programming in any Microsoft supported language\\n\\n• Experience with migrating from Tableau, QlikView, IBM SPSS, Cognos to other Analytics/BI Platform\\n\\n• Masters in statistics heavy degree plan\\n\\n• Microsoft Certifications\\n\\nAbout Business Unit\\nIBM Services is a team of business, strategy and technology consultants that design, build, and run foundational systems and services that is the backbone of the world's economy. IBM Services partners with the world's leading companies in over 170 countries to build smarter business by reimagining and reinventing through technology, with its outcome-focused methodologies, industry-leading portfolio and world class research and operations expertise leading to results-driven innovation and enduring excellence.\\n\\nYour Life @ IBM\\nWhat matters to you when youre looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nBENEFITS\\nHealth Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.\\n\\n•http://www-01.ibm.com/employment/us/benefits/\\n•https://www-03.ibm.com/press/us/en/pressrelease/50744.wss\\n\\nCAREER GROWTH\\nOur goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal talent strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:\\n•Skill development: helping our employees grow their foundational skills\\n•Finding the dream job at IBM: navigating our company with the potential for many careers by channeling an employees strengths and career aspirations\\n•Diversity of people: Diversity of thought driving collective innovation\\n\\nCORPORATE CITIZENSHIP\\nWith an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!\\n\\n•http://www.ibm.com/ibm/responsibility/initiatives.html\\n•http://www.ibm.com/ibm/responsibility/corporateservicecorps\\n\\nLocation Statement\\nFor additional information about location requirements, please discuss with the recruiter following submission of your application.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.\",\n",
       "  'Data Scientists at Octane deliver data driven insights that support our Credit and Risk functions. As such, you will be responsible for optimizing product performance through experimentation, design, business analytics and predictive analytics.\\n\\nSpecifically, you will:\\nIdentify business opportunities, measure KPIs to craft compelling stories, make data-driven recommendations, and drive informed actions\\nLead and manage the development of machine learning powered predictive models for Octane Lending’s real-time credit and fraud detection systems\\nMine portfolio performance to drive insights into what’s working, what isn’t and make data-driven suggestions/recommendations that will positively impact underwriting performance and portfolio quality\\nDesign, analyze, and interpret the results of experiments that could optimize product performance\\nWork cross-functionally with our engineering and product teams to ensure data is being captured and reports are accurate\\nRequirements\\n\\nSuccess in this role hinges on your technical aptitude, quantitative abilities, and business acumen: you know how to plow through data with SQL/Python/R/Tableau, surface insights using math/statistics/ML techniques, and measure the business impact using efficiency/conversion/profit metrics.\\n\\nSpecifically, your background includes:\\nAn advanced degree in Math, Statistics, Physics, Engineering or another quantitative field, followed by 5+ years of experience inclusive of business analytics, performance analysis, mathematical modeling, and/or data science\\nSolid proficiency with SQL, Python, R, Tableau\\nStrong analytical, problem solving, critical thinking, and stakeholder management skills\\nPassion for data and its ability to drive serious business impact\\nBenefits\\nA working environment filled with passionate, happy, smart people\\nHealth Care Plan (Medical, Dental & Vision)\\nRetirement Plan (401k)\\nLife Insurance (Basic, Voluntary & AD&D)\\nPre-Tax Commuter Benefits\\nPaid Time Off (Vacation, Sick & Public Holidays)\\nShort Term & Long Term Disability\\nOffice snacks, full fridge, and drinks\\nMonthly team happy hours and lunches',\n",
       "  \"Company Description\\n\\nDailymotion is the leading video discovery destination & technology that learns about your tastes over time, constantly surfacing the best, most relevant content on the web. Our mission is to provide the best video user experience for consumers on the market, connecting publishers and advertisers to engaged viewers who turn to Dailymotion for their daily fix of the most compelling music, entertainment, news and sports content around.\\n\\nThrough partnerships with the world's leading publishers and content creators, including CBS, CNN, Fox Sports, GQ, Mashable, Universal Music Group, VICE and more, Dailymotion commands 4 billion monthly pageviews across its mobile app, desktop and connected TV experiences. Dailymotion is owned by Vivendi, one of the largest mass-media corporations in the world.\\n\\nAs part of our growing activities, we have built our own ad stack (SSP) to enhance our programmatic advertising capabilities, deliver new monetization solutions for our ecosystem of online, mobile and TV, and provide innovative marketing solutions for advertisers.\\n\\nJob Description\\n\\nAre you someone with a passion for digging into large distributed pools of data, unearthing insights, and transforming them into a visually compelling form that appeals both to technical and non-technical audiences? Then this is the job for you!\\n\\nOn a daily basis, you will be responsible for:\\nPerforming data analysis on products managed by Dailymotion’s AdTech tribe, including our SSP that serves billions of requests daily, and extract actionable insights that will be used to drive decisions across the business\\nStatistically validating and A/B testing hypotheses generated by Business, Product and Data Science teams.\\nDriving the collection of new data that would help build the next generation of algorithms (E.g. audience segmentation, contextual targeting, bidder behavior monitoring)\\nBuilding easy to use dashboards (E.g. Tableau) to help visualize insights and KPIs interactively\\nAutomating analysis into monitoring and alerting tools to continuously drive value for business owners\\nProactively owning the end-to-end delivery of projects, from design, to development, testing, and operations.\\nQualifications\\n\\n· Bachelor’s degree, or higher, in Business, Finance, Statistics, or related field.\\n\\n· >5 years of business / data analysis experience\\n\\n· Previous ad-tech experience (required)\\n\\n· Extensive experience solving analytical problems using quantitative approaches\\n\\n· Comfortable with manipulation and analysis of complex, high volume, high dimensional data from varying sources (E.g. SQL, AWS/GCP, Excel)\\n\\n· Solid Experience of Data Visualization Tools (E.g. Tableau suite)\\n\\n· Strong intellectual curiosity and ability to structure and solve difficult problems with minimal supervision\\n\\n· Ability to communicate complex quantitative analysis in a clear, precise and actionable manner in English. French is a plus.\\n\\n· A background in Machine Learning and Statistics is a plus\\n\\n· Programming in Python is a plus\\n\\nAdditional Information\\n\\nLocation: New York\\nType of contract: Full-time\\nStart Date: ASAP\\n\\n• Flexible time off, vacation, holidays, sick-leave so you can take time off when you need to\\n• Fitness club membership to NY Health & Racquet Club\\n• 100% healthcare coverage starting on day 1\\n• Commuter benefits\\n• 401k Contribution\\n• Paid parental leave\\n• Fully stocked kitchens with free snacks and drinks\\n\\nIf you want to explore Dailymotion culture a little further please check out:\\n\\n1./ Our BuiltIn page: https://www.builtinnyc.com/company/dailymotion\\n\\n2./ Our Recent Global Hackathon in November 2018. https://www.dailymotion.com/video/x70val9\\n\\n3./ Welcome to the Jungle page: https://www.welcometothejungle.co/companies/dailymotion/team\\n\\nAll your information will be kept confidential according to EEO guidelines.\",\n",
       "  'Job Description\\nSenior Data Scientist\\nNew York, New York\\n$140,000-160,000 base salary + bonus + benefits\\n\\nOne of my newest clients in New York City is a fast-growing established startup within the pharmaceutical space. They have seen great success with their niche focus of applying data science methodologies to speed up the backend of clinical trials. They are partnered with some of the largest players in the pharma space.\\n\\nThey need an experienced Data Scientist to come on and be a key contributor for their expanding team.\\n\\nTHE ROLE\\nYou will analyze medical records with advanced statistical techniques to develop better tools for understanding of the oncology and health space.\\nYou will report directly into the VP of Analytics and work closely on technical direction.\\nYou will implement and design code and build out to production using various advanced statistical techniques with access to various datasets.\\nWork closely with the Product team building models from concept to production.\\nYOUR SKILLS AND EXPERIENCE\\n\\nThe successful Senior Data Scientist will likely have the following skills and experience:\\nHeavy experience using Python, R, PySpark, Hadoop and AWS for predictive modeling and data manipulation\\n3+ years experience building statistical models in similar setting\\nPrior experience as a leading projects a plus\\nWorking knowledge of EMR/EHR data or experience within pharmaceuticals preferred\\nBayesian statistics experience required\\nExperience with longitudinal or causal analysis for healthcare claims analysis\\nMS/PhD in Statistics, Applied Mathematics or STEM field preferred\\nMachine Learning, Deep Learning or Natural Language Processing experience a plus\\nTHE BENEFITS\\n\\nA competitive base salary of $140,000-160,000 + bonus + benefits\\n\\nHOW TO APPLY\\n\\nPlease register your interest by sending your résumé to Tim Jonas via the Apply link on this page.\\n\\nKEYWORDS\\n\\nData Science | Python | SQL | AWS | Statistics | Analytics | Healthcare | Startup | PySpark | Pharmaceuticals\\nCompany Description\\nData and Analytics recruitment is our core business and we’re proud to say, our customers believe we’re good at it. In our most recent customer satisfaction survey, 95% of respondents said that they would recommend Harnham.\\n\\nHarnham has actively chosen to focus on Data and Analytics, we’ve immersed ourselves in this market and are now an integral part of this business community.\\n\\nOur capability has grown to provide recruitment services and advice across the Marketing Analytics, Credit Risk, Data Science, Data and Technology and Digital sectors.',\n",
       "  \"Prudential Financial, Inc. is a multinational financial services leader\\nwith operations in the United States, Asia, Europe, and Latin America.\\nLeveraging its heritage of life insurance and asset management expertise,\\nPrudential is focused on helping individual and institutional customers grow\\nand protect their wealth through a variety of products and services, including\\nlife insurance, annuities, retirement-related services, mutual funds, asset\\nmanagement, and real estate services. The company’s well-known Rock® symbol is an icon of\\nstrength, stability, expertise and innovation that has stood the test of time. ISG\\nTechnology is an integral part of Prudential’s Individual Solutions Group\\n(ISG) and is driven by data. Capitalizing on data aggregation, analytics, and\\ninsights is paramount to driving growth and business success.\\n\\nAbout the Role:\\n\\nAs we create a diverse, growing, and dynamic workforce within a technology\\ncommunity that's respected for delivering high-tech solutions, data-driven\\nsolutions are at the core of it all. To this end, ISG Technology is seeking a\\nqualified Senior Data Scientist with experience in designing solutions to\\nbusiness problems using machine learning, big data techniques and technologies.\\nThe ideal candidate is a data-driven problem-solver with an ability to translate\\nbusiness requirements into mathematical models. The candidate will also have an\\nability to develop solutions to complex mathematical and data problems,\\ndiscover insights and identify opportunities through the use of statistical,\\nalgorithmic, data-mining and visualization techniques. Partnering with a team\\nof technology and business data specialists the candidate will use their\\nanalytic and engineering skills to integrate and prepare large, varied\\ndatasets, and architecting specialized solutions, in cloud environments, employing\\na range of machine learning and data science techniques, while effectively\\ncommunicating results to various interested parties. This is a dynamic\\nenvironment where the right candidate will be comfortable and capable of\\noperating in “test and learn” mode, and work independently.\\n\\nKey Responsibilities\\nUtilize\\nemerging data technologies and techniques to solve data driven problems\\nLead, influence and participate in the end-to-end data\\nlifecycle activities\\nIdentify business problems, engage with business teams & build\\nhypotheses\\nDefine and build compute environments and automated data\\npipelines\\nDrive data engineering activities: discovery, sourcing,\\nwrangling, cleansing and modeling\\nApply Machine Learning techniques and various algorithms to\\nprovide problem solving insights\\nCreate meaningful insights through data\\nvisualization & simple, effective results communication\\nWork as a liaison between business partners\\nand junior data scientists to refine data science activity\\nParticipate\\nin management meetings to help establish data roadmap and strategies\\nExperience with algorithms and programming to efficiently process\\nlarge datasets and apply treatments, filters and conditions to your data\\nExperience with big data technologies such as Hadoop 2.0,\\nHBase, Mongo, Informatica, Cloudera, Vertica, Green Plum, Netezza\\nProficient with practical experience with data engineering;\\ndata discovery, cleansing, wrangling, and modeling:\\nThe ability to organize and “wrangle”\\nlarge datasets so that you can get actionable insights from them. This\\nmay include finding innovative ways to combine fields of data that don’t\\nnaturally mesh together.\\nExperience defining and creating automated data pipelines,\\nand performing advanced data manipulation\\nInterest\\nin understanding the business problems and how the work impacts the\\nbusiness\\nEnthusiastic\\nyet humble – you are excited about the work you do, but you are also\\nhumble enough to embrace feedback – you don’t need to be the smartest\\nperson in the room.\\nExperience with machine learning methods,\\ntechniques and algorithms; classification and regression, clustering,\\ndecision trees, neural networks, time series analysis, etc.\\nFirm understanding of statistics: statistical tests, distributions, maximum likelihood estimators,\\netc.\\nExperience with visualization and reporting technologies such\\nas Tableau, Power BI, and/or programmatic visualizations in Plotly, SAS,\\nPython, etc.\\nThe ability to create meaningful data\\nvisualizations that communicate your findings and demonstrate how your\\ninsights create business impact.\\nExcellent written and verbal communication skills\\nAn entrepreneurial mindset with the ability to adopt novel\\nsolutions in a fast-paced environment, to deliver results quickly, help\\njunior team members along the way.\\nAble to work collaboratively as a senior\\nmember of the team, but with creative and individual thinking\\nPrevious\\nexperience in working in high growth, data driven, and/or innovative\\nbusinesses\\nExperience\\nwith Lean and/or SCRUM development methodologies a plus\\nMS\\nor PhD in Computer Science, Math or Engineering or other quantitative\\nfield. Work experience can substitute for experience\\nExperience with time series analysis is a plus\\nExperience with AWS machine learning suite of tools is a plus\\nExperience with applying data science to problems in insurance\\nand/or other financial industries is a plus\\nExperience\\nwith analyzing data from social media platforms a plus\\nKnowledge of image data analysis and computer vision a plus\\nKnowledge of natural language processing a plus\\nPreferred but not\\nrequired qualifications\",\n",
       "  \"Group One Trading is one of the largest proprietary options market making firms in the US-listed market. The Business Intelligence team supports the various functional groups by providing analytics and infrastructure to decision-makers across the firm. Interfacing with trading, risk, finance, and operations the group's tasks are myriad and require drive, attention to detail, and a commitment to improving the quality of the product every day.\\n\\nDescription:\\n\\nA data engineer at Group One will be responsible for designing and maintaining data pipelines used throughout the firm. Their technical leadership will be used to make the existing systems more efficient and accessible while also developing new methods that can accommodate a growing problem space. By working collaboratively with different functional groups throughout the firm, they will be able to apply their technical expertise and creativity to identify data sets that can be used to research new opportunities, audit processes, and identify areas for improvement.\\n\\nThe ideal candidate will have experience integrating data consumption from multiple sources and liaising across teams. They will have expertise in making design decisions for consuming, processing, and reporting on data. While designing data pipelines that reduce maintenance costs and increase automation, they will ensure that the systems are extensible and flexible for changing markets. They will establish and maintain database standards and provide data solutions to both Business Intelligence and other data consumers in the firm.\\n\\nDesired Skills:\\nStrong communication skills, including explaining technical problems and solutions to a non-technical audience;\\nTroubleshooting problem reports in a dynamic and live production environment;\\nExperience gathering functional requirements from a varied user base;\\nDeveloping complex T-SQL queries across multiple datasets;\\nImplementing scheduled jobs and stored procedures;\\nUsing SSIS and SSAS to manage ETL and data warehousing;\\nGenerating reports and building validation checks for data integrity, logic and quality;\\nDesigning infrastructure and implementation of database management best practices;\\nData validation and schema design;\\nData analysis with Excel, SQL Server, SSRS Microsoft Report Builder, and Power BI;\\nTime series analysis of financial data.\\nExperience:\\n\\nEducation: Master's degree in Information Systems, Computer Engineering or related OR Bachelor's degree in the same with two additional years of professional experience\\n\\nProfessional: Two years of experience in a position developing and maintaining information or computer systems in the financial or equities trading industry preferred.\\n\\nJob Posted by ApplicantPro\",\n",
       "  'Company Overview\\n\\n\\nAt Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. For the 28th year, MSK has been named a top hospital for cancer by U.S. News & World Report. We are proud to be on Becker’s Healthcare list as one of the 150 Great Places to Work in Healthcare in 2018, as well as one of Glassdoor’s Employees’ Choice Best Place to Work for 2018. We’re treating cancer, one patient at a time. Join us and make a difference every day.\\n\\nJob Description\\n\\n\\nWe are seeking a Computational Biologist to join our team and provide bioinformatics support in the development of next-generation sequencing applications to characterize the spectrum and clinical significance of genetic mutations in human tumors. Our lab uses mouse models, RNA interference (RNAi), and cancer genomics to identify components of tumor suppressor gene networks and understand the molecular determinants of treatment response. The goal is to identify genes required for tumor maintenance and to understand the mechanisms involved in tumor progression. These genes may be potential therapeutic targets to various cancer genotypes, such as lung, liver, colon, stomach, pancreatic, and prostate cancer.\\n\\nYou are:\\nExperienced in the analysis of epigenetics data from high sequencing experiments and knowledge of single-cell RNA-sequencing technologies and ability to analyze and interpret single-cell RNA-seq data a plus.\\nYou Will:\\n\\n\\nDesign, develop, and operate computational pipelines for the analysis of next-generation sequencing data.\\nUtilize gene set enrichment analysis to identify altered biological pathways that correlate with tumor progression.\\nEvaluate performance of novel technology platforms and assays.\\nWork directly with research groups to analyze biomedical data and address scientific problems with bioinformatics techniques and tools\\nDevelop, refine, and improve bioinformatics tools for interpretation of large genomic data sets.\\nManage and archive large data sets from next-generation sequencing experiments\\nDoes this sound like a match? If yes, you should apply.\\n\\nYou Need:\\nExperience with programming skill, such as R, Python, MatLab, etc.\\nAnalysis of NGS data – RNA-seq and ChIP-seq\\nVariant calling – point mutations, indel, structural rearragenement, copy number changes\\nBuilding analysis pipeline, basic genomic sequencing alignment\\n#LI-POST\\n\\nClosing\\n\\n\\nMSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.\\n\\nFederal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.',\n",
       "  'Job Description\\nLead Data Scientist\\n\\nNew York, NY\\n\\nTarget Comp: USD $180,000\\n\\nTHE COMPANY\\n\\nHarnham are partnered with one of the leaders in the healthcare space who are seeking to add a strong core data scientist to their team. Use your machine learning skills to solve problems by developing and implementing machine learning models into production. Come mentor junior team members and guide best practices.\\n\\nTHE ROLE- Lead Data Scientist\\n\\nAs the Lead Data Scientist, you will:\\nApply machine learning techniques on various data science projects\\nBuild recommendation systems to target the right customers and identify the best practice of sending information to those people\\nUsing reinforcement learning to optimize experiments\\nGuide junior data scientists and show them the best approaches to use skills\\nWork with stakeholders to best incorporate guidelines for experimentation\\nAnalyzing results to make sure experiments are being designed correctly\\nYOUR SKILLS & EXPERIENCE\\nMasters or PhD in a relevant field\\nExtensive experience programming in Python or R\\nExperience in big data technologies (Pyspark, Scala, Hive or Hadoop)\\nStrong experience developing and implementing machine learning models into production\\nPresenting final models and projects to business leaders for approval\\nExperience mentoring or leading team members a plus\\nExperience with Healthcare a plus\\nTHE BENEFITS\\n\\nTarget compensation of $180,000\\n\\nHOW TO APPLY:\\nPlease register your interest by sending your CV to David Izso via the Apply link on this page.\\nKEYWORDS\\nData Scientist Machine Learning | Reinforcemen | Leadership | Healthcare | Data Science | Big Data\\nCompany Description\\nData and Analytics recruitment is our core business and we’re proud to say, our customers believe we’re good at it. In our most recent customer satisfaction survey, 95% of respondents said that they would recommend Harnham.\\n\\nHarnham has actively chosen to focus on Data and Analytics, we’ve immersed ourselves in this market and are now an integral part of this business community.\\n\\nOur capability has grown to provide recruitment services and advice across the Marketing Analytics, Credit Risk, Data Science, Data and Technology and Digital sectors.',\n",
       "  'Principal Data Scientist - AI Solutions\\nThis Jobot Job is hosted by: Dani Schlarmann\\nAre you a fit? Easy Apply now by clicking the \"Apply Now\" button and sending us your resume.\\nSalary: $140,000 - $200,000\\nA bit about us:\\nBased in the New York City Metro, we are a fast growing company that provides AI/ML Solutions to companies in a variety of industries. We provide innovative solutions that are practical, scalable, and enable high value insights and outcomes. We are looking for a Principle Data Scientist to help lead our team as we try to infect change with some of the world\\'s largest enterprise and government entities.\\n\\nIf you are a Principle Data Scientist with great technical and team building skills, then please read on.\\n\\nWhat can we do for you?\\nCompetitive Base Salary!\\nExtremely Competitive Equity Package!\\nFlexible Work Schedules!\\nAccelerated Career Growth!\\nIs your background a fit? We are looking for\\nAdvanced degree (MS/PhD) in a quantitative discipline (e.g., Applied Math, Statistics, Engineering, Computer Science, Physics, etc.),\\n5+ of experience developing and deploying machine learning based solutions in a Data Science role.\\nExperience of building successful machine learning based solutions in healthcare industry; experience of handling health systems datasets\\nStrong working experience and knowledge of Python and SQL\\nExperience with Cloud technologies such as Docker/Kubernetes\\nExperience with Big Data technologies such as Hadoop, Spark or PySpark, Kafka, and NoSQL\\nWhy join us?\\nWe can offer you the opportunity to work with some of the largest enterprise clients in the world! Together we can translate their data into insights that propel their business through improved profit, performance and reduced risk.\\n\\nInterested in hearing more? Easy Apply now by clicking the \"Apply Now\" button.',\n",
       "  \"At GenapSys we are developing a DNA-sequencing technology that will revolutionize the world of healthcare and diagnostics. Our system will enable fast, accurate, and ultra-low cost genetic tests in order to better understand (and ultimately treat) genetic diseases such as Cancer, Parkinson's, Diabetes, and Alzheimer's, just to name a few. GenapSys was founded by Dr. Hesaam Esfandyarpour and incubated over 6 years (2004-2010) at Stanford University.\\n\\nOur team brings together an incredibly diverse and multidisciplinary set of backgrounds and skills – from electrical and mechanical engineers to physicists, chemists, microfluidic engineers, molecular biologists, bioinformaticians, mathematicians, and more. Together we will advance scientific and medical knowledge and improve human health through cheaper, faster, and more accurate DNA sequencing technologies. We are working cross-functionally to make a great impact in the world – join us on that mission!\\n\\nAs the scientific expert in our products, the Field Applications Scientist (FAS) will build long-term relationships and establish rapport with GenapSys customers. You will provide wet-lab, data analysis, and technical education on our Sequencing products as needed. Support interfaces include in-person, phone and email communications. The FAS will travel to customer sites and trade shows in the field for pre- and post-sale activities. Solves customer escalations effectively. Acts as a trusted expert to our customers, increasing customer satisfaction and loyalty as well as contributing to the overall success of the business through customer success and sharing customer voice in the development of new products.\\n\\nResponsibilities\\nProviding scientific/technical consultation to customers in partnership with the sales and marketing to ensure customers understand and use GenapSys products to best suit their needs.\\nDelivering scientific/technical presentations for internal or customer facing events.\\nLeading hands-on product and software trainings in customer laboratories.\\nReporting on relevant activities in the field on a regular basis to sales partners, management, R&D, and product marketing.\\nMonitoring technical trends and competitive products in the genomics markets.\\nProviding remote and/or on-site troubleshooting assistance to customers in partnership with our internal technical support team and service repair depot.\\nProactively partnering with sales and technical support teams to ensure customer support needs are achieved in a timely manner and appropriately documented.\\nProvides and continuously update customer contact relevant information. Track, record and document relevant information from all technical enquiries from customers.\\nStays current on scientific and industry trends and innovations.\\nParticipates in inter-departmental projects.\\nMay be required to perform other related duties as needed.\\nMinimum Requirements\\nBA or BS in Biology, Genetics, Bioinformatics or related scientific field.\\nMS or PhD is strongly preferred.\\nA minimum of 2+ years of molecular biology laboratory or related work experience.\\nExperience in Next Generation Sequencing (NGS) technologies\\nExperience in sample and library preparation for sequencing is preferred\\nRelevant Skills and Experience:\\nMust be technically competent in the molecular biology lab, along with good people skills, to work with a variety of diverse customers, including principle investigators, scientists, technicians, and informaticists.\\nExcellent communication skills; both verbally and written. Experience delivering seminars and leading group discussions and data reviews a plus.\\nSound analytical reasoning and problem-solving skills.\\nInnovative, independent, driven, and results-oriented.\\nMethodical and comprehensive, yet flexible and able to easily adjust to evolving requirements and navigate in a fast-paced environment to meet tight schedules.\\nAbility to travel >50%, including overnight and international travel.\\nStrong desire to work in a demanding, high-energy, hands-on startup environment.\\nAbility to effectively represent customer needs with cross functional teams, including marketing, R&D, operations, and sales.\\nAbility to lift 30 Lbs. without assistance.\\nMaintain a valid driver's license and safe driving record.\\nWhat we offer:\\nAn amazing mission: Enabling everyone to decode the living world!\\nCompetitive compensation and stock options.\\nCompetitive benefits: medical, dental and vision benefits for employees and dependents, fully sponsored life, short and long-term disability insurance, and 401(k) retirement savings plan.\\nFree daily catered gourmet lunches and snacks.\\nWeekly fancy High Tea and happy hours.\\nFree complex onsite gym membership.\\nCasual, but high-performing work environment.\\nA diverse and inclusive workplace where we learn from each other is an integral part of GenapSys' culture. We actively welcome people of different backgrounds, experiences, abilities, and perspectives. We are an equal opportunity employer and a great place to work. Join us and help us achieve our mission!\\n\\nNote to Recruiters and Placement Agencies:\\n\\nGenapsys does not accept unsolicited agency resumes. Please do not forward unsolicited agency resumes to our website or to any Genapsys employee. Genapsys will not pay fees to any third party agency or firm and will not be responsible for any agency fees associated with unsolicited resumes. Unsolicited resumes received will be considered the property of Genapsys.\",\n",
       "  \"Company Overview\\n\\n\\nAt Memorial Sloan Kettering (MSK), we’re not only changing the way we treat cancer, but also the way the world thinks about it. By working together and pushing forward with innovation and discovery, we’re driving excellence and improving outcomes. For the 28th year, MSK has been named a top hospital for cancer by U.S. News & World Report. We are proud to be on Becker’s Healthcare list as one of the 150 Great Places to Work in Healthcare in 2018, as well as one of Glassdoor’s Employees’ Choice Best Place to Work for 2018. We’re treating cancer, one patient at a time. Join us and make a difference every day.\\n\\nJob Description\\n\\n\\nWould you like to utilize your outstanding data engineering skills to support MSK’s mission? Do you want to be part of a team that values your contributions and encourages you to grow?\\n\\nOur Data Strategy team is committed to facilitating the efficient delivery of reliable data to the physicians and researchers working to discover more-effective strategies to prevent, control, and ultimately cure cancer in the future!\\n\\nWe are seeking a Sr Data Engineer who understands the critical value that prepared data provide for next-generation analytics. You will support the work of Data Strategy by managing the full development life cycle of complex projects requiring integration to other systems including analysis, design, programming, implementation and support, you will exponentially reduce the time-to-insight for healthcare professionals, researchers, and the advanced analytics teams providing innovative, best-in-class solutions.\\n\\nSample Projects:\\nMSKTranslate: A program designed to transform free-text fields using common external ontologies, which provides clearer links within clinical information regarding patient outcomes\\nGenomic data integration, which allows principal investigators to match patients to the most appropriate clinical trials\\nYou will:\\nMaintain relationships with many partners including developers from other divisions, physicians, administrators, researchers, and technical specialists throughout the international cancer care community.\\nMentor others in technical standard methodologies and problem-solving, both within the MSKCC community and the cancer data consortium's in which it participates.\\nFunction as the technical lead by crafting and architecting elegant data delivery and analysis solutions that integrate seamlessly with existing technologies. These solutions will be innovative and solve complex issues, and a high degree of problem-solving and creative thinking are required.\\nKeep abreast of developments in technology as well as healthcare, science, analytics, and the broader world to identify novel ways to make quality data available efficiently.\\nBe responsible for all project planning and coordination of assigned projects, including keeping the project timeline on track and managing user and management expectations.\\nYou have:\\nBachelor's degree in Computer Science, Mathematics\\nMinimum of 5 years in a professional environment using Java\\nDemonstrated history of technical leadership and project management within a data-driven environment\\nOutstanding CS fundamentals and analytical skills\\nProficiency with SQL\\nNice to have:\\nExperience with Noble Coder/TIES or other Named Entity Recognition (NER) tools\\n2-4 years of experience as a Software Engineer focusing in Java or a Master’s Degree in CS or related field.\\nExperience and comfort with XML\\nFamiliarity with REST API\\n#LI-POST\\n\\nClosing\\n\\n\\nMSK is an equal opportunity and affirmative action employer committed to diversity and inclusion in all aspects of recruiting and employment. All qualified individuals are encouraged to apply and will receive consideration without regard to race, color, gender, gender identity or expression, sexual orientation, national origin, age, religion, creed, disability, veteran status or any other factor which cannot lawfully be used as a basis for an employment decision.\\n\\nFederal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.\",\n",
       "  \"Job Description\\nNO VISAS. NO H1B. NO OPT. WE CANNOT SPONSOR. US CITIZEN OR GREENCARD ONLY.\\nCONTRACT TO FULL TIME HIRE AFTER A 6-8 MONTH PERIOD.\\nOur client a leading Digital Media company headquartered in Manhattan is looking for driven individuals to join our team of passionate data engineers and data scientists in creating their generation of data products and capabilities. Candidates should possess deep knowledge of machine learning algorithms along with modern data processing technology stacks and have a strong problem solving skills. We are a small and efficient team building scalable data pipelines on cloud platform for data analysis and reporting process and providing a solution with machine learning and data mining techniques.\\nResponsibilities\\n-Implement machine learning based predictive models such as churn prediction.\\n-Write production worthy code, ensuring it runs efficiently on the target system.\\n-Apply data analytics and statistical techniques to identify relevant ML features.\\n-Overcome common ML problems such as overfitting, cold start, and logging.\\n-Detect flaws in the existing machine learning systems and suggest improvements.\\n-Collaborate with management to align developed models with stakeholders’ needs and\\nbusiness requirements.\\n-Develop, deploy, and maintain the data products and systems with documentations.\\nQualifications\\n-Bachelor’s degree or Master’s degree from Computer Science, Data Science, or equivalent practical experience. Master’s degree preferred.\\n-Strong knowledge of machine Learning algorithms.\\n-Strong understanding of programming and computer science fundamentals.\\n-Experience in producing algorithms and improving their efficiency and runtime.\\n-Fluent in Python and SQL.\\n-Experience with cloud platform is a plus.\\n-Knowledge of Big Data technologies such as Spark or Hadoop is a plus.\\n-Ability to learn modern data technologies.\\nCompany Description\\nABOUT DELPHI-US\\nDelphi-US provides world class talent acquisition solutions to our clients nationwide. We are experts in Information Technology, Engineering, Professional, and select specialized talent recruitment. With a collective 30+ years in our field, Delphi-US brings true best practices to the industry, and a fundamental approach that drives excellence in results for our clients and consultants. Known as 'The Peacemakers in The Talent War', Delphi-US seeks to join the best and brightest of our knowledge workforce to select employers of choice throughout the United States.\",\n",
       "  \"Type: Full Time\\nExperience: All Levels\\n\\nWho we are:\\n\\nWe are Dataminr. Our advanced AI platform delivers relevant, actionable information alerts in real time. We detect the earliest signals on real-time risks within an increasingly diverse and complex landscape of publicly available information, enabling more effective risk mitigation strategies, faster response, and better crisis management for public and commercial sector organizations across the globe.\\n\\nWe've grown to over 450 talented employees across six global offices and raised $392 million in funding at a $1.6 billion valuation in our most recent investment round. Our culture promotes cross-team interaction, work-life balance and the sharing of information and ideas because it enables us to do our best work together and have fun.\\n\\nWe are a mission-driven company committed to the power of AI and real-time information as a force for good in the world. Join us and help the world operate in real time.\\n\\nWho you are:\\n\\nYou're a research scientist who has published in top tier conferences and are passionate about having a positive impact on the world. You are enthusiastic about working on some of the most challenging problems in AI and directly contributing to products while advancing the state of the art. You are very creative, can work independently, and are an excellent collaborator and communicator.\\n\\nResponsibilities:\\nPerform research to advance the state of the art and solve specific problems at scale in one or more of the following areas: image and video classification, object recognition, event detection, forensics, caption generation, etc.\\nWork closely with a diverse, interdisciplinary team to deliver value to customers (existing and new products).\\nContribute to the research community via publications in top tier venues, participation in program committees, etc.\\nExcel in placing a human-centered focus on the work (context, end-user impact, etc), finding solutions that work in practice and have significant impact.\\nSenior candidates are expected to lead technical areas and/or people.\\nMinimum qualifications:\\nPh.D. in Computer Science or Electrical Engineering (Computer Vision).\\nExperience in one or several of the following topics above, with an emphasis on real time and scalability.\\nOutstanding publication record (CVPR, ICCV, NIPS, ICML, etc.).\\nExperience in Java, Scala, Python, or similar languages.\\nDemonstrated ability to work independently, set up experiments and demonstrate progress through principled used of metrics.\\nFamiliarity with machine learning and deep learning tools (TensorFlow, PyTorch, Scikit-learn, etc)\\nProfessional experience in industry (requirement varies depending on level; recent graduates must have had relevant internships).\\nWhy you should work here:\\n\\nWe recognize and reward hard work with:\\ncompetitive compensation package including company equity.\\npaid benefits for employees and their dependents, including medical, dental, vision, disability and life insurance.\\n401(k) savings plan with company matching.\\nflexible spending account for out-of-pocket medical, transit, parking and dependent care expenses.\\nWe want you to be your best, authentic self by supporting you with:\\na diverse, driven, and passionate team of coworkers who want you to succeed.\\nopportunities to own and drive important critical projects.\\nindividual Learning and Development fund and professional training.\\ngenerous leave and flexible hours.\\ndaily catered lunch and a fully stocked kitchen.\\nAnd more!\\nNote: For this role, only candidates with Ph.D.s in the areas above will be considered. If you do not meet that requirement please consider either the Data Scientist or other positions.\\nDataminr is an equal opportunity and affirmative action employer. Individuals seeking employment at Dataminr are considered without regards to race, sex, color, creed, religion, national origin, age, disability, genetics, marital status, pregnancy, unemployment status, sexual orientation, citizenship status or veteran status.\\n\\n#LI-BM\",\n",
       "  'General Information\\n\\nRef #: 27449\\n\\nFunctional Area: Technology\\n\\nEmployee Type: Full Time\\n\\nLocation: Austin\\n\\nExperienced Required: Please See Below\\n\\nEducation Required: Bachelors Degree\\n\\nJob Posting Shift: 1st\\n\\nDate published: 26-Jun-2019\\n\\nAbout Us:\\n\\n\\nWe are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.\\n\\nPIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.\\n\\nPosition Description:\\n\\n\\nAs a data engineer on our team, we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. In this role you will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.\\n\\nPosition Requirements:\\n\\n\\n2+ years of experience with building end-to-end scalable production-grade data pipelines\\nKnowledge of data warehousing\\nUnderstanding of modern data architecture, data modeling, and data management principles\\nExperience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nGood foundation in data structures and algorithms\\nUnderstanding with Relational and NoSQL databases to help teams best organize their data for analysis\\nExperienced in OOP design and development, preferably in Python\\nKnowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\nPREFERRED QUALIFICATIONS\\nDomain knowledge of Financial Services\\nHands-on experience with working with Cloud technologies, including AWS\\nExperience with CI/CD methodologies\\nExperience with *nix environments, including shell script development\\n\\nWe are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.\\n\\nOur technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.\\nWhy PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).\\n\\nPIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.\\n\\nBenefits:\\n\\n\\nPIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:\\nMedical, dental, and vision coverage\\nLife insurance and travel coverage\\n401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment\\nWork/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs\\nCommunity involvement opportunities with The PIMCO Foundation in each PIMCO office',\n",
       "  'At SailPoint, we do things differently. We understand that a fun-loving work environment can be highly motivating and productive. When smart people work on intriguing problems, and they enjoy coming to work each day, they accomplish great things together. With that philosophy, we’ve assembled the best identity team in the world that is passionate about the power of identity.\\n\\nAs the fastest-growing, independent identity and access management (IAM) provider, SailPoint helps hundreds of global organizations securely and effectively deliver and manage user access from any device to data and applications residing in the data center, on mobile devices, and in the cloud. The company’s innovative product portfolio offers customers an integrated set of core services including identity governance, provisioning, and access management delivered on-premises or from the cloud (IAM-as-a-service).\\n\\nSailPoint is seeking a Sr/Staff Data Software Engineer to help build a new cloud-based identity analytics product incorporating real-time data pipelines, machine learning algorithms and multi-tenancy support. We are looking for well-rounded backend or full stack engineers who are passionate about building and delivering reliable, scalable microservices and infrastructure for SaaS products.\\n\\nResponsibilities\\nCollaborate with peers on requirements, designs, code reviews, and testing\\nProduce designs and rough estimates, and implement features based on product requirements\\nDeliver efficient, maintainable, robust Java/Scala based microservices\\nProduce unit and end-to-end tests to improve code quality and maximize code coverage for new and existing features\\nProductize and operationalize machine learning algorithms\\nActively engage in technology discovery that can be applied to the product\\nRequirements\\n7-10 years of professional software development experience\\n2+ years of data engineering or related experience\\nStrong Java and/or Scala experience\\nExperience with Agile development practices and continuous delivery\\nProficient understanding of distributed computing principles. microservice architectures and patterns\\nExperience with integration of data from multiple data sources\\nExperience writing unit and integration tests\\nGreat communication skills\\nBS in Computer Science or a related experience\\nPreferred\\nExperience with Cloud computing architectures (AWS, Google Cloud)\\nExperience with Kafka, Flink/Spark, Elasticsearch technologies or related\\nExperience integrating data pipelines for machine learning\\nExperience with container technologies (Docker, Kubernetes, etc.)\\nExperience with NoSQL databases, such as Redshift, Cassandra, DynamoDB\\nExperience instrumenting code for gathering production performance metrics\\nCompensation and benefits\\nExperience a Small-company Atmosphere with Big-company Benefits\\nCompetitive pay, 401(k) and comprehensive medical, dental and vision plans\\nRecharge your batteries with a flexible vacation policy and paid holidays\\nGrow with us with both technical and career growth opportunities\\nEnjoy a healthy work-life balance with flexible hours, family-friendly company events and charitable work\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.\\n\\nAll qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.',\n",
       "  \"The home search starts online, but the real estate industry is often optimized for in-person, one-on-one service. That's a fantastic experience once you connect with the right professional, but finding the right fit isn't always a smooth process. Opcity built a nationwide real-time data and technology platform combining cutting edge deep learning, business analytics and human intuition with the latest web, mobile and digital telephony technologies to enable our team of professionals, and thousands of real estate agents and brokers, to make sure we connect every home buy with the right agent at the right time so more time is spent finding a home and less time finding the perfect agent.\\n\\nThe Data Science Engineer will support our software developers, database architects, data analysts and data scientists on data initiatives and will ensure optimal data delivery architecture is consistent throughout ongoing projects. The Date Science Engineer will consider optimizing or even re-designing our company’s data architecture to support our next generation of products and data initiatives. The Data Science Engineer should have the drive and ability to meet with relevant stakeholders, understand and model relevant data, and effectively communicate with an engineering team to put models into production. Demonstrating lifelong learning and collaboration; and willingness to take advice and fill gaps in knowledge is essential.\\nDuties & Responsibilities:\\nCreate and maintain optimal data pipeline architecture,\\nAssemble large, complex data sets that meet business requirements.\\nIdentify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL and AWS technologies.\\nBuild analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.\\nWork with stakeholders including the Engineering, Product and Data and teams to assist with data-related technical issues and support their data infrastructure needs.\\nCreate data tools for analytics and data scientist team members that assist them in building and optimizing our product into an innovative industry leader.\\nWork with data and analytics experts to strive for greater functionality in our data systems.\\nWhat We Like To See / Measures of Success:\\nM.S. or Ph.D. in Computer Science, or other quantitative field or a B.A./B.S. with 3+ years professional or research experience\\nProficiency with SQL\\nProficiency with Python\\nExpert in AWS technologies\\nEnd-to-end experience with data, including querying, aggregation, analysis, and visualization\\nWillingness to collaborate and communicate with others to solve a problem\\nExperience supporting and working with cross-functional teams in a dynamic environment\",\n",
       "  \"The home search starts online, but the real estate industry is often optimized for in-person, one-on-one service. That's a fantastic experience once you connect with the right professional, but finding the right fit isn't always a smooth process. Opcity built a nationwide real-time data and technology platform combining cutting edge deep learning, business analytics and human intuition with the latest web, mobile and digital telephony technologies to enable our team of professionals, and thousands of real estate agents and brokers, to make sure we connect every home buy with the right agent at the right time so more time is spent finding a home and less time finding the perfect agent.\\n\\nDescription:\\n\\nData Support Engineers assist the development team by using their data and analytical skills to improve customer experience and product performance. Data Support Engineers triage and troubleshoot data inconsistencies across our products, perform manual data fixes, and consult with development stakeholders to build reporting and key metrics.\\nJob Duties / Responsibilities:\\nYou will identify, report and act on issues that lead to customer dissatisfaction\\nYou will collaborate with the development team to troubleshoot customer issues, find root causes, and generate solutions in a timely manner\\nYou will consult with development stakeholders to build reporting and key metrics\\nYou will support field teams by answering product questions and performing manual data fixes\\n\\nExperience & Skill Requirements:\\nStrong proficiency in SQL\\nExposure to programming tools such as Python, Ruby, C#, Java\\nStrong analytical and troubleshooting abilities\\nExperience reasoning about complex systems and technology platforms\\nExcellent oral and written communication skills\\nWhat We Like To See:\\nPrior experience working with product and engineering teams\\nSelf-starter and high degree of motivation to go above and beyond the task at hand\\nExcellent attention to detail and strong outcome-oriented mindset\\nComfort working in a fast-paced environment with multiple stakeholders\\nThe Perks:\\nOnsite gym with showers and lockers\\nFree lunch provided daily\\nLocated on private members only disc golf course\\nWorking with a highly-motivated team with a proven track record of success that also has a lot of FUN!\\nCasual work environment, rewards, recognition and fun events\\nCompetitive pay\\nMedical, Vision, Dental, Disability and Life Insurance plus Flexible and Dependent Care Spending Accounts\\nInvestment in growing your career and providing you opportunity to have an immediate and massive impact on a growing company with a revolutionary business model\\n\\n*No relocation is provided for this vacancy, local candidates preferred. Must be eligible to work in the U.S. for any employer.\",\n",
       "  \"At SpringML, we are all about empowering the 'doers' in companies to make smarter decisions with their data. Our predictive analytics products and solutions apply machine learning to today's most pressing business problems so customers get insights they can trust to drive business growth. We are a tight knit, friendly team of passionate and driven people who are dedicated to learning, get excited to solve tough problems and like seeing results, fast.\\n\\nYour primary role will be to design and build data pipelines. You will be focused on designing and implementing solutions on Hadoop, Spark, Pig, Hive. In this role you will be exposed to Google Cloud Platform including Dataflow, BigQuery and Kubernetes so the ideal candidate will have a strong big data technology foundation and bring a passion to learn new technologies. If you believe you have these skills please email your resume to info@springml.com.\\n\\nRequired Skills:\\n4-7 years Python and Java programming\\n3-5 years knowledge of Java/J2EE\\n3-5 years Hadoop, Big Data ecosystem experience\\n3-5 years of Unix experience\\nBachelors in Computer Science (or equivalent)\\nDuties and Responsibilities:\\nDesign and develop applications utilizing the Spark and Hadoop Frameworks or GCP components.\\nRead, extract, transform, stage and load data to multiple targets, including Hadoop, Hive, BigQuery.\\nMigrate existing data processing from standalone or legacy technology scripts to Hadoop framework processing.\\nShould have experience working with gigabytes/terabytes of data and must understand the challenges of transforming and enriching such large datasets.\\nAdditional Skills that are a plus:\\nC, Perl, Javascript or other programming skills and experience a plus\\nProduction support/troubleshooting experience\\nData cleaning/wrangling\\nData visualization and reporting\\nDevops, Kubernetes, Docker containers\",\n",
       "  'About Sysco LABS\\n\\nSysco LABS is a technology-focused division within Sysco, dedicated to reimagining foodservice through innovation. An extension of Sysco’s commitment to deliver exceptional products and services to the foodservice industry, Sysco LABS uses customer and market intelligence, data-driven insights and agile technology development to rethink the entire foodservice ecosystem.\\n\\nOur innovation will improve everything from the ordering process, inventory, pricing and automation to the in-restaurant customer experience. Operating with the mindset of a startup and backed by the authoritative expertise of an industry leader, Sysco LABS’ mission is to improve the Sysco customer experience and consistently deliver cost savings and new innovations through technology.\\n\\nAbout the Role\\n\\nAs a Data Engineer at SyscoLABS you will build scalable data and analytics models and architecture from event formation to ingestion to reporting from scratch. You will work on all aspects of the system from stream configuration, to ETL, to aggregate tables/cubes for reporting needs. You will make an impact in creating robust systems that are used by every team. You will help shape the vision and architecture of the end-to-end data pipeline while following industry best practices. You will work with all facets of the business from dev ops, to development, to consumers of the data. You will work with data stakeholders to understand their needs and translate those into executable plans. You will be part of an experienced data team and work with passionate leaders.\\n\\nWhat you’ll bring to the table\\n5+ years of experience in data engineering\\nExperience working closely with analysts and business stakeholders\\nProficient in SQL and Python\\nProficient in Redshift and Postgres\\nProficient in AWS platform and technologies including Kinesis, Firehose, and Glue\\nCloud storage such as S3\\nFamiliar with Data Lake technologies like EMR, Athena, hive, presto, parquet\\nFamiliar with Event streaming and processing.\\nExperienced in orchestrating data ingestion and validation.\\nExperienced in ETL for reporting/business users (marketing, finance, product owners)\\nExperience with Tableau and Tableau Server a plus\\nExperience working with Virtual Container Environment (eg Docker, ECS) a plus.\\nPerks at Sysco LABS:\\nCompetitive base & bonus packages\\nFun, casual, fast-paced work environment filled with talented colleagues\\nBrand-new high-tech office space located in the heart of East 6th St, (walking distance to the MetroRail)\\nGenerous PTO policy\\nFlexible work environment that encourages work/life balance\\nFully stocked break room & weekly catered lunch\\nExcellent medical, dental and vision benefits and company 401(k) program\\nAt Sysco LABS you will be part of a team that values trust, learning, and working together to solve problems. Your efforts will help to build groundbreaking experiences for our customers. Come join us!\\n\\nWe embrace diversity and equal opportunity in a serious way. We are committed to building a team that represents a variety of backgrounds, perspectives, and skills. The more inclusive we are, the better our work will be.',\n",
       "  'General Information\\n\\nRef #: 25383\\n\\nFunctional Area: Business Management\\n\\nEmployee Type: Full Time\\n\\nLocation: Austin\\n\\nExperienced Required: Please See Below\\n\\nEducation Required: Bachelors Degree\\n\\nJob Posting Shift: 1st\\n\\nDate published: 26-Feb-2019\\n\\nAbout Us:\\n\\nWe are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.88 trillion in assets for clients around the world. PIMCO has over 2,800 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.\\n\\nPIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.\\n\\nPosition Description:\\n\\nPacific Investment Management Company (PIMCO) is seeking a motivated, hands-on client data quality analyst for its Client Data Oversight team. In this role the incumbent will facilitate collaboration with the team that manages distribution data and IT resources to deliver business value through creating, maintaining, and presenting actionable client data insights.\\n\\nThe incumbent will partner with internal business members and support the delivery of data projects by profiling datasets, communicating data model requirements, validating quality of data, and day-to-day oversight of the client data quality. This function supports client business growth for PIMCO through improved data quality / completeness and simplified data management.\\n\\nThe successful incumbent will possess strong analytics skills, a demonstrated ability to analyze data to enhance and operate processes that create and maintain data in the client data warehouse. This role requires a continuous analytical assessment of the client reference master data and CRM data to assess the effectiveness and quality of distribution data management efforts and ensure that client information is of the highest accuracy and completeness prior to populating PIMCO’s internal systems and reporting applications.\\n\\nResponsibilities\\n\\nKey responsibilities include, but are not limited to, the following:\\n\\nSupport the client data profile quality efforts to assess and ensure ongoing accuracy of information across PIMCO and with suppliers and customers.\\nConsolidate data providing a common view of a client while also offering the flexibility to meet business unit information needs.\\nDevelop data prototypes to confirm business requirements and expectations to IT.\\nDevelop and participate in the review of requirements, data mappings, use cases and other artifacts to communicate the system requirements to the technical team.\\nEnrich the client data through routine uploads of new dimensional data.\\nProvide client support for questions regarding data.\\nProvide ad hoc querying support for data analysis by various client groups.\\n\\nPosition Requirements:\\n\\nBachelor Degree required, preferably in a business- or technology-related field of study. A Master’s Degree is preferred.\\nMinimum 2 years professional experience with data analysis, including an understanding of entity relationship modeling and/or dimensional modeling.\\nDemonstrated knowledge and experience with key Data Management principles, master data management, reference data management, data cataloguing, policies/procedures, management information and/or data engineering.\\nA high technical aptitude, including proficiency in SQL and relational database concepts, as well as Microsoft Office tools, including Excel.\\nProactively curious, self-sufficient professional able to convey complex ideas and datasets to actionable uses for CRM, reporting, and business analytics.\\nAdditional Desired Skills/Experience:\\nWorking knowledge of the financial services industry.\\nWorking knowledge of Python desirable.\\nDemonstrated ability to document requirements and participate in key requirement workshops.\\nProfessional Skills Requirement:\\nExcellent communication (verbal and written), facilitation, and interpersonal skills.\\nStrong attention to detail and quality, with the ability to keep larger goals in mind.\\nActive learner and continuously eager to acquire new skills to succeed in a dynamic evolving environment.\\nResults-oriented and entrepreneurial, proven ability to work creatively and analytically.\\nStrong process-orientation with a client-servicing mind-set.\\nExcellent organizational and time management skills.\\nHighly motivated with the ability to thrive in a fast-paced, high energy, and demanding team-oriented environment.\\nAbility to work collaboratively and independently while managing multiple projects, assignments and/or responsibilities.\\n\\nBenefits:\\n\\nPIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:\\nMedical, dental, and vision coverage\\nLife insurance and travel coverage\\n401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment\\nWork/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs\\nCommunity involvement opportunities with The PIMCO Foundation in each PIMCO office',\n",
       "  'Who are we? Raybeam Inc. is a software engineering consulting company focused on strategic consulting, business intelligence, and online/database marketing for the past twenty years. We have offices near Boston and San Francisco and support a strong list of clients including Google, Facebook, Microsoft, eBay, One Kings Lane, Disney and Hilton Worldwide. We are in the process of opening a new office in Austin, TX.\\nWhat do we do? We provide technology solutions by architecting and developing enterprise systems using a variety of programming languages, tools and platforms. This can range from building data warehouses, to web applications to implementing reporting platforms. We work in small teams, own the projects that we work on, and have direct input into the business decisions of our clients.\\nWhat are we looking for? We are looking for a technically savvy database-oriented Analyst or Data-Engineer with good people skills and the ability to pick up new business concepts and technologies.\\nThe ideal candidate will possess:\\nA strong to very strong working knowledge of SQL and python.\\nAn ability to write and troubleshoot complex SQL procedures.\\nThe desire to understand business events through data.• An understanding of Data Warehousing and ETL techniques.\\nHigh level understanding in at least one scripting language such as Ruby, Shell, Python.\\nAn interest in learning large data set processing with MapReduce/Hadoop/Pig/etc.\\nA minimum of 8 years experience.\\nLinux skills are a plus.\\nGood client relations skills strongly preferred.\\nIf you are interested in applying for the position please click on the link below to take a 10 minute quiz.\\n\\nhttp://careerseval.raybeam.com/sign_in\\n\\nPlease note that Raybeam, Inc. is not E verified and is unable to provide sponsorship. We will only consider local candidates. Recent grads are encouraged to apply, and an MBA is desirable. This is a full time contract role. Thank You',\n",
       "  'Come join our team at Cerity! We are a cutting edge insure tech company working with some really cool technologies. We have an opening for a data engineer on our growing team. We are looking for highly motivated, startup minded engineers to join our team of stellar engineers. Our data engineers work with latest big data tech including NoSQL (DynamoDB), AWS Aurora, AWS Data pipeline, Kinesis and Snowflake DB. Come help us revolutionize insurance technology.\\n\\nResponsibilities will include:\\n\\n- Designing and Implementing RDBMS Data Models\\n\\n- Designing and Implementing NoSQL Data Models\\n\\n- Implementing ETL solutions\\n\\n- Working with DevOps to establish infrastructure as code\\n\\n- Fixing any issues that arise with data functionality\\n\\nRequirements\\n\\n- 7+ Years Data Engineering\\n\\n- 5+ Years RDBMS Management\\n\\n- 2+ Years of NoSQL\\n\\n- AWS Ecosystem knowledge\\n\\n- Solid Data Modeling/Design Experience\\n\\nBenefits\\n\\n- Competitive Salaries\\n\\n- Annual Bonus Program\\n\\n- Great Health and other Benefits\\n\\n- Unlimited PTO',\n",
       "  \"Introduction\\nAt IBM we have an amazing opportunity to transform the world with cognitive technology. By using the vast amounts of information available today to identify new patterns and make new discoveries, we are helping cities become smarter, hospitals transform patient care, financial institutions minimize risk, and pharmaceuticals find cures for rare diseases.\\n\\nData scientists work with enterprise leaders and key decision makers to solve business problems by preparing, analyzing, and understanding data to deliver insight, predict emerging trends, and provide recommendations to optimize results. Data scientists use a variety of data (structured, unstructured, IoT streaming), analytics, AI tools, and programming languages often using a cloud infrastructure to handle the volume and veracity of data streams.\\n\\nArmed with data, modeling expertise, and analytic results, the data scientist communicates conclusions and recommendations to stakeholders in an organization's leadership structure. Business acumen is an important skill for data scientists to effectively communicate their findings to business leaders, data scientists need strong consulting, communication, visualization, and storytelling skills.\\n\\nYour Role and Responsibilities\\nSTART DATES FOR THIS POSITION ARE IN 2020\\n\\nData Scientists are in demand across IBM's growth areas. If hired, you will be matched to a team based on business demand, location and fit. Join the forward-thinking teams at IBM solving some of the worlds most complex problems there is no better place to grow your career!\\n\\nWhat Youll Do as an Entry-Level Data Scientist:\\nYou will implement and validate predictive models as well as create and maintain statistical models with a focus on big data.\\nYou will be exposed to and incorporate a variety of statistical and machine learning techniques such as logistic regression, experimental design, generalized linear models, mixed modeling, CHAID/decision trees, neural networks and ensemble models.\\nYoull communicate with internal and external clients to understand business needs and provide analytical solutions.\\nYou will use leading edge tools such as COGNOS, Watson Studio and Watson Machine Learning.\\nYoull work in an Agile, collaborative environment, partnering with other scientists, engineers, and database administrators of all backgrounds and disciplines to bring analytical rigor and statistical methods to the challenges of predicting behaviors.\\nWho You Are:\\nYou are great at solving problems, debugging, troubleshooting, and designing & implementing solutions to complex technical issues.\\nYou thrive on teamwork and have excellent verbal and written communication skills.\\nYou have strong technical and analytical abilities, a knack for driving impact and growth, and some experience with programming/scripting in a language such as Java or Python.\\nYou have a basic understanding of statistical programming in a language such as R, SAS, or Python.\\nYou have an interest in, understanding of, or experience with Design Thinking and Agile Development Methodologies\\nRequired Professional and Technical Expertise\\nBasic understanding of statistical programming in a language such as R, SAS, or Python.\\nExperience with programming/scripting in a language such as Java or Python.\\nKnowledge of statistical concepts such as regression, time series, mixed model, Bayesian, clustering, etc., to analyze data and provide insights.\\nPreferred Professional and Technical Expertise\\nAdvanced knowledge of statistical concepts such as regression, time series, mixed model, Bayesian methods, clustering, etc., to analyze data and provide insights.\\nAbout Business Unit\\nNo matter where you work in IBM, you are making an impact. As an Early Professional with IBM, you will be taking on a key role with one of our industry-leading business units to work on the technology that is solving our most challenging problems and changing the way the world thinks.\\n\\nYour Life @ IBM\\nWhat matters to you when youre looking for your next career challenge?\\n\\nMaybe you want to get involved in work that really changes the world? What about somewhere with incredible and diverse career and development opportunities where you can truly discover your passion? Are you looking for a culture of openness, collaboration and trust where everyone has a voice? What about all of these? If so, then IBM could be your next career challenge. Join us, not to do something better, but to attempt things you never thought possible.\\n\\nImpact. Inclusion. Infinite Experiences. Do your best work ever.\\n\\nAbout IBM\\nIBMs greatest invention is the IBMer. We believe that progress is made through progressive thinking, progressive leadership, progressive policy and progressive action. IBMers believe that the application of intelligence, reason and science can improve business, society and the human condition. Restlessly reinventing since 1911, we are the largest technology and consulting employer in the world, with more than 380,000 IBMers serving clients in 170 countries.\\n\\nLocation Statement\\nWe consider qualified applicants with criminal histories, consistent with applicable law.\\n\\nBeing You @ IBM\\nIBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status.\",\n",
       "  \"Company Description\\n\\nnull\\nJob Description\\n\\nOur Data Engineering team builds and maintains a secure, scalable, flexible and user-friendly analytics hub that allows us to make informed and data-driven decisions. They also construct and curate business-critical data sets that allow us to realize the value of all the data we collect.\\n\\nA Data Engineer utilizes a multidisciplinary approach to providing ETL solutions for the business, combining technical, analytical, and domain knowledge. The perfect applicant for this role has strong development skills, experience transforming and profiling data to determine risks associated with proposed analytics solutions, a willingness to continually interface with analysts in order to determine an optimal approach, and an eagerness to explore data sources to understand the availability, utility, and integrity of our data.\\n\\nWhat you'll own:\\n\\nData pipeline / ETL development:\\nBuilding and enhancing data curation pipelines using tools like SQL, Python, Glue, Spark and other AWS technologies\\nFocus on data curation on top of datalake data to produce trusted datasets for analytics teams\\nData Curation:\\nProcessing and cleansing data from a variety of sources to transform collected data into an accessible and curated state for Analysts and Data Scientists\\nMigrating self-serve data pipeline to centrally managed ETL pipelines\\nAdvanced SQL development and performance tuning\\nSome exposure to Spark, Glue or other distributed processing frameworks helpful\\nWork with business data stewards & analytics team to research and identify data quality issues to be resolved in the curation process\\nData Modeling:\\nDesign and build master dimensions to support analytic data requirements\\nReplacing legacy data structures with new datasets sourced from streaming data feeds from the core product and other operational systems\\nDesign, build and support pipelines to deliver business critical datasets\\nResolve complex data design issues & provide optimal solutions that meet business requirements and benefit system performance\\nQuery Engine Expertise & Performance Tuning:\\nAssist Analytics teams with tuning efforts\\nCurated dataset design for performance\\nOrchestration:\\nManagement of job scheduling\\nDependency management mapping and support\\nDocumentation of issue resolution procedures\\nData Access\\nDesign and management of data access controls mapped to curated datasets\\nLeveraging devops best practices, such as IAC and CI/CD to build upon a scalable and extensible data environment\\n\\nExperience you'll need:\\nStrong experience designing and building end-to-end data pipelines\\nExtensive SQL development experience\\nKnowledge of data management fundamentals and data storage principles\\nData modeling:\\nNormalization\\nDimensional/OLAP design and data warehousing\\nMaster data management patterns\\nModeling trade-offs impacting data management & processing/query performance\\nKnowledge of distributed systems as it pertains to data storage, data processing and querying\\nExtensive experience in ETL and DB performance tuning\\nHands on experience with a scripting language (Python, bash, etc.)\\nSome experience with Hadoop, Spark, Kafka, Impala, or other big data technologies helpful\\nFamiliarity with the technology stacks available for:\\nMetadata management: Data Governance, Data Quality, MDM, Lineage, Data Catalog etc.\\nData management, data processing and curation:\\nPostgres, Hadoop, Hive, Impala, Presto, Spark, Glue, etc.\\nExperience in data modeling for batch processing and streaming data feeds; structured and unstructured data\\n\\nExperience in data security / access management, data cataloging and overall data environment management\\n\\nExperience with cloud services such as AWS and APIs helpful\\n\\nYou’d be a great fit if your current track record looks like this:\\n5+ years of progressive experience data engineering and data warehousing\\nExperience with a variety of data management platforms (e.g. RDBMS (Postgres), Hadoop (CDH, EMR))\\nExperience with high performance query engines (Hive, Impala, Presto, Athena, MPP engines like RedShift)\\nStrong capability to manipulate and analyze complex, high-volume data from a variety of sources\\nEffective communication skills with technical team members as well as business partners. Able to distill complex ideas into straightforward language\\nAbility to problem solve independently and prioritize work based on the anticipated business value\",\n",
       "  \"At RetailMeNot, we believe that gaining valuable insights using our data is core to our future success. The Data team at RetailMeNot is responsible for developing core datasets and for exposing data services consumed by product, data science and business teams. Daily, we collect approximately a terabyte of analytics events and process hundreds of terabytes of data. Our team works efficiently to deliver new features for real-time and batch processing services. We use primarily AWS cloud services and Kubernetes to build and deploy services quickly, at scale and with no downtime.\\n\\nThis team is integral to the RetailMeNot business, so we need engineers who can deliver results while understanding the structure of a large system. We provide cross-team leadership that ensures that RetailMeNot code meets a consistent standard while building the platform of the future. With this team, your daily activities will involve oversight, mentoring, delivering key pieces of functionality, and collaborating with technology leadership to plan the technical roadmap for RMN.\\n\\nWe are constantly evolving both the software and the teams that deliver it. If you’re someone who enjoys taking on new challenges, working in a rapidly changing environment, learning new skills, and applying it all to solve large and impactful business problems, then we want you to be a part of the team.\\nWho You Are\\nYou have 4+ years work experience\\nYou are highly skilled using Scala, Python (Spark), Linux, Docker, Git, and Amazon Web Services (or have translatable experience with similar toolsets)\\nYou have extensive SQL experience on a variety of RDBMS, and enjoy optimizing queries as well as designing efficient data models\\nYou have developed scalable solutions using both SQL and NoSQL (Hadoop) databases. Working with data sets comprised of millions or billions of records is comfortable\\nYou are familiar with one or more cluster-computing frameworks (Spark)\\nYou strive to identify simple solutions to complex problems, can identify a minimal viable product and enjoy iterative development\\nYou are able to accurately estimate tasks, identify dependencies and dedicatedly solve problems to ensure commitments are met\\nYou recognize that your success depends upon enabling your fellow team members to succeed; taking time to help others energizes you\\nYou enjoy gathering requirements from non-technical coworkers and delivering solutions that meet their needs and exceed their expectations\\nYou derive satisfaction from enabling the business to succeed and delighting coworkers, not building technology for its own sake\\nYou have a work ethic that inspires your fellow team members to give their best\\nWhat You'll Do\\nImplement data system for both real-time and warehouse applications\\nDevelop ETL processes that ensure data is accurate and available within SLAs\\nEnhance data models by developing integrations with business partners\\nSeek opportunities for performance improvement and implement optimizations\\nCreate dashboards that provide insight into the health of data integrations, ETL processes and data sets\\nMentor junior data engineers on standard methodologies and provide code review when needed\\nWho We Are\\nWe hire intelligent people and give them the autonomy to be creative, have an impact, and share standard methodologies.\\nWe have a generous leave policy for new parents and 401k matching.\\nWe have a thriving Diversity and Inclusion program that gives back to the community and supports multiple Austin events and organizations with like-minded goals throughout the year.\\nWe serve breakfast Mondays and Fridays, lunch four days a week, provide all the snacks you could dream of, and have our own coffee bar run by trained baristas.\\nWe have a Friday board game happy hour for co-workers to mingle or just relax.\\nWe provide reimbursements for cell phones and gym memberships.\\nWe have an outstanding open vacation policy.\\n\\nRewards*\\nWe offer an opportunity to be an integral part of a company that eagerly pursues disruption in its space to continue to drive innovation and lead the competition. Benefits of being an employee of RetailMeNot, Inc. include, but are not limited to the following:\\n- Competitive base & bonus packages; salary negotiable\\n- Long Term Incentive Plan\\n- Performance based rewards & recognition for your hard work and service\\n- Very competitive benefits packages, including best-in-class parental leave\\n- Open & flexible PTO\\n- Cell phone & gym membership reimbursements\\n- Fully stocked break room & onsite catered breakfasts & lunches multiple days/week\\n\\n*Some rewards do not apply to contract workers or interns.\\nAbout Us\\nRetailMeNot, Inc. is a leading savings destination bringing people and the things they love together through savings with retailers, brands, restaurants and pharmacies. RetailMeNot makes everyday life more affordable through online and in-store coupon codes, cash back offers, discount gift cards, and the RetailMeNot Genie browser extension. Savings are also provided in consumers’ mailboxes through the RetailMeNot Everyday™ direct mail package, and at the pharmacy with RxSaver by RetailMeNot.\\n\\nRetailMeNot is a wholly owned subsidiary of Harland Clarke Holdings. http://www.retailmenot.com/corp or follow @RetailMeNot on social media.\\n\\nU.S. Equal Employment Opportunity/Affirmative Action Information\\nIndividuals seeking employment at RetailMeNot, Inc. are considered without regards to race, color, creed, religion, gender, gender identity, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, or any other protected classification. You are being given the opportunity to provide the following information in order to help us align with federal and state Equal Employment Opportunity/Affirmative Action record keeping, reporting, and other legal requirements.\",\n",
       "  'About JASK\\n\\nJASK was founded in 2015 and is headed by industry leaders with decades of experience solving real-world security operations center (SOC) issues.\\n\\nThe JASK team is dedicated to modernizing security operations to reduce organizational risk and improve human efficiency. Through technology consolidation, enhanced AI and machine learning, the JASK Autonomous Security Operations Center (ASOC) platform automates the correlation and analysis of threat alerts, helping SOC analysts focus on highest-priority threats, streamlining investigations and delivering faster response times.\\n\\nBacked by Kleiner Perkins, Dell Technologies Capital and TenEleven Ventures, JASK is dual-headquartered in San Francisco and Austin.\\n\\nOVERVIEW\\n\\nAt JASK, we are building a data platform designed to power the analytics and investigations that are common in the Security Operation Centers of large enterprises. It is designed to accept hundreds of billions of events from security-relevant data sources (detection products, network sensors, log shippers, inventory systems) per day. It is cloud-native, with no plan to support an on-premise deployment. It is multi-tenant, and is designed to simultaneously process events from thousands of our customers. And, it is security-focused it is designed to perform the kinds of stateful analyses that security analysts demand.\\n\\nWe are building this platform the way startups should with ruthless prioritization, and with a live and demanding customer base. By joining as a Data Engineer, you have the opportunity to make our vision a reality, one feature at a time.\\n\\nWhat You Will be Doing\\nBuilding, improving, maintaining, and scaling stream processing services.\\nWriting code. Reviewing code. Revising code.\\nGiving feedback on our standards. Holding your team mates to them.\\nCollaborating with teammates on major feature designs. Sometimes, you will own features, sometimes others will.\\nHelping our team grow organically. We value referrals. We value your feedback on candidates.\\nWho You Are\\nYou are a software engineer. (We treat our data systems as software systems, and engineer them accordingly.)\\nYou love working with data. (Small data. Big data. All the data.)\\nYou are excited to optimize for events per second (not requests per second).\\nYou have experience working with (or, at minimum a desire to work with) the technologies we use: Kafka, RocksDB, ElasticSearch, JanusGraph, Postgres, Spark, HBase.\\nYou know (and want to write software in) Scala.\\nYou love collecting data about your software as much as writing software that collects data. We measure everything. We make data-driven decisions.\\nYou are collaborative. Nothing this hard can be accomplished by working alone. We work as a team.\\nWHY JASK\\n\\nAt JASK, we are dedicated to transforming the way organizations manage security operations with legacy solutions. Through innovation, collaboration and teamwork we will drive innovation in autonomous SOC technology.\\n\\nOur Company Values:\\n\\nResults. Above all else, we expect results. We honor our commitments. We say what we do and do what we say. We value quantitative measurements over qualitative ones. We deliver results.\\n\\nPerseverance. We get up after we fall down or get pushed. We never, ever, ever quit.\\n\\nCollaboration. Consensus building and collaborative planning. All voices should be heard and considered. Only then will decisions be made and acted upon.\\n\\nIntegrity and Respect. Ethics and morals. Respect people of differing ethnicities, cultures, languages, backgrounds, educations, lifestyles, ideas and experiences. We assume positive intent from everyone.\\n\\nExcellence. Each person should seek excellence in everything they do, every day. Consciously challenge each other to exert their best in their role to advance the company mission.\\n\\nJASK is most decidedly an equal opportunity employer. We want applicants of diverse background and hire without regard to color, gender, religion, national origin, citizenship, disability, age, sexual orientation, or any other characteristic protected by law',\n",
       "  \"At Saatva we are looking for someone passionate about data to join our engineering team and assist in the development of our data pipeline and warehouse solutions.\\n\\nAs a Data Engineer at Saatva, you will:\\nDesign and build scalable data pipeline solutions\\nDesign schemas for efficient storage and query execution\\nParticipate in data model design and implementation processes\\nEnhance data models by developing integrations with business partners and build the infrastructure required for optimal ETL of data from a wide variety of data sources\\nWork with data scientists, analysts, software engineers, executives, and product managers to design and deliver products and functionality to address analytical and functional data needs\\nWork alongside site reliability engineers to ensure the availability and performance of our data platform\\nWhat you need to bring to the table:\\n1+ years of professional experience in data engineering and / or software development\\nGood knowledge of SQL and query performance optimization\\nDevelopment experience with Python and REST APIs\\nComfortable with AWS data and analytics products\\nExperience with Tableau or other data visualization tools\\nBachelor's degree in Computer Science or a related discipline\\nBonus\\nExperience with data science / machine learning tools on AWS\\nAffirmative Action/Diversity Inclusion at Whitestone Home Furnishings, LLC:\\n\\nAt Whitestone Home Furnishings, LLC, we are committed to a culture of diversity and inclusiveness, as demonstrated through our recruitment, retention and employee development programs. We maintain an environment that respects and builds on the assets and talents of each person, valuing their differences. We also engage in good faith efforts to maintain an environment free from discrimination and harassment in strict compliance with applicable laws, and consider all qualified candidates for employment without regard to their race/ ethnicity, national origin, color, religion, gender, sexual orientation, gender identity or expression, age, disability or medical condition, protected veteran or military status, criminal record history, marital status, or status in any group or class protected by applicable federal, state or local law. We also engage in affirmative action to employ and advance in employment qualified women, minorities, disabled individuals and protected veterans. Maintaining a diverse and inclusive workforce is a win/win, and provides Whitestone Home Furnishings, LLC with the opportunity to leverage our top talent to provide innovative solutions to our clients.\",\n",
       "  'Job Description\\n\\nAre you passionate about data ecosystems and the ability to use data to drive actionable change? If so this role with our team at Atlassian is for you. As a Data Engineer on the Customers Success and Support team, you\\'ll be driving our business to scale via building and improving our data infrastructures and data pipeline.\\nSome examples of what you will be doing are:\\nArchitect, build, launch, and manage data models to enable analytics\\nDesign, build, and manage data warehouse\\nDesign, build, improve, and manage data pipelines and ETL jobs\\nCreate ETL scripts via SQL/HiveQL/SparkSQL\\nAutomate data pipeline and reporting processes\\nBuild data expertise and own data quality for the awesome pipelines you build\\nSome of the tools/languages/systems you\\'ll use are:\\nHive, Spark, Postgres, Presto\\nAmazon EC2, EMR, S3\\nPython\\nDocker\\nTableau\\nSourceTree and Bitbucket\\nLinux Shell\\nYou\\'ll work together with other data engineers, analysts, project managers, and subject matter experts to deliver impactful outcomes to the organization. You\\'ll participate in multiple concurrent high-visibility projects along with occasional ad-hoc questions from your internal customers.\\n\\nWe continually require modifications to the data pipeline for improvements in quality, speed, and features. In this role you\\'ll focus on building out the future state of our data pipeline. You\\'ll help design event collection infrastructure, build data models, and ETL processes to collect, extract, and clean the data for subsequent reporting and analysis. The target is making our data model more scalable, reliable, maintainable, and better integrated with other parts of our data ecosystem.\\n\\nMore about you\\n\\nYou\\'ve been in a data engineering role for 5+ years and have a BS degree in Engineering, Computer Science, or other technical discipline. You value high-quality work with attention to detail and have a track record of delivering both. You combine curiosity with critical thinking and good judgment, and like asking \"why\" to unravel a seemingly complex problem and get to the root cause. You know how to gather, document, and interpret business requirements.\\n\\nYou\\'re a wizard with SQL; better than anyone else you know. You\\'ve got practical experience working with large structured or unstructured datasets. You enjoy thinking about improvements to the ways in which data is consumed and then figuring out how to make it happen via reporting platforms and visualizations. You have a nearly insatiable desire to learn new concepts and technologies and apply them to your work.\\n\\nWhen you encounter a problem you come up with multiple solutions, weigh the tradeoffs and efforts, identify the best path forward, and exercise good judgment to drive ahead. You\\'re comfortable interacting with people across all levels of an organization and can field questions during a presentation like a pro. You\\'re self-driven and find ways to be impactful.\\n\\nYou\\'re quick on your feet and take on challenges with ease. You can take an ambiguous assignment and derive valuable insight. You use multiple tools and methods to find solutions, and couple that with intuition and quick tests to prioritize how to unravel complicated problems.\\n\\n\\nMore about our team\\n\\nYou\\'ll be joining a growing analytics and project delivery team located in multiple regions across the globe. We challenge each other constantly to improve our work and ask hard questions. We\\'re direct, focused, and demand excellence, but there\\'s laughter in every meeting because we thoroughly enjoy the work we do and the impact it has. We\\'re constantly growing, learning, adapting, and trying new things. BBQ, tacos, and coffee are a few of our favorite things.\\n\\nMore about our benefits\\n\\nWhether you work in an office or a distributed team, Atlassian is highly collaborative and yes, fun! To support you at work (and play) we offer some fantastic perks: ample time off to relax and recharge, flexible working options, five paid volunteer days a year for your favourite cause, an annual allowance to support your learning & growth, unique ShipIt days, a company paid trip after five years and lots more.\\n\\nMore about Atlassian\\n\\nCreating software that empowers everyone from small startups to the who’s who of tech is why we’re here. We build tools like Jira, Confluence, Bitbucket, and Trello to help teams across the world become more nimble, creative, and aligned—collaboration is the heart of every product we dream of at Atlassian. From Amsterdam and Austin, to Sydney and San Francisco, we’re looking for people who want to write the future and who believe that we can accomplish so much more together than apart. At Atlassian, we’re committed to an environment where everyone has the autonomy and freedom to thrive, as well as the support of like-minded colleagues who are motivated by a common goal to: Unleash the potential of every team.\\n\\nAdditional Information\\n\\nWe believe that the unique contributions of all Atlassians is the driver of our success. To make sure that our products and culture continue to incorporate everyone\\'s perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status.\\n\\nAll your information will be kept confidential according to EEO guidelines.',\n",
       "  'Square Root is built on understanding our customers\\' data more deeply than they do, and our data engineers are instrumental in that. In this role, you\\'ll take heterogeneous, unstructured data, mine it for insights, and apply it to business problems in innovative ways. While you don\\'t need tons of experience, you\\'ll need to be sharp and possess a genuine interest in using data to solve business problems. You\\'ll be working side by side with experienced cloud architects and data engineers. We want someone that will grow with us and is comfortable with the idea of programming, algorithmic thinking, and automated testing.\\n\\nSound like your kind of challenge? Dig in to learn more!\\n\\nThe Gig\\nYou\\'ll design, develop, and improve our ETL Engine to allow for rapid customer implementations, minimal customer investment, data quality and consistency, flexibility, durability, and availability.\\nYou\\'ll design scalable systems that work in concurrency and are fault-tolerant.\\nAs part of our implementation process, you\\'ll work directly with customers to integrate their business data. You\\'ll work closely with different teams at Square Root to deliver enhancements, ensure operational stability, and define + refine product features.\\nYou\\'ll help expand the scalability of our system and maintain our customer relationships as we adapt to more clients and larger data volumes.\\nWe\\'re all about driving action from data, and that includes putting our enterprise data at the fingertips of all Radicals to empower our team.\\nYou\\'ll formulate metrics for business users using math, forecast models, and statistical packages with Python to compute them.\\nAbout You\\nA Master\\'s degree in Computer Science, Computer Engineering, Information Technology, Information Sciences, or a related technical field.\\n2+ years\\' experience in a professional environment. You\\'ve practically applied engineering principles to the design and development of a data warehouse and scalable ETL pipeline.\\n2+ years\\' experience in SQL and Python. You must be able to write joins and use aggregate functions in SQL as well as understand data structures in Python.\\n1+ years\\' experience working with visualization technology such as Tableau or Looker.\\n1+ years\\' experience in cloud computing, including Amazon S3 and AWS EMR as well as familiarity with Microsoft Azure or Google Cloud Platform.\\n1+ years\\' experience with Big Data technologies such as Hadoop, Spark, Hive, and Kafka.\\n1+ years\\' experience with Linux and Jenkins or similar technology.\\nYou\\'re familiar with a data lake pattern and understand the process of a slowly changing dimension.\\nYou enjoy digging into a problem and investigating potential causes and solutions. You think creatively and use your reasoning skills to discover connections and potential strategies.\\nYou\\'re a thoughtful communicator able to articulate data insights to non-technical customers.\\nYou\\'re an active listener. Once you understand internal and external customer problems, you\\'re excited to propose and debate solutions which elevate the team or business.\\nYou\\'re calm in a crisis and have examples of navigating difficult situations and relationships.\\nYou keep up with the latest and greatest in technology and bring insights back to the team.\\nYou\\'ve demonstrated an eagerness to learn, ability to adapt and perfect your work, and willingness to seek out help and put it to good use.\\nOur Radical Culture\\n\\nOur culture is at the core of everything we do. As we grow, we\\'re not only looking to hire the best and brightest, but we\\'re also looking for people that share our values. This is the code we live by:\\nThink big. Do bigger. Big ideas are meant to be pursued. We have a bias for action, iteration, and impact.\\nBe Customer Inspired. Our customers\\' toughest challenges inspire us to build innovative software. We delight them by deeply understanding their business and driving results.\\nPartner. We\\'re approachable, dependable, and collaborative. We go above and beyond to help our customers, our partners, and one another succeed.\\nThrive. We revere personal and professional growth. We recognize individuality, embrace authenticity, and celebrate each other\\'s success.\\nThe Good Stuff\\n\\nFounded in Austin, Texas, we\\'ve been bootstrapped and profitable since our start in 2006. Along the way, we\\'ve added a slew of perks inspired by our team of Radicals. Here are some fan favorites:\\nFlexibility: no dress code, unlimited PTO, and paid parental leave\\nCompetitive benefits and compensation\\nA cozy campus of 1920\\'s craftsman homes in downtown\\nTeam lunches, stocked kitchen + bar, coffee-snob compliant coffee machines\\nRadicals get $3,000 a year to learn anything (seriously!). PEOPLE Magazine recognized our Learn Anything program in their 2019 list of \"50 Companies that Care,\" located here.\\nWe\\'re always looking for people to add to and evolve our culture + company. We want you to bring your unique experience and perspective, so don\\'t worry about checking all the boxes. If our values and mission resonate with you, please apply! Additionally, Square Root is an Equal Opportunity Employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, gender and sexual identity, national origin, disability status, protected veteran status or any other characteristic protected by law.',\n",
       "  \"We are looking for a highly motivated Data Engineer to join our Data Engineering Team. You will be part of a center of excellence that enables best in class leveraging of data to support advanced analytics. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis? If so, H-E-B Data Solutions team is seeking an expert Data Engineer to build high quality, scalable and resilient distributed systems that power H-E-B's analytics platform and data pipelines.\\n\\nWe use a diverse technology stack such as Teradata, MicroStrategy, Hadoop, Kafka, Spark, and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job. The Partner will be able think outside of the box and should have passion for building analytics solutions to enable business in making time sensitive and critical decisions.\\n\\nKey Qualifications\\n\\n· In-depth understanding of data structures and algorithms\\n\\n· experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data\\n\\n· Database development experience with Relational or MPP/distributed systems such as Oracle/Teradata/RedShift/Hadoop\\n\\n· Programming experience in building high quality software in Java, Python or Scala preferred\\n\\n· Experience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs\\n\\n· You will demonstrate excellent understanding of development processes and agile methodologies\\n\\n· Strong analytical and interpersonal skills\\n\\n· Self-driven, highly motivated and ability to learn quick\\n\\n· Experience with or advance courses on data science and machine learning is ideal\\n\\n· Work/project experience with big data and advanced programming languages is a plus\\n\\n· Experience developing Big Data/Hadoop applications using java, Spark, Hive, Nifi, Kafka, and Map Reduce is a huge plus.\\n\\nDescription\\n\\nYou will build and design data structures on MPP platform like Teradata, Hadoop to provide efficient reporting and analytics capability. Design and build highly scalable data pipelines using new generation tools and technologies like Spark, Kafka to induct data from various systems. Translate complex business requirements into scalable technical solutions meeting data warehousing design standards. Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency Build dashboards using Self-Service tools like Tableau and perform data analysis to support business Collaborate with multiple cross functional teams and work on solutions which has larger impact on H-E-B.\\n\\nEducation & Experience\\n\\n· 6 years of professional experience as a data engineer or in a similar role\\n\\n· 4 years of database development experience with relational databases such as Oracle/MS SQL/PostgreSQL\\n\\n· Bachelor’s degree in Computer Science or Engineering, or equivalent experience\\n\\n· Demonstrated ability to adapt to new technologies and learn quickly.\\n\\nDEVS3232\",\n",
       "  \"</strong>\\nRole Number:\\n200068319\\nAre you ready to apply your educational experience to real-world problems? Are you passionate about applying your data skills in a real-world tech environment? Imagine what you could do here.\\n\\nAt Apple, new ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Join Apple, and help us leave the world better than we found it. Apple’s Strategic Data Services (SDS) team is responsible for mitigating fraud, waste and abuse company-wide while optimizing and empowering our customers and internal partners. SDS Data Science Engineering is building an environment to enable ground breaking data analysis over Petabytes of data. We work side-by-side with data scientists and implement scalable, easy-to-use systems and tools. We are seeking a customer-focused, passionate and driven Data Science Engineer with experience in building analytic tools and solutions.\\n<b>\\nKey Qualifications\\nMastery of one of Python, Java, Scala, C++ or equivalent language.\\nExperience with Relational databases and NoSQL databases.\\nDemonstrated understanding of the full software development lifecycle.\\nExcellent problem solving, critical thinking, and communication skills.\\nSolid grasp of computer science fundamentals including data structures and algorithms.\\nSolid ability to evaluate and apply new technologies in a short time.\\nSelf-motivated, proactive, and solution-oriented.\\n<b>\\nDescription\\nWith the expansive data we have, our job is to build meaningful data relationships and engagement experiences for our internal customers. If you’re interested in being a part of a team that’s constantly learning and problem-solving, we’d love to talk with you. As a Data Science Engineer on the SDS team, you will work closely with Data Scientists and other Data Science Engineers to lead the design and implementation of systems and tools to support the fraud prevention efforts of SDS.\\n\\nYou will be:\\n\\n•Developing and implementing production software for preventing fraud.\\n•Responsible for system architecture design.\\n•Working with external infrastructure teams to drive the development of infrastructure needs. •Innovating by recognizing opportunities for automation and tools improvements.\\n•Responsible for developing and implementing process improvements to bring efficiency and stability to fraud analytics.\\n•Responsible for technical leadership for a team of data scientists.\\n•Lead the team to increase the level of maturity and skill in analytical software development. •Responsible for release engineering.\\n<b>\\nEducation & Experience\\nBS degree in computer science or equivalent field plus 7-10 years experience, or equivalent.\\n<b>\\nAdditional Requirements\",\n",
       "  'General Information\\n\\nRef #: 27451\\n\\nFunctional Area: Technology\\n\\nEmployee Type: Full Time\\n\\nLocation: Austin\\n\\nExperienced Required: Please See Below\\n\\nEducation Required: Bachelors Degree\\n\\nJob Posting Shift: 1st\\n\\nDate published: 28-Jun-2019\\n\\nAbout Us:\\n\\n\\nWe are PIMCO, a leading global asset management firm. We manage investments and develop solutions across the full spectrum of asset classes, strategies and vehicles: fixed income, equities, commodities, asset allocation, ETFs, hedge funds and private equity. PIMCO is one of the largest investment managers, actively managing more than $1.84 trillion in assets for clients around the world. PIMCO has over 2,700 employees in 17 offices globally. PIMCO is recognized as an innovator, industry thought leader and trusted advisor to our clients.\\n\\nPIMCO is one of the world’s premier fixed income investment managers with thousands of professionals around the world united in a single purpose: creating opportunities for our clients in every environment. Since 1971, we have brought innovation and expertise to our partnership with the institutions, financial advisors and millions of individual investors who entrust us with their assets. We aspire to cultivate performance and leadership through empowering our people, diversity of thought, and a commitment to an inclusive culture that engages in our global communities.\\n\\nPosition Description:\\n\\n\\nAs a Sr. Data Engineer on our team we will be working on a number of high-profile projects that will require you to collaborate with key partners and develop data solutions that enable insights into our clients through an evolving data architecture and drive our next generation data platform. A data engineer will meet with relevant partners, understand and model data assets, and constantly find opportunities to optimize and evolve the underlining data platform. You will work with software developers, data architects, data analysts, and data scientists to solve complex business problems. Demonstrating lifelong learning and collaboration to fill gaps in knowledge is essential.\\n\\nPosition Requirements:\\n\\n\\n5+ years of experience with building end-to-end scalable production-grade data pipelines\\nIn-depth knowledge of data warehousing and master data management\\nExpertise with modern data architecture, data modeling, and data management principles\\nHands-on experience with modern data pipeline technologies (Spark, Flink, Airflow, Beam, etc.)\\nSolid foundation in data structures, algorithms, and software design\\nExpertise with Relational and NoSQL databases to help teams best organize their data for analysis\\nSkilled in OOP design and development, preferably in Python\\nAdvanced knowledge in writing, understanding and tuning PL/SQL and/or T-SQL\\nBS/BA degree in Computer Science, Engineering or related fields or equivalent experience\\n\\nPREFERRED QUALIFICATIONS\\nDomain knowledge of Financial Services\\nHands-on experience with working with Cloud technologies, including AWS\\nSkilled at designing and implementing ETL frameworks\\nExperience with CI/CD methodologies\\nExperience with *nix environments, including shell script development\\nWe are PIMCO, a global investment management firm with a singular focus on preserving and enhancing investors’ assets. We manage investments for institutions, financial advisors and individuals, helping millions of people around the world meet their financial goals.\\n\\nOur technology powers the firm’s global trading platform. We employ sophisticated and cutting edge technology tools that support PIMCO’s core investment management strategy.\\nWhy PIMCO? At PIMCO you will join a dynamic, constantly evolving global firm that pushes you to grow, lead and innovate. You will be client- focused and work on technologies that will be put to immediate use. You will be part of a team whose members are encouraged to speak up with an idea or challenge existing views, regardless of title or tenure. You will have the opportunity to receive competitive compensation and other attractive benefits (Please see below).\\n\\nPIMCO’s Technology Team is organized in small, focused, agile groups, that either work closely with business units to deliver value or develop core technologies that lever the product teams. Our environment fosters innovation and promotes entrepreneurial spirit, and we use top of the line tools. PIMCO recognizes the paramount role of tech now and in the future and invests in technology accordingly. Technology careers are available in Newport Beach, Austin, New York, London, Munich, Singapore and Tokyo.\\n\\nBenefits:\\n\\n\\nPIMCO is committed to offering a comprehensive portfolio of employee benefits designed to support the health and wellbeing of you and your family. Benefits vary by location but may include:\\nMedical, dental, and vision coverage\\nLife insurance and travel coverage\\n401(k) (defined contribution) retirement savings, retirement plan, pension contribution from your first day of employment\\nWork/life programs such as flexible work arrangements, parental leave and support, employee assistance plan, commuter benefits, health club discounts, and educational/CFA certification reimbursement programs\\nCommunity involvement opportunities with The PIMCO Foundation in each PIMCO office',\n",
       "  'Data Intelligence Engineer\\n\\nRound Rock,TX\\n\\nDell provides the technology that transforms the way we all work and live. But we are more than a technology company — we are a people company. We inspire, challenge and respect every one of our over 100,000 employees. We also provide them with unparalleled growth and development opportunities. We can’t wait for you to discover this for yourself as a Data Intelligence Engineer in our Dell Digital IT Analytics and Data Science organization team in Round Rock, TX.\\n\\nThe Dell Digital IT Analytics and Data Science organization is responsible for insights and recommendations to optimize the user experiences. This includes analysis focused on application speed, stability, and user experience. Such work includes data ingestion, data munging, and statistical analysis. Effective outputs require simple user-friendly synthesis of information, including well-articulated insights.\\n\\nThe team needs individuals with strong business and analytic knowledge to deliver high value insight and recommendations. We are looking for a candidate who has a strong background in analytics, with proven abilities to be innovative and creative, as well as strong collaborator and communicator enabling influence.\\n\\nResponsibilities:\\nProvides actionable recommendations on how to optimize the user experience, by considering speed, stability, and key user features\\nDelivers on analytics roadmap, while always identifying new opportunities and prioritization recommendations\\nWork with data engineers, data scientist, architects and business to design actionable strategic projects and break complex problems into smaller actionable tasks.\\nLeverage analytical insights to drive support product development, operational optimization to improve key business/IT metrics.\\nLead cross-functional engagements to define problem statements, collect data, build analytical models and make recommendations.\\nEstablish relationship with key business partners or Point of Contacts to accelerate issues fix.\\nContribute to improve business Processes and reduce overall issues.\\nRequirements:\\n5+ years of related experience with a bachelor’s degree; or 3+ years with a master’s degree in Engineering, Computer Science, or Business Degree; or equivalent experience.\\nExperience in most big data and statistical analytic tools: Python, Hadoop (Hive), SQL, Splunk.\\nExperience in Salesforce Lightning CRM or Financial Force or ClickSoftware or OpenText application Analytics, development or Support.\\nExperience in machine learning algorithms/concepts.\\nHands-on analytics capabilities, expertise in analytical problem-solving techniques and frameworks and the ability to deal with large volumes of data.\\nStrong teamwork, project management skills, business acumen skills.\\nStrong communication skills, including the ability to convey analytic insights effectively to both IT and business audiences.\\nPreferences:\\nMBA or Engineering Degree from top schools\\nManagement Consulting background\\nExperience in one of e-commerce analytics tools: SiteCatalyst, Discover, Insights, Google Analytics\\nUnderstanding of support and deployment processes and or broader Tech Support ecosystem is a plus\\nDell is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Dell are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Dell will not tolerate discrimination or harassment based on any of these characteristics. Dell encourages applicants of all ages. Learn more about Diversity and Inclusion at Dell here.',\n",
       "  'Description:\\nPrimary Job Duties:\\nBuild and maintain Bioinformatics data processing environments.\\nCreate DDL scripts for new schemas and support for database updates, restructuring, etc.\\nWork with enormous amounts of data loading, migration, etc. and work on optimizing CRUD operations\\nBuild and maintain SQL scripts and PL/SQL procedures.\\nBuild and maintain Java based web services.\\nAnalyse and troubleshoot data and web service issues and implement smart improvements and solutions.\\nMinimum Qualifications:\\nA Bachelors Degree in Computer Science, Engineering, Biology, or equivalent experience.\\n3-5 years of experience\\nHighly proficient in Oracle SQL and PL/SQL.\\nVery good working knowledge of Oracle and MySQL databases.\\nVery good knowledge of Core Java.\\nGood understanding of Java EE (Servlets, JSP).\\nGood working knowledge of scripting languages like PERL.\\nGood working knowledge of UNIX / Linux systems.\\nBasic knowledge of Bio Sciences and Genetics is highly preferred.\\nFamiliarity with Bioinformatics tools is a plus.\\nPreferably has experience with LSF - Load Sharing Facility.\\nExperience with Agile Software Development process a plus.\\nExcellent communication / documentation skills.\\nMust be detail oriented and a self-starter.\\nMulti-tasking with good follow through skills, good communication skills.\\nAbility to work well in a team environment.\\nStrong problem solving, debugging and troubleshooting skills using latest tools and technology.\\nAbility to work alone and accomplish tasks without supervision.\\nprovided by Dice',\n",
       "  \"ABOUT THIS ROLE\\nAs a member of our engineering team, you will work on the design, implementation and delivery of data platform frameworks, pipelines, microservices, and other features that build on our high-value data assets. Your customers will include data analytics, marketing, and leadership teams as well as our Care Partners (hospitals and physicians) and external data partners, in concert with our Enterprise Data Warehouse delivery teams.\\nRESPONSIBILITIES\\nEvaluate and build proofs of concept for Cloud PaaS and IaaS offerings for data liquidity, data management and storage, data pipelines built on both traditional ETL as well as streaming platforms, master data management, data stewardship, record-linkage, NLP services, and more.\\nWrite traditional code and server-less functions using the right language for the task, which may be SQL, Python, C#, Java, PowerShell, SSIS/BIML, and others.\\nParticipate in build-buy-open source decisions for parsing and managing industry standard formats such as FHIR/NDJSON, pipe-and-hat HL7, and x12 EDI\\nEvaluate, select, and apply Cloud and OO design and resiliency patterns\\nBuild APIs and data microservices to share our data with internal and external partners, and write interfaces to public data sets to enrich our analytics data stores\\nProvide subject matter expertise on performance tuning and query optimization to full-stack peers, data analysts, and EDW developers\\nParticipate in building and owning a DevOps culture\\nContinuously document your code, framework standards, and team processes\\nEDUCATION, TRAINING, AND PROFESSIONAL EXPERIENCE\\n1 - 5 years of experience in an enterprise or commercial software development environment\\nExtensive experience developing data-intensive solutions against an RDBMS, such as SQL Server, Postgres or Oracle.\\nHighly skilled writing SQL queries, DML and DDL, CDC/change tracking patterns, indexes and performance tuning.\\nEnterprise development experience coding in at least one, but preferably more than one, procedural/OO language, e.g., C#, Java, JavaScript, Python, C++, PowerShell\\nProficiency in using OOTB components, as well as implementing custom components or frameworks, on at least one traditional ETL platform, preferably SSIS, Informatica, or Talend\\nTeam player who is not afraid to ask questions, take risks, share in owning team victories as well as team failures\\nGood communicator – both written and verbal – with high emotional intelligence\\nAbility to focus on MVP and shipping software while remaining cognizant of the long-term costs of technical debt\\nHealthcare data background a must\\nMUST HAVE THE RIGHT TO WORK IN THE US WITHOUT VISA SPONSORSHIP\\nIdeal candidates come to the table with one or more additional competencies, such as:\\nExposure to Enterprise Data Warehouse , Data Lake, Big Data, unstructured data, in-memory data stores\\nFamiliarity with NoSQL database systems such as MongoDB, Cassandra, CosmosDB, neo4j etc.\\nFamiliarity with Kimball-like star and snowflake data models and columnstore Indexing\\nExperience building metadata-driven data pipeline frameworks for quickly mapping, onboarding, and ingesting data from a wide variety of partner sources\\nEnterprise experience with data movement and management in the Cloud utilizing some combination of Azure and/or AWS features such as Data Factory, Blob Storage, Service Bus, Kafka, Redis, S3 Buckets, Azure Automation, Machine Learning, elastic search, Glue etc.\\nData Science training or experience to better understand and collaborate with one of our key data consumers (notably, this is still an engineering role and not a data science role)\\nCRM experience, such as MS Dynamics or SalesForce\\nABOUT US\\nAt Bright Health, we brought together the brightest minds from the health care industry and consumer technology and together we created Bright Health: a new, brighter approach to healthcare, built for individuals. Our plans are easy to manage, personalized and more affordable, giving people the quality care they deserve. Through our exclusive care partnerships with leading health systems in local communities we are reshaping how people and physicians achieve better health together.\\nBright Health is tripling its footprint in 2019 to offer a variety of health insurance plans to more individuals. Bright Health operates health insurance offerings across Individual and Family Plan segments and the Medicare Advantage space in Alabama, Arizona, Colorado, Ohio, New York and Tennessee.\\nWe’re Making Healthcare Right. Together.\\n\\nWe've won some fun awards like: Great Places to Work, Modern Healthcare, Forbes, etc. But more than anything, we're a group of people who are really dedicated to our mission in healthcare. Come join our growing team!\\nAs an Equal Opportunity Employer, we welcome and employ a diverse employee group committed to meeting the needs of Bright Health, our consumers, and the communities we serve. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.\\n\\nBRIGHT ON!\",\n",
       "  \"About Dosh:\\nDosh is revolutionizing the way the world does cash back. We are the fastest growing app that finds cash for consumers and helps businesses acquire and retain customers. Dosh subscribers (Doshers) get cash back for traveling, shopping, and dining out at thousands of merchants nationwide. Using breakthrough technology to eliminate wasted advertising spend, Dosh puts that money back into the pockets of consumers and businesses.\\n\\nWe’re a two-sided marketplace offering merchants/brands an online-to-offline performance based advertising solution through card-linked offers. For our Doshers, we are providing a completely frictionless way to get cash back. No cutting coupons. No scanning receipts. No hassle. Just cold, hard cash. Doshers can then transfer the cash directly into a bank account, PayPal, Venmo, or donate to a charity conveniently through our app! Our product is disrupting the trillion dollar advertising industry by connecting merchants directly with their consumers.\\n\\nImpact:\\nAs a Data Engineer at Dosh, your role is to deliver the information your business partners need to grow the business. You will assist business teams in drawing insights from our data. You are someone who wants to see the impact of their work and make a difference every day. You know what it takes to deliver quality reporting and business intelligence solutions to the organization. You understand the value and benefit of solid data practices and how to translate that into satisfied business partners.\\n\\nYou are an experienced Software Engineer. You are skilled in extracting, transforming, and loading data. You are able to translate business requests into database or streaming data designs. You have experience working with relational and nosql databases for consumption by numerous consumer applications and business orgs.\\n\\nYou are someone who is passionate about data-driven approaches. You enjoy exploring large data sets and get excited about learning new technologies and learning in a collaborative environment. You are skilled at eliciting requirements from a wide range of different teams.\\n\\nWhat you’ll bring:\\nExperience leveraging SQL, Python, Scala, Matlab or other tools to manipulate data and set up automated processes as per business requirements\\nFamiliarity with analytical techniques and machine learning workflows\\nFamiliarity with financial data sets and use cases\\nStrong familiarity with AWS\\nKnowledge of Lambda architectures\\nExperience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets\\nExperience with Big Data Technologies (S3, Kinesis, Hadoop, Hive, Hbase, Pig, Spark, etc.)\\nExperience leading large-scale data warehousing and analytics projects, including using AWS technologies – Redshift, S3, EC2, Data-pipeline and other big data technologies\\nExperience building data products incrementally and integrating and managing datasets from multiple sources\\nStrong ability to interact, communicate, present and influence within multiple levels of the organization\\nTrack record of manipulating, processing, and extracting value from large datasets\\nExcellent communication skills to be able to work with business owners to develop and define key business questions and to build data sets that answer those questions\\n\\nWhy Dosh:\\nThe opportunity to join us on our mission to positively impact millions of people's lives financially\\nA ground floor opportunity with the Dosh team\\nThe chance to work in a fast-paced start-up environment with experienced industry leaders\\nFun, driven, innovative and supportive co-workers who will push you to be your best\\nCompetitive compensation and benefits\\n\\nAt Dosh, we celebrate and support all differences. Dosh is proud to be an equal opportunity workplace. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex national origin, sexual orientation, age, citizenship, marital, disability & veteran status.\\n\\nAuthorization to work in the U.S. is a precondition of employment. At this time, Dosh regrets that it is unable to sponsor employment visas or consider individuals on time-limited visa status for this position.\",\n",
       "  \"Crowdskout is looking for a Data Engineer that can help us expand our data pipeline infrastructure. Crowdskout's product has most recently been centered in the CRM space, but we are looking to change that. Currently, we process millions of data points through multiple data pipelines to feed into a suite of databases. We are preparing for 10x growth both in the volume of data processed and the speed in which that data can be available and actionable. To accomplish this we are looking for someone who can build out highly scalable data solutions.\\n\\nIf you are highly motivated, super passionate about democracy, and want to join a close-knit team that is looking to build great things together, Crowdskout may be for you. This is a full-time position in Austin, TX; Raleigh-Durham, NC; Salt Lake City, UT; or Chicago, IL.\\n\\nResponsibilities:\\nDesign, build, scale, and maintain multiple data pipelines\\nWork closely with business owners and external stakeholders to provide actionable data\\nEnsure data accuracy and reliability\\nRequirements:\\nExperience building large scale streaming and batch data pipelines\\nExperience using Big Data technologies (Spark, EMR, hadoop, data lakes, etc.)\\nMastery of multiple databases (e.g. MongoDB, MySQL, etc.)\\nUnderstanding of data security best practices\\nExtras:\\nAWS data technologies (e.g. Kenesis, Glue, RDS, Athena, etc.)\\nExperience building out data warehouse infrastructure\\nSoftware development using PHP\\nDevOps or System Admin experience\\nData Science exploration and modeling\\nCrowdskout is an equal opportunity employer that encourages diversity across all spectrums in its hiring, without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, or any other protected factor. With that being said, we wouldn't be able to accommodate candidates in need of work sponsorship at this time since we are a small company. If you find this role interesting and you hit on the elements above, please apply!\",\n",
       "  \"About the Position\\n\\n\\nAt Rev, we realize that the data we possess is one of our most important assets. From data on user interactions with our platform to extensive repositories of AI training data, we have a rich asset that you can help us capitalize on. Until now, accessing this data has been the responsibility of software engineers, product managers and business analysts. Now, Rev is hiring an experienced data engineer to build our data engineering infrastructure from the ground up.\\n\\nWho we're looking for\\n\\n\\nAs the first dedicated data engineer you will take the lead in designing and implementing Rev's data warehouse and data pipeline. The right candidate will be an engineer who is passionate about data engineering, knows industry best practices, has implemented them before and can put those practices into use at Rev. Once hired, you will work closely with the VP of Engineering to assess the data needs of Rev's product, marketing, sales, ops and R&D teams. Then, you will design and implement a data engineering infrastructure that will scale with our growing data needs.\\n\\nResponsibilities\\nWork with the VP of Engineering to select the technologies and tools Rev should use in setting up our data engineering infrastructure\\nBuild a data warehouse to store data from disparate sources (SQL database, mixpanel, google analytics, zendesk, etc)\\nDesign the schema for analytical data storage\\nBuild out Rev's data pipeline\\nWork with consumers of the data throughout the organization to ensure needs are being met\\nQualifications\\n4+ years data engineering experience\\n4+ years of experience with higher level programming language (i.e. Python, Java, JS, etc)\\nDeep understanding of the technologies and tools used in the data engineering field\\nHands on experience building scalable data engineering infrastructure\\nExcellent communication skills to work with Rev's engineering leadership and consumers of the data throughout the organization\",\n",
       "  'Key Qualifications:\\nStrong programming skills in Java and Scala\\nExperience with Big Data applications that use Spark, Hive, Kafka, Hadoop, and Oozie\\nKnowledge of build and test tools such as Maven, Gradle, SBT, and JUnit\\nGood understanding of relational and NoSQL databases\\nExperience writing and optimizing SQL queries\\nExperience in developing ETL data pipelines\\nMonitor and improve data and BI processes to meet firm SLAs and increase efficiency\\nDefine user stories as a part of the BI scrum team by analyzing business requirements, defining technical specifications and sizing the development effort.\\nLeverage technology capabilities and standards while working with technology system owners to identify appropriate data sources, define and build required data transformation logic.\\nPerform detailed data analysis to derive insights and deliver operational mechanism such as dashboards or ad-hoc reports.\\n\\nEEO Statement(Equal employment opportunity):\\nCompunnel is a proud equal opportunity employer. We cherish diversity and harbour inclusivity. Any discrimination on basis of race, religion, color, nationality, gender, sexual orientation or other applicable legally protected characteristics is forbidden.',\n",
       "  'Job Description\\nTreehouse Technology Group (TTG) is looking for a self-motivated data engineer to provide ongoing support for data warehousing and data integration projects. This position will be maintaining data models for business intelligence, analytics, and dash-boarding projects across a variety of industries. We love all-things-data here at TTG and we are looking for other ambitious, driven individuals who want to work with us and learn quickly. Critical thinking, business acumen, and communication are heavily valued and deeply encouraged. If you are interested in learning, growing professionally and working with a great team, this position is for you. Send me a message or send an email and cover letter to info@treehousetechgroup.com\\n\\nResponsibilities:\\nProvide ongoing support, changes, and troubleshoot custom data warehouse (OLAP) and ETL solutions\\nProvide ongoing support for custom data integration solutions\\nWork directly with customers to ensure high uptimes and quick response times for systems built by TTG\\nEnsure best practices for data integrity, quality, access and security\\nDesign logical data models and physical database designs optimized for performance, availability and reliability.\\nMaintain systems and ensure that they scale\\nSecondary Job Responsibilities:\\nDesign, implement, update, and maintain dashboards and reports\\nDesign and implement tools to automate database operations and monitoring\\nTuning and optimization of backend and frontend data operations.\\nScripting and automation to support development, QA and production database environments and deployments\\nMinimum Qualifications:\\nPassion for data technology\\nB.S. degree in Computer Science or related discipline, with strong academic record\\n1- 3 years experience in a Database Management role (e.g., DBA, ETL, Data Analyst), and/or working with relational databases, preferably MySQL and Microsoft SQL Server\\nStrong work ethic, sense of ownership and initiative\\nStrong background in Agile software development\\nStrong analytical, programming and problem solving skills\\nSelf-motivated, organized, ability to handle multiple projects\\nPreferred Qualifications:\\nExperience with AWS\\nExperience with SSIS, Talend Open Studio\\nExperience in database tuning, performance optimization, replication\\nExperience with at least one scripting language\\nKnowledge of financial services industry (alt. health care, logistics, cpg, web analytics, marketing)\\n\\nCompany Description\\nTreehouse Technology Group is a technology strategy and software development and integration firm. We provide both product and strategy consulting, in addition to technical implementation and project execution. We believe in an independent approach to technology solutions and work with firms to define, deliver and execute their technology strategy.\\n\\nAccordingly, we perform vendor analysis and technical development based on industry best practices. When an off-the-shelf solution does not meet the needs of our partners, custom application development is necessary to bridge the gap between best-in-breed (specialized) and ERP solutions (general).\\n\\n\\n\\nTTG prides itself on our ability to solve complex business problems with custom-tailored solutions that support the ever-changing business environment of our partners. We focus on business value first, aligning the strategy and direction of our partners with their specific technical needs, before providing a technology roadmap to implement the mutually established vision. By working closely with our partners and obtaining feedback along the way, we guarantee a positive experience that yields fruitful results.',\n",
       "  \"Posted: Oct 14, 2019\\nWeekly Hours: 40\\nRole Number:\\n200041718\\nAt Apple, excellent ideas have a way of becoming extraordinary products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish. Would you like to work in a fast-paced environment where your technical abilities will be challenged on a day-to-day basis?\\n\\nThe people here at Apple don’t just build products — we craft the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that supports the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it.\\nKey Qualifications\\nWe would like for you to have In-depth understanding of data structures and algorithms\\nWe are looking for experience in designing and building dimensional data models to improve accessibility, efficiency, and quality of data\\nDatabase development experience with Relational or MPP/distributed systems such as Oracle/Teradata/Vertica/Hadoop\\nWe are seeking programming experience in building high quality software in Java, Python or Scala preferred\\nExperience in designing and developing ETL data pipelines. Should be proficient in writing Advanced SQLs, Expertise in performance tuning of SQLs\\nYou will demonstrate excellent understanding of development processes and agile methodologies\\nStrong analytical and interpersonal skills\\nSelf-driven, highly motivated and ability to learn quick\\nExperience with or advance courses on data science and machine learning is ideal\\nWork/project experience with big data and advanced programming languages is a plus\\nExperience developing Big Data/Hadoop applications using java, Spark, Hive, Oozie, Kafka, and Map Reduce is a huge plus\\nDescription\\nApple's Global Business Intelligence (GBI) team is seeking a Data Engineer to join a team focusing on building solutions for data privacy controls and features. The team member will build high quality, scalable and resilient solution that power apple's analytics platform and data pipelines to meet data privacy standards. Apple's Enterprise Data warehouse system cater to a wide variety of real-time, near real-time and batch analytical solutions. We use a diverse technology stack such as Teradata, HANA, Vertica, Hadoop, Druid, Kafka, Spark, and Cassandra and beyond. Designing, Developing and scaling these big data technologies are a core part of our daily job.\\n\\nRESPONSIBILITIES\\nDemonstrate strong emotional intelligence to understand people, and their perspectives. Should have willingness to collaborate in cross functional team setup. This would be a top priority.\\nAnalytical mindset to understand data, business needs, and come up with engineering solutions that scale to answer broad problems across the organization balancing complexity and simplicity.\\nProvide ongoing proactive communication and collaboration throughout the organization.\\nDesign and build highly scalable solution using Apple internal frameworks built on Spark, Kafka, Cassandra, Teradata and many other industry standard frameworks.\\nInnovate and drive business process improvements.\\nSoftware engineering mindset with ability to write elegant, maintainable code, and follow engineering best practices.\\nExtend support to production support, framework and platform teams in exigent situation and when required.\\nEducation & Experience\\nBachelor’s Degree\",\n",
       "  'What youll be called: Data Engineer\\n\\nWhere youll work: KWRI HeadquartersAustin, TX\\n\\nNamed a Happiest Company to Work for in 2019; one of the Best Places to Work in Austin, TX; and featured on the Training Magazine Training 125 list seven times, Keller Williams Realty International (KWRI) thrives within a creative and collaborative culture where transforming the real estate industry through technology is our primary goal.\\n\\nKW Technology is the foremost provider of real estate solutions, offering the most comprehensive end-to-end portfolio of products, services and training in the industry. Our Data Engineering team converts agent and consumer challenges into intuitive, insight-enhanced technology and consumer experiences using tools such as Python, Hadoop, Spark, MySQL, MongoDB and Snaplogic.\\n\\nWhat youll do:\\n\\nDesign, develop and implement data infrastructure and best-in-class pipelines that collect, connect, centralize and curate data from various internal and external data sources. You will ensure that architectures support the needs of the business, and recommend ways to improve data reliability, efficiency.\\n\\nEssential Duties and Responsibilities:\\nDesign, develop, and implement data infrastructure and pipelines that collect, connect, centralize, and curate data from various internal and external data sources\\nParticipate in data architecture discussions to understand target data structures, required data transformations and deliver data pipelines/ETL loading processes that meet requirements.\\nPerform detailed exploration of new internal and external source data to perform source-to-target mapping to inform the development of new data pipelines/flows.\\nWork in close collaboration with your data-minded colleagues focused on back-end (microservice) development, business intelligence reporting, machine learning and artificial intelligence models.\\nInvestigate the root cause of data-related issues and implement viable, sustainable solutions to correct issues.\\nPerform database administration activities such as refreshes, updates, migrations, etc. in support of data pipeline maintenance.\\nMinimum Qualifications:\\nBachelors degree in Computer Science, Information Management, Data Science, Analytics or related field or equivalent experience.\\n3 or more years of experience as a data engineer on enterprise-level data solutions, specifically as a Data Engineer or ETL Developer.\\n2 or more years of experience working with relational and unstructured databases and enterprise data warehouses, such as work with MySQL, PostgreSQL, MongoDB, SQL Server, or Oracle.\\nExperience with Spark, Presto, Hive and/or other map/reduce \"big data\" systems and services.\\nExperience in SQL and Python for scripting automation.\\nPreferred Qualifications:\\nMasters degree in Information Management, Data Science, Analytics or related field.\\nExperience building open source data pipeline systems such as AirFlow, Hadoop or Kafka.\\nFamiliar working in a Cloud environment (AWS or GCP) with a subset of the following tools or their equivalent - Redshift, RDS, S3, EC2, Lambda, Kinesis, Elasticsearch, EMR, BigQuery, GCS.\\nWho are we?\\n\\nKeller Williams Realty Inc. is the largest real estate company by agent count across the globe and is number one in units and volume in the United States. Founded in 1983, we pride ourselves on an agent-centric, technology-driven and education-based culture that rewards agents as stakeholders. Keller Williams Realty International (KWRI), is the companys corporate headquarters located in Austin, TX. Here, through a focus on cutting edge technology, education, and products and services, we support our agents and associates to create careers worth having, businesses worth owning, lives worth living, experiences worth giving and legacies worth leaving.'],\n",
       " 'hq_city': ['Boston',\n",
       "  'Boston',\n",
       "  'New York',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Boston',\n",
       "  'Fairfax',\n",
       "  'Framingham',\n",
       "  'New York',\n",
       "  'Lucerne',\n",
       "  'Ashburn',\n",
       "  'Redwood City',\n",
       "  'Boston',\n",
       "  'London',\n",
       "  'Springfield',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Braintree',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Somerville',\n",
       "  'Mc Lean',\n",
       "  'Lucerne',\n",
       "  'Framingham',\n",
       "  'Cambridge',\n",
       "  'Springfield',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Mattapan',\n",
       "  'Boston',\n",
       "  'Seattle',\n",
       "  'Columbus',\n",
       "  'Boston',\n",
       "  'Acton',\n",
       "  'North Brunswick',\n",
       "  'Springfield',\n",
       "  'Natick',\n",
       "  'Boston',\n",
       "  'Indianapolis',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Cambridge',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Bengaluru',\n",
       "  'Woburn',\n",
       "  'Louisville',\n",
       "  'Boston',\n",
       "  'Providence',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Southfield',\n",
       "  'Troy',\n",
       "  'Farmington Hills',\n",
       "  'Detroit',\n",
       "  'Grand Rapids',\n",
       "  'New York',\n",
       "  'Grugliasco',\n",
       "  'Southfield',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Oakland',\n",
       "  'Troy',\n",
       "  'Mason',\n",
       "  'Southfield',\n",
       "  'Seattle',\n",
       "  'Seattle',\n",
       "  'Tampa',\n",
       "  'Reston',\n",
       "  'Detroit',\n",
       "  'Bloomfield Hills',\n",
       "  'Red Bank',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Southfield',\n",
       "  'South Burlington',\n",
       "  'Troy',\n",
       "  'Basking Ridge',\n",
       "  'South San Francisco',\n",
       "  'Dearborn',\n",
       "  'Detroit',\n",
       "  'Aberdeen',\n",
       "  'Detroit',\n",
       "  'Jacksonville',\n",
       "  'Phoenix',\n",
       "  'Fort Worth',\n",
       "  'Olathe',\n",
       "  'Detroit',\n",
       "  'Dearborn',\n",
       "  'South San Francisco',\n",
       "  'Detroit',\n",
       "  'Detroit',\n",
       "  'Toronto',\n",
       "  'Farmington Hills',\n",
       "  'Detroit',\n",
       "  'Livonia',\n",
       "  'Reston',\n",
       "  'Dearborn',\n",
       "  'Detroit',\n",
       "  'Dearborn',\n",
       "  'Dearborn',\n",
       "  'Southfield',\n",
       "  'Fremont',\n",
       "  'Shrewsbury',\n",
       "  'Columbus',\n",
       "  'Boston',\n",
       "  'Dearborn',\n",
       "  'Dearborn',\n",
       "  'Troy',\n",
       "  'Dearborn',\n",
       "  'Detroit',\n",
       "  'Troy',\n",
       "  'Troy',\n",
       "  'Dearborn',\n",
       "  'Detroit',\n",
       "  'Bangalore',\n",
       "  'Detroit',\n",
       "  'East Thetford',\n",
       "  'Princeton',\n",
       "  'Phoenix',\n",
       "  'Elgin',\n",
       "  'Detroit',\n",
       "  'Atlanta',\n",
       "  'Dearborn',\n",
       "  'Basking Ridge',\n",
       "  'Dublin',\n",
       "  'Phoenix',\n",
       "  'Phoenix',\n",
       "  'Southfield',\n",
       "  'Southfield',\n",
       "  'Palatine',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Newark',\n",
       "  'Santa Cruz',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Long Beach',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Chicago',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Burbank',\n",
       "  'Boston',\n",
       "  'Chandler',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Norwalk',\n",
       "  'New York',\n",
       "  'Chicago',\n",
       "  'Redmond',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'Cary',\n",
       "  'New York',\n",
       "  'Paris',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Lyndhurst',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'Antioch',\n",
       "  'Chicago',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'San Jose',\n",
       "  'Boston',\n",
       "  'New York',\n",
       "  'Shenzhen',\n",
       "  'New York',\n",
       "  'Chicago',\n",
       "  'Middlesex',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Mumbai',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'Princeton',\n",
       "  'New York',\n",
       "  'Seattle',\n",
       "  'Mc Lean',\n",
       "  'New York',\n",
       "  'Issy-les-Moulineaux',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Helsinki',\n",
       "  'New York',\n",
       "  'Brooklyn',\n",
       "  'Berlin',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Aurora',\n",
       "  'New York',\n",
       "  'Paris',\n",
       "  'London',\n",
       "  'London',\n",
       "  'Redwood City',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Dublin',\n",
       "  'Rockwall',\n",
       "  'Atlanta',\n",
       "  'Mumbai',\n",
       "  'Iselin',\n",
       "  'San Mateo',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'Littleton',\n",
       "  'Guilford',\n",
       "  'New York',\n",
       "  'Santa Cruz',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Seattle',\n",
       "  'Detroit',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Irvine',\n",
       "  'Toronto',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'Holon',\n",
       "  'New York',\n",
       "  'Tarrytown',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Northville',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'Beaverton',\n",
       "  'New York',\n",
       "  'Menlo Park',\n",
       "  'Armonk',\n",
       "  'New York',\n",
       "  'Burbank',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Mountain View',\n",
       "  'New York',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Washington',\n",
       "  'New York',\n",
       "  'Boston',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Hartford',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Plano',\n",
       "  'New York',\n",
       "  'Mountain View',\n",
       "  'New York',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Princeton',\n",
       "  'Armonk',\n",
       "  'New York',\n",
       "  'Mc Lean',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Weston',\n",
       "  'Norwalk',\n",
       "  'Litchfield Park',\n",
       "  'Charlotte',\n",
       "  'New York',\n",
       "  'Stamford',\n",
       "  'Portland',\n",
       "  'Seattle',\n",
       "  'Monmouth Junction',\n",
       "  'New York',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Montreal',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Los Angeles',\n",
       "  'New York',\n",
       "  'Los Angeles',\n",
       "  'Farmington',\n",
       "  'New York',\n",
       "  'Guilford',\n",
       "  'Mc Lean',\n",
       "  'Redwood City',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Newark',\n",
       "  'Mountain View',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'Stamford',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Brooklyn',\n",
       "  'New York',\n",
       "  'Newark',\n",
       "  'North Brunswick',\n",
       "  'Netanya',\n",
       "  'Evanston',\n",
       "  'Boston',\n",
       "  'New York',\n",
       "  'Bengaluru',\n",
       "  'Greenwich',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Hartford',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Philadelphia',\n",
       "  'Brooklyn',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Bronx',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Paris',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'Wichita',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Austin',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'London',\n",
       "  'Helmetta',\n",
       "  'Norwalk',\n",
       "  'Rolling Meadows',\n",
       "  'Leverkusen',\n",
       "  'Pasadena',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Portland',\n",
       "  'Hamilton',\n",
       "  'New York',\n",
       "  'Stamford',\n",
       "  'Norwalk',\n",
       "  'Edison',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Cherry Hill',\n",
       "  'New York',\n",
       "  'Hoboken',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Los Angeles',\n",
       "  'New York',\n",
       "  'Chicago',\n",
       "  'Alpharetta',\n",
       "  'Stamford',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Pittsburgh',\n",
       "  'New York',\n",
       "  'Arlington',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Arlington',\n",
       "  'Jersey City',\n",
       "  'Florham Park',\n",
       "  'Englewood',\n",
       "  'New York',\n",
       "  'Norwich',\n",
       "  'Woodcliff Lake',\n",
       "  'New York',\n",
       "  'Roseland',\n",
       "  'New York',\n",
       "  'Irvine',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Burlington',\n",
       "  'South Plainfield',\n",
       "  'Ventura',\n",
       "  'New York',\n",
       "  'Ventura',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Mountain View',\n",
       "  'New York',\n",
       "  'Hoboken',\n",
       "  'Brooklyn',\n",
       "  'New York',\n",
       "  'Irving',\n",
       "  'Seattle',\n",
       "  'Boston',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Hartford',\n",
       "  'New York',\n",
       "  'Menlo Park',\n",
       "  'New York',\n",
       "  'Baltimore',\n",
       "  'Chicago',\n",
       "  'Atlanta',\n",
       "  'New York',\n",
       "  'Seattle',\n",
       "  'Seattle',\n",
       "  'Los Angeles',\n",
       "  'Stockholm',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'Mountain View',\n",
       "  'Munich',\n",
       "  'New York',\n",
       "  'Iselin',\n",
       "  'Littleton',\n",
       "  'Hartford',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Irvine',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'Tuticorin',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'West Palm Beach',\n",
       "  'Cary',\n",
       "  'Elk Grove Village',\n",
       "  'Menlo Park',\n",
       "  'Hartford',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'Iselin',\n",
       "  'San Mateo',\n",
       "  'New York',\n",
       "  'Menlo Park',\n",
       "  'North Brunswick',\n",
       "  'Gottingen',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'Hong Kong',\n",
       "  'Washington',\n",
       "  'Palo Alto',\n",
       "  'Monmouth Junction',\n",
       "  'Shenzhen',\n",
       "  'Seattle',\n",
       "  'New York',\n",
       "  'East Lansing',\n",
       "  'New York',\n",
       "  'Florham Park',\n",
       "  'Port Washington',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Cincinnati',\n",
       "  'New York',\n",
       "  'Portland',\n",
       "  'Mountain View',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Atlanta',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Berlin',\n",
       "  'New York',\n",
       "  'Charlotte',\n",
       "  'New York',\n",
       "  'San Jose',\n",
       "  'Springfield',\n",
       "  'Boston',\n",
       "  'Boston',\n",
       "  'Marina del Rey',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Rockleigh',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Menlo Park',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Mc Lean',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'New York',\n",
       "  'Northville',\n",
       "  'San Francisco',\n",
       "  'New York',\n",
       "  'Norcross',\n",
       "  'London',\n",
       "  'Armonk',\n",
       "  'New York',\n",
       "  'Paris',\n",
       "  'London',\n",
       "  'Newark',\n",
       "  'Chicago',\n",
       "  'New York',\n",
       "  'London',\n",
       "  'Irvine',\n",
       "  'Redwood City',\n",
       "  'New York',\n",
       "  'Newport',\n",
       "  'New York',\n",
       "  'Newport Beach',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'Pleasanton',\n",
       "  'San Mateo',\n",
       "  'Newport Beach',\n",
       "  'Newburyport',\n",
       "  'Austin',\n",
       "  'Armonk',\n",
       "  'Farmington',\n",
       "  'Austin',\n",
       "  'Austin',\n",
       "  'New York',\n",
       "  'Sydney',\n",
       "  'Austin',\n",
       "  'San Antonio',\n",
       "  'Cupertino',\n",
       "  'Newport Beach',\n",
       "  'Round Rock',\n",
       "  'Pune',\n",
       "  'Minneapolis',\n",
       "  'Austin',\n",
       "  'Washington',\n",
       "  'Austin',\n",
       "  'Plainsboro',\n",
       "  'Austin',\n",
       "  'Cupertino',\n",
       "  'Austin'],\n",
       " 'hq_state_code': ['MA',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'VA',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'Switzerland',\n",
       "  'VA',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'United Kingdom',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'VA',\n",
       "  'Switzerland',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'WA',\n",
       "  'OH',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'NJ',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'IN',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'India',\n",
       "  'MA',\n",
       "  'KY',\n",
       "  'MA',\n",
       "  'RI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'NY',\n",
       "  'Italy',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'CA',\n",
       "  'MI',\n",
       "  'OH',\n",
       "  'MI',\n",
       "  'WA',\n",
       "  'WA',\n",
       "  'FL',\n",
       "  'VA',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'NJ',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'VT',\n",
       "  'MI',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'NC',\n",
       "  'MI',\n",
       "  'FL',\n",
       "  'AZ',\n",
       "  'TX',\n",
       "  'KS',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'CA',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'Canada',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'VA',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'OH',\n",
       "  'MA',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'India',\n",
       "  'MI',\n",
       "  'VT',\n",
       "  'NJ',\n",
       "  'AZ',\n",
       "  'IL',\n",
       "  'MI',\n",
       "  'GA',\n",
       "  'MI',\n",
       "  'NJ',\n",
       "  'Ireland',\n",
       "  'AZ',\n",
       "  'AZ',\n",
       "  'MI',\n",
       "  'MI',\n",
       "  'IL',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'AZ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'WA',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'NC',\n",
       "  'NY',\n",
       "  'France',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'IL',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'China',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'India',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'WA',\n",
       "  'VA',\n",
       "  'NY',\n",
       "  'France',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'Finland',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'Germany',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'NY',\n",
       "  'France',\n",
       "  'United Kingdom',\n",
       "  'United Kingdom',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'Ireland',\n",
       "  'TX',\n",
       "  'GA',\n",
       "  'India',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'CO',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'WA',\n",
       "  'MI',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'Canada',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'Israel',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'MI',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'OR',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'DC',\n",
       "  'NY',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'TX',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'VA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'MA',\n",
       "  'CT',\n",
       "  'AZ',\n",
       "  'NC',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'OR',\n",
       "  'WA',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'Canada',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'VA',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NJ',\n",
       "  'Israel',\n",
       "  'IL',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'India',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'PA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'France',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'KS',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'TX',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'United Kingdom',\n",
       "  'NJ',\n",
       "  'CT',\n",
       "  'IL',\n",
       "  'Germany',\n",
       "  'CA',\n",
       "  '061',\n",
       "  'NY',\n",
       "  'OR',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'CT',\n",
       "  'NJ',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'IL',\n",
       "  'GA',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'PA',\n",
       "  'NY',\n",
       "  'VA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'VA',\n",
       "  'NJ',\n",
       "  'NJ',\n",
       "  'CO',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'VT',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'TX',\n",
       "  'WA',\n",
       "  'MA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'MD',\n",
       "  'IL',\n",
       "  'GA',\n",
       "  'NY',\n",
       "  'WA',\n",
       "  'WA',\n",
       "  'CA',\n",
       "  'Sweden',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'Germany',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'CO',\n",
       "  'CT',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'India',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'FL',\n",
       "  'NC',\n",
       "  'IL',\n",
       "  'CA',\n",
       "  'CT',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NJ',\n",
       "  'Germany',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'Hong Kong',\n",
       "  'DC',\n",
       "  'CA',\n",
       "  'NJ',\n",
       "  'China',\n",
       "  'WA',\n",
       "  'NY',\n",
       "  'MI',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'OH',\n",
       "  'NY',\n",
       "  'OR',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'GA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'Germany',\n",
       "  'NY',\n",
       "  'NC',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'MA',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NJ',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'VA',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'MI',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'GA',\n",
       "  'United Kingdom',\n",
       "  'NY',\n",
       "  'NY',\n",
       "  'France',\n",
       "  'United Kingdom',\n",
       "  'NJ',\n",
       "  'IL',\n",
       "  'NY',\n",
       "  'United Kingdom',\n",
       "  'CA',\n",
       "  'CA',\n",
       "  'NY',\n",
       "  'RI',\n",
       "  'NY',\n",
       "  'CA',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'CA',\n",
       "  'CA',\n",
       "  'CA',\n",
       "  'MA',\n",
       "  'TX',\n",
       "  'NY',\n",
       "  'UT',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'NY',\n",
       "  'Australia',\n",
       "  'TX',\n",
       "  'TX',\n",
       "  'CA',\n",
       "  'CA',\n",
       "  'TX',\n",
       "  'India',\n",
       "  'MN',\n",
       "  'TX',\n",
       "  'DC',\n",
       "  'TX',\n",
       "  'NJ',\n",
       "  'TX',\n",
       "  'CA',\n",
       "  'TX'],\n",
       " 'size': ['500',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '500',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '200',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '200',\n",
       "  '500',\n",
       "  '200',\n",
       "  '50',\n",
       "  '200',\n",
       "  '500',\n",
       "  '200',\n",
       "  '50',\n",
       "  '500',\n",
       "  '500',\n",
       "  '500',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '500',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '500',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '500',\n",
       "  '500',\n",
       "  '200',\n",
       "  '200',\n",
       "  '500',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '500',\n",
       "  '50',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '500',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '50',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '1000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '50',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '1000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '500',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '200',\n",
       "  '10000',\n",
       "  '50',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '1000',\n",
       "  '50',\n",
       "  '500',\n",
       "  '5000',\n",
       "  '200',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '1000',\n",
       "  '200',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '5000',\n",
       "  '10000',\n",
       "  '10000',\n",
       "  '500',\n",
       "  '200',\n",
       "  '50',\n",
       "  '200',\n",
       "  '5000',\n",
       "  '50',\n",
       "  '10000',\n",
       "  '500'],\n",
       " 'industry': ['Health Care Services & Hospitals',\n",
       "  'Insurance Carriers',\n",
       "  'Computer Hardware & Software',\n",
       "  'Farm Support Services',\n",
       "  'Internet',\n",
       "  'Advertising & Marketing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Education Training Services',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'Insurance Carriers',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Insurance Carriers',\n",
       "  'Computer Hardware & Software',\n",
       "  'IT Services',\n",
       "  'Advertising & Marketing',\n",
       "  'Internet',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Publishing',\n",
       "  'Internet',\n",
       "  'Banks & Credit Unions',\n",
       "  'Education Training Services',\n",
       "  'Consumer Products Manufacturing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Insurance Carriers',\n",
       "  'Internet',\n",
       "  'Colleges & Universities',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Internet',\n",
       "  'Accounting',\n",
       "  'Insurance Carriers',\n",
       "  'Health Care Products Manufacturing',\n",
       "  'IT Services',\n",
       "  'Insurance Carriers',\n",
       "  'Computer Hardware & Software',\n",
       "  'Insurance Carriers',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Publishing',\n",
       "  'Insurance Carriers',\n",
       "  'Internet',\n",
       "  'Insurance Carriers',\n",
       "  'Computer Hardware & Software',\n",
       "  'IT Services',\n",
       "  'Aerospace & Defense',\n",
       "  'Insurance Carriers',\n",
       "  'Farm Support Services',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Internet',\n",
       "  'Lending',\n",
       "  'Internet',\n",
       "  'Lending',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Lending',\n",
       "  'IT Services',\n",
       "  'Telecommunications Services',\n",
       "  'Industrial Manufacturing',\n",
       "  'IT Services',\n",
       "  'Real Estate',\n",
       "  'Internet',\n",
       "  'Advertising & Marketing',\n",
       "  'Building & Personnel Services',\n",
       "  'IT Services',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Accounting',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Colleges & Universities',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Real Estate',\n",
       "  'Computer Hardware & Software',\n",
       "  'Computer Hardware & Software',\n",
       "  'Insurance Agencies & Brokerages',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Lending',\n",
       "  'IT Services',\n",
       "  'Insurance Carriers',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'IT Services',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Consulting',\n",
       "  'Lending',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'IT Services',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Real Estate',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'IT Services',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Architectural & Engineering Services',\n",
       "  'Publishing',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Logistics & Supply Chain',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Accounting',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Lending',\n",
       "  'Electrical & Electronic Manufacturing',\n",
       "  'Utilities',\n",
       "  'Consulting',\n",
       "  'IT Services',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Industrial Manufacturing',\n",
       "  'Lending',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'IT Services',\n",
       "  'Consulting',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Transportation Equipment Manufacturing',\n",
       "  'Cable, Internet & Telephone Providers',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Motion Picture Production & Distribution',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Sports & Recreation',\n",
       "  'Insurance Carriers',\n",
       "  'Internet',\n",
       "  'Hotels, Motels, & Resorts',\n",
       "  'Consulting',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Consumer Product Rental',\n",
       "  'Motion Picture Production & Distribution',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'IT Services',\n",
       "  'Internet',\n",
       "  'Music Production & Distribution',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Research & Development',\n",
       "  'Other Retail Stores',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Advertising & Marketing',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Real Estate',\n",
       "  'IT Services',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'IT Services',\n",
       "  'Real Estate',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'Travel Agencies',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Travel Agencies',\n",
       "  'Insurance Carriers',\n",
       "  'IT Services',\n",
       "  'Advertising & Marketing',\n",
       "  'Beauty & Personal Accessories Stores',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Stock Exchanges',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Consulting',\n",
       "  'Photography',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Brokerage Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Banks & Credit Unions',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Radio',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Gift, Novelty & Souvenir Stores',\n",
       "  'Health, Beauty, & Fitness',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Sports & Recreation',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'Consulting',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Insurance Carriers',\n",
       "  'Insurance Carriers',\n",
       "  'Internet',\n",
       "  'Consulting',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Financial Transaction Processing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'K-12 Education',\n",
       "  'Advertising & Marketing',\n",
       "  'IT Services',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Publishing',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Other Retail Stores',\n",
       "  'Lending',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Advertising & Marketing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Banks & Credit Unions',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Wholesale',\n",
       "  'Advertising & Marketing',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Consulting',\n",
       "  'Aerospace & Defense',\n",
       "  'Advertising & Marketing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Consulting',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Consumer Products Manufacturing',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'Sports & Recreation',\n",
       "  'Motion Picture Production & Distribution',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Consulting',\n",
       "  'General Merchandise & Superstores',\n",
       "  'Advertising & Marketing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Lending',\n",
       "  'Internet',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Internet',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Insurance Carriers',\n",
       "  'Computer Hardware & Software',\n",
       "  'K-12 Education',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Internet',\n",
       "  'Education Training Services',\n",
       "  'IT Services',\n",
       "  'Advertising & Marketing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Computer Hardware & Software',\n",
       "  'Colleges & Universities',\n",
       "  'Food Production',\n",
       "  'Video Games',\n",
       "  'Food Production',\n",
       "  'Computer Hardware & Software',\n",
       "  'IT Services',\n",
       "  'Insurance Agencies & Brokerages',\n",
       "  'Consulting',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Internet',\n",
       "  'Colleges & Universities',\n",
       "  'Internet',\n",
       "  'Financial Analytics & Research',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Computer Hardware & Software',\n",
       "  'Food & Beverage Stores',\n",
       "  'Department, Clothing, & Shoe Stores',\n",
       "  'Financial Analytics & Research',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Banks & Credit Unions',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Consulting',\n",
       "  'Consulting',\n",
       "  'Advertising & Marketing',\n",
       "  'Consumer Products Manufacturing',\n",
       "  'Consulting',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Education Training Services',\n",
       "  'Travel Agencies',\n",
       "  'Membership Organizations',\n",
       "  'IT Services',\n",
       "  'Internet',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Aerospace & Defense',\n",
       "  'Financial Analytics & Research',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Banks & Credit Unions',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Advertising & Marketing',\n",
       "  'Consumer Product Rental',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'IT Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Computer Hardware & Software',\n",
       "  'Internet',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Advertising & Marketing',\n",
       "  'Accounting',\n",
       "  'Department, Clothing, & Shoe Stores',\n",
       "  'Other Retail Stores',\n",
       "  'Colleges & Universities',\n",
       "  'Motion Picture Production & Distribution',\n",
       "  'IT Services',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Consulting',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'IT Services',\n",
       "  'Brokerage Services',\n",
       "  'Consumer Product Rental',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Financial Analytics & Research',\n",
       "  'Insurance Carriers',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Advertising & Marketing',\n",
       "  'Municipal Governments',\n",
       "  'Building & Personnel Services',\n",
       "  'Other Retail Stores',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Colleges & Universities',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Insurance Carriers',\n",
       "  'Publishing',\n",
       "  'Advertising & Marketing',\n",
       "  'Oil & Gas Exploration & Production',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Internet',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Consumer Products Manufacturing',\n",
       "  'Accounting',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Internet',\n",
       "  'News Outlet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'Financial Analytics & Research',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Financial Analytics & Research',\n",
       "  'Publishing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Computer Hardware & Software',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Education Training Services',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Research & Development',\n",
       "  'Music Production & Distribution',\n",
       "  'Lending',\n",
       "  'Consulting',\n",
       "  'Consulting',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Financial Analytics & Research',\n",
       "  'Computer Hardware & Software',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Lending',\n",
       "  'Federal Agencies',\n",
       "  'IT Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Federal Agencies',\n",
       "  'Research & Development',\n",
       "  'Internet',\n",
       "  'Cable, Internet & Telephone Providers',\n",
       "  'Telecommunications Services',\n",
       "  'Food & Beverage Manufacturing',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Internet',\n",
       "  'Consulting',\n",
       "  'Banks & Credit Unions',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Other Retail Stores',\n",
       "  'Computer Hardware & Software',\n",
       "  'Advertising & Marketing',\n",
       "  'Energy',\n",
       "  'Internet',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Financial Analytics & Research',\n",
       "  'Logistics & Supply Chain',\n",
       "  'Travel Agencies',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'General Merchandise & Superstores',\n",
       "  'Other Retail Stores',\n",
       "  'Advertising & Marketing',\n",
       "  'IT Services',\n",
       "  'Photography',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Financial Transaction Processing',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Insurance Carriers',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Financial Transaction Processing',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Internet',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Computer Hardware & Software',\n",
       "  'Insurance Carriers',\n",
       "  'Insurance Agencies & Brokerages',\n",
       "  'Health, Beauty, & Fitness',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Insurance Carriers',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Financial Analytics & Research',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Telecommunications Services',\n",
       "  'Architectural & Engineering Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Financial Analytics & Research',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Research & Development',\n",
       "  'Lending',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Insurance Carriers',\n",
       "  'Consulting',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Advertising & Marketing',\n",
       "  'Stock Exchanges',\n",
       "  'Financial Analytics & Research',\n",
       "  'Consulting',\n",
       "  'Advertising & Marketing',\n",
       "  'Advertising & Marketing',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Telecommunications Services',\n",
       "  'IT Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'IT Services',\n",
       "  'Miscellaneous Manufacturing',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Accounting',\n",
       "  'Consulting',\n",
       "  'Computer Hardware & Software',\n",
       "  'Banks & Credit Unions',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'Motion Picture Production & Distribution',\n",
       "  'Financial Analytics & Research',\n",
       "  'Health, Beauty, & Fitness',\n",
       "  'Accounting',\n",
       "  'Banks & Credit Unions',\n",
       "  'Sports & Recreation',\n",
       "  'Internet',\n",
       "  'Insurance Carriers',\n",
       "  'Advertising & Marketing',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'Advertising & Marketing',\n",
       "  'Internet',\n",
       "  'Financial Analytics & Research',\n",
       "  'Internet',\n",
       "  'Food & Beverage Manufacturing',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Computer Hardware & Software',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Internet',\n",
       "  'Travel Agencies',\n",
       "  'Consulting',\n",
       "  'Internet',\n",
       "  'Banks & Credit Unions',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Education Training Services',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Computer Hardware & Software',\n",
       "  'IT Services',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'TV Broadcast & Cable Networks',\n",
       "  'IT Services',\n",
       "  'Staffing & Outsourcing',\n",
       "  'IT Services',\n",
       "  'Lending',\n",
       "  'Internet',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Stock Exchanges',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Biotech & Pharmaceuticals',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Staffing & Outsourcing',\n",
       "  'Internet',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Real Estate',\n",
       "  'Real Estate',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Computer Hardware & Software',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'Consulting',\n",
       "  'Insurance Carriers',\n",
       "  'IT Services',\n",
       "  'Education Training Services',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Home Furniture & Housewares Stores',\n",
       "  'Computer Hardware & Software',\n",
       "  'Computer Hardware & Software',\n",
       "  'Grocery Stores & Supermarkets',\n",
       "  'Computer Hardware & Software',\n",
       "  'Investment Banking & Asset Management',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Health Care Services & Hospitals',\n",
       "  'Internet',\n",
       "  'Enterprise Software & Network Solutions',\n",
       "  'Internet',\n",
       "  'IT Services',\n",
       "  'IT Services',\n",
       "  'Computer Hardware & Software',\n",
       "  'Real Estate']}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('E:/OSU - MSBAN/3rd Sem/MSIS 5600/glassdoor.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
